

:::{.content-visible when-format="html"}

# More Models {#sec-app-more-models}


This is the list of models and related that includes those we've seen, some we've just mentioned, and some we've not explored at all. 


## Linear Models {#sec-app-more-linear}

**Simplified Linear Models**

- correlation
- t-test and ANOVA
- chi-square

**Generalize Linear Models and related**

- True GLM e.g. gamma
- Other distributions: beta regression, tweedie, t (so-called 'robust'), truncated
- Censored outcomes: Survival models, tobit
- Nonlinear regression 
- Modeling other parameters (e.g. heteroscedastic models)

**Multivariate/multiclass/multipart**

- Multivariate regression (multiple targets)
- Multinomial/Categorical/Ordinal regression (>2 classes)
- MANOVA/Linear Discriminant Analysis (these are identical, and can handle multiple outputs or >=2 classes)
- Zero (or some number) -inflated/hurdle/altered
- Mixture models and Cluster analysis
- Two-stage least squares, instrumental variables
- SEM, simultaneous equations
- PCA, Factor Analysis
- Mixture models
- Structural Equation Modeling, Graphical models generally


**Other Random Effects**

- Gaussian process regression
- Spatial models (CAR, SAR, etc.)
- Time series models (ARIMA and related, e.g. state space)
- Factor analysis


All of these are explicitly linear models or can be framed as such, and compared to what you've already seen, only require only a tweak or two - e.g. a different distribution, a different link function, penalizing the coefficients, etc. In other cases, we can bounce from one to the another. For example we can reshape our multivariate regression to be amenable to a mixed model approach, and get the exact same results. We can potentially add a random effect to any model, and that random effect can be based on time, spatial or other considerations. The important thing to know is that the linear model is a very flexible tool that expands easily, and allows you to model most of the types of outcomes were interested in. As such, it's a very powerful approach to modeling.


## Other Machine Learning Models {#sec-app-more-ml}

There are models you'll typically only see when you're in a machine learning context, as they often do not provide specific parameters of interest like coefficients or variance components, nor have easy ways to estimate uncertainty. While that leads the focus toward prediction, most of these are no longer performant compared to tools used today. Still, they can be interesting historically or conceptually, some are special cases of more widely used techniques, and some can still be used as baseline models.

**Standard Regression/Classification**

- k-Nearest neighbors regression
- Naive Bayes
- Support Vector Machines, Boltzmann Machines
- Projection pursuit regression
- (Hidden) Markov Models
- Undirected graphs, Markov Random Fields, Network analysis
- Single Decision trees, CART, C4.5, etc.

**Latent Models**

- PCA, probabilistic PCA, ICA, SVD
- Latent Dirichlet Allocation, Latent Semantic Analysis
- (Non-negative) Matrix Factorization
- Dirichlet process




## Other Deep Learning Models {#sec-app-more-dl}

We haven't delved into the world of deep learning as much as there hasn't yet been a 'foundational' model for tabular data of the sort we've focused on. However most of the models that make headlines today are built upon simpler models, even going back to the basic multilayer perceptron. Here are some of the models you might see in the wild:

- Convolutional Neural Networks
- Recurrent Neural Networks
- Long Short-Term Memory Networks
- Transformers
- Autoencoders
- Generative Adversarial Networks
- Extreme Learning Machines
- Graph Neural Networks
- Factorization Machines
- Attention Mechanisms
- Reinforcement Learning


Convolutional neural networks as currently implemented can be seen going back to LeNet in the late 1990s, and took off several years later with AlexNet and VGG. ResNet (residual networks) and Densenet are more recent examples of CNNs, though even they are have been around for several years at this point. Even so, several of these still serve as baseline models for image classification and object detection, either in practice or as a reference point for current model performance.

NLP and language processing more generally can be seen as evolving from matrix factorization and LDA, to neural network models such as word2vec and GloVe. In addition, the temporal nature of text suggested time-based models even more statistical ones like hidden markov models way back in the day. But in the neural network domain, we have standard Recurrent networks, then LSTMs, GRUs, Seq2Seq, and more that continued the theme. Now the field is dominated by attention-based transformers, of which BERT variants and GPT are among the most famous examples of these, but there are many others that have been developed in the last few years, offered from Meta, Google, Anthropic and others.

<!-- 
could mention specific/historical models here, like lenet, alexnet, resnet, YOLO, bert, GANs, LSTM, etc.
talk about DL applied to tabular 
autoencoders, VAEs, etc. -->

:::

