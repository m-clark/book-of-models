
# Other to come

We'll be adding more models and related topics here, including some we've mentioned and some we've not seen at at all.

## Simulation



<!-- TODO: any other code demos for models in the text  will be web only -->
## Bayesian Demonstration

Metropolis-Hastings demo

reference nice shiny app https://github.com/tomicapretto/shiny-hmc


```{r}
#| echo: false
#| eval: false
#| label: r-metropolis-hastings
# Define the log-likelihood function for linear regression
log_likelihood <- function(beta, X, y, sigma_sq) {
    y_hat <- X %*% beta
    residuals <- y - y_hat
    log_likelihood <- -0.5 * length(y) * log(2 * pi * sigma_sq) - 0.5 * sum(residuals^2) / sigma_sq
    return(log_likelihood)
}

# Define the prior distribution for beta
prior_beta <- function(beta) {
    prior_mean <- rep(0, length(beta))
    prior_sd <- rep(10, length(beta))
    log_prior <- sum(dnorm(beta, mean = prior_mean, sd = prior_sd, log = TRUE))
    return(log_prior)
}

# Define the prior distribution for sigma
prior_sigma <- function(sigma_sq) {
    alpha <- 2
    beta <- 2
    # log_prior <- dgamma(1/sigma_sq, shape = alpha, rate = beta, log = TRUE)
    log_prior <- extraDistr::dinvgamma(sigma_sq, alpha = alpha, beta = beta, log = TRUE) 

    return(log_prior)
}

# Define the proposal distribution for beta
proposal_beta <- function(beta, scale) {
    beta_proposal <- rnorm(length(beta), mean = beta, sd = scale)
    return(beta_proposal)
}

# Define the proposal distribution for sigma
proposal_sigma <- function(sigma_sq, scale) {
    # sigma_proposal <- rgamma(1, shape = sigma_sq / scale, rate = scale)
    sigma_proposal <- extraDistr::rinvgamma(1, alpha = sigma_sq / scale, beta = scale)
    return(sigma_proposal)
}

# Set up the data
# set.seed(123)
# n <- 100
# X <- cbind(1, rnorm(n), rnorm(n), rnorm(n))
# beta_true <- c(1, 2, 3, 4)/4
# sigma_true <- 1
# y <- X %*% beta_true + rnorm(n, sd = sigma_true)

# Set up the Metropolis-Hastings algorithm
# n_iter <- 10000


# Run the Metropolis-Hastings algorithm
mh = function(
    X,
    y,
    beta = rep(0, ncol(X)), 
    sigma_sq = .5, 
    scale_beta = 0.1, 
    scale_sigma = 1,
    chains = 2,
    warmup = 1000,
    n_iter = 2000,
    seed = 123
) {
    set.seed(seed)

    result <- list()
    beta_start <- beta
    sigma_sq_start <- sigma_sq

    for (c in 1:chains){
        acceptance_beta <- 0
        acceptance_sigma <- 0
        beta_samples <- matrix(0, n_iter, ncol(X))
        sigma_sq_samples <- rep(0, n_iter)

        if (c > 1) {
            beta <- beta_start
            sigma_sq <- sigma_sq_start
        }       

        for (i in 1:n_iter) {
            # Update beta
            beta_proposal <- proposal_beta(beta, scale_beta)
            log_ratio_beta <- log_likelihood(beta_proposal, X, y, sigma_sq) + prior_beta(beta_proposal) -
                log_likelihood(beta, X, y, sigma_sq) - prior_beta(beta)
            if (log(runif(1)) < log_ratio_beta) {
                beta <- beta_proposal
                acceptance_beta <- acceptance_beta + 1
            }
            beta_samples[i, ] <- beta

            # Update sigma_sq
            sigma_sq_proposal <- proposal_sigma(sigma_sq, scale_sigma)
            log_ratio_sigma <- log_likelihood(beta, X, y, sigma_sq_proposal) + prior_sigma(sigma_sq_proposal) -
                log_likelihood(beta, X, y, sigma_sq) - prior_sigma(sigma_sq)
            if (log(runif(1)) < log_ratio_sigma) {
                sigma_sq <- sigma_sq_proposal
                acceptance_sigma <- acceptance_sigma + 1
            }
            sigma_sq_samples[i] <- sigma_sq
        }
    
        message("Acceptance rate for beta:", acceptance_beta / n_iter, "\n")
        message("Acceptance rate for sigma:", acceptance_sigma / n_iter, "\n")


        result[[c]] = list(
            beta = beta_samples[-(1:warmup), ], 
            sigma_sq = sigma_sq_samples[-(1:warmup)],
            # y_rep = X %*% t(beta_samples[-(1:warmup), ])
            # +rnorm(n_iter - warmup, sd = sqrt(sigma_sq_samples[-(1:warmup)]))
            y_rep = t(X %*% t(beta_samples[-(1:warmup), ]) + rnorm(n_iter - warmup, sd = sqrt(sigma_sq_samples[-(1:warmup)])))
        )
    } 
    result
}

X_train = df_happiness |>
    select(life_exp, gdp_pc, corrupt) |>
    as.matrix()

our_result = mh(
    X = cbind(1, X_train),
    y = df_happiness$happiness, 
    beta = c(mean(df_happiness$happiness), rep(0, ncol(X_train))),
    sigma_sq = var(df_happiness$happiness),
    scale_sigma = .5,
    warmup = 1000,
    n_iter = 2000
)

str(our_result)
```

```{r}
#| echo: false
#| label: fig-r-bayesian-estimation
#| fig-cap: Bayesian estimation results

# Plot the posterior distributions with ggplot, using the bayesplot package
# par_chains = map(our_result, \(x) {
#     x = cbind(x$beta, x$sigma_sq)
#     colnames(x) = c("Intercept", "Life Exp.", "GDP_PC", "Corrupt", "Sigma")
#     x
# }) 
# y_rep_chains = map(our_result, \(x) x$y_rep)

# # show trace plots for all betas and sigma
# # performance::performance_mse(model_compare)

# bayesplot::mcmc_intervals_data(par_chains, point_est = 'mean') |> 
#     select(parameter, mean=m, q.05 =l, q.95=h) |> 
#     gt()
# bayesplot::mcmc_combo(par_chains)
# bayesplot::pp_check(
#     df_happiness$happiness, 
#     rbind(
#         y_rep_chains[[1]][1:10,],
#         y_rep_chains[[2]][1:10,]
#     ), 
#     fun =  bayesplot::ppc_dens_overlay
# )

# save(
#     par_chains,
#     y_rep_chains,
#     file = "estimation/data/bayes_estimation.RData"
# )
```


## Linear Programming


## Boosting {#app-boosting}

import boosting_demo.R



<!-- TODO: Maybe move to appendix but summarize here -->
:::{.content-visible when-format="html"}


## More Models {#sec-app-more-models}

This is the list of models and related that includes, some we've mentioned and some we've not seen at at all. 


### Linear Models

**Simplified Linear Models**

- correlation
- t-test and ANOVA
- chi-square

**Generalize Linear Models and related**

- True GLM e.g. gamma
- Other distributions: beta regression, tweedie, t (so-called 'robust'), truncated
- Censored outcomes: Survival models, tobit
- Nonlinear regression 
- Modeling other parameters (e.g. heteroscedastic models)

**Multivariate/multiclass/multipart**

- Multivariate regression (multiple targets)
- Multinomial/Categorical/Ordinal regression (>2 classes)
- MANOVA/Linear Discriminant Analysis (these are identical, and can handle multiple outputs or >=2 classes)
- Zero (or some number) -inflated/hurdle/altered
- Mixture models and Cluster analysis
- Two-stage least squares, instrumental variables
- SEM, simultaneous equations
- PCA, Factor Analysis
- Mixture models
- Structural Equation Modeling, Graphical models generally


**Other Random Effects**

- Gaussian process regression
- Spatial models (CAR, SAR, etc.)
- Time series models (ARIMA and related, e.g. state space)
- Factor analysis


All of these are explicitly linear models or can be framed as such, and compared to what you've already seen, only require only a tweak or two - e.g. a different distribution, a different link function, penalizing the coefficients, etc. In other cases, we can bounce from one to the another. For example we can reshape our multivariate regression to be amenable to a mixed model approach, and get the exact same results. We can potentially add a random effect to any model, and that random effect can be based on time, spatial or other considerations. The important thing to know is that the linear model is a very flexible tool that expands easily, and allows you to model most of the types of outcomes were interested in. As such, it's a very powerful approach to modeling.


### Other Machine Learning Models

There are models you'll typically only see when you're in a machine learning context, as they often do not provide specific parameters of interest like coefficients or variance components, nor have easy ways to estimate uncertainty. While that leads the focus toward prediction, most of these are no longer performant compared to tools used today. Still, they can be interesting historically or conceptually, some are special cases of more widely used techniques, and some can still be used as baseline models.

**Standard Regression/Classification**

- k-Nearest neighbors regression
- Naive Bayes
- Support Vector Machines, Boltzmann Machines
- Projection pursuit regression
- (Hidden) Markov Models
- Undirected graphs, Markov Random Fields, Network analysis
- Single Decision trees, CART, C4.5, etc.

**Latent Models**

- PCA, probabilistic PCA, ICA, SVD
- Latent Dirichlet Allocation, Latent Semantic Analysis
- (Non-negative) Matrix Factorization
- Dirichlet process




### Other Deep Learning

We haven't delved into the world of deep learning as much as there hasn't yet been a 'foundational' model for tabular data of the sort we've focused on. However most of the models that make headlines today are built upon simpler models, even going back to the basic multilayer perceptron. Here are some of the models you might see in the wild:

- Convolutional Neural Networks
- Recurrent Neural Networks
- Long Short-Term Memory Networks
- Transformers
- Autoencoders
- Generative Adversarial Networks
- Extreme Learning Machines
- Graph Neural Networks
- Factorization Machines
- Attention Mechanisms
- Reinforcement Learning


Convolutional neural networks as currently implemented can be seen going back to LeNet in the late 1990s, and took off several years later with AlexNet and VGG. ResNet (residual networks) and Densenet are more recent examples of CNNs, though even they are have been around for several years at this point. Even so, several of these still serve as baseline models for image classification and object detection, either in practice or as a reference point for current model performance.

NLP and language process can be seen as evolving from matrix factorization and LDA to neural network models such as word2vec and GloVe. In addition, the temporal nature of text suggested time-based models even more statistical ones like hidden markov models way back in the day. But in the neural network domain, we have standard Recurrent networks, then LSTMs, GRUs, Seq2Seq, and more that continued the theme. Now the field is dominated by attention-based transformers, of which BERT and GPT are the most famous examples of these, but there are many others that have been developed in the last few years.

<!-- 
could mention specific/historical models here, like lenet, alexnet, resnet, YOLO, bert, GANs, LSTM, etc.
talk about DL applied to tabular 
autoencoders, VAEs, etc. -->

:::

