# Danger Zone {#sec-danger-zone}

The following are very brief thoughts on common pitfalls in modeling we thought of while writing this book. This is not meant to be comprehensive, but rather a quick reference for common things that can trip folks up. We're not going to tell you not to do these things, but we are going to tell you to be very careful if you do, and be ready to defend your choices.

## Things to Generally Avoid



When it comes to conducting models in data science, a lot can go wrong, and in many cases it's easy to get lost in the weeds and lose sight of the bigger picture. In this chapter, we'll discuss some common pitfalls that can sneak up on you when you're working on a data science project, and others that just came to mind while we were thinking about it. This is pretty opinionated, but it is largely based on the sorts of things we've seen across many academic disciplines and industries. While some of these may not apply to your situation, we think you'll find most of these are reasonable suggestions. The following groups of focus attempt to reflect the content of the book as it was presented, but 



TLDR:
- Don't get hung up on statistical significance
    - Solution: focus on practical implications
- Avoid using specialized yet dated techniques, metrics, or other tools that solve problems that don't exist anymore
    - Solution: use tools that are common, well-understood, and still solve the problem
- Feature importance is difficult to assess in the best of circumstances (i.e. almost never), and rarely changes anything about what you're doing from a modeling perspective.
    - Solution: focus on model performance and interpretability. A feature can be the best of a bad lot.
- Don't rely on defaults for complex models
    - Solution: tune your models, and understand the model well enough to set options that make sense for your situation.
- Don't rely on a single metric to assess model performance
    - Solution: use a suite of metrics that are appropriate for your situation
- Don't rely on a single model to make decisions
    - Solution: use a set of models to tackle a problem
- Tweaking your optimizer is generally not going to make a huge difference
    - Solution: Focus on the model architecture and data
- Improving your data is generally going to make a bigger difference than improving your model
    - Solution: Focus on data quality and data collection when possible
- Don't forget to transform your features
    - Solution: use transformations to help model convergence, and potentially improve interpretability
- Don't binarize data just because you think it's easier
    - Solution: Use the data in its raw form when possible, transform if appropriate. Binarizing continuous data can lead to loss of information and worse practical application. Alternate solution - use tree-based models that will use discretized forms of data appropriately, but ultimately use the range available.



## Linear Models and Related Statistical Endeavors

Statistical models are a powerful tool for understanding the relationships between variables in a dataset. They are also excellent at

### Statistical Significance

One of the most common mistakes when conducting statistical linear models is simply relying too heavily on the statistical result. Statistical significance is simply not enough to determine feature importance or model performance. When complex statistical models are applied to small data, the results are typically very noisy and statistical significance can be misleading. This also means that 'big' effects can be a reflection of that noise, rather than something meaningful.

Focusing on statistical significance can lead you down other dangerous paths. For example, relying on statistical tests of assumptions instead of visualizations or practical metrics can lead you to believe that your model is valid when it is not. Using a statistical testing approach to select features can often result in incorrect choices about feature contributions, as well as poorer models. 

### Ignoring Complexity

While techniques like standard linear/logistic regression and GLMs are valid and very useful, for many modeling contexts they may be too simplified to capture the complexity of the data generating process. This can lead to underfitting, where the model is too simple to capture the underlying structure of the data.

On the other side of the coin, many applications of statistical models ignore model assessment on a separate dataset, which can lead to overfitting. This makes generalization of statistical and other results more problematic. Such applications typically use a single model as well, and so may not be indicative of the best approach that could be taken.


### Using Dated Techniques

This is not specific to the statistical linear modeling realm, but there are many applications of statistical models that rely on outdated techniques, metrics, or other tools that solve problems that don't exist anymore. For example, using stepwise/best subset regression for feature selection is not really viable when more principled approaches like ridge/lasso/elastic net are available. Likewise, we can't really think of a case where something like MANOVA/discriminant analysis would provide the best answer to a data problem, or where a pseudo-R^2^ would be a metric that would help us understand a model better or make a decision about it. 

Statistical analysis has been around a long time, and many of the techniques that have been developed are still valid, useful, and very powerful. But some mostly reflect the limitations of the time they were developed. Others were an attempt to take something that was straightforward for simpler settings (e.g. linear regression) and apply to settings where it doesn't make sense (nonlinear, non-gaussian, etc.). Even when still valid, there are may be better alternatives available now.


### Assuming Simpler is More Interpretable

Standard linear models are often used because of their interpretability, but in many of these modeling situations, interpretability can be difficult to obtain without using the same amount of effort one would for more complex models. Many statistical/linear models employ interactions,  employ nonlinear feature-target relationships (e.g. GLM/GAMs). If your goal is interpretability, many of these approaches can be as difficult to interpret as a features in a random forest. They still have the added benefit of more reliable uncertainty estimation, but one should not assume you will have a result as simple as a coefficient in a linear regression just because you didn't use a deep learning model.

### Problematic Model Comparison

statistical, r^2^, etc.

:::{.callout type='info', title='Garden of Forking Paths'}
A common issue in statistical and machine learning modeling is the **garden of forking paths**. This is the idea that there are many different ways to analyze a dataset, and that the results of these analyses can be very different. When you don't have a lot of data, or when the data is complex and the data generating process is not well understood. In these cases, the interpretation of a single model from the many that are actually employed can be misleading, and can lead to incorrect conclusions about the data.
:::

## Estimation

### If I Just Tweak This... What if I Just...


From traditional statistical models to deep learning, the more you know about the underlying modeling process, the more apt you are to tweak some aspect of the model to try and improve performance. When you start thinking about changing optimizer options, link/activation functions, learning rates, etc., you can easily get lost in the weeds. This would be okay if it actually made a big difference, but in many, or maybe even most cases, it doesn't or there are ways to not have to make the choice in the first place. More to the point, if this sort of estimation parameter tweaking does make a notable difference, that probably means you have a bigger problem with your model architecture or data.

Unless you are using a bleeding edge technique, much of the work has been done for you by folks who had a lot more time to work on these aspects of the model. As such, spending a lot of time trying to decide on ML vs. REML, which variant of an Adam optimizer to use, learning rate of 1e-3 vs. 1e-4, 5 vs. 10 fold CV, or whether you should use ReLU, GELU, or Swish activations, a dropout rate of .1 or .2, etc., a lot of these choices won't matter, or shouldn't if more important aspects of your model and data are in order.


### Everything is Fine

There is a flip side to the previous point, and that is that many assume that the default settings for complex models are good enough. We all do this when venturing into the unknown, but this is not always the case. Many of the more complex models have defaults geared toward a 'just works' setting rather than a 'production' setting. For example, the default number of boosting rounds for xgboost will rarely be adequate[^num_boost_round].

[^num_boost_round]: The number is actually dependent on other parameters, like whether early stopping is used, the number of classes, etc.

### Just Bootstrap It!

When it comes to uncertainty estimation, many advanced modeling tools leave that to the user, and when the developers are pressed on how to get uncertainty estimates, they often will just suggest to bootstrap the result. While the bootstrap is a powerful tool for inference, it isn't appropriate just because you decide to use it. The suggestion to use it is often made in the context of a complex modeling situation where it would be very (prohibitively) computationally expensive, and in other cases the properties of the results are not well understood. Other methods of prediction inference, such as conformal prediction, maybe better suited to the task. In general, if a package developer suggests you bootstrap because their package doesn't have any means of uncertainty estimation, you should be wary.