# Danger Zone {#sec-danger-zone}

![](img/chapter_gp_plots/gp_plot_11.svg){width=75%}


When it comes to conducting models in data science, a lot can go wrong, and in many cases it's easy to get lost in the weeds and lose sight of the bigger picture. In this chapter, we'll discuss some common pitfalls that can sneak up on you when you're working on a data science project, and others that just came to mind while we were thinking about it. The topics are based on things we've commonly seen in consulting across many academic disciplines and industries, and attempts a very general overview. The following groups of focus attempt to reflect the content of the book as it was presented.

<!-- 
The following are very brief thoughts on common pitfalls in modeling we thought of while writing this book. This is not meant to be comprehensive, but rather a quick reference for common things that can trip folks up.  -->

<!-- We're not going to tell you not to do these things, but we are going to tell you to be very careful if you do, and be ready to defend your choices. -->

<!-- ## Things to Generally Avoid -->





TLDR:

- Don't get hung up on statistical significance
    - Solution: focus on practical implications
- Avoid using specialized yet dated techniques, metrics, or other tools that solve problems that don't exist anymore
    - Solution: use tools that are common, well-understood, and still solve the problem
- Feature importance is difficult to assess in the best of circumstances (i.e. almost never), and rarely changes anything about what you're doing from a modeling perspective.
    - Solution: focus on model performance and interpretability. A feature can be the best of a bad lot.
- Don't rely on defaults for complex models
    - Solution: tune your models, and understand the model well enough to set options that make sense for your situation.
- Don't rely on a single metric to assess model performance
    - Solution: use a suite of metrics that are appropriate for your situation
- Don't rely on a single model to make decisions
    - Solution: use a set of models to tackle a problem
- Tweaking your optimizer is generally not going to make a huge difference
    - Solution: Focus on the model architecture and data
- Improving your data is generally going to make a bigger difference than improving your model
    - Solution: Focus on data quality and data collection when possible
- Don't forget to transform your features
    - Solution: use transformations to help model convergence, and potentially improve interpretability
- Don't binarize data just because you think it's easier
    - Solution: Use the data in its raw form when possible, transform if appropriate. Binarizing continuous data can lead to loss of information and worse practical application. Alternate solution - use tree-based models that will use discretized forms of data appropriately, but ultimately use the range available.



## Linear Models and Related Statistical Endeavors

Statistical models are a powerful tool for understanding the relationships between variables in a dataset. They are also excellent at

### Statistical Significance

One of the most common mistakes when conducting statistical linear models is simply relying too heavily on the statistical result. Statistical significance is simply not enough to determine feature importance or model performance. When complex statistical models are applied to small data, the results are typically very noisy and statistical significance can be misleading. This also means that 'big' effects can be a reflection of that noise, rather than something meaningful.

Focusing on statistical significance can lead you down other dangerous paths. For example, relying on statistical tests of assumptions instead of visualizations or practical metrics can lead you to believe that your model is valid when it is not. Using a statistical testing approach to select features can often result in incorrect choices about feature contributions, as well as poorer models. 

### Ignoring Complexity

While techniques like standard linear/logistic regression and GLMs are valid and very useful, for many modeling contexts they may be too simplified to capture the complexity of the data generating process. This can lead to underfitting, where the model is too simple to capture the underlying structure of the data.

On the other side of the coin, many applications of statistical models ignore model assessment on a separate dataset, which can lead to overfitting. This makes generalization of statistical and other results more problematic. Such applications typically use a single model as well, and so may not be indicative of the best approach that could be taken.


### Using Dated Techniques

This is not specific to the statistical linear modeling realm, but there are many applications of statistical models that rely on outdated techniques, metrics, or other tools that solve problems that don't exist anymore. For example, using stepwise/best subset regression for feature selection is not really viable when more principled approaches like the lasso are available. Likewise, we can't really think of a case where something like MANOVA/discriminant function analysis would provide the best answer to a data problem, or where a pseudo-R^2^ would be a metric that would help us understand a model better or make a decision about it. 

Statistical analysis has been around a long time, and many of the techniques that have been developed are still valid, useful, and very powerful. But some reflect the limitations of the time in which they were developed. Others were an attempt to take something that was straightforward for simpler settings (e.g. linear regression) and apply to settings where it doesn't make sense (nonlinear, non-gaussian, etc.). Even when still valid, there are may be better alternatives available now.


### Assuming Simpler is More Interpretable

Standard linear models are often used because of their interpretability, but in many of these modeling situations, interpretability can be difficult to obtain without using the same amount of effort one would for more complex models. Many statistical/linear models employ interactions,  employ nonlinear feature-target relationships (e.g. GLM/GAMs). If your goal is interpretability, many of these approaches can be as difficult to interpret as a features in a random forest. They still have the added benefit of more reliable uncertainty estimation, but one should not assume you will have a result as simple as a coefficient in a linear regression just because you didn't use a deep learning model.

### Problematic Model Comparison

statistical, r^2^, etc.

:::{.callout type='info' title='Garden of Forking Paths'}
A common issue in statistical and machine learning modeling is the **garden of forking paths**. This is the idea that there are many different ways to analyze a dataset, and that the results of these analyses can be very different. When you don't have a lot of data, or when the data is complex and the data generating process is not well understood. In these cases, the interpretation of a single model from the many that are actually employed can be misleading, and can lead to incorrect conclusions about the data.
:::

## Estimation

### What If I Just Tweak This... 


From traditional statistical models to deep learning, the more you know about the underlying modeling process, the more apt you are to tweak some aspect of the model to try and improve performance. When you start thinking about changing optimizer options, link/activation functions, learning rates, etc., you can easily get lost in the weeds. This would be okay if you knew ahead of time it would make a big difference. However, in many, or maybe even most cases, this sort of tweaking doesn't improve model results, or there are ways to not have to make the choice in the first place. More to the point, if this sort of estimation parameter tweaking does make a notable difference, that probably means you have a bigger problem with your model architecture or data.

Unless you are using a bleeding edge technique, much of the work has been done for you by folks who had a lot more time to work on these aspects of the model. As such, spending a lot of time trying to decide on ML vs. REML, which variant of an Adam optimizer to use, learning rate of 1e-3 vs. 1e-4, 5 vs. 10 fold CV, or whether you should use ReLU, GELU, or Swish activations, a dropout rate of .1 or .2, etc., a lot of these choices won't matter, or shouldn't if more important aspects of your model and data are in order.


### Everything is Fine

There is a flip side to the previous point, and that is that many assume that the default settings for complex models are good enough. We all do this when venturing into the unknown, but this is not always the case. Many of the more complex models have defaults geared toward a 'just works' setting rather than a 'production' setting. For example, the default number of boosting rounds for xgboost will rarely be adequate[^num_boost_round].

[^num_boost_round]: The number is actually dependent on other parameters, like whether early stopping is used, the number of classes, etc.

### Just Bootstrap It!

When it comes to uncertainty estimation, many advanced modeling tools leave that to the user, and when the developers are pressed on how to get uncertainty estimates, they often will just suggest to bootstrap the result. While the bootstrap is a powerful tool for inference, it isn't appropriate just because you decide to use it. The suggestion to use it is often made in the context of a complex modeling situation where it would be very (prohibitively) computationally expensive, and in other cases the properties of the results are not well understood. Other methods of prediction inference, such as conformal prediction, maybe better suited to the task. In general, if a package developer suggests you bootstrap because their package doesn't have any means of uncertainty estimation, you should be wary.


## Machine Learning

### General Modeling Issues

We see a lot of issues creep upon with machine learning models, and many of them are the same as those that come up with statistical models, but some are more unique to the machine learning world. A starting point is that many forget to create a baseline model, and instead jump right into a complicated model. This is a problem because it is hard to improve performance if you don't know what a good baseline score is. So create that baseline model and iterate from there.

A related point is that many will jump into machine learning models without fully investigating the data. Standard exploratory data analysis (EDA) is a prerequisite for any modeling, and can go a long way toward saving time and effort in the modeling process.  It's here you'll find problematic cases and features, and can explore ways to deal with it.

When choosing a model or set of models, one should have a valid reason for the choice. Some less stellar reasons include using a model just because it seems popular, or just because it's the only one you know. In addition, as mentioned with other types of models, you want to avoid using older methods that really don't perform well in most situations compared to others[^oldml].

[^oldml]: As we mentioned in the statistical section, many older methods are still valid and useful, but it's not clear what would be gained by using things like a standard support vector machine or hidden markov model related to more recently developed or other techniques that have shown more flexibility.

- Thinking deep learning will solve all your problems. If you are dealing with standard, tabular data, deep learning will mostly just increase computational complexity and time, with no guarantee of increased performance.

- Comparing models on different datasets. If you run different models on separate data, there is no objective way to compare them. As an example, the accuracy may be higher on one dataset just because the baseline rate is much higher.



### Classification


Machine learning is not synonymous with a classification problem, but this point seems to be lost on many. As an example, many will split their target just so they can do classification, when the target is a more expressive continuous variable. This is a problem because you are unnecessarily diminishing the reliability of the target score, and losing information about it. This can lead to a well known statistical issue - **attenuation of the correlation** between variables.

:::{panel-tabset}

##### R
```{r}
#| label: simulate_binarize_r
#| results: hide 
simulate_binarize <- function(N = 1000, correlation = .5, num_simulations = 100, bin_y_only = FALSE) {
    correlations <- list()
    
    for (i in 1:num_simulations) {
        # Simulate two variables with the given correlation
        
        xy = MASS::mvrnorm(
            n = N, 
            mu = c(0, 0), 
            Sigma = matrix(c(1, correlation, correlation, 1), 
            nrow = 2),
            empirical = FALSE
        )

        # binarize on median split
        
        if (bin_y_only) {
            x_bin = xy[, 1]
        } else {
            x_bin = ifelse(xy[, 1] >= median(xy[, 1]), 1, 0)
        }
        
        y_bin = ifelse(xy[, 2] >= median(xy[, 2]), 1, 0)
        
        raw_correlation <- cor(xy[, 1], xy[, 2])
        binarized_correlation <- cor(x_bin, y_bin)
        
        correlations[[i]] <- tibble(
            sim = i,
            raw_correlation,
            binarized_correlation
        )
    }

    cors =  bind_rows(correlations)
    cors
}

simulate_binarize(correlation = .25, num_simulations = 5)
```

##### Python

```{python}
#| label: simulate_binarize_py
#| eval: false
import numpy as np
import pandas as pd

def simulate_binarize(N = 1000, correlation = .5, num_simulations = 100, bin_y_only = False):
    correlations = []
    
    for i in range(num_simulations):
        # Simulate two variables with the given correlation
        xy = np.random.multivariate_normal(
            mean = [0, 0], 
            cov = [[1, correlation], [correlation, 1]], 
            size = N
        )

        # binarize on median split
        if bin_y_only:
            x_bin = xy[:, 0]
        else:
            x_bin = np.where(xy[:, 0] >= np.median(xy[:, 0]), 1, 0)
        y_bin = np.where(xy[:, 1] >= np.median(xy[:, 1]), 1, 0)
        
        raw_correlation = np.corrcoef(xy[:, 0], xy[:, 1])[0, 1]
        binarized_correlation = np.corrcoef(x_bin, y_bin)[0, 1]
        
        correlations.append({
            'sim': i,
            'raw_correlation': raw_correlation,
            'binarized_correlation': binarized_correlation
        })

    cors = pd.DataFrame(correlations)
    return cors

simulate_binarize(correlation = .25, num_simulations = 5)
```
:::

The following plot shows the case where we only binarize the target variable for 500 simulations. The correlation between the raw and binarized variables is .25, .5, or .75, but the attenuated correlation  in the binarized case is still notable. This is because the binarization process has removed the correlation between the variables.

```{r}
#| label: fig-simulate_binarize_plot
#| fig-cap: Density plots of raw and binarized correlations
#| echo: false
#| eval: false

# do simulations for .25, .5, .75 at 100 sims each. Plot the density plots of each faceting on the baseline correlation, and color the type of correlation (raw vs. binarized)

correlations = bind_rows(
    simulate_binarize(correlation = .25, num_simulations = 1000, bin_y_only = TRUE) |> mutate(correlation = '.25'),
    simulate_binarize(correlation = .50, num_simulations = 1000, bin_y_only = TRUE) |> mutate(correlation = '.50'),
    simulate_binarize(correlation = .75, num_simulations = 1000, bin_y_only = TRUE) |> mutate(correlation = '.75')
)

correlations |> 
    # select(-sim) |>
    pivot_longer(
        cols = c(raw_correlation, binarized_correlation), 
        names_to = 'type', 
        values_to = 'Correlation'
    ) |>
    ggplot(aes(x = Correlation)) +
    geom_density(
        aes(color = type, fill = type), 
        alpha = 2/3, 
        position = 'identity', 
        trim = TRUE
    ) +
    scale_fill_manual(values = c('white', okabe_ito[['darkblue']])) +
    scale_color_manual(values = c('gray50', okabe_ito[['darkblue']])) +
    facet_wrap(~correlation, ncol = 1) +
    theme(
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        # legend.background = element_rect(color = NA),
        strip.text = element_text(size = 16)
    )

ggsave('img/danger-binarize_corr.svg', width = 8, height = 6)
```

![Density plots of raw and binarized correlations](img/danger-binarize_corr.svg){width=75% #fig-simulate_binarize_plot}

Common issues with ML classification don't end here however.  Another problem is that many will use a simple .5 cutoff for binary classification, when it is probably not the best choice in most classification settings. Related to this many only focus on accuracy as a metric for performance. Others are more useful in many situations, or just add more information to assess the model. Each metric has its own pros and cons, so you should evaluate your model's performance with a suite of metrics.


### Ignoring Uncertainty

It is very common in ML practice to ignore uncertainty in your predictions or metrics. This is a problem because there is always uncertainty, and acknowledging that it is there will be helpful for mitigating performance expectations. This is especially true when you are using a model in a production setting, where the model's performance can have real-world consequences.

It is difficult, and often computationally hard to get uncertainty estimates for many of the black-box techniques that are popular in ML. Some might suggest that there is enough data such that uncertainty is not needed, but this would have to be demonstrated in some fashion, and there is always increased uncertainty for prediction on new data. In general, there are ways to get uncertainty estimates for these models, e.g., bootstrapping, conformal prediction, and it is worth the effort to do so.


### Hyperfocus on Feature Importance

Researches and businesses often have many questions about which features in an ML model are important. This is a difficult question to answer, and the answer is often not practically useful. Most ML models are going to have interactions, so the importance of a feature is going to be dependent on the other features in the model. If you can't disentangle the effects of one feature from another, where either may or may not be important on their own, or maybe neither is important except in the presence of the other, then trying to talk about a single feature's relative worth is often a misguided endeavor.

Even if we can deem a variable 'important', this doesn't imply a causal relationship, and it doesn't mean that the variable is the best of the features you have. Another might not be as important on its own, but is even more important in the presence of other features. 

What's more, just because an importance metric may deem a feature as not important, that doesn't mean it has no effect on the target. It may be that the feature is correlated with other features that are more important, and so the importance metric is just reflecting that. It may also just mean that the importance metric is not well suited to assessing that feature's contribution.

The reality is that multiple valid measures of importance can come to wildly different conclusions about the importance of a feature, even within the same model setting. One should be very cautious in how they interpret these.

:::{.callout type='info' title='SHAP for Feature Importance'}
SHAP values are meant to assess local, i.e. observation level, feature contributions to a prediction. They are not meant to be used for assessing global feature importance, and can be misleading if used in this way. Often average SHAP values will mostly just reflect the distribution of the feature.
:::


### Other


- Letting training leak into test set. Doing this gives your model an unfair advantage when it is time for testing, leading you to believe that your model is doing better than it really is.


- Forgetting that the data is more important than your modeling technique. The axiom will always ring true: 90% of analytics work is on data prep.

- Forgetting you need to be able to explain your model and code to someone else. The only good model is a useful one; if you can't explain it to someone, it probably isn't useful.

- Assuming that grid search is good enough for all or even most cases. Not only is it computationally expensive, but you very well might miss tuning parameter values that are outside of the grid; instead, you might consider Bayesian Optimization.

- Ignoring temporal/spatial data structure. People will often forget about the effects of time and space on relationships; fortunately, many methods exist for exploring these important effects.




## Data


- Forgetting to scale your data. You don't want the scale of the data to be something that the model hones in on.

- Forgetting to transform your features. Many models perform better when working with variables that are on the same scale.

- Using simple imputation (like the mean, modal category) when you're missing a lot of data. While model-based imputation is always preferred (e.g., MICE), it might not work too well when you have a lot of missingness. 

- Making something binary when it naturally isn't and has notable variability. This is effectively burning information about the variable.

- Dropping/modifying extreme values. They are valid values, so you might just need to use a different distribution.

- Forgetting that large data can get small very quickly due to class imbalance, interactions, etc.
