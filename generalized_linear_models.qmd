# Generalized Linear Models {#sec-glm}

```{r}
#| label: setup-glm
#| include: false
# source("load_packages.R")
# source("setup.R")
# source("functions/utils.R")

reticulate::use_condaenv("book-of-models")
```

```{r}
#| include: false
#| label: setup-glm-r

df_reviews = read_csv("data/movie_reviews.csv")

# for the by-hand option
X = df_reviews |> 
    select(word_count, male = gender) |> 
    mutate(male = ifelse(male == 'male', 1, 0)) |> 
    as.matrix()

y = df_reviews$rating_good
```


```{python}
#| include: false
#| label: setup-glm-py

import pandas as pd
import numpy as np

df_reviews = pd.read_csv("data/movie_reviews.csv")

# for the by-hand option
X = (
  df_reviews[['word_count', 'gender']]
  .rename(columns = {'gender': 'male'})
  .assign(male = np.where(df_reviews[['gender']] == 'male', 1, 0))
)

y = df_reviews["rating_good"]
```

What happens when your target variable isn't really something you feel comfortable assuming is 'normal'? Maybe you've got a binary condition, like good or bad, or maybe you've got a count of something, like the number of times a person has been arrested. In these cases, you can use a linear regression, but it often won't do exactly what you want. Instead, you can *generalize* your approach to handle these scenarios.

**Generalized linear models** exist to map different distributions into linear space. This allows us to use the same linear model framework that we've been using, but with different types of data. These models *generalize* the linear model to incorporate different distributions of the target variable, while still using the same framework.

## Key Ideas {#sec-glm-key}

- A simple tweak to our previous approach allows us to generalize our linear model to account for other settings.
- Common distributions such as binomial, poisson, and others can often do better for us both in terms of model fit and interpretability.
- Getting familiar with just a couple distributions will allow you to really expand your modeling repertoire.


### Why this matters {#sec-glm-why}

The linear model is powerful on its own, but even more so when you realize you can extend many other data settings, some of which are implicitly nonlinear! When we want to classify observations, count them, or deal with proportions and other things, simple tweaks of our standard linear model allow us to handle such situations.

### Good to know {#sec-glm-good2know}

Generalized linear models are a broad class of models that extend the linear model to different distributions of the target variable. In general, you'd need to have a pretty good grasp of linear regression before getting too carried away here.

## Distributions & Link Functions {#sec-glm-distributions}

Remember how linear models really enjoy the whole Gaussian, i.e. 'normal', distribution scene? The essential form of the linear model can be expressed as follows[^conditional]:

[^conditional]: The y in the formula is more properly expressed as $y | X$, where X is the matrix of features. We'll keep it simple here.

$$
\mu = \alpha + X\beta
$$
$$
y \sim \textrm{Normal}(\mu,\sigma)
$$

But not all data follows a Gaussian distribution. Instead, we often find some other distribution would be appropriate, so we need a way to incorporate different distributions of the target into our model. Distributions cannot do it alone! We also need a **link function** to connect the linear model we've been using to the distribution we want to apply.

From a theoretical perspective, link functions are tricky to get your head around.

- *Find the exponential of the response's density function and derive the canonical link function*...

From a conceptual perspective, all they are doing is allowing our linear combination of features to "link" to a distribution function's mean or other parameters. If you know a distribution's 'canonical' link function, that is all the deeper you will probably ever need to go. At the end of the day, these link functions will convert the target to an unbounded continuous variable. The take-away here is that the link function describes how the mean is generated from the (linear) predictor.

## Logistic Regression {#sec-glm-logistic}


As we've seen elsewhere, you will often have a binary variable that you might want to use as a target -- it could be dead/alive, lose/win, quit/retain, etc. You might be tempted to use a linear regression, but you will quickly find that it is not the best option if you're interested in the probability of one category versus the other. So let's try something else.

<!-- TODO: can probably move distribution part of this section to appendix as part of general discussion of some distributions worth knowing, esp. as this is focused on binomial as a count, and the model focus is on the binary/bernoulli special case. -->

### The Binomial Distribution {#sec-glm-binomial}

**Logistic regression** is different than linear regression mostly because instead of that nice continuous target, we are dealing with a target that takes the form of a binary variable, one that we often assume comes from a **binomial distribution**. The binomial distribution doesn't have the $\mu$ or $\sigma^2$ like the Gaussian - instead we have *p* and *n*, where *p* is a probability and *n* is the number of trials. We tend to talk about *p* with regard to the probability of a specific event happening (heads, wins, defaulting, etc.).

Let's see how the binomial distribution looks with 100 trials and probabilities of "success" at *p = * .25, .5, and .75:

```{r}
#| echo: false
#| label: fig-binomial
#| fig-cap: "Binomial distributions for different probabilities"

library(ggplot2)

library(dplyr)

binom.25 = dbinom(1:100, size = 100, prob = .25)

binom.5 = dbinom(1:100, size = 100, prob = .5)

binom.75 = dbinom(1:100, size = 100 , prob = .75)

as.data.frame(rbind(binom.25, binom.5, binom.75)) %>% 
  mutate(prob = row.names(.)) %>% 
  tidyr::gather(., "key", "value", -prob) %>% 
  mutate(key = as.numeric(gsub("V", "", key)), 
         prob = gsub("binom", "", .$prob)) %>% 
  ggplot(., aes(x = key, y = value, fill = prob)) + 
  geom_col(alpha = .5, color = NA) + 
  scale_fill_manual(values = okabe_ito) +
  labs(x = "Number of Successes", y = "Probability") +
  guides(fill = guide_legend(title = "Probability\nValues"))
```

If we examine the distribution for a probability of .5 in @fig-binomial, we will see that it is centered over 50 - this would suggest that we have the highest probability of encountering 50 successes if we ran 100 trials.   Shifting our attention to a .75 probability of success, we see that our density is sitting over 75. In practice we probably end up with something around that value, but on average and over repeated runs of 100 trails, the value would be $p\cdot n$.

Since we are dealing with a number of trials, it is worth noting that the binomial distribution is a discrete distribution. If we have any interest in knowing the probability for a number of successes we can use the following formula:

$$P(x) = \frac{n!}{(n-x)!x!}p^xq^{n-x}$$

While we don't need to dive into finding those specific values for the binomial distribution, we can spend our time exploring how it looks in linear model space:

$$
\textrm{logit}(p) = \alpha + X\beta
$$

$$y \sim \textrm{Binomial}(n, p) \\ $$

The *logit* function is defined as:

$$\textrm{log}\frac{p}{1-p}$$

We are literally just taking the log of the odds (the log odds become important later).

Now we can map this back to our model:

$$\textrm{log}\frac{p}{1-p} = \alpha + X\beta$$

And finally, we can take that logistic function and invert it (the **inverse-logit** function) to produce the probabilities.

$$p = \frac{\textrm{exp}(\alpha + X\beta)}{1 + \textrm{exp}(\alpha + X\beta)}$$

or equivalently:

$$p = \frac{1}{1 + \textrm{exp}(-(\alpha + X\beta))}$$

Whenever we get coefficients for the logistic regression model, we are always going to get them as log odds, but usually we exponentiate them to get the **odds ratio**. For example, if we have a coefficient of 1.5, we would say that for every one unit increase in the predictor, the odds of the target being a "success" increase by 1.5 times. 

### Probability, Odds, and Log Odds {#sec-glm-binomial-prob}

Probability lies at the heart of all of this, so let's look at the relationship between the probability and log odds. In our model, the log odds are the linear combination of our features. Let's say we have a model that produces those values. We can then convert them from the linear space to the (nonlinear) probability space with our inverse-logit function.

```{r}
#| echo: false
#| label: fig-probs
probability = seq(.0001, .9999, by = .01)
```


```{r}
#| echo: false
#| label: fig-prob-log-odds
#| fig-cap: "Log odds and probability values"
ggplot(
  tibble(log_odds = log(probability / (1- probability)), probability = probability),
  aes(log_odds, probability)) +
  geom_point() +
  labs(x = "Log Odds", y = "Probability")
```

This is the classic logistic function, where we can see that the probability of success approaches 0 when the log odds are negative and approaches 1 when the log odds are positive. The main message here is that we can take a bounded variable in probability and transform it to continuous space, and vice versa.

### A Preliminary Model {#sec-glm-binomial-model}

We are going to return to our movie review data, and we are going to use `rating_good` as our target. Before we get to modeling, see if you can find out the frequency of "good" and "bad" reviews. We will use `word_count` and `gender` as our features. Before we move on, though, find the probability of getting a "good" review.

<!-- TODO: Do we want to leave model fitting via optim to estimation chapter -->

:::{.panel-tabset}

##### R

```{r}
#| label: import-data-r
df_reviews = read_csv("data/movie_reviews.csv")

X = df_reviews |> 
    select(word_count, male = gender) |> 
    mutate(male = ifelse(male == 'male', 1, 0)) |> 
    as.matrix()

y = df_reviews$rating_good
```

##### Python

```{python}
#| label: import-data-py
import pandas as pd
import numpy as np

df_reviews = pd.read_csv("data/movie_reviews.csv")

X = (
  df_reviews[['word_count', 'gender']]
  .rename(columns = {'gender': 'male'})
  .assign(male = np.where(df_reviews[['gender']] == 'male', 1, 0))
)

y = df_reviews["rating_good"]
```

:::


For an initial logistic regression model, we can use standard and common functions in our chosen language.

:::{.panel-tabset}

##### R

```{r}
#| label: model-logistic-r
model_logistic = glm(
    rating_good ~ word_count + gender, 
    data = df_reviews,
    family = binomial
)

summary(model_logistic)
```

##### Python

```{python}
#| label: model-logistic-py
import statsmodels.api as sm
import statsmodels.formula.api as smf

model_logistic = smf.glm(
  'rating_good ~ word_count + gender', 
  data = df_reviews,
  family = sm.families.Binomial()
  ).fit()

model_logistic.summary()
```

:::

### Interpretation and Visualization {#sec-glm-binomial-interpret}

We need to know what those results mean. The coefficients that we get from our model are in log odds. We can exponentiate them to get the odds ratio, but we can also exponentiate them and divide by 1 + that value to get the probability. Interpreting log odds is difficult, but we can at least get a feeling for them directionally. A log odds of 0 would indicate no relationship between the feature and target. A positive log odds would indicate that an increase in the feature will increase the log odds of moving from "bad" to "good", whereas a negative log odds would indicate that a decrease in the feature will decrease the log odds of moving from "bad" to "good".  On the log odds scale, the coefficients are symmetric as well, such that, e.g., a +1 coefficient denotes a similar increase in the log odds as a -1 coefficient denotes a decrease.

We can convert those log odds to help make some more sense from them. When we exponentiate the log odds coefficients, we are given the odds ratio.  This is the ratio of the odds of the outcome (i.e., success from our binomial distribution) occurring for a one unit increase in the feature. 

```{r}
#| echo: false
#| label: show-odds-ratio
#| tbl-cap: "Odds ratios for logistic regression model"
lo_result = parameters::model_parameters(model_logistic) 
or_result = exp(lo_result$Coefficient)

lo_result |> 
  as_tibble() |> 
  select(Parameter, Coefficient) |> 
  mutate(`Exp. Coef` = exp(Coefficient)) |> 
  gt()
```


Fortunately, the intercept is easy -- it is the odds of a "good" review when word count is 0 and gender is "female". We see that we've got an odds ratio of `r round(or_result[2], 2)` for the word count variable and `r round(or_result[3], 2)` for the male variable. An odds ratio of 1 means that there is no change in the odds of the outcome occurring, that the feature does not influence the target. An odds ratio of less than 1 means that the odds of the outcome's chosen class occurring decreases as the feature increases. An odds ratio of greater than 1 means that the odds of the outcome occurring increase as the feature increases.

It is more intuitive to interpret the probability, so we'll get predicted probabilities for different values of the features. The way we do this is through the (inverse) link function, which will convert our log odds of the linear predictor to probabilities. We can then plot these probabilities to see how they change with the features.

```{r}
#| echo: false
#| label: fig-logistic-regression-word-count
ggeffects::ggpredict(model_logistic, terms = c("word_count")) |> 
  plot(color = okabe_ito[2]) +
  scale_fill_manual(values = okabe_ito) +
  labs(x = "Word Count", y = "% Good Rating", title = "Logistic Regression Predictions") +
  theme_clean() 
```


In @fig-logistic-regression-word-count, we can see a clear negative relationship between the number of words in a review and the probability of being considered a "good" movie. As we get over 20 words, the predicted probability of being a "good" movie is less than .2. We also don't see much difference between male and female ratings. Note that in both cases, we're getting predictions for one feature while the other feature is held at zero or its reference level.


```{r}
#| echo: false
#| label: fig-logistic-regression-gender
#| fig-cap: "Logistic regression predictions for gender feature"
ggeffects::ggpredict(model_logistic, terms = c("gender")) |> 
  plot(
    color = okabe_ito[2], 
    dot_size = 4, 
    dot_alpha = 1,
    line_size = 1
  ) +
  scale_fill_manual(values = okabe_ito) +
  labs(x = "", y = "% Good Rating", title = "Logistic Regression Predictions") +
  theme_clean() 
```



### Objective Function & Model Fitting {#sec-glm-binomial-objective}

Let's see how we can pick that work apart to create our own functions to help us demystify the process. We can use maximum likelihood estimation to estimate the parameters of our model, which is the approach used by standard package functions. Feel free to skip this part if you only wanted conceptual, but for even more information on maximum likelihood estimation, see @sec-estim-maxlike where we take a deeper dive into the topic and with a similar function.  The following code is a simple but effective version of what goes on behind the scenes with 'glm' type functions.

:::{.panel-tabset}

##### R

```{r}
#| label: glm-ml-r
my_glm = function(par, X, y, family = "binomial") {
    # add an column for the intercept
    X = cbind(1, X)

    # Calculate the linear predictor
    mu = X %*% par # %*% is matrix multiplication

    # get the likelihood for the binomial or poisson distribution
    if (family == "binomial") {
      # Convert to a probability ('logit' link/inverse)
      p = 1 / (1 + exp(-mu))
      L = dbinom(y, size = 1, prob = p, log = TRUE)
    }
    else if (family == "poisson") {
      # Convert to a count ('log' link/inverse)
      p = exp(mu)
      L = dpois(y, lambda = p, log = TRUE)
    }

    # return the negative sum of the log-likelihood (for minimization)
    -sum(L) 
}
```

##### Python

```{python}
#| label: glm-ml-py

from scipy.stats import poisson, binom
def my_glm(par, X, y, family = "binomial"):
    # add an column for the intercept
    X = np.column_stack((np.ones(X.shape[0]), X))

    # Calculate the linear predictor
    mu = X.dot(par)
    
    # get the likelihood for the binomial or poisson distribution
    if family == "binomial":
        p = 1 / (1 + np.exp(-mu))
        L = binom.logpmf(y, 1, p)
    elif family == "poisson":
        lambda_ = np.exp(mu)
        L = poisson.logpmf(y, lambda_)
    
    # return the negative sum of the log-likelihood (for minimization)
    return -np.sum(L)
```

:::


Now that we have our objective function, we can fit our model.  We will use the `optim` function in R and the `minimize` function in Python.

:::{.panel-tabset}

##### R

```{r}
#| label: logreg-ml-r-run
#| results: 'hide'
init = rep(0, ncol(X) + 1)

names(init) = c('intercept', 'b1', 'b2')

fit_logistic = optim(
  par = init,
  fn  = my_glm,
  X   = X,
  y   = y,
  control = list(reltol = 1e-8)
)

fit_logistic$par
```

##### Python

```{python}
#| label: logreg-ml-py-run
#| results: 'hide'
import numpy as np
from scipy.optimize import minimize

init = np.zeros(X.shape[1] + 1)

fit_logistic = minimize(
    fun = my_glm,
    x0 = init,
    args = (X, y),
    method = 'BFGS'
)

fit_logistic.x
```

:::

And here is our comparison table. Looks like our little function worked well!

```{r}  
#| echo: false
#| label: tbl-logistic-ml

tibble(
  Parameter = c("Intercept", "Word Count", "Male"),
  Ours = fit_logistic$par,
  Standard = model_logistic$coefficients
) |> 
    as_tibble() |> 
    gt(decimals = 4)
```


:::{.callout type='info' title="A Note on $R^2$ for Logistic Regression" collapse="true"}
Logistic regression does not have an $R^2$ value in the way that a linear regression model does. Instead, there are pseudo-$R^2$ values, but they are not the same as the $R^2$ value that you are used to seeing. You can find a great breakdown of different pseudo methods [here](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/).
:::

## Poisson Regression {#sec-glm-poisson}

**Poisson regression** also belongs to the class of generalized linear models. It's used when you have a count variable as your target. The nature of a count variable is a bit different, since it starts at 0 and can only be a whole number. A lot of times it is naturally skewed as well. So we'd like a model better suited to this situation.

### The Poisson Distribution {#sec-glm-poisson-distribution}

The Poisson distribution is very similar to the binomial distribution, because the binomial is also a count distribution and in fact generalizes the poisson[^poisbin]. The main difference is in its parameter: Poisson has a single parameter noted as $\lambda$. This rate parameter is going to estimate the expected number of events during a time interval. This can be accidents in a year, pieces produced in a day, or hits during the course of a baseball season. We can find the rate by determining the number of events per interval, multiplied by the interval length.

[^poisbin]: If your binomial regarded very small rates, where the number of trials is very large relative to the number of successes (relatively small $p$), you would find that the binomial distribution would converge to the poisson distribution.

$$\frac{\text{event}}{\text{interval}}*\text{interval length} $$

To put some numbers to that, if we have 1 accident per week in a factory and we are observing a whole year, we would have a rate of $(1 / 7) * 28 = 4$ accidents per month.

Let's see what that particular distribution might look like in @fig-poisson-distribution:

```{r}
#| echo: false
#| label: fig-poisson-distribution
#| fig-cap: "Poisson distribution for a rate of 4"

ggplot(tibble(x = 0:20), aes(x)) +
    geom_col(aes(y = dpois(x, (1 / 7) * 28))) +
    labs(x = "Number of Accidents", y = "Probability")
```

We can also see what it looks like for different rates (some places might be safer than others) in @fig-poisson-distribution-rates.

```{r}
#| echo: false
#| label: fig-poisson-distribution-rates
#| fig-cap: "Poisson distributions for different rates"
ggplot() +
    geom_col(
        data = data.frame(x = 1:28, y = dpois(1:28, (1 / 7) * 28)),
        mapping = aes(x, y), width = .97, alpha = .5, fill = okabe_ito[1], color = NA
    ) +
    geom_col(
        data = data.frame(x = 1:28, y = dpois(1:28, (3 / 7) * 28)),
        mapping = aes(x, y), width = .97, alpha = .5, fill = okabe_ito[2], color = NA
    ) +
    geom_col(
        data = data.frame(x = 1:28, y = dpois(1:28, (5 / 7) * 28)),
        mapping = aes(x, y), width = .97, alpha = .5, fill = okabe_ito[3], color = NA
    )
```

:::{.callout-tip title="More Poisson" collapse="true"}
A cool thing about these distributions is that they can deal with different *exposure* rates. You don't need observations recorded over the same interval length, because you can adjust for them appropriately. They can also be used to model inter-arrival times and time-until-events.  Also note how the distribution becomes more symmetric and bell-shaped as the rate increases. For large mean counts, you can just go back to using the normal distribution.
:::

Let's make a new variable that will count the number of times a person uses a personal pronoun word in their review. We'll use it as our target variable and see how it relates to the number of words and gender in a review as we did before.

:::{.panel-tabset}

##### R

```{r}
#| label: create-poss-pronoun-r
#| fig-show: 'hide'
df_reviews$poss_pronoun = stringr::str_count(
  df_reviews$review_text, 
  "\\bI\\b|\\bme\\b|\\b[Mm]y\\b|\\bmine\\b|\\bmyself\\b"
)

hist(df_reviews$poss_pronoun)
```

##### Python

```{python}
#| label: create-poss-pronoun-py
#| fig-show: 'hide'
df_reviews['poss_pronoun'] = df_reviews['review_text'].str.count(
  "\\bI\\b|\\bme\\b|\\b[Mm]y\\b|\\bmine\\b|\\bmyself\\b"
  )

df_reviews['poss_pronoun'].hist()
```

:::

```{r}
#| echo: false
#| label: fig-poss-pronoun

ggplot(df_reviews, aes(poss_pronoun)) +
    geom_histogram(binwidth = .5, fill = okabe_ito[1]) +
    labs(x = "Number of Personal Pronouns", y = 'Count')
```

### Model {#sec-glm-poisson-model-fitting}

Recall that every GLM distribution has a link function that works best for it. The poisson distribution uses a log link function:

$$\text{log}(\lambda) = \alpha + X\beta$$
$$y = \textrm{Poisson}(\lambda)$$

Using the log link keeps the outcome non-negative like we want.  For model fitting with, with standard functions all we have to do is switch the family from binomial to poisson. The default link is the 'log', so we don't have to specify it explicitly.

:::{.panel-tabset}

##### R

```{r}
#| label: model-poisson-r
#| results: 'hide'
model_poisson = glm(
  poss_pronoun ~ word_count + gender,
  data = df_reviews,
  family = poisson
)

summary(model_poisson)

exp(model_poisson$coefficients)

fit_poisson = optim(
  par = c(0, 0, 0),
  fn = my_glm,
  X = X,
  y = df_reviews$poss_pronoun,
  family = "poisson"
)

fit_poisson$par
```


##### Python

```{python}
#| label: model-poisson-py
#| results: 'hide'
import statsmodels.api as sm
import statsmodels.formula.api as smf

model_poisson = smf.glm(
  formula = "poss_pronoun ~ word_count",
  data = df_reviews,
  family = sm.families.Poisson()
).fit()

model_poisson.summary()        

np.exp(model_poisson.params)

fit_poisson = minimize(
  fun = my_glm,
  x0 = np.zeros(X.shape[1] + 1),
  args = (X, df_reviews["poss_pronoun"], "poisson")
)
```

:::

Like with logistic, we can exponentiate the coefficients to get what's now referred to as the [rate ratio](https://en.wikipedia.org/wiki/Rate_ratio#:~:text=In%20epidemiology%2C%20a%20rate%20ratio,any%20given%20point%20in%20time.). This is the ratio of the rate of the outcome occurring for a one unit increase in the feature. 

```{r}
#| echo: false
#| label: show-rate-ratio
#| tbl-cap: "Rate ratios for poisson regression model"

log_result = parameters::model_parameters(model_poisson)
rr_result = exp(log_result$Coefficient)

log_result |> 
  as_tibble() |> 
  select(Parameter, Coefficient) |> 
  mutate(
    `Exp. Coef` = exp(Coefficient),
    `Our result (raw)` = fit_poisson$par
  ) |> 
  gt(decimals = 3)
```

We are going to interpret this similarly as with other linear models. The slight wrinkle here is that we are looking at the log counts - remember that we specified the log link function. In other words, an increase in one review word leads to an expected log count `r word_sign(log_result$Coefficient[2], c('increase', 'decrease'))` of ~`r round(log_result$Coefficient[2], 2)`. We can exponentiate this to get `r round(rr_result[2], 2)` - which tells us that every added word in a review gets us a `r scales::label_percent()(round(rr_result[2], 2) - 1) ` `r word_sign(log_result$Coefficient[2], c('increase', 'decrease'))` in the number of possessive pronouns - probably not a surprising result we hope - wordier stuff has more types of words! But as usual, the visualization tells the real story. Notice too that the predictions are not discrete, but continuous, because predictions here are the expected or average counts.

```{r}
#| echo: false
#| label: fig-poisson-regression
#| fig-cap: "Poisson regression predictions for word count feature"
ggeffects::ggpredict(model_poisson, terms = "word_count") |> 
  plot(colors = okabe_ito) +
  scale_y_continuous(breaks = 1:5) +
  theme_clean() +
  labs(x= "Word Count", y = "Count", title = "Poisson Regression Predictions")
```

With everything coupled together, we have an interpretable coefficient for `word_count`, a clear plot, and adequate model fit. Therefore, we might conclude that there is a positive relationship between the number of words in a review on the number of times a person uses a personal possessive.

:::{.callout type='info' title="Nonlinear linear models?" collapse="true"}
Did you notice that both our effects for word count in the logistic (@fig-logistic-regression-word-count) and poisson (@fig-poisson-regression) models were not exactly the straightest of lines? Once we're on the probability and count scales, we're not going to see the same linear relationships that we might expect from a basic linear model due to the transformation. If we plot the effect on the log-odds or log-count scale, we're back to straight lines. This is a first taste in how the linear model can be used to get at nonlinear relationships, which are of the focus of @sec-lm-extend.
:::



## Wrapping Up {#sec-glm-wrap}

So at this point you have standard linear regression with the gaussian (normal) distribution for continuous targets, logistic for binary ones, and poisson for counts. These models combine to provide you all you need for starting out, and all serve well as baseline models for comparison to more complex methods (@sec-ml-baseline). That is, however, just a tiny slice of the potential distributions that you could use, and there are still others both in the [GLM family](https://en.wikipedia.org/wiki/Exponential_family) proper and in related scenarios. Here is a quick list[^glmlist]:

[^glmlist]: There is not strict agreement about what qualifies.

Other Core GLM (available in standard functions):

- **Gamma**: For continuous, positive targets that are skewed.
- **Inverse Gaussian**: For continuous, positive targets that are skewed and have a long tail.

Others (some fairly common):

- **Beta**: For continuous targets that are bounded between 0 and 1.
- **Log-Normal**: For continuous targets that are skewed. Essentially what you get with linear regression and logging the target[^lognorm].
- **Tweedie**: Generalizes several core GLM family distributions.

[^lognorm]: [But there is a variance issue to consider](https://stats.stackexchange.com/questions/99867/difference-between-log-normal-distribution-and-logging-variables-fitting-normal).

In the ballpark:

- **Negative Binomial**: For count targets that are overdispersed.
- **Multinomial**: Typically used for categorical targets with more than two categories, but like the binomial, it is actually a more general (multivariate) count distribution.
- **Student t**: For continuous targets that are distributed similar to normal but with heavier tails.
- **Quasi \***: For example quasipoisson. These served a need at one point that is best served by other means.

You'll typically need separate packages to fit some of these, but most often the tools keep to a similar functional approach. The main thing is to know that certain distributions might fit your data a bit better than others, and that you can use both the same basic framework and mindset to fit them, and maybe get a little closer to the answer you seek about your data!

### The Thread {#sec-glm-thread}

GLMs extend your standard linear model as a powerful tool for modeling a wide range of data types. They are a great way to get started with more complex models, and even allow us to linear models in a not so linear way. It's best to think of GLMs more broadly than the strict statistical definition, and consider many models like ordinal regression, ranking models, survival analysis, and more as part of the same extension.

### Choose Your Own Adventure {#sec-glm-adventure}

At this point you have a pretty good sense of what linear models have to offer, but there's even more! You can start to look at more complex models that build on these ideas, like mixed models, generalized additive models and more in @sec-lm-extend. You can also feel confident heading into the world of machine learning (@sec-ml-core-concepts), where you'll find additional ways to think about your modeling approach.



### Additional Resources {#sec-glm-resources}

If you are itching for a textbook, there isn't any shortage of them out there and you can essentially take your pick, though most purely statistical treatments are going to be a bit dated at this point, but still accurate and maybe worth your time. 


- *Generalized Linear Models* (@mccullagh_generalized_2019) is a classic text on the subject, but it is a bit dense and not for the faint of heart, or even @nelder_generalized_1972, which is a very early treatment.

For more accessible fare that doesn't lack on core details either:

- *An Introduction to Generalized Linear Models* is generally well regarded (@dobson_introduction_2018).
- *Generalized Linear Models* is more accessible (@hardin_generalized_2018).
- Roback and Legler's [*Beyond Multiple Linear Regression*](https://bookdown.org/roback/bookdown-BeyondMLR/), available for free.
- *Applied Regression Analysis and Generalized Linear Models*  (@fox_applied_2015)
- *Generalized Linear Models with Examples in R* (@dunn_generalized_2018) 


## Exercise

<!-- TODO: Add data links -->

Use the classic heart disease data to conduct a logistic regression and see how well you can predict the presence of heart disease with features such as age, sex, cholesterol, and more. (@sec-dd-hear-failure)


Use the fish data to conduct a poisson regression and see how well you can predict the number of fish caught based on the other variables like how many people were on the trip, how many children, whether live bait was used etc. (@sec-dd-fish)
