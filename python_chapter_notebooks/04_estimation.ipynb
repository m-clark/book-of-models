{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_happiness = (\n",
    "#     pd.read_csv('https://tinyurl.com/worldhappiness2018')\n",
    "#     .dropna()\n",
    "#     .rename(columns = {'happiness_score': 'happiness'})\n",
    "# )\n",
    "\n",
    "df_happiness = (\n",
    "    pd.read_csv('https://tinyurl.com/worldhappiness2018')\n",
    "    .dropna()\n",
    "    .rename(\n",
    "        columns = {\n",
    "            'happiness_score': 'happiness',\n",
    "            'healthy_life_expectancy_at_birth': 'life_exp',\n",
    "            'log_gdp_per_capita': 'log_gdp_pc',\n",
    "            'perceptions_of_corruption': 'corrupt'\n",
    "        }\n",
    "    )\n",
    "    .assign(\n",
    "        gdp_pc = lambda x: np.exp(x['log_gdp_pc']),\n",
    "    )    \n",
    "    [['country', 'happiness','life_exp', 'gdp_pc', 'corrupt']]\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_happiness[['life_exp_sc', 'gdp_pc_sc', 'corrupt_sc']] = scaler.fit_transform(\n",
    "    df_happiness[['life_exp', 'gdp_pc', 'corrupt']]\n",
    ")\n",
    "df_happiness = df_happiness.drop(columns = ['life_exp', 'gdp_pc', 'corrupt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE comparison between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>5.087318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.635838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model       MSE\n",
       "0     A  5.087318\n",
       "1     B  0.635838"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_happiness['happiness']\n",
    "\n",
    "# Calculate the error for the guess of four\n",
    "prediction = np.min(df_happiness['happiness']) + 1 * df_happiness['life_exp_sc']\n",
    "mse_model_A   = np.mean((y - prediction)**2)\n",
    "\n",
    "# Calculate the error for our other guess\n",
    "prediction = y.mean() + .5 * df_happiness['life_exp_sc']\n",
    "mse_model_B  = np.mean((y - prediction)**2)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Model': ['A', 'B'],\n",
    "    'MSE': [mse_model_A, mse_model_B]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later comparison\n",
    "model_lr_happy = smf.ols('happiness ~ life_exp_sc', data = df_happiness).fit()\n",
    "\n",
    "def ols(par, X, y):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the predicted values\n",
    "    y_hat = X @ par  # @ is matrix multiplication\n",
    "    \n",
    "    # Calculate the mean of the squared errors\n",
    "    value = np.mean((y - y_hat)**2)\n",
    "    \n",
    "    # Return the objective value\n",
    "    return(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.793842044979073"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "guesses = pd.DataFrame(\n",
    "    product(\n",
    "        np.arange(1, 7, 0.1),\n",
    "        np.arange(-1, 1, 0.1)\n",
    "    ),\n",
    "    columns = ['b0', 'b1']\n",
    ")\n",
    "\n",
    "# Example for one guess\n",
    "ols(\n",
    "    par = guesses.iloc[0,:],\n",
    "    X = df_happiness['life_exp_sc'],\n",
    "    y = df_happiness['happiness']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>objective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.490789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      b0   b1  objective\n",
       "899  5.4  0.9   0.490789"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses['objective'] = guesses.apply(\n",
    "    lambda x: ols(\n",
    "        par = x, \n",
    "        X = df_happiness['life_exp_sc'], \n",
    "        y = df_happiness['happiness']\n",
    "    ),\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "min_loss = guesses[guesses['objective'] == guesses['objective'].min()]\n",
    "\n",
    "min_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 0.48851727833540676\n",
       "        x: [ 5.445e+00  8.838e-01]\n",
       "      nit: 3\n",
       "      jac: [-9.313e-08  7.004e-07]\n",
       " hess_inv: [[ 5.190e-01 -9.564e-02]\n",
       "            [-9.564e-02  9.810e-01]]\n",
       "     nfev: 12\n",
       "     njev: 4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_result = minimize(\n",
    "    fun  = ols,\n",
    "    x0   = np.array([1., 0.]),\n",
    "    args = (\n",
    "        np.array(df_happiness['life_exp_sc']), \n",
    "        np.array(df_happiness['happiness'])\n",
    "    ),\n",
    "    method  = 'BFGS',   # optimization algorithm\n",
    "    tol     = 1e-6,     # tolerance\n",
    "    options = {\n",
    "        'maxiter': 500  # max iterations\n",
    "    }\n",
    ")\n",
    "\n",
    "our_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10798193, 0.22184167])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two example life expectancy scores, at the mean (0) and 1 sd above\n",
    "life_expectancy = np.array([0, 1])\n",
    "\n",
    "# observed happiness scores\n",
    "happiness = np.array([4, 5.2])\n",
    "\n",
    "# predicted happiness with rounded coefs\n",
    "mu = 5 + 1 * life_expectancy\n",
    "\n",
    "# just a guess for sigma\n",
    "sigma = .5\n",
    "\n",
    "# likelihood for each observation\n",
    "L = norm.pdf(happiness, loc = mu, scale = sigma)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35819022,  5.44483213,  0.88382377])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_likelihood(par, X, y):\n",
    "    \n",
    "    # setup\n",
    "    X = np.c_[np.ones(X.shape[0]), X] # add a column of 1s for the intercept\n",
    "    beta   = par[1:]         # coefficients\n",
    "    sigma  = np.exp(par[0])  # error sd, exp keeps positive\n",
    "    N = X.shape[0]\n",
    "\n",
    "    LP = X @ beta            # linear predictor\n",
    "    mu = LP                  # identity link in the glm sense\n",
    "\n",
    "    # calculate (log) likelihood\n",
    "    ll = norm.logpdf(y, loc = mu, scale = sigma)\n",
    "\n",
    "    value = -np.sum(ll)      # negative for minimization\n",
    "\n",
    "    return(value)\n",
    "\n",
    "our_result = minimize(\n",
    "    fun  = max_likelihood,\n",
    "    x0   = np.array([1, 0, 0]),\n",
    "    args = (\n",
    "        np.array(df_happiness['life_exp_sc']), \n",
    "        np.array(df_happiness['happiness'])\n",
    "    )\n",
    ")\n",
    "\n",
    "our_result['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use lambda_ because lambda is a reserved word in python\n",
    "def ridge(par, X, y, lambda_ = 0):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the predicted values\n",
    "    mu = X @ par\n",
    "    \n",
    "    # Calculate the error\n",
    "    value = np.sum((y - mu)**2)\n",
    "    \n",
    "    # Add the penalty\n",
    "    value = value + lambda_ * np.sum(par**2)\n",
    "    \n",
    "    return(value)\n",
    "\n",
    "our_result = minimize(\n",
    "    fun  = ridge,\n",
    "    x0   = np.array([0, 0, 0, 0]),\n",
    "    args = (\n",
    "        np.array(df_happiness.drop(columns=['happiness', 'country'])),\n",
    "        np.array(df_happiness['happiness']), \n",
    "        0.1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.439975  ,  0.52188496,  0.43554025, -0.10484643])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_result['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    par, \n",
    "    X, \n",
    "    y, \n",
    "    tolerance = 1e-3, \n",
    "    maxit = 1000, \n",
    "    learning_rate = 1e-3\n",
    "):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    # initialize\n",
    "    beta = par\n",
    "    loss = np.sum((X @ beta - y)**2)\n",
    "    tol = 1\n",
    "    iter = 1\n",
    "\n",
    "    while (tol > tolerance and iter < maxit):\n",
    "        LP = X @ beta\n",
    "        grad = X.T @ (LP - y)\n",
    "        betaCurrent = beta - learning_rate * grad\n",
    "        tol = np.max(np.abs(betaCurrent - beta))\n",
    "        beta = betaCurrent\n",
    "        loss = np.append(loss, np.sum((LP - y)**2))\n",
    "        iter = iter + 1\n",
    "\n",
    "    output = {\n",
    "        'par': beta,\n",
    "        'loss': loss,\n",
    "        'MSE': np.mean((LP - y)**2),\n",
    "        'iter': iter,\n",
    "        'fitted': LP\n",
    "    }\n",
    "\n",
    "    return(output)\n",
    "\n",
    "our_result = gradient_descent(\n",
    "    par = np.array([0, 0, 0, 0]),\n",
    "    X = df_happiness[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']].to_numpy(),\n",
    "    y = df_happiness['happiness'].to_numpy(),\n",
    "    learning_rate = 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.43691264,  0.51898243, -0.10687814,  0.4370022 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_result['par']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x6/4jhswqxj0sqf_gkgq6lw6l880000gn/T/ipykernel_69776/3050765792.py:55: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  fits[i] = LP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.42766433,  0.53158177, -0.14934094,  0.39770852])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    par, # parameter estimates\n",
    "    X,   # model matrix\n",
    "    y,   # target variable\n",
    "    learning_rate = 1, # the learning rate\n",
    "    stepsize_tau = 0,  # if > 0, a check on the LR at early iterations\n",
    "    average = False    # a variation of the approach\n",
    "):\n",
    "    # initialize\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # shuffle the data\n",
    "    idx = np.random.choice(\n",
    "        df_happiness.shape[0], \n",
    "        df_happiness.shape[0], \n",
    "        replace = False\n",
    "    )\n",
    "    X = X[idx, :]\n",
    "    y = y[idx]\n",
    "    \n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    beta = par\n",
    "\n",
    "    # Collect all estimates\n",
    "    betamat = np.zeros((X.shape[0], beta.shape[0]))\n",
    "\n",
    "    # Collect fitted values at each point))\n",
    "    fits = np.zeros(X.shape[0])\n",
    "\n",
    "    # Collect loss at each point\n",
    "    loss = np.zeros(X.shape[0])\n",
    "\n",
    "    # adagrad per parameter learning rate adjustment\n",
    "    s = 0\n",
    "\n",
    "    # a smoothing term to avoid division by zero\n",
    "    eps = 1e-8\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        Xi = X[None, i, :]\n",
    "        yi = y[i]\n",
    "\n",
    "        # matrix operations not necessary here,\n",
    "        # but makes consistent with previous gd func\n",
    "        LP = Xi @ beta\n",
    "        grad = Xi.T @ (LP - yi)\n",
    "        s = s + grad**2 # adagrad approach\n",
    "\n",
    "        # update\n",
    "        beta = beta - learning_rate / \\\n",
    "            (stepsize_tau + np.sqrt(s + eps)) * grad\n",
    "\n",
    "        betamat[i, :] = beta\n",
    "\n",
    "        fits[i] = LP\n",
    "        loss[i] = np.sum((LP - yi)**2)\n",
    "\n",
    "    LP = X @ beta\n",
    "    lastloss = np.sum((LP - y)**2)\n",
    "\n",
    "    output = {\n",
    "        'par': beta,          # final estimates\n",
    "        'par_chain': betamat, # estimates at each iteration\n",
    "        'MSE': lastloss / X.shape[0],\n",
    "        'fitted': LP\n",
    "    }\n",
    "\n",
    "    return(output)\n",
    "\n",
    "X_train = df_happiness[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']]\n",
    "y_train = df_happiness['happiness']\n",
    "\n",
    "our_result = stochastic_gradient_descent(\n",
    "    par = np.array([np.mean(df_happiness['happiness']), 0, 0, 0]),\n",
    "    X = X_train.to_numpy(),\n",
    "    y = y_train.to_numpy(),\n",
    "    learning_rate = .15,\n",
    "    stepsize_tau = .1\n",
    ")\n",
    "\n",
    "our_result['par']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.45437658,  0.51165618, -0.10404313,  0.45536819])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap(X, y, nboot=100, seed=123):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # initialize\n",
    "    beta = np.empty((nboot, X.shape[1]))\n",
    "    \n",
    "    # beta = pd.DataFrame(beta, columns=['Intercept'] + list(cn))\n",
    "    mse = np.empty(nboot)    \n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for i in range(nboot):\n",
    "        # sample with replacement\n",
    "        idx = np.random.randint(0, N, N)\n",
    "        Xi = X[idx, :]\n",
    "        yi = y[idx]\n",
    "\n",
    "        # estimate model\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        mod = model.fit(Xi, yi)\n",
    "\n",
    "        # save results\n",
    "        beta[i, :] = mod.coef_\n",
    "        mse[i] = np.sum((mod.predict(Xi) - yi)**2) / N\n",
    "\n",
    "    # given mean estimates, calculate MSE\n",
    "    y_hat = X @ beta.mean(axis=0)\n",
    "    final_mse = np.sum((y - y_hat)**2) / N\n",
    "\n",
    "    output = {\n",
    "        'par': beta,\n",
    "        'mse': mse,\n",
    "        'final_mse': final_mse\n",
    "    }\n",
    "\n",
    "    return(output)\n",
    "\n",
    "our_result = bootstrap(\n",
    "    X = df_happiness[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']],\n",
    "    y = df_happiness['happiness'],\n",
    "    nboot = 250\n",
    ")\n",
    "\n",
    "np.mean(our_result['par'], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-of-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
