# Extending the Linear Model {#sec-lm-extend}


```{r}
#| label: setup-extensions
#| include: false
source("load_packages.R")
source("setup.R")
source("functions/utils.R")

library(reticulate)
use_condaenv("book-of-models")
```

```{r}
#| include: false
#| label: r-data-import

df_reviews = read.csv("data/movie_reviews_processed.csv")
df_happiness = read.csv("data/world_happiness_2018.csv")
```

```{python}
#| include: false
#| label: py-data-import

import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

df_reviews = pd.read_csv("data/movie_reviews_processed.csv")
df_happiness = pd.read_csv("data/world_happiness_2018.csv")
```

With just linear model and generalized linear models, we have a solid foundation for modeling, and we've seen how there is a notable amount we can do with a conceptually simple approach. We've also seen how we can extend the linear model to handle different types of target distributions to help us understand and make some inferences about the relationships between our features and target.

In this chapter, we want to show you how to extend the linear model even further with still common tools. These particular methods are also good examples of how we can think about our data and approach in different ways, and can serve as a good starting point for even more techiques you may want to explore in the future. A thread that binds these techniques together is the ability to use a linear model to explore nonlinear relationships!

## Key Ideas {#sec-lm-extend-key-ideas}

- The linear and generalized linear models are great and powerful starting points for modeling, but there's even more we can do!
- **Linear models can be used to model nonlinear feature-target relationships**
- Various technqiues are availabel allow us to model relationships that are not linear or monotonic, and can help us to better understand our data, even while still being linear models.
- While these seem like very different approaches, we can still use our linear model concepts and approach at the core, take similar estimation steps, and even have similar, albeit more, interpretation.


### Why this matters? {#sec-lm-extend-why}

The linear model is a great starting point for modeling. It is a simple approach that can be used to model a wide variety of relationships between features and targets, and it's also a great way to get a feel for how to think about modeling. But linear and generalized models are just the starting point, and the models depicted here are very common extensions used in a variety of disciplines and industries. More generally, the following techniques allow for nonlinear realationships will still employing a linear model approach. This is a very powerful combination, and it's good to be aware of these tools.


### Good to know {#sec-lm-extend-good}

While these models are extensions of the linear model, they are not necessarily more complex, but it can take a bit more effort to interpret. You likely want to be fairly comfortable with standard linear models at least before you start to explore these extensions.



TODO: This can be a chapter with a general focus on nonlinearities: intearctions in general and mixed model, gam (effects vary with self or other), quantile (effects vary with target). Also remind GLM as introducing nonlinearity.

## Interactions {#sec-lm-interactions}

Things can be quite complex in a typical model with multiple features, but just adding features may not be enough to capture the complexity of the relationships between features and target. Sometimes, we need to consider how features interact with each other to better understand the relationships between features and target. A common way to add complexity in linear models is through **interactions**. This is where we allow the effect of a feature to vary depending on the values of another feature, or even itself! 

As a conceptual example, we might expect that the effect of the number of children in the home on a movie's rating is different for movies from different genres (much higher for kids movies, maybe lower for horror movies), or that genre and season work together in some way to affect rating (e.g. action movies get higher ratings in summer).  We might also consider that the length of a movie might plateau or even have a negative effect on rating after a certain point, i.e., it would have a **curvilinear** effect. All of these are types of interactions we can explore. Interactions allow us to incorporate nonlinear relationships into the model, and so greatly extend the linear model's capabilities - we basically get to use a linear model in a nonlinear way!  

With that in mind, let's explore how we can add interactions to our models. Going with our first example, let's see how having kids impacts the relationship between genre and rating. We'll start with a standard linear model, and then add an interaction term. Using a formula approach makes it very straightforward to add an interaction term. We just need to add a `:` between the two features we want to interact.

:::{.panel-tabset}

##### R

```{r}
#| label: model-interaction-r
df_reviews = read_csv("data/movie_reviews_processed.csv")

model_base = lm(rating ~ children_in_home + genre, data = df_reviews)
model_interaction = lm(rating ~ children_in_home * genre, data = df_reviews)

# summary(model_interaction)
```

##### Python

```{python}
#| echo: true
#| label: model-interaction-py
#| eval: false

import pandas as pd
import statsmodels.formula.api as smf

df_reviews = pd.read_csv("data/movie_reviews_processed.csv")

model_base = smf.ols(
  formula = 'rating ~ children_in_home + genre', 
  data = df_reviews
).fit()

model_interaction = smf.ols(
  formula = 'rating ~ children_in_home * genre', 
  data = df_reviews
).fit()

model_interaction.summary()
```

:::


```{r}
#| echo: false
#| label: model-interaction-get-reference-group
ref = model_base$xlevels$genre[1]
```

Here is a quick look at the model output for the interaction vs. no interaction interaction model. Starting with the base model, the coefficients look like what we've seen before, but we have several coefficients for genre. The reason is that genre is composed of several categories, and converted to a set of dummy variables (refer to @sec-lm-categorical-features and @sec-data-cat). In the base model, the intercept tells us what the mean is for the reference group, in this case `r ref`, and the genre coefficients tell us the difference between the mean for that genre and the reference. For example, the mean rating for `r ref` is `r round(coef(model_base)['(Intercept)'], 2)`, and the difference between that genre rating for the drama genre is `r round(coef(model_base)['genreDrama'], 2)`. Adding the two gives us the mean for drama movies `r round(coef(model_base)['(Intercept)'], 2)` + `r round(coef(model_base)['genreDrama'], 2)` = `r round(coef(model_base)['(Intercept)'] + coef(model_base)['genreDrama'], 2)`. We also have the coefficient for the numbre of children in the home, and this does not vary by genre.

```{r}
#| echo: false
#| label: tbl-model-interaction-output
#| tbl-cap: Model coefficients with interaction

library(broom)
library(gt)

tidy_base = tidy(model_base) |> 
  rename(
    feature = term,
    coef = estimate,
    se = std.error,
    t = statistic,
    p = p.value
  )

tidy_interaction = tidy(model_interaction) |> 
  rename(
    feature = term,
    coef = estimate,
    se = std.error,
    t = statistic,
    p = p.value
  )

nkids_coef = round(coef(model_interaction)['children_in_home'], 2)
sign_nkids_coef = sign(nkids_coef)
nkids_comedy_coef = round(coef(model_interaction)['children_in_home:genreComedy'], 2)
nkids_kids_coef = round(coef(model_interaction)['children_in_home:genreKids'], 2)
sign_nkids_kids_coef = sign(nkids_kids_coef)
# mean(df_reviews$children_in_home == 0)
# mean(df_reviews$genre == ref)

# Create a gt table from the tidy coefficients
# COLOR CODING WON'T WORK IN LATEX BECAUSE IT SUCKS
tidy_base |> 
  full_join(tidy_interaction, by = 'feature', suffix = c('_base', '_inter')) |>
  select(feature, starts_with('coef')) |>
  gt(decimals = 4) |> 
  sub_missing(
    missing_text = "",
  ) |> 
  # gt_highlight_rows(
  #   columns = vars(coef_base, coef_inter),
  #   rows = str_detect(feature, 'children_in_home$|children.*Kids'),
  #   fill = okabe_ito[5],
  #   alpha = .1
  # ) |>
  tab_caption("Model coefficients with and without an interaction") 
```

But we have an interaction in our other model, and an interaction basically tells us that the effects of feature A change depending on the values of feature B and vice versa. In this setting, feature A can be children in the home or genre, and B can be genre or children in the home.  So let's start with the coefficient for children in the home. It is `r nkids_coef`, which means that for every additional child in the home, the rating `r ifelse(sign_nkids_coef, 'increases', 'decreases')` by that amount. But! Due to our interaction, we now interpret that as just the effect of children in the home when genre is the reference group `r ref`. Now let's look at the interaction effect for children in home and the kids genre. It is `r nkids_kids_coef`, which means that for the kids genre, the *effect of having children in the home `r ifelse(sign_nkids_kids_coef, 'increases', 'decreases')`* by that amount. So our actual effect for an additional child in the home for the kids genre is  `r nkids_coef` + `r nkids_kids_coef` = `r round(nkids_coef + nkids_kids_coef, 2)` `r ifelse(sign_nkids_coef + sign_nkids_kids_coef, 'increase', 'decrease')` in the review rating.  It is also correct to say that the difference in rating between the kids genre and the reference group `r ref` is `r nkids_kids_coef`, but, when with an increase in children in the home, the difference in rating between the kids genre and the reference group `r ref` `r ifelse(sign_nkids_kids_coef, 'increases', 'decreases')` by `r abs(nkids_kids_coef)`. In other words, it is a *difference in differences*[^diffindiff].

[^diffindiff]: Some models that employ an interaction that investigates categorical group differences like this actually call their model a difference-in-difference model.

When we talk about differences in coefficients, across values of features, it can get a little bit hard to follow. In every case that you employ an interaction, you should look at the interaction visually. Here is a plot of the predictions from the interaction model. We hightlight the predictions for the kids genre, and we can see that the effect of children in the home is strongest for kids movies than for other genres, which makes a whole lot of sense! In other genres, the effect of having children seems to have little effect, and in others it still has a positive effect, but not as strong as for kids movies.



```{r}
#| echo: false
#| label: fig-model-interaction-plot
#| fig-cap: Interaction plot

library(ggeffects)

p_init = plot(
  ggpredict(model_interaction, terms = c("children_in_home", "genre")), 
  alpha = .05, 
  # line_size = I(rep(c(rep(.10, 7), 2), 4)) # values of children in home = 0:3, kids is last genre, but it's ignored anyway
  line_size = 1
  )$data |>
  as_tibble()

p_init |> 
  ggplot(aes(x = x, y = predicted, color = group, fill = group)) +
  geom_line(linewidth = .5) +
  # geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1, color = NA) +
  geom_borderline(linewidth = 2.5, data = p_init |> filter(group == 'Kids') ) +
  # scale_color_manual(values = okabe_ito, aesthetics = c('color', 'fill')) +
  scale_color_brewer(palette = 'Set1', aesthetics = c('color', 'fill')) +
  scale_y_continuous(breaks = seq(0, 5, .25)) +
  labs(x = "Children in home", y = "Predicted rating") +
  theme_clean() +
  theme(
    panel.grid.major = element_line(color = alpha('black', .05)),    
  )

# plot(modelbased::estimate_expectation(model_interaction), si_alpha = .01)

```


So we can see that interactions can allow a linear effect to vary depending on the values of another feature. But the real take home message from this is that *the general effect is actually not linear*! The effect changes depending on the setting. Furthermore, the coefficient for children in the home is only the effect of children in the home when genre is the reference group, or more generally, when other features are at their reference group or zero if they are numeric. Whenever you have interactions, you really can't talk about a singular effect of a feature, but rather the effect of a feature at a particular setting of the other features. Some think this is a drawback, but it's actually the reality of most feature-target relationships. Interactions allow us to model more complex relationships between features and target, and they are very common in practice.


### Average Effects {#sec-lm-extend-avgeffect}

So what is the effect of children in the home? Or genre, for that matter? We can't really say, because the effect of one feature depends on the setting of the other feature. We can say what the effect of a feature is *on average* across the settings of the other features. This is called the **average marginal effect**[^ME]. We can compute this by averaging the effect of a feature across the values of the other features. 

[^ME]: These results are provided by the [marginaleffects]{.pack} package, which is great for this and has pretty much no equal in the Python realm.

```{r}
#| echo: false
#| label: tbl-model-interaction-avg-effect
#| tbl-cap: Average Marginal Effects of Children in the Home
library(marginaleffects)
me_children = marginaleffects::avg_slopes(model_interaction, variables = c("children_in_home"))
me_children |> 
  as_tibble() |> 
  select(-s.value) |> 
  gt() |> 
  tab_header(
    title = "", 
    # subtitle = "Average coefficient for children in home averaged over genre"
  ) |> 
  fmt_number(
    columns = vars(estimate), 
    decimals = 3
  )
```


So-called **marginal effects** and related approaches such as SHAP values (see @sec-model-explore-shap-values) attempt to boil down the effect of a feature to a single number, but this is difficult even in the simpler GLM settings, and downright misleading in more complex settings like our interaction model. Here we see the average coefficient for children in the home is `r round(me_children$estimate, 2)`, but we saw in @tbl-model-interaction-output that this is slightly larger than what we would estimate in the non-interaction model, and we saw in @fig-model-interaction-plot it's actually near zero (flat) for most genres. So what is the average effect really telling us? Consider a more serious case of drug effects across demographic groups, where the effect of the drug is much stronger for some groups than others. Would you want your doctor to prescribe you a drug based on the average effect across all groups or the specific group to which you belong?

In the end, when it comes to interactions, it's better to think about the effect of a feature in terms of the setting of the other features it interacts with. It's even better to visualize the effect of a feature across a range of settings of the other features to get the best understanding of how the relationship changes. It's also good to think about what the actual prediction for your outcome is at key values of the features, and how that changes depending on what the feature values are. This is what we've done here with interactions, and it's a good approach to take in general.


### ANOVA

A common method for summarizing categorical effects in linear models is through **analysis of variance** or ANOVA. ANOVA breaks down the variance in a target attributable to different features or their related effects such as interactions. It's a bit beyond the scope here to get into all the details, but both base R and [statsmodels]{.pack} have functions for this as demonstrated here.

:::{.panel-tabset}

##### R


```{r}
#| label: anova-r
#| eval: false

anova(model_base)
```

##### Python

```{python}
#| label: anova-py
#| eval: false
import statsmodels.api as sm

smf.stats.anova_lm(model_base)
```

:::

 In this case, it doesn't appear that the interaction effect is statistically significant if we use the typical .05 cut-off.

```{r}
#| echo: false
#| label: tbl-anova-r
#| tbl-cap: ANOVA table for interaction model
anova(model_interaction) |> 
  tidy() |> 
  rename(
    feature = term,
    df = df,
    sum_sq = sumsq,
    mean_sq = meansq,
    f = statistic,
    p = p.value
  ) |>
  gt() |> 
  sub_missing(
    missing_text = "",
  )
```

The ANOVA approach can be generalized to provide a statistical test to compare models. For example, we can compare the base model to the interaction model to see if the interaction model is a better fit. However, it's entirely consistent with just looking at the interaction result in the ANOVA for the interaction model, so doesn't provide additional information, and the only models that can be compared in a meaningful way must be nested in this way, i.e., one model is a subset of the other.


It's perhaps worth noting that ANOVA is often confused with being a model itself. When people use it as such, it is just a linear regression with only categorical features, something that can typically only happen within strict experimental designs that ignore interactions with continuous features. It's pretty difficult to think of a linear regression setting where no continuous features would be of interest, but back when people were doing this stuff by hand, they just categorized everything to enable this approach. It's a bit of a historical artifact, but still might be useful for exploratory purposes. Beyond that, ANOVA can be used to compare models more generally, but other approaches are a little more general or not confined to nested models- ones that can be seen as subsets of another.



## Mixed Models {#sec-mixed-models}

<!-- ### Why Should You Care? {#sec-mixed-models-why}

Structures within your data are important and paying attention to how data might be grouped in some way will let you generate more insights about how observations within and between groups might behave. For example, people working in the same organization, or students in the same school, are likely to have a more similar experiences than those from different organizations or schools. This shared connection within groups may ultimately affect what we see in our chosen outcomes. Mixed models will let us handle grouped data such as this, all while getting the benefits of a standard linear model. -->

### Knowing Your Data {#sec-mixed-models-knowing}

As much fun as modeling is, knowing your data is far more important. You can throw any model you want at your data, from simple to fancy, but you can count on disappointment if you don't fundamentally know the structure that lies within your data. Let's take a look at the following visualizations. In @fig-length-release-rating, we see a positive relationship between the length of the movie and ratings. 

```{r}
#| echo: false
#| label: fig-length-release-rating
#| fig-cap: Linear relationship between length of movied and rating.
ggplot(df_reviews, aes(length_minutes, rating)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  theme_clean()
```

We could probably just stop there, but we might be ignoring something substantial within our data: genre. We might want to ask a question, "Does this relationship work the same way across the different genres?"

```{r}
#| echo: false
#| label: fig-length-genre-rating
#| fig-cap: Genre Effects on Length and Rating
#| out-width: 100%

# p_mod = lm(rating ~ length_minutes*genre, 
#           df_reviews |> mutate(genre = recode(genre, "Action/Adventure" = "Act/Adv")))

# plot(modelbased::estimate_expectation(p_mod, trend = 'length_minutes', at = 'genre')) +
#   scale_x_discrete(guide = guide_axis(n.dodge=2)) +
#   labs(title = '', y = '') +
#   theme_clean()

mods = df_reviews |> 
  nest_by(genre) |>
  mutate(model = list(lm(rating ~ length_minutes, data = data))) |>
  summarize(broom::augment(model))

p_slopes = ggplot(mods, aes(length_minutes, .fitted, color = genre)) +
  geom_borderline(lwd = 1.5) +
  scale_y_continuous(limits = c(1, 5)) +
  labs(title = '', y = '', x = '') 

p_mod = lm(rating ~ genre, df_reviews |> mutate(genre = recode(genre, "Action/Adventure" = "Act/Adv")))

p_dat = plot(modelbased::estimate_means(p_mod))$data

p_group_means = p_dat |> 
  ggplot(aes(x = genre, y = Mean)) +
  geom_dotplot(
    aes(x = genre, y= rating),
    binaxis='y', 
    stackdir='center',
    dotsize=.1,
    color = 'black',
    data = df_reviews |> 
    mutate(genre = recode(genre, "Action/Adventure" = "Act/Adv")), 
    position = position_jitter(width = .0, height = .15),
    alpha = .1
  ) +
  geom_pointrange(
    aes(ymin = CI_low, ymax = CI_high, color = genre),
    fatten = 5,
    alpha = 1,
    show.legend = FALSE
  ) +
  scale_x_discrete(guide = guide_axis(n.dodge=2)) +
  labs(title = '', y = '', x = '') +
  scale_y_continuous(limits = c(1, 5))
  


see::plots(
  p_slopes,
  p_group_means
) + 
  plot_layout(guides = 'collect') &
  scale_color_brewer(palette = 'Set1') &
  theme(panel.grid.major.y = element_line(color = scales::alpha('black', .1)))


# Calculate ICC for each feature
# feature_icc = purrr::map_df(stringr::str_subset(colnames(df_reviews), '_0|_sc'), function(feature) {
#   model = lme4::lmer(paste("rating ~", feature, "+ (1 +", feature, "| genre)", sep = ""), data = df_reviews)
#   as_tibble(performance::icc(model))
# }, .id = 'feature')

# feature_icc |> 
#   mutate(feature = stringr::str_subset(colnames(df_reviews), '_0|_sc')) |> 
#   arrange(ICC_adjusted)
```

A very quick examination of @fig-length-genre-rating might suggest that the rating varies by genre, and that the relationship between length and rating varies significantly over the different genres. The group means in the right panel show variability across genre. In addtion, on the left panel, some genres show a strong positive relationship, some show less of a positive relationship, a couple even show a negative relationship, and one even looks flat. We can also see that they would have different intercepts. This is a very important thing to know about your data! If we had just run a model with length as a feature and nothing else, we would have missed this important information.

<!-- 
Just for fun, try to group by genre and summarize it by the mean rating. You'll find that the averages are very different across the genre! Then, subtract the overal mean rating from those values, so that they represent deviations from the mean. Keep them handy! -->

### Overview of Mixed Models {#sec-mixed-models-overview}

Clearly genre is offering some type of additional information to the model, but how can we incorporate that into our model? An interaction might come to mind at first, and that's the right way to think about it! A **mixed model** can be used to get at that type of relationship into our model, which we can think of as a group interaction, without much hassle and additional explanability.

Before going too much further, the term *mixed model* is as vanilla as we can possibly make it, but you might have heard of different flavors of them before. You might have heard of *hierarchical linear models*, or *multilevel models*, or maybe *mixed-effects models* tossed around before. Maybe you've even been exposed to ideas like *random effects* or *random slopes*. These are in fact all instances of what we're calling a mixed model. 

What makes a model a *mixed* model? The mixed model is characterized by the idea that a model can have **fixed effects** and **random effects**. Fortunately, you've already encountered *fixed* effects -- those are the features that we have been using in all of our models so far! We are assuming a single true parameter (coefficient/weight) for each of those features to estimate, and that parameter is *fixed*.

In the mixed model context, the *random effect* typically comes from some type of specific distribution, almost always a normal distribution, that contributes uniquely to the variance in the outcome. This distribution of effects can be characterized from something like a grouping variable (such as genre), such that we let those parameters, i.e. coefficients (or weights), vary across the groups, creating the observed distribution of values. 



<!-- A classic example of such grouping structure is students in a classroom. Those groups can also be nested within larger groups -- students nested within classrooms, nested within schools, nested within districts. But the grouping structure can also capture the notion of a repeated measure for an individual, like repeated test scores.  -->

<!-- While we aren't rejecting the idea of a mean with these mixed-models, we are implying that each group (whether it is a group, nested group, or repeated observations from a person) has its own unique mean that can be useful for modeling the target.  -->

Formally, we might specify something like this:

$$
\text{rating} = b_{\text{0[genre]}} + b_\text{length}*\text{length}
$$

We are explicitly saying that genre has its own unique effect for this model in the form of specific intercepts for each genre. This means that whenever an observation belongs to a specific genre, it will have an intercept reflect that genre, and that means that two observations with the same length but from different genres would have different predictions.


We also posit that those come from a *random* distribution. We can specify that as:

$$b_{\text{0[genre]}} \sim \text{N}(b_\text{intercept}, \sigma_\text{int\_genre})$$

This means that the random intercepts will be normally distributed and the overall intercept is just the mean of those random intercepts, and with its own variance, an extra parameter we'll eventually have to estimate as part of the model. Another very common depiction is:

$$\text{re}_{[\text{int\_genre}]} \sim \text{N}(0, \sigma_\text{int\_genre})$$

$$b_{\text{0[genre]}} = b_\text{intercept} +\text{re}_{[\text{int\_genre}]}$$

The same approach would apply with a random slope, where we would have a random slope for each group, and that random slope would be normally distributed with its own variance.

$$b_{\text{length[genre]}} \sim \text{N}(b_\text{length}, \sigma_\text{length\_genre})$$



<!-- TODO: Not sure need another example?

Before we go to modeling our reviews, let's consider an example training program to increase vertical jump, with average vertical increases of 2 inches. That really doesn't sound all that impressive; however, that increase came across 5 distinct groups: NBA players, NFL players, NHL players, MLB players, and data analysts. We can be pretty certain that each group has a very different vertical jump distribution coming into this training program. Given the amount of jumping that NBA players do, this program is unlikely to produce dramatic increases in vertical jump for them. We would probably expect modest gains in the NFL and even greater gains within NHL and MLB players. Where we are going to see the best gains, though, is from data analysts -- they might want to jump to conclusions, but don't need to jump over their computers very often. Now that we know the additional information, can we just look at the average increase and be satisfied? Probably not. Instead, we need to look at the group that individuals might be in and judge accordingly. A mixed-model is going to let us to model all of this without too much work. -->


### Using a Mixed Model  {#sec-mixed-models-using}

To use mixed models, at a minimum we have to specify a group effect in some way, but that's the primary difference from our approaches used for linear or generalized linear models previously. We can specify a random effect in a few different ways, but we'll start with the simplest way, which is to just add a random effect to the model.


:::{.panel-tabset}

##### R

We'll use the [lme4]{.pack} package in R which is the most widely used package for mixed models. 

```{r}
#| eval: false
#| label: r_mixed_model

library(lme4)

# random intercepts are specified by a 1
fit_ran_int = lmer(
  rating ~ length_minutes_sc + (1 | genre), 
  df_reviews
)

fit_ran_slope = lmer(
  rating ~ length_minutes_sc + (1 + length_minutes_sc | genre), 
  df_reviews
)

summary(fit_ran_int)
summary(fit_ran_slope)
```

##### Python

As with our recommendation with GAMs later, you really should just use R for mixed models. The functionality is overwhelmingly better there. However, you can use [statsmodels]{.pack} in Python to fit mixed models[^mixedpy]. But as an example, this doesn't even converge with default settings even after scaling the data, so we had to switch the optimization method. These results correspond with the R results.

[^mixedpy]: One of your authors worked for several years with the key developer of the mixed models functionality in [statsmodels]{.pack}. As such, we can say there is zero doubt about the expertise going into its development, as there are few in the world with such knowledge. Even so, the functionality is not as mature or as expansive as what you get in R.


```{python}
#| eval: false
#| label: python_mixed_model
import statsmodels.api as sm

fit_ran_int = sm.MixedLM.from_formula(
  "rating ~ length_minutes_sc", 
  df_reviews, 
  re_formula= '1',
  groups=df_reviews["genre"]
)

fit_ran_slope = sm.MixedLM.from_formula(
  "rating ~ length_minutes_sc ", 
  df_reviews, 
  re_formula= 'length_minutes_sc',
  groups=df_reviews["genre"]  
)

fit_ran_int = fit_ran_int.fit()
fit_ran_slope = fit_ran_slope.fit(maxiter=1000)

fit_ran_int.summary()
fit_ran_slope.summary()
```

:::

With @tbl-mixed-model we can see some typical output from a mixed model. The fixed effect part is your basic GLM result and interpreted as such. Nothing new there, and we can see a general positive relationship between length and rating, but maybe not a strong one. But the random effects are where the action is! We can see the standard devation (or variance) of the random effects, i.e., the intercepts and slopes. We can also see the standard deviation of the residual, which conceptually identical to your standard regression model's residual standard deviation, but won't be the same value. We can also see the correlation between the random intercepts and random slopes. Depending on your tool, the default may be in terms of variances and covariances rather than standard deviations and correlations, but you would not see anything fundamentally different.

```{r}
#| echo: false
#| label: tbl-mixed-model
#| tbl-cap: Mixed model results

library(lme4)

# random intercepts are specified by a 1
fit_ran_int = lmer(
  rating ~ length_minutes_sc + (1 | genre), 
  df_reviews
)

fit_ran_slope = lmer(
  rating ~ length_minutes_sc + (1 + length_minutes_sc | genre), 
  df_reviews
)

vc = fit_ran_slope |> mixedup::extract_vc(ci_level = 0)
vcor = fit_ran_slope |> mixedup::extract_vc(ci_level = 0, show_cor = TRUE)
vcor = vcor$Cor$genre
cor_sign = ifelse(sign(vcor[2]) > 0, 'positive', 'negative')


# mixedup::summarise_model(fit_ran_slope) 
# library(flextable)

# NOTE: flextable continues to have issues with pdf; I've seen the same undefined error creep up repeatedly on the issues, and the attempts to fix seem to be in vain. Unfortunately, it's also the better format for mixed and GAM models. Maybe revisit or hope gtsummary eventually does better.
# flextable::as_flextable(fit_ran_slope) |> 
#   set_header_labels(
#     estimate = 'Estimate',
#     std.error = 'Std. Err.',
#     statistic = 't value',
#     p.value = 'Pr(>|t|)'
#   ) 
# broom.mixed::tidy(fit_ran_slope)
# gtsummary::tbl_regression(fit_ran_slope, tidy_fun = broom.mixed::tidy) # doesn't include intercept and default is 'include everything'.
broom.mixed::tidy(fit_ran_slope) |>
    mutate(
      effect = ifelse(str_detect(effect, 'ran_pars'), 'Random', 'Fixed'),
      term = ifelse(str_detect(term, 'Observation'), '', term),
      term = str_remove_all(term, '\\(|\\)'),
    ) |>
    group_by(effect) |>
    gt() |> 
    sub_missing(
      missing_text = "",
    )

```

In this case, we can see notable variability attributable to the random effects. How do we know? Well, if if our rating is on a 1-5 scale, and we naturally have a standard deviation of `r round(sd(df_reviews$rating), 2)` for rating before accounting for anything else, we mights surmise that having an effect of that size for just genre (roughly `r round(vc$sd[1], 2)`)  is a relatively notable amount. We can also see that the correlation between the random intercepts and random slopes is  `r cor_sign`, which means that the groups with higher intercepts have `r ifelse(cor_sign == 'positive', 'more positive', 'more negative')` slopes. Now let's look at the estimates for the random effects for the model with both intercepts and slopes[^mixedup].

[^mixedup]: One of your authors provides a package for mixed models in R called [mixedup]{.pack}. It provides a nice way to extract random effects and summarize such models ([link](https://github.com/m-clark/mixedup)).


:::{.panel-tabset}

##### R

```{r}
#| label: r_mixed_model_random_effects

ranef = ranef(fit_ran_slope)
# mixedup::extract_random_effects(fit_ran_slope) # prettier version
```

##### Python

```{python}
#| eval: false
#| label: python_mixed_model_random_effects

ranef = pd.DataFrame(fit_ran_slope.random_effects).T
```

:::

```{r}
#| echo: false
#| label: fig-random-effects
#| fig-cap: Random effects for mixed model


x = visibly::plot_coefficients(fit_ran_slope, ranef = TRUE, which_ranef = 'genre') 

(
  x[[1]] + labs(y = 'Intercept') + 
  x[[2]] + labs(y = 'Length')
) &
  scale_x_discrete(guide = guide_axis(n.dodge=2)) &
  theme_clean() 
```

How do we interpret these *deviations*? For the intercept plot, we see that Kids, Sci-Fi, and Action/Adventure have a lower default value for rating, while Drama and Comedy start off relatively higher. These values reflect what's happening when the length is zero, which, since it's standardized, means that we're talking about what's happening for an average length movie.

Comedy's estimated trend over length suggests that it has a smaller slope, and relative to the global slope, this means a negative relationship for Comedy movies, i.e. longer is not better! Longer romance movies, on the other hand, have an even larger positive coefficient, and so seem to do better seem to do better than short ones- maybe all the short ones are mostly awful Rom-Coms!

:::{.callout-tip title='Always scale features for mixed models'}

Your authors have run a lot of these models. Save yourself some trouble and standardize or otherwise scale your features before fitting the model. Just trust us, or at least, don't be surprised when your model doesn't converge.

:::


### Mixed Model Summary {#sec-mixed-models-summary}

Even with just one feature, we certainly had a lot to talk about! This is just a glimpse of what mixed models have to offer, and the approach can be even richer than what we've just seen.  But you might be asking- Why don't I just put genre into the model like other categorical features? In the case of genre, that's okay, but doing even just that would add several coefficients to the model before counting any interactions. Now consider thousands of United States county voting percentages for elections over time- would you just put 3000+ county indicator variables into the model as is? You can try, but you'll likely run into estimation problems for typical GLM settings. In addition, as we saw, mixed models can correlate the random effects, which can be very useful for understanding the relationships between the groups. Furthermore, mixed models estimate the correlation of the observations within groups. Default mixed models assume this correlation is constant, but this can be modified to allow for different correlation structures. For example, in a longitudinal study you might want to assume that the correlation between observations within a group decreases as the time between observations increases. This is a very common approach for longitudinal data, where the correlation between observations decreases as the time between observations increases.

In general mixed models provide several advantages for the data scientist:

- Any coefficient can be allowed to vary by groups, including other random effects. It actually is just an interaction in the end as far as the linear predictor is concerned.
- The group-specific effects are *penalized*, which shrinks them toward the overall mean, and makes this a different approach from just adding a 'mere interaction'. This helps to avoid overfitting, and that penalty is related to the variance estimate of the random effect. In other words, you can think of it as running a penalized linear model where the penalty is applied to the group-specific effects.
- Also unlike standard interaction approaches, we can estimate the covariance of the random effects, which can be useful for understanding the relationships between the groups. We can specify different covariance structures for observations within groups.
- Standard modeling approaches actually only estimate the variance part of the random effects, and get the estimated group-specific effects via a predictive method as part of model post-processing. This allows only the variances and covariances of the random effects to require estimation, rather than a weight or coefficient for every group.
- The group effects are like a very simplified **embedding**, where we have taken a cateogrical feature and turned it into a numeric one, like those shown in @fig-random-effects. This may help you understand other embedding techniques that are used in other places like deep learning if you think of this as the simplest embedding approach.
- When you start to think aobut random effects and/or distributions for effects, you're already thinking like a Bayesian, who is always thinking about the distributions for various effects. Mixed models are a perfect segue from standard linear model estimation to Bayesian estimation, where everything is random.
- The random effect is akin to a latent variable of 'unspecified group causes'. This is a very powerful idea that can be used in many different ways, but importantly, you might want to start thinking about how you can figure out what those 'unspecified' causes may be!
- Group effects will almost always improve your model's performance relative to not having them, especially if you weren't including those groups in your model because of how many there were.


In short, mixed models are a fun way to incorporate additional interpretive color to your model, while also getting several additional benefits to help you understand your data!


<!-- 
### Using a Mixed Model  {#sec-mixed-models-standard}

Now let's fit some mixed models with `lme4` or `statsmodels`. We will also create `null models` (i.e., models with an intercept only), since we will need some infomration from them later.



:::{.panel-tabset}

##### R
```{r}
library(lme4)

fit_mer = lmer(rating ~ total_reviews_sc + (1 | genre), 
               df_reviews, 
               REML = FALSE)

summary(fit_mer)

null_model = lmer(rating ~ 1 + (1 | genre), 
                   df_reviews, 
                   REML = FALSE)

summary(null_model, correlation = FALSE)
```

##### Python

```{python}
import statsmodels.api as sm

fit_mer = sm.MixedLM.from_formula("rating ~ total_reviews_sc", df_reviews, 
                                  groups=df_reviews["genre"])

fit_mer = fit_mer.fit()

fit_mer.summary()

null_model = sm.MixedLM.from_formula("rating ~ 1", df_reviews, 
                                     groups=df_reviews["genre"])
```

:::



What can we take from these summaries, and what are we getting beyond the linear model? 

For starters, we should notice a change in our standard errors -- by integrating information about the groups, we are getting a better sense of how much uncertainty our model contains at the global average level.

We also see some additional information for our random effects. The standard deviation is telling us how much the rating moves around based upon genre after getting the information from our fixed effects (i.e., the rating can move around nearly .3 points from genre alone). 

We can use information for the random effects components to get some additional information. For instance, we can calculate the proportion of variance in scores that is accounted for by the genre alone:

$$\frac{\text{intercept variance}}{\text{intercept variance} + \text{residual variance}}$$

With our values, we have:

$$
.08309 / (.08309 + .25482) = 0.2458939
$$

This is what is known as the intraclass correlation (ICC). It ranges from 0 (no variance between clusters) to 1 (variance between clusters). With an ICC of .25, we can say that 25% of the variance in scores is accounted for by the genre alone.


TODO: WEED PULLING

We can also use various bits of information in our output to create different *R^2* values. 

For the first level of the model, we can calculate the $R^2$ as:

$$R^2_1 - \frac{\sigma^2_{M1} + \tau^2_{M1}}{\sigma^2_{M0} + \tau^2_{M0}}$$

We can pull that information from our two model outputs:

```{r}
m1Sigma = .08309 # full model random effect intercept

m1Tau = .25482 # full model random effect residual

m0Sigma = .07557 # null model random effect intercept

m0Tau = .31371 # null model random effect residual

1 - ((m1Sigma + m1Tau) / (m0Sigma + m0Tau))
```

The fixed effect of number of reviews accounts for nearly 13% of the variation within the reviews.

The second level can be calculated as:

$$R^2_2 = 1 - \frac{\sigma^2_{M1} / B + \tau^2_{M1}}{\sigma^2_{M0} / B + \tau^2_{M0}}$$

We see that we added a *B* into the mix here and it is just the average size of the level 2 units (1000 observations / 8 genres). 

```{r}
level2Mean = 1000 / 8

r2Numerator = m1Sigma / level2Mean + m1Tau

r2Denominator = m0Sigma / level2Mean + m0Tau

1 - (r2Numerator / r2Denominator)
```

Which gives us .18 or 18% of the variation in scores is accounted for by the genre alone.

You can also get these values in R like this:

```{r}
performance::r2(fit_mer)
MuMIn::r.squaredGLMM(fit_mer)
```

Here we have two values: the marginal R2 (R2m) and the conditional R2 (R2c). You can think of the marginal values as the standard type of R2 -- it is the variability explained by the fixed effects part of the model (it is what we have already done above). The conditional R2 is using both fixed and random effects, so you can think of it as the toal variability explained. Clearly, we can subtract the two to get a better understanding of the effect of the random effects. With $.362 - .154 = .208$, we can say that the random effects account for 20.8% of the variation in ratings. 

Notice that those values are a bit different than what we produced by hand. Packages are now using a revised method proposed by Nakagawa, Johnson, and Schielzeth (2017) that is a bit more accurate than the previous method.

We can also get a good sense of the random effect estimates:

```{r}
#| echo: false
#| label: fig-random-effects-2
#| fig-cap: Random effects estimates
library(sjPlot)

plot_model(fit_mer, type = "re") + 
  theme_clean()
```

For @fig-random_effects, the easiest way to think about it is that the values are effects for each individual random effect (i.e., each genre's random intercept). Since we are just dealing with intercepts right now, they are the deviations from the fixed intercept. The intercept for the model is ~2.6 and the random effect for `comedy` is .48. If we wanted to predict scores for a comedy movie, we would take 2.6 + .48 for the intercept portion of the model (the same would go for any random slopes in the model). In the end, it is showing how much the intercept shifts from genre to genre, and some genres have a positive effect beyond the average and others have a negative effect (`sci-fi`, for instance, is well below the global average).

Remember your group-by and summarize task earlier? Each point is the difference between a genre's average and the overall average -- values in blue are higher than the average and values in red are lower than the average. With that, the average rating for `comedy` is much better than the global average rating, while `sci-fi` is much worse than the global average rating.

### Model Matrix

Let's start our homebrewing adventures by creating a model matrix. We are going to use the `model.matrix()` function to create our model matrix. We are going to use the `user_age` variable as our fixed effect and `genre` as our random effect. We are going to use the `factor()` function to make sure that `genre` is treated as a categorical variable. We are also going to use the `-1` to remove the intercept from the model matrix. 

:::{.panel-tabset}

##### R

```{r}
#| echo: true
#| label: mixed_model_prep

X = model.matrix(~total_reviews_sc, df_reviews)
Z = model.matrix(~factor(df_reviews$genre) - 1)

colnames(Z) = paste0("released_", sort(unique(df_reviews$genre)))

y = df_reviews$rating
```

##### Python

```{python}
#| echo: true
#| label: py_mixed_model_prep

X = df_reviews[['total_reviews_sc']]
X = sm.add_constant(X)
X = X.to_numpy()

Z = pd.get_dummies(df_reviews['genre'], drop_first=True)
Z = Z.to_numpy()

y = df_reviews['rating']
y = y.to_numpy()
```

:::

### Likelihood Function

:::{.panel-tabset}

##### R

```{r, r_data_read}
#| echo: true
#| label: mixed_model_ll

mixed_log_likelihood = function(y, X, Z, theta) {
  tau   = exp(theta[1])
  sigma = exp(theta[2])
  n = length(y)
  
  # evaluate covariance matrix for y
  e  = tcrossprod(Z)*tau^2 + diag(n)*sigma^2
  b  = coef(lm.fit(X, y))
  mu = X %*% b

  ll = mvtnorm::dmvnorm(y, mu, e, log = TRUE)
  -ll
}
```

##### Python

```{python}
#| echo: true
#| label: py_mixed_model_ll
from scipy import stats

def mixed_log_likelihood(theta, y, X, Z):
    tau = np.exp(theta[0])
    sigma = np.exp(theta[1])
    n = len(y)
    
    e = (Z.dot(Z.T) * tau**2) + (np.eye(n) * sigma**2)
    b = np.linalg.lstsq(X, y, rcond=None)[0]
    mu = X.dot(b) 
    
    ll = stats.multivariate_normal.logpdf(y, mu, e)
    return -ll
```

:::

### Model Fitting

:::{.panel-tabset}

##### R

```{r}
#| label: r-mixed_model_fit
param_init = c(0, 0)

names(param_init) = c('tau', 'sigma')

fit = optim(
  fn  = mixed_log_likelihood,
  X   = X,
  y   = y,
  Z   = Z,
  par = param_init,
  # control = list(reltol = 1e-10)
)

exp(fit$par) # compare to sd of random effects
```

##### Python

```{python}
#| eval: false
#| label: mixed_model_fit

from scipy import optimize as opt
def mixed_log_likelihood(theta, y, X, Z):
    tau = np.exp(theta[0])
    sigma = np.exp(theta[1])
    n = len(y)
    
    e = (Z.dot(Z.T) * tau**2) + (np.eye(n) * sigma**2)
    b = np.linalg.lstsq(X, y, rcond=None)[0]
    mu = X.dot(b) 
    
    ll = stats.multivariate_normal.logpdf(y, mu, e)
    return -ll
theta = np.array([0, 0])

mixed_log_likelihood(theta, y, X, Z)

fit = opt.minimize(
    fun=mixed_log_likelihood,
    x0=theta,
    args=(y, X, Z),
    # tol=1e-5
)

np.exp(fit.x) # compare to sd of random effects
```

:::

 -->


## Additive Models {#sec-gam}

> Wiggle, wiggle, wiggle, yeah!
> -- LMFAO

TODO: GAM gets a bit deep, but we do want to keep discussion of penalized appoach, and ultimately a word on the random effect connection.

But what if we want to allow the effect of a feature to vary depending on its own values? This is called a **curvilinear** effect, and we can use a linear model to capture this as well.
<!-- 
### Why Should You Care? {#sec-gam-why}

Not every relationship is linear and not every relationship is monotonic. Sometimes, you need to be able to model a relationship that has a fair amount of nonlinearity -- they can appear as slight curves, waves, and any other type of wiggle that you can imagine. Additive models will give you the ability to model those nonlinear relationships between your features and target. -->

### When Straight Lines Aren't Enough {#sec-gam-curve}

Fitting a line through your data is always going to be the best approach. While doing so often give us a wonderful ability to say important things about the relationships between variables and how one variable might influence another. What if we just want to dispense with the notion that we need to fit a straight line through some mass of the data? What if we relax the idea that we need a straight line and think in terms of fitting something curvy through the data?


TODO: Maybe make a single plot.

In other words, we can go from the straight line here:
  
```{r}
#| echo: false
#| label: fig-regular-linear-line
#| fig-cap: A standard linear model

df_happiness = read_csv('data/world_happiness_2018.csv')

df_happiness |> 
  ggplot(aes(healthy_life_expectancy_at_birth, happiness_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_clean()
```


To the curve seen here:
  
```{r, gam_model_line}
#| echo: false
#| label: fig-gam-model-line
#| fig-cap: A generalized additive model

df_happiness |> 
  ggplot(aes(healthy_life_expectancy_at_birth, happiness_score)) +
  geom_point() +
  geom_smooth(method = 'gam', se = FALSE) +
  theme_clean()
```

That curved line in @fig-gam-model-line is called a **spline**. It is created by a feature and expanding it to multiple columns, each of which is a function of the original feature. We then a fit a model to that data as usual. Oddly enough, the result is that we can use a linear model to fit a curve through the data. While this might not give us the same tidy explanation that a typical line would offer, we will certainly get better prediction, and a better understanding of the reality and complexity of the true relationship. But often it's useful for exploratory purposes, and tools like ggplot, plotly[^ggplotly] and others make it easy to do so.

[^ggplotly]: [Plotly]{.pack} is directly available in R and Python, and [plotnine]{.pack} is the [ggplot]{.pack} equivalent in Python.


:::{.panel-tabset}

##### R

```{r}
#| eval: false
#| label: r_gam_ggplot

x = rnorm(1000)
y = sin(x)

tibble(x, y) |> 
  ggplot(aes(x = x, y = y)) +
  geom_smooth(method = 'gam', se = FALSE) 
```

##### Python

```{python}
#| eval: false
#| label: python_gam_model

import plotly.graph_objects as go
import numpy as np

x = np.random.normal(size = 1000)
y = np.sin(x)

fig = go.Figure()
fig.add_trace(
  go.Scatter(
    x = x, 
    y = y,
    line_shape = 'spline'
  )
)
```

:::

Such models belong to a broad group of *generalized additive models* (**GAM**s). When we used an interations, we explored how the feature-target relationship varies with another feature. When we fit mixed models and interactions earlier, we focused on our feature and its relationship to the target at different values of other features. When we use a GAM, we are going to focus on our feature, and see how the relationship changes at different values for it. How are we going to do this, you might ask? Conceptually, we will have a model that looks like this:

$$
y = f(x) + \epsilon
$$

This isn't much different than before, and technically, it really isn't. It's the same linear combination of features we have with a basic linear model. The difference is that we are going to let $f(x)$ be a function of a particular feature $x$ that allows us to capture other types of relationships by expanding the feature $x$ in different ways. Some approaches can be quite complex, tackling spatial, temporal, or other aspects of the data. But on the practical side are just extra columns in the **model matrix** that find their way into the model fitting function like any other feature.

These additive features will allow us to capture nonlinearities in our data very nicely. At this point, you might be asking yourself, "Why couldn't I just use some type of polynomial regression or even a nonlinear regression?". Of course you could, but both have limitations relative to a GAM. If you are familiar with polynomial regression, where we add columns that are squares, cubes, etc. of the original feature, you can think of GAMs as a more general approach, and very similar in spirit. But with a lack of penalization, the typical polynomial regression tends to overfit the data you currently have, and **you** are forcing curves to fit through the data. To use a nonlinear model, you need to know what the underlying nonlinear form actually looks like before you can even specify the model, and without taking extra steps, such models likewise can tend to overfit. Furthermore, outside of well-known physical, chemical, or biological processes, it's rarely clear what the underlying functional form should be.


A GAM handles this situation a little better in that it will produce a curve that will provide a good fit to the data without the need to know the underlying functional form. Additionally, the default penalized approach will help prevent overfitting for smaller and/or more complex settings. Note also that we can do this for multiple features at once, and we can even include interactions between features. We can also use different types of splines to capture different types of nonlinearities.  Here is another formal definition of a GAM that makes more clear we can deal with mulitple features.

$$
\hat{y} = \sum \mathbf{X_j\beta_j}
$$

In this case, each $X_j$ is a matrix of the feature and its **basis expansion**, and the $\beta_j$ are the coefficients for each of those basis expansion columns. But a specific X could also just be a single feature and it's coefficient to model a linear relationship. The nice thing is that you don't have to worry about the details of the basis expansion -- the package you choose will take care of that for you. You will have different options, and often the default is fine, but sometimes you'll want to play with both the technique, and how 'wiggly' you want the curve to be.

### A Standard GAM {#sec-gam-standard}

Now that you have some background, let's give this a shot! In most respects, we can use the same sort of approach as we did with our other linear model examples. For our exmaple here, we'll use model what was depicted in figure @fig-gam-model-line, which looks at the relationship between the `healthy_life_expectancy_at_birth` and `happiness_score` variables from the world happiness data.

:::{.panel-tabset}

##### R

We'll use the very powerful `mgcv` package in R. The `s` function will allow us to use a spline approach to capture the nonlinearity.

```{r, r_gam_demo}
#| eval: false
#| label: r_gam_demo
library(mgcv)

df_happiness = read_csv('data/world_happiness_2018.csv')

gam_model = gam(
  happiness_score ~ s(healthy_life_expectancy_at_birth, bs = "bs"), 
  data = df_happiness
)

summary(gam_model)
```

##### Python

We can use the [statsmodels]{.pack} package in Python to fit a GAM, or alternatively, [pygam]{.pack}, and for consistency with previous models we'll choose the former. Honestly though, you should use R's [mgcv]{.pack}, as both require notably more work without much of the functionality. In addition, there is an ecosystem of R packages to further extend [mgcv's]{.pack} capabilities.

```{python, python_gam_demo}  
#| eval: false
#| label: python_gam_demo

import statsmodels.api as sm

from statsmodels.gam.api import GLMGam, BSplines
import pandas as pd

df_happiness = pd.read_csv('data/world_happiness_2018.csv')

bs = BSplines(df_happiness['healthy_life_expectancy_at_birth'], df=[9])

gam_happiness = GLMGam.from_formula(
  'happiness_score ~ healthy_life_expectancy_at_birth', 
  smoother = bs,
  data = df_happiness
)
  
gam_happiness_result = gam_happiness.fit()

gam_happiness_result.summary()
```

:::

```{r}
#| echo: false
#| label: tbl-gam-model-output
#| tbl-cap: GAM model output

library(mgcv)

df_happiness = read_csv('data/world_happiness_2018.csv')

gam_model = gam(
  happiness_score ~ s(healthy_life_expectancy_at_birth, bs = "bs"), 
  data = df_happiness
)

# library(flextable)
# looks best but not easy to manipulate at all, docs for models are non-existent, the only thing it says about signif is to remove the R global option which then proceeds to f up the footer
# this is awful to have to do and not at all generalizable, but it works for now.

d_0 = flextable::as_flextable(gam_model)$body$dataset  |>
    mutate(
      Term = ifelse(Term == '(Intercept)', 'Intercept', Term),
      Component = str_remove(Component, 'A. |B. '),
      across(where(is.numeric), \(x) round(x, 2))
    ) |> 
    select(-signif)
header_smooth = str_to_upper(c('','', 'edf','Ref.df','F.value','p.value'))
d_0[2,] = header_smooth
  
d_0 |> 
    gt() |> 
    tab_style(locations = cells_body(rows = 2), style = cell_text(size = 'small', color = 'gray50'))  |> # px or words but not actual font size specification
    tab_style(locations = cells_column_labels(), style = cell_text(color = 'gray50'))


```

When you look at the model output, what you get will depend a lot on the tool you use, and the details are mostly beyond the scope we want to present here ([check out this for more](https://m-clark.github.io/generalized-additive-models/application.html)). But in general, the following information will be provided as part of the summary or as an attribute of the model object:

- **coefficients**: The coefficients for each of the features in the model. For a GAM, these are the coefficients for the basis expansion columns, as well as standard linear feature effects. Typically, the total effect for a smooth term is displayed in the summary rather than the coefficients for each basis expansion column. Above we have the intercept and the summarized smooth term.

- **global test of significance**: Some tools will provide a test of the significance of the entire feature, as opposed to just the individual coefficients. This is a test of whether the feature is useful in the model at all.

- **edf**/**EDoF**: Effective degrees of freedom. This is a measure of wiggle in the relationship between the feature and the target. The higher the value, the more wiggle you have. If you have a value close to 1, then you have a linear relationship. With our current result, we can be pretty confident that a nonlinear relationship gives a better idea about the relationship between `healthy_life_expectancy_at_birth` and `happiness_score` than a linear one.

- **R-squared**: Adjusted/Pseudo $R^2$ or *deviance explained*. This is a measure of how much of the variance in the target is explained by the model. The higher the value, the better the model. Deviance explained is an analog to the unadjusted $R^2$ value for a Gaussian model that is used in the GLM setting. It's fine as a general assessment of prediction-target correspondence, but don't believe the actual value since we're not in a basic OLS setting.

Far more important than any of these is the visual interpration, and we can get plots from GAMs easily enough (results not shown).




<!-- For the results of our gam model, one of the best places to look first is the `edf`/`EDoF` column or attribute. This column indicates the *effective degrees of freedom*. You can think of it as a measure of wiggle in the relationship between the predictor and the target. The higher the value, the more wiggle you have. If you have a value close to 1, then you have a linear relationship. With our current result, we can be pretty confident that a nonlinear relationship gives a better idea about the relationship between `total_reviews` and `rating` than a linear relationship. -->


<!-- 
Depending on the tool we used, we may also get information about our Adjusted $R^2$ (**R-sq.(adj)**) and **Deviance explained**, which is an analog to the unadjusted $R^2$ value for a Gaussian model. We also have **GCV** -- the generalized cross validation score. It is an estimate of the mean square prediction error based on a leave-one-out cross validation estimation process. Naturally, the lower the GCV, the better the model. -->

TODO: NEED VISUAL


:::{.panel-tabset}

##### R

```{r}
#| eval: false
#| label: r_gam_plot

plot(gam_model)
```

##### Python

```{python}
#| eval: false
#| label: python_gam_plot

res_bs.plot_partial(0, cpr=True)
```

:::


Unfortunately the default package plots are not pretty, and sadly aren't provided in the same way we'd expect for interpretation. But they're fine for a quick look at your wiggly result. We provide a better looking one her[^gamplot]. The main interpretation is that there is not much relationship between `healthy_life_expectancy_at_birth` and `happiness_score` until you get to about 60 years of life expectancy, and then it increases at a faster rate.  Various tools are available to easily plot the derivatives for more understanding.

[^gamplot]: We used the [see]{.pack} in R for a quick plot. We also recommend its functionality via the [gratia]{.pack} package to visualize the derivatives, which will show more of where the effect is changing most.

```{r}
#| echo: false
#| label: fig-gam-plot
#| fig-cap: Visualizing a GAM
# TODO: GRAB DATA FROM SEE PACKAGE RESULT AND REPLOT TO CONTROL COLOR/ALPHA ETC.


gam_model = mgcv::gam(
  happiness_score ~ s(healthy_life_expectancy_at_birth, bs = "bs"), 
  data = df_happiness
)

library(see)
# can't change alpha?
plot(
  modelbased::estimate_expectation(gam_model, alpha = .1)
) +
  labs(title = 'Results from a GAM')

```
<!-- 

TODO: Move to estimation, but may need to nix for pdf brevity, and needs some work to fit well.

### Splines {#sec-gam-splines}

Now that you've gotten a taste of a standard way of specifying a GAM, let's roll our own. We are going to need to generate several functions to make this work. The first will be to produce the *cubic spline*. Do take note that there are many different types of splines that could be used.

:::{.panel-tabset}

##### R

```{r, r_cubic_spline}
#| echo: true
#| label: r_cubic_spline
cubic_spline = function(x, z) {
  ((z - 0.5)^2 - 1/12) * ((x - 0.5)^2 - 1/12)/4 -
    ((abs(x - z) - 0.5)^4 - (abs(x - z) - 0.5)^2 / 2 + 7/240) / 24
}
```

##### Python

```{python, python_cubic_spline}
#| echo: true
#| label: python_cubic_spline
import numpy as np

def cubic_spline(x,z):
    return (((z - 0.5)**2 - 1/12) * ((x - 0.5)**2 - 1/12)/4 -
            ((np.abs(x - z) - 0.5)**4 - (np.abs(x - z) - 0.5)**2 / 2 + 7/240) / 24)
```

:::

### Model Matrix Function {#sec-gam-model-matrix}

Then we a function to produce the model matrix:

:::{.panel-tabset}

##### R

```{r, r_model_matrix}
#| echo: true
#| label: r_model_matrix
splX = function(x, knots) {
  q = length(knots) + 2        # number of parameters
  n = length(x)                # number of observations
  X = matrix(1, n, q)          # initialized model matrix
  X[ ,2] = x                   # set second column to x
  X[ ,3:q] = outer(x, knots, FUN = cubic_spline) 
  X
}
```

##### Python

```{python, python_model_matrix}
#| echo: true
#| label: python_model_matrix

def splX(x, knots):
    q = len(knots) + 2
    n = len(x)
    X = np.ones((n, q))
    X[:,1] = x
    for i in range(2, q):
        X[:,i] = cubic_spline(x, knots[i-2])
    return X
```

This model matrix will help us to produce an *unpenalized spline*. 

:::



### Model Matrix {#sec-gam-model-matrix}

We can create a model with 4 knots -- you can think of knots as places where individual regression lines will get joined together. You can always experiment with more or less knots. Once we have our knots ready, we can create the model matrix.

As soon as you create your `X` object, you should take a look at it. It will be a matrix with 4 columns. The first column will be all 1s, the second column will be the scaled `user_age`, and the last two columns will be the cubic splines.

:::{.panel-tabset}

##### R

```{r, r_knots}
#| echo: true
#| label: r_knots
knots = 1:4/5
```

```{r, r_model_matrix_spline}
#| echo: true
#| label: r_model_matrix_spline
rating = df_reviews$rating

x = df_reviews$total_reviews_sc

X = splX(x, knots)            
```

```{r, model_matrix_head}
#| echo: true
#| label: model_matrix_head
head(X)
```

##### Python

```{python, python_knots}
#| echo: true
#| label: python_knots

knots = np.arange(1, 5) / 5
```

```{python, python_model_matrix_spline}
#| echo: true
#| label: python_model_matrix_spline

x = df_reviews['total_reviews_sc']

rating = df_reviews['rating']

X = splX(x, knots)
```

```{python, py_model_matrix_head}
#| echo: true
#| label: py_model_matrix_head

X[:5,:]
```

:::

### Model Fitting {#sec-gam-model-fitting}

Now that we have a model matrix, `X`, we can fit the model. All of the hardwork was done in creating the model matrix and we can just use `lm` or `OLS` to fit the model.
  
:::{.panel-tabset}

##### R
```{r, r_cubic_model_fitting}
#| echo: true
#| label: r_cubic_model_fitting
fit_lm = lm(rating ~ X - 1)

fit_lm
```

##### Python

```{python, python_cubic_model_fitting}
#| echo: true
#| label: python_cubic_model_fitting
import statsmodels.api as sm

fit_lm = sm.OLS(rating, X).fit()

fit_lm.summary()
```

:::

### Prediction

We can set some prediction values for this model:

TODO: fix this to reflect total reviews, not 0:1; Don't worry until we know where it goes.

:::{.panel-tabset}

##### R
  
```{r, r_predictions}
#| echo: true
#| label: r_predictions
# xp = seq(0, 1, by = .01)
xp = seq(-3, 3, by = .01)
Xp = splX(xp, knots)  
```

##### Python

```{python, python_predictions} 
#| echo: true

xp = np.arange(0, 1, 0.01)
Xp = splX(xp, knots)
```

:::

While creating those predictions is nice, using them to visualize the model is far more helpful.
  
```{r, spline_viz}
#| echo: false
#| label: fig-spline_viz
#| fig-cap: Visualizing cubic regression spline
ggplot(aes(x = x, y = rating), data = data.frame(x, rating)) +
  geom_point() +
  geom_smooth(method = "gam", se = FALSE, color = "#00AAFF") +
  theme_clean()
```

In @fig-spline_viz, we can see that the relationship starts a bit flat, increases, and then flattens out again. This is a pretty good example of a nonlinear relationship.

### Penalized Cubic Spline {#sec-gam-penalty}

Recall that this is an unpenalized cubic spline. If we want to have a finer degree of control over that wiggly line, we can include a **lambda penalty**. 

We'll need to change up our spline function just a bit.

:::{.panel-tabset}

##### R
  
```{r, r_spline_penalty}
#| echo: true
#| label: r_spline_penalty
splS = function(knots) {
  q = length(knots) + 2
  S = matrix(0, q, q) 
  S[3:q, 3:q] = outer(knots, knots, FUN = cubic_spline)
  S
}
```

##### Python

```{python, python_spline_penalty}
#| echo: true
#| label: python_spline_penalty

def splS(knots):
    q = len(knots) + 2
    S = np.zeros((q, q))
    S[2:, 2:] = cubic_spline(knots, knots[:,None])
    return S

```

:::

We also need to be able to take the square root of our entire matrix. This is a bit more complicated than it sounds. We need to take the eigenvalue decomposition of the matrix, take the square root of the eigenvalues, and then recombine the matrix.

:::{.panel-tabset}

##### R

```{r, r_matrix_square}
#| echo: true
#| label: r_matrix_square
mat_sqrt = function(S) {
  d = eigen(S, symmetric = TRUE)
  rS = d$vectors %*% diag(d$values^.5) %*% t(d$vectors)
  rS
}
```

##### Python

```{python, python_matrix_square}
#| echo: true
#| label: python_matrix_square

def mat_sqrt(S):
    w, v = np.linalg.eig(S)
    rS = v @ np.diag(w**.5) @ v.T
    return rS
```

:::

### Penalized Model Fitting Function {#sec-gam-penalty-model}

With those functions in hand, we can create the function to fit the entire model.

:::{.panel-tabset}

##### R
  
```{r, r_penalized_fit}
#| echo: true
#| label: r_penalized_fit
prs_fit = function(y, x, knots, lambda) {
  q  = length(knots) + 2    # dimension of basis
  n  = length(x)            # number of observations
  Xa = rbind(splX(x, knots), mat_sqrt(splS(knots))*sqrt(lambda)) # augmented model matrix
  y[(n + 1):(n+q)] = 0      # augment the data vector
  
  lm(y ~ Xa - 1) # fit and return penalized regression spline
}
```

##### Python

```{python, python_penalized_fit} 
#| echo: true
#| label: python_penalized_fit
def prs_fit(y, x, knots, lamba):
    q = len(knots) + 2
    n = len(x)
    Xa = np.vstack(
      (splX(x, knots), mat_sqrt(splS(knots))*np.sqrt(lamba))
      )
    y_add = np.zeros(q)
    y = y.to_numpy()
    y = np.concatenate((y, y_add), axis = 0)
    return sm.OLS(y, Xa).fit()
```

:::

Notice again that magic happens in the model matrix, but that we are still just using `lm` or `OLS` to fit the model.

### Penalized Model Fitting {#sec-gam-penalty-fitting}

Let's stick with 4 knots and see what happens when we set our lambda to .1:

:::{.panel-tabset}

##### R
  
```{r, r_lambda_1}
#| echo: true
#| label: r_lambda_1
knots = 1:4/5

fit_penalized = prs_fit(
  y = rating,
  x = x,
  knots = knots,
  lambda = .1
) 

Xp = splX(xp, knots) 
```

##### Python

```{python}
#| eval: false # change if used later
#| label: python_lambda_1

knots = np.arange(1, 5)/5

fit_penalized = prs_fit(
    y = y,
    x = x,
    knots = knots,
    lamba = .1
)

Xp = splX(xp, knots)
```

:::

As shown in @fig-r_lambda_1_viz, there is some wiggle to that line, but it is not as extreme as what we saw with our unpenalized cubic spline. 

TODO: fix plot scale (doesn't match raw or scaled data)

```{r}
#| echo: false
#| label: fig-r_lambda_1_viz
#| fig-cap: GAM model with lambda set to .1
ggplot(aes(x = x, y = rating), data = data.frame(x, rating)) +
  geom_point() +
  geom_line(aes(x = xp, y = fit[,1]),
            data = tibble(xp, fit = Xp %*% coef(fit_penalized)),
            color = "#00AAFF") +
  theme_clean()
```

We can test out what happens at different lambda values:
  
```{r, lambda_value_viz}
#| echo: false
#| label: fig-lambda_value_viz
#| fig-cap: GAM model with different lambda values
plot_data = purrr::map_df(c(.9, .5, .1, .01, .001), ~{
  fit_penalized = prs_fit(
    y = rating,
    x = x,
    knots = knots,
    lambda = .x
  ) 
  Xp = splX(xp, knots)
  
  results = data.frame(
    x = xp,
    y = Xp %*% coef(fit_penalized),
    lambda = as.factor(.x)
  )
  
  return(results)
})

ggplot(aes(x = x, y = rating), data = data.frame(x, rating)) +
  geom_point() +
  geom_line(
    aes(x = x, y = y, color = lambda),
    data = plot_data
  ) +
  scale_color_manual(values = okabe_ito[c(-1,-4)]) +
  coord_cartesian(xlim = c(-2, 2)) +
  theme_clean()
```

What can we take from @fig-lambda_value_viz? As lambda values get closer to 1, we see lines that look very similar to a standard linear model. If you recall the our function to fit the model, we multiplied the square root of the matrix by the square root of the lambda value; since the square root of 1 is 1, we wouldn't see anything too interesting happen. As our lambda value gets lower, we see an increasing amount of wiggle happen. 

Naturally, this is a great time to think about how these models would work on new data. As lambda gets smaller, we are fitting our in-sample data much better. How do you think this would fare with unseen data? If you'd say that we would do well with training and horrible on testing, we'd likely agree with you.  
-->

To summarize, we can use a GAM to model nonlinear relationships between our features and target. We can use splines to capture those nonlinearities, and we can use a penalized approach to control the amount of wiggle in our model. What's more we can interact the wiggle with other categorical and numeric features to capture even more complexity in our data.  Because of this, GAMs are a very powerful modeling tool that take us a step toward more complex models, but without the need to go all the way to a neural network or other more complex model, and they can still provide statistical inference information as a default. A great tool to have in your modeling toolbox!





## Quantile Regression {#sec-lm-extend-quantile}


> Oh, you think the median is your ally. But you merely adopted the median; I was born in it, molded by it. I didn't see anything interesting until I was already a man. And by then, it was nothing to me but illuminating.
> -- Bane (probably)


People generally understand the concept of the arithmetic mean. You see it some time during elementary school, it gets tossed around in daily language (usually using the word "average"), and it is statistically important. After all, where would the normal distribution be without a mean? Why, though, do we feel so tied to it from a regression modeling perspective? Yes, it has handy features, but it is also a bit restrictive to the types of relationships that it can actually model well. 

Here we'll show you what to do when the mean betrays you -- and trust us, the mean will betray you at some point!

<!-- ### Why Should You Care? {#sec-quantile-why}

Sometimes the mean doesn't make as much sense to focus on for summarizing our data, whether due to extreme scores that have too much influence, or you're interested at how your model is performing in different parts of your data, such as the top 10%, quantile regression can be helpful. Quantile regression will give you the ability to model the relationship between your features and target at different quantiles of your target, with the median being a starting point. Maybe people who are older are more likely to rate movies higher, but that relationship is stronger for people who rate movies higher than the median. Quantile regression will let you model that relationship. -->

### When The Mean Breaks Down {#sec-quantile-break}

In a perfect data world, we like to assume the mean is equal to the middle observation of the data: the *median*. But that is only when things are symmetric though, and usually our data comes loaded with challenges. Skewness and even just a few extreme scores in your data may cause a rift between the median and the mean. 

Let's say we take the integers between 1 and 10, and find the mean. 

$$\frac{1+2+3+4+5+6+7+8+9+10}{10} =  5.5$$

The middle value in that vector of numbers would also be 5.5. 

What happens we replace the 1 with a more extreme value, like -10?

$$\frac{-10+2+3+4+5+6+7+8+9+10}{10} =  4.5$$

With just one dramatic change, our mean went down by a whole point. The median observation, though, is still 5.5. In short, the median is invariant to wild swings out in the tails of your numbers.  

You might be saying to yourself, "Why should I care about this central tendency chicanery?" Let us tell you why you should care -- the least squares approach to the standard linear model dictates that the regression line needs to be fit through the means of the variables. If you have extreme scores that influence the mean, then your regression line will also be influenced by those extreme scores.

Consider the following regression line:
  
```{r}
#| echo: false
#| label: fig-linear-line-no-extremes
#| tbl-cap: Linear line without extreme scores

library(ggplot2)

set.seed(123)
N = 1000 
k = 2   
X = matrix(rnorm(N * k), ncol = k)  
y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5)  

dfXy = data.frame(X, y)

ggplot(dfXy, aes(x = X1, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("x") +
  ylab("y") +
  theme_clean()
```

Now, what would happen if we replaced a few of our observations with extreme scores?
  
```{r linear_line_extremes}
#| echo: false
#| label: fig-linear-line-extremes
#| fig-cap: Linear line with extreme scores
new_df = dfXy

new_df$y[dfXy$y > quantile(dfXy$y, .95) & 
           dfXy$X1 > quantile(dfXy$X1, .95)] = rnorm(10, 2.5, .1)

new_df$X1[dfXy$y > quantile(dfXy$y, .95) & 
            dfXy$X1 > quantile(dfXy$X1, .95)] = rnorm(10, 2.5, .1)

ggplot(new_df, aes(x = X1, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "x", y = "y") +
  theme_clean()
```

With just a casual glance, it doesn't look like our two regression lines are that different. They both look like they have a similar positive slope, so all should be good. To offer a bit more clarity, though, let's put those lines in the same space:
  
```{r}
#| echo: false
#| label: fig-both-linear-lines
#| fig-cap: Line lines with and without extreme scores

ggplot() +
  geom_smooth(
    aes(X1, y, color = "No extreme scores"), 
    data = dfXy, 
    method = 'lm',
    se = FALSE
  ) +
  geom_smooth(
    aes(X1, y, color = "Extreme scores"), 
    data = new_df, 
    method = 'lm',
    se = FALSE
  ) +
  xlab("x") +
  ylab("y") +
  # theme_clean() +
  scale_color_manual(
    name='',
    breaks=c('No extreme scores', 'Extreme scores'),
    values=c('No extreme scores'=okabe_ito[5], 'Extreme scores'=okabe_ito[6])
  ) +
  theme(
    panel.grid.major = element_line(color = alpha('black', .1)),
    # panel.grid.minor = element_line(color = alpha('black', .1)),
  )
```

With 1000 observations, we see that having just 10 relatively extreme scores is enough to change the regression line, even if just a little. But that little bit can mean a huge difference for predictions or just the conclusions we come to.

There are a few approaches we could take here, with common approaches being dropping those observations or Windsorizing them. Throwing away data because you don't like the way it behaves is nearing on statistical abuse, and Windsorization is just replacing those extreme values with numbers that you like a little bit better. Let's not do that!

A better answer to this challenge might be to not fit the regression line through the mean, but the median instead. This is where a model like **quantile regression** becomes handy. Formally, the objective function for the model can be expressed as:
<!-- 
$$
Q_{Y\vert X}(\tau) = X\beta_\tau
$$

Where we can find the estimation of $\beta_\tau$ as:

TODO: change a bit to other loss function depiction, add the explicit quantile function -->

$$
\text{Objective} =  \Sigma \left((\tau - 1)\sum_{y_{i}<q}(y_{i}-q)+\tau\sum_{y_{i}\geq q}(y_{i}-q) \right)
$$

With quantile regression, we are given an extra parameter for the model: $\tau$ or *tau*. The tau parameter let's us choose which quantile we want to use for our line fitting. Since the median splits the data in half, we can translate that to a quantile of .5. The objective function treats positive residuals differently than negative residuals. If the residual is positive, then we multiply it by the tau value. If the residual is negative, then we multiply it by -1 plus the tau value.


We can again use our movie reviews data. Let's say that we are curious about the relationship between the `word_count` variable and the `rating` variable to keep things simple. To make it even more straightforward, we will use the standardized (scaled) version of the variable. In our default approach, we will start with a median regression, in other words, a quantile of .5.


<!-- 
### Data Import and Preparation {#sec-quantile-data}

TODO: Rescale total_reviews or use the scaled version

:::{.panel-tabset}

##### R

```{r}
#| eval: false
#| label: r_data_prep
df_reviews = read.csv("data/movie_reviews_processed.csv")

df_reviews = na.omit(df_reviews)

X = df_reviews$total_reviews
X = cbind(1, X)
y = df_reviews$rating
```

##### Python

```{python}
#| eval: false
#| label: py_data_prep
import pandas as pd
import numpy as np

df_reviews = pd.read_csv("data/movie_reviews_processed.csv")

df_reviews = df_reviews.dropna()

X = pd.DataFrame(
  {'intercept': 1, 
  'total_reviews': df_reviews['total_reviews']}
)
y = df_reviews['rating']
```
::: 
 -->


:::{.panel-tabset}

##### R

```{r}
#| echo: true
#| label: r_quantreg
#| results: hide
library(quantreg)

model_median = rq(
  rating ~ word_count_sc, 
  tau = .5,
  data = df_reviews
)

summary(model_median)
```

##### Python

```{python} 
#| label: py_quantreg
#| results: hide
import pandas as pd
import statsmodels.formula.api as smf

df_reviews = pd.read_csv("data/movie_reviews_processed.csv")

model_median = smf.quantreg('rating ~ word_count_sc',  data = df_reviews)
model_median = model_median.fit(q = .5)
                           
model_median.summary()                           
```

:::


```{r}
#| echo: false
#| label: tbl-quantile-model-output
#| tbl-cap: Quantile regression model output

broom::tidy(model_median) |> 
  rename(
    feature = term,
    coef = estimate
  ) |> 
  select(-tau) |> 
  gt()
```

Fortunately, our interpretation of this result isn't all that different from a standard linear model -- the rating should decrease by `r round(coef(model_median)['word_count_sc'],2)` for every bump in standard deviation for number of words, which in this case is about `r round(sd(df_reviews$word_count))` words. However, this is concerns the rating median, not the mean, like the standard linear model. 

Quantile regression is not a one-trick-pony. Remember, it is called quantile regression -- not median regression. Being able to compute a median regression is just the default. What we can do also is to model different quantiles of the same data. It gives us the ability to answer brand new questions -- does the relationship between user age and their ratings change at different quantiles of rating? Very cool!

Instead of a single model to capture the trend through the mean of the data, we can now examine the trends within 5 different quantiles of the data - .1, .3 .5, .7, and .9.  We aren't limited to just those quantiles though, and you can examine any of them that you might find interesting.  Here is a plot of the results of these models.

```{r}
#| echo: false
#| label: fig-quantile-lines
#| fig-cap: Quantile regression lines
tau_values = c(.1, .3, .5, .7, .9)

quant_values = purrr::map_df(tau_values, ~{
  result = coef(rq(rating ~ word_count_sc, tau = .x, 
                    data = df_reviews))
  result$tau = .x
  result
})

colnames(quant_values) = c("intercept", "word_count_sc", "tau")

quant_values$tau = as.factor(quant_values$tau)

ggplot() +
  geom_point(
    data = df_reviews, 
    mapping = aes(word_count_sc, rating), 
    alpha = .25
  ) +
  geom_abline(
    aes(slope = word_count_sc, intercept = intercept, color = tau),
    linewidth = 2,
    data = quant_values
  ) +
  labs(x = "Word Count (Standardized)", y = "Rating") +
  scale_color_manual(values = okabe_ito[c(-1,-4)])
```


If we had to put some words to our visualization, we could say that all of the quantiles show a negative relationship. The 10th and 90th quantiles show the weakest relationship, while those in the middle show a notably stronger relationship. We can also see that that the 90th percentile is better able to capture those values that would otherwise be deemed as outliers using other standard techniques.

```{r}
#| echo: false
#| label: tbl-quantile-model-output-multi-quants
#| tbl-cap: Quantile regression model output

rq(
  rating ~ word_count_sc, 
  tau = tau_values, 
  data = df_reviews
) |> 
  broom::tidy() |> 
  rename(
    feature = term,
    coef = estimate,
    quantile = tau
  ) |>
  gt()
```



TODO: MOVE TO ESTIMATION OR ONLINE ONLY

### Quantile Loss Function {#sec-quantile-loss}

Now that we know how to use standard functions for quantile regression, let's see one way that we can create a least squares loss function for fitting a linear regression model and compare it with a function for quantile loss.

:::{.panel-tabset}

##### R

```{r}
#| echo: true
#| label: quantile_loss
quantile_loss = function(par, X, y, tau) {
  
  linear_parameters = X %*% par
  
  residual = y - linear_parameters
  
  loss = ifelse(
    residual < 0, 
    (tau-1)*residual, 
    tau*residual
  )
  
  sum(loss)
}
```

##### Python

```{python}
#| echo: true
def quantile_loss(par, X, y, tau):
  linear_parameters = X.dot(par)
  
  residual = y - linear_parameters
  
  loss = []
  
  loss = np.where(
    residual < 0, 
    (tau-1)*residual, 
    tau*residual
  )

  # for i in residual:
  #   if i < 0: loss.append((-1 + tau)*i)
  #   else: loss.append(tau*i)
  
  return sum(loss)

```

:::

You'll notice right away that we have a few differences. Our quantile loss function includes the **tau** argument, which will let us set our quantile of interest; naturally, it can be any value between 0 and 1. The residual is multiplied by the tau value, only if the residual is greater than 0. If the residual is negative, we need to add tau to -1. Since we need a positive value for our loss values, we will multiply our negative residuals by the negative value produced from -1 plus our tau value. After that, we just sum all of those positive loss values and do our best to minimize that summed value. 

### Model Fitting {#sec-quantile-model}

Now that we have our data and our loss function, we can fit the model almost exactly like our standard linear model. Again, note the difference here with our tau value, which we've set to .5 to represent the median.

:::{.panel-tabset}

##### R

```{r}
#| echo: true
#| label: r_quantile_optim

X = cbind(1, df_reviews$word_count_sc)
y = df_reviews$rating

optim(
  par = c(intercept = 0, word_count_sc = 0),
  fn  = quantile_loss,
  X   = X,
  y   = y,
  tau = .5
)$par
```

##### Python

```{python}
from scipy.optimize import minimize
import numpy as np

X = pd.DataFrame(
  {'intercept': 1, 
  'word_count_sc': df_reviews['word_count_sc']}
)
y = df_reviews['rating']

minimize(
  quantile_loss, 
  x0 = np.array([0, 0]), 
  args = (X, y, .5)
  ).x
```

:::


::: {.callout-tip title='Another Interaction' collapse="true"}
One way to interpret this result is that we have a nonlinear relationship between the word count and the rating, in the same we we had an interaction previously. In this case, our effect of number of reviews interacts with the target! In other words, we have a different word count effect for different ratings. This is a bit of a mind bender, but it's a good example of how a linear approach can be used to model quirky relationships!
:::



## Performance Comparisons

TODO: UPDATE WITH CURRENT MODELS

Just for giggles, we should see how all of our models perform:

```{r, model_performace_comp}
#| echo: false
#| label: tbl-model-performance-comp
#| tbl-cap: Comparing model performance with RMSE
library(mgcv)
model_data = data.frame(rating = df_reviews$rating, 
                         total_reviews = df_reviews$total_reviews_sc, 
                         genre = df_reviews$genre)
lm_test = lm(rating ~ total_reviews, 
              data = model_data) 
median_test = rq(rating ~ total_reviews, 
                  data = model_data) 
gam_test = gam(rating ~ 
                  s(total_reviews, bs = "cr", fx = FALSE, m = .001), 
                data = model_data) 

fit_mer = lmer(rating ~ total_reviews + (1 | genre), 
               model_data, 
               REML = FALSE)

gt(
  tibble(model = c("standard", "median", "gam", "mixed"), 
             rmse = c(modelr::rmse(lm_test, model_data),
                      modelr::rmse(median_test, model_data),
                      modelr::rmse(gam_test, model_data), 
                      modelr::rmse(fit_mer, model_data))
  )
)
```

Let's check out the results in @tbl-model-performance-comp. Unsurprisingly, the standard linear model and the median regression were pretty close to each other. GAM offered a small bump in performance, but our best model came from the mixed model. This finding may or may not surprise you -- as you spend more time with models, you often encounter situations where simple models outperform more complex models, or are on par with them. Here, we are seeing that the mixed model is offering us a better fit to the data than the other models. However, that doesn't mean that you can just go right to the mixed model. You need to know your data and know what you are trying to accomplish.

## Wrapping Up {#sec-lm-extend-wrap}

The standard linear model is useful across many different data situations. It does, unfortunately, have some issues when data becomes a little bit more "real". When you have extreme scores or relationships that a standard model might miss, you don't need to abandon your linear model in favor of something more exotic. Instead, you might just need to think about how you are actually fitting the line through your data. 

TODO: ADD Exercise

## Next Steps {#sec-lm-extend-next-steps}

No matter how much we cover in this book, there is always more to learn. Here are some additional resources that you might find helpful related to this task. But if you've got a good grip on linear models and related topics, feel free to try out some machine learning @sec-ml-core-concepts!



If you want absolute depth on quantile regression, we will happily point you to the OG of quantile regression, Roger Koenker. His book, *Quantile Regression* is a must read for anyone wanting to dive deeper into quantile regression [-@koenker_quantile_2005-1], or just play around with his R package `quantreg`. *Galton, Edgeworth, Frisch, and prospects for quantile regression in econometrics* is another resource from him.

If you want to dive more into the GAM world, we would recommend that you start with the **Moving Beyond Linearity** chapter in *An Introduction to Statistical Learning* [@james_introduction_2021]. Not only do they have versions for both R and Python, but both have been made [available online](https://www.statlearning.com/). If you are wanting more after that, you can't beat Simon Wood's book, *Generalized Additive Models: An Introduction with R* [-@wood_generalized_2017], or a more digestible covering of the same content by one of your own humble authors [@clark_generalized_2022].

There is no shortage of great references for mixed effects models. If you are looking for a great introduction to mixed models, we would recommend to start with yet another tutorial by one of your fearless authors! Michael Clark's *Mixed Models with R* [-@clark_mixed_2023], is a great introduction to mixed models and is [freely available](https://m-clark.github.io/mixed-models-with-R/). If you want to dig just a little deeper, the `lme4` vignette for [*Fitting Linear Mixed-Effects Models Using lme4*](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf) is a great resource.