[
  {
    "objectID": "knowing_models.html",
    "href": "knowing_models.html",
    "title": "2  Knowing Your Model",
    "section": "",
    "text": "2.1 Key Ideas",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#sec-knowing-key",
    "href": "knowing_models.html#sec-knowing-key",
    "title": "2  Knowing Your Model",
    "section": "",
    "text": "Metrics can help you assess how well your model is performing, and they can also help you compare different models.\nDifferent metrics can be used depending on the goals of your model.\nVisualizations can help you understand how your model is making predictions and which variables are important.\nFeature importance is very difficult to ascertain even in the simplest of models, but there are tools to help you understand how much each feature contributes to a prediction.\n\n\n2.1.1 Why this matters\nIt’s never good enough to simply get model results. You need to know how well your model is performing and how it is making predictions. You also should be comparing your model to other alternatives. Doing so provides more confidence in your model and helps you to understand how it is working, and just as importantly, where it fails. This is actionable knowledge.\n\n\n2.1.2 Good to know\nThis takes some of the things we see in other chapters on linear models and machine learning. We’d suggest have linear model basics down pretty well.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#sec-knowing-model-metrics",
    "href": "knowing_models.html#sec-knowing-model-metrics",
    "title": "2  Knowing Your Model",
    "section": "2.2 Model Metrics",
    "text": "2.2 Model Metrics\nA first step in understanding our model can be done with summary statistics, typically called metrics Regression and classification have different metrics for assessing model performance. We want to give you a sample of some of the more common one, but we also want to acknowledge that there are many more that you can use! We would always recommend looking at a few different metrics to get a better sense of how your model is performing.\nTable 2.1 illustrates some of the most commonly used performance metrics. Just because these are popular or applicable for your situation, doesn’t mean they are the only ones you can or even should use. Nothing keeps you from using more than one metric for assessment, and in fact, it is often a good idea to do so. Your should have a working knowledge of these.\nTODO: check table for pdf\n\n\n\n\n\n\nTable 2.1: Commonly used performance metrics in machine learning.\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Metric\n      Description\n      Other Names/Notes\n    \n  \n  \n    \n      Regression\n    \n    RMSE\nRoot mean squared error\nMSE (before square root)\n    MAE\nMean absolute error\n\n    MAPE\nMean absolute percentage error\n\n    RMSLE\nRoot mean squared log error\n\n    R-squared\nAmount of variance shared by predictions and target\nCoefficient of determination\n    Deviance/AIC\nGeneralization of sum of squared error \nAlso \"deviance explained\" for similar R-sq interpretation\n    \n      Classification\n    \n    Accuracy\nPercent correct\nError rate is 1 - Accuracy\n    Precision\nPercent of positive predictions that are correct\nPositive Predictive Value\n    Recall\nPercent of positive samples that are predicted correctly\nSensitivity, True Positive Rate\n    Specificity\nPercent of negative samples that are predicted correctly\nTrue Negative Rate\n    Negative Predictive Value\nPercent of negative predictions that are correct\n\n    F1\nHarmonic mean of precision and recall\nF-Beta1\n    AUC\nArea under the ROC curve\n\n    False Positive Rate\nPercent of negative samples that are predicted incorrectly\nType I Error, alpha\n    False Negative Rate\nPercent of positive samples that are predicted incorrectly\nType II Error, beta, Power is 1 - beta\n    Phi\nCorrelation between predicted and actual\nMatthews Correlation\n    Log loss\nNegative log likelihood of the predicted probabilities\n\n  \n  \n  \n    \n      1 Beta = 1 for F1\n    \n  \n\n\n\n\n\n\n\n\n\n\n2.2.1 Regression Metrics\nRecall that a primary goal of our standard linear model is to produce predictions. Since we are predicting a specific value, we need to be able to compare that prediction to its corresponding observed value. The closer our prediction is to the actual value, the better our model is performing. As we saw in the above table, when we have a numeric target there are quite a few metrics that help us understand prediction-target correspondence, so let’s look at some of those.\nBut before we create a model to get us started, we are going to read in our data and then create two different splits within our data: a training set and a testing set. In other words, we are going to partition our data so that we can train a model and then see how well that model performs with new data1.\n\n\n\n\n\n\nSplitting Data\n\n\n\nThis basic split is the foundation of cross-validation. Cross-validation is a method for partitioning data into training and non-training sets in a way that allows you to better understand the model’s performance. You’ll find more explicit demonstration in the machine learning chapter Chapter 7.\n\n\n\nRPython\n\n\n\ndf_reviews = read.csv(\n  \"data/movie_reviews_processed.csv\"\n)\n\ninitial_split = sample(\n  x = 1:nrow(df_reviews), \n  size = nrow(df_reviews) * .75, \n  replace = FALSE\n)\n\ntraining_data = df_reviews[initial_split, ]\n\ntesting_data = df_reviews[-initial_split, ]\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\ndf_reviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\ntraining_data, testing_data = train_test_split(\n    df_reviews, \n    test_size = 0.25, \n    random_state = 123\n)\n\n\n\n\nYou’ll notice that we created training data with 75% of our data and we will use the other 25% to test our model. With training data in hand, let’s produce a model to predict review rating. We’ll use scaled versions of several features, and use the ‘year’ features starting at year 0, which is the earliest year in our data. Finally we also include the genre of the movie as a categorical variable.\n\nRPython\n\n\n\nmodel_train_reg = lm(\n  rating ~ \n    review_year_0 \n    + release_year_0 \n    + age_sc \n    + length_minutes_sc \n    + total_reviews_sc \n    + word_count_sc \n    + genre \n    ,\n    training_data\n)\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# we'll use 'features' later also\nfeatures = [\n    \"review_year_0\", \n    \"release_year_0\",\n    \"age_sc\", \n    \"length_minutes_sc\", \n    \"total_reviews_sc\", \n    \"word_count_sc\", \n    \"genre\", \n    ]\n\nmodel =  'rating ~ ' + \" + \".join(features)\n\nmodel_train_reg = smf.ols(\n    formula = model,\n    data = training_data\n).fit()\n\n\n\n\nNow that we have a model on our training data, we can use it to make predictions on our test data:\n\nRPython\n\n\n\npredictions = predict(model_train_reg, newdata = testing_data)\n\n\n\n\npredictions = model_train_reg.predict(testing_data)\n\n\n\n\nThe goal now is to find out how close our predictions match reality. Let’s look at them first:\n\n\n\n\n\n\n\n\nFigure 2.1\n\n\n\n\n\nObviously, our points do not make a perfect line, which would indicate perfect prediction, so we’d like to determine how far off we are. There are a number of metrics that can be used to measure this. We’ll go through a few of them here.\n\n2.2.1.1 R-squared\nAnyone that has done linear regression has come across the \\(R^2\\) value. It is a measure of how well the model explains the variance in the target. One way to calculate it is as follows:\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\]\nwhere \\(y_i\\) is the observed value, \\(\\hat{y}_i\\) is the predicted value, and \\(\\bar{y}\\) is the mean of the observed values. The \\(R^2\\) value is a measure of how much variance in the target (the denominator) is attibutable to the model’s predictions (numerator). It is a value between 0 and 1, with 1 indicating that the model explains all of the variance in the target.\nMore simply, \\(R^2\\) is the squared correlation of our predicted values and the target. In that sense it can almost always be useful as a descriptive measure, just like we use means and standard deviations in exploratory data analysis. However, it is not so great at telling us about predictive quality. Why? Take your predictions from a our rating model, and add 10 to them, or make them all negative. In both cases your predictions would be ridiculous, but your \\(R^2\\) will be the same. Another problem is that for training data, \\(R^2\\) will always increase as you add more variables to your model, whether they are useful or pure noise! This is why we use other metrics to assess predictive quality.\n\nRPython\n\n\n\n1 - sum((testing_data$rating - predictions)^2) / sum((testing_data$rating - mean(testing_data$rating))^2)\n\n[1] 0.522237\n\nyardstick::rsq_trad_vec(testing_data$rating, predictions)\n\n[1] 0.522237\n\n# conceptually identical, but slight difference due to how internal calculations are done\ncor(testing_data$rating, predictions)^2 \n\n[1] 0.5225642\n\nyardstick::rsq_vec(testing_data$rating, predictions)\n\n[1] 0.5225642\n\n\n\n\n\nfrom sklearn.metrics import r2_score\n\n1 - np.sum((testing_data.rating - predictions)**2) / np.sum((testing_data.rating - np.mean(testing_data.rating))**2)\n\n0.508431158347433\n\nr2_score(testing_data.rating, predictions)\n\n0.508431158347433\n\n# conceptually identical, but slight difference due to how calculations are done\nnp.corrcoef(testing_data.rating, predictions)[0, 1]**2\n\n0.5147329632453266\n\n\n\n\n\n\n\n\n\n\n\nR-squared variants\n\n\n\nThere are different versions of R-squared. ‘Adjusted’ R-squared is a common one, and it penalizes the model for adding features that don’t really explain the target variance. This is a nice sentiment, but its difference versus the standard R-squared would only be noticeable for very small datasets. Some have also attempted to come up with R-squared values that are more appropriate for GLMs for count, binary and other models. Unfortunately, these ‘pseudo-R-squared’ values are not as interpretable as the original R-squared, and generally suffer several issues.\n\n\n\n\n2.2.1.2 Mean Squared Error\nOne of the most common performance metrics for numeric targtes is the mean squared error (MSE) and its square root, root mean squared error (RMSE). The MSE is the average of the squared differences between the predicted and actual values. It is calculated as follows:\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nMSE is a great metric for penalizing large errors. Since errors are squared, the larger the error, the larger the penalty. As mentioned, the root mean squared error (RMSE) is just the square root of the MSE. Like MSE, RMSE is a great metric for penalizing large errors, but if you want to that approach and still have a metric that is in the same units as the original data, RMSE is the metric for you. It is calculated as follows:\n\\[RMSE = \\sqrt{MSE}\\]\n\nRPython\n\n\n\nmean((testing_data$rating - predictions)^2)\n\n[1] 0.1858096\n\nyardstick::rmse_vec(testing_data$rating, predictions)^2\n\n[1] 0.1858096\n\nsqrt(mean((testing_data$rating - predictions)^2))\n\n[1] 0.4310564\n\nyardstick::rmse_vec(testing_data$rating, predictions)\n\n[1] 0.4310564\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error\n\nnp.mean((testing_data.rating - predictions)**2)\n\n0.20798285555421575\n\nmean_squared_error(testing_data.rating, predictions)\n\n0.20798285555421575\n\nnp.sqrt(np.mean((testing_data.rating - predictions)**2))\n\n0.4560513738102493\n\nmean_squared_error(testing_data.rating, predictions, squared = False)\n\n0.4560513738102493\n\n\n\n\n\n\n\n2.2.1.3 Mean Absolute Error\nThe mean absolute error (MAE) is the average of the absolute differences between the predicted and actual values. It is calculated as follows:\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\]\nMAE is a great metric when all you really want to know is how far off your predictions are from the actual values. It is not as sensitive to large errors as the MSE.\n\nRPython\n\n\n\nmean(abs(testing_data$rating - predictions))\n\n[1] 0.3383954\n\nyardstick::mae_vec(testing_data$rating, predictions)\n\n[1] 0.3383954\n\n\n\n\n\nfrom sklearn.metrics import mean_absolute_error\n\nnp.mean(abs(testing_data.rating - predictions))\n\n0.3704072983307527\n\nmean_absolute_error(testing_data.rating, predictions)\n\n0.3704072983307527\n\n\n\n\n\n\n\n2.2.1.4 Mean Absolute Percentage Error\nThe mean absolute percentage error (MAPE) is the average of the absolute differences between the predicted and actual values, expressed as a percentage of the actual values. It is calculated as follows:\n\\[MAPE = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{|y_i - \\hat{y}_i|}{y_i}\\]\n\nRPython\n\n\n\nmean(\n  abs(testing_data$rating - predictions) / \n    testing_data$rating\n) * 100\n\n[1] 12.00139\n\nyardstick::mape_vec(testing_data$rating, predictions)\n\n[1] 12.00139\n\n\n\n\n\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nnp.mean(\n    abs(testing_data.rating - predictions) / \n    testing_data.rating\n) * 100\n\n13.464399850975898\n\nmean_absolute_percentage_error(testing_data.rating, predictions) * 100\n\n13.464399850975898\n\n\n\n\n\n\n\n2.2.1.5 Which To Use?\nIn the end, it won’t hurt to look at a few of these metrics to get a better idea of how well your model is performing. You will always be using these metrics to compare different models, so use a few of them to get a better sense of how well your models are performing relative to one another. Does adding a variable help drive down RMSE, indicating that the variable helps to reduce large errors? In other words, does adding complexity to your model provide a big reduction in error? If adding variables doesn’t help reduce error, do you really need to include it in your modelU+0203D;\n\n\n\n2.2.2 Classification Metrics\nWhenever we are classifying outcomes, we don’t have the same ability to compare a predicted score to an observed score – instead, we are going to use the predicted probability of an outcome, establish a cut-point for that probability, convert everything below that cut-point to 0, and then convert everything at or above that cut-point to 1. We can then compare a table predicted and actual classes.\nLet’s start with a model to predict whether a review is “good” or “bad”. We will use the same training and testing data that we created above.\n\nRPython\n\n\n\nmodel_train_class = glm(\n  rating_good ~ \n    genre + review_year_0 \n    + release_year_0 \n    + age_sc \n    + length_minutes_sc \n    + total_reviews_sc \n    + word_count_sc \n    + genre     \n    , \n    training_data, \n    family = binomial\n)\n\nsummary(model_train_class)\n\n\nCall:\nglm(formula = rating_good ~ genre + review_year_0 + release_year_0 + \n    age_sc + length_minutes_sc + total_reviews_sc + word_count_sc + \n    genre, family = binomial, data = training_data)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.86459    0.43000  -4.336 1.45e-05 ***\ngenreComedy        2.36609    0.40016   5.913 3.36e-09 ***\ngenreDrama         2.10262    0.26193   8.027 9.95e-16 ***\ngenreHorror        0.01011    0.41956   0.024  0.98077    \ngenreKids          0.07226    0.36804   0.196  0.84434    \ngenreOther         0.08938    0.36778   0.243  0.80798    \ngenreRomance       0.43456    0.37733   1.152  0.24946    \ngenreSci-Fi        0.01289    0.41785   0.031  0.97539    \nreview_year_0      0.03553    0.01767   2.011  0.04434 *  \nrelease_year_0     0.02941    0.01037   2.835  0.00458 ** \nage_sc            -0.27453    0.09328  -2.943  0.00325 ** \nlength_minutes_sc  0.63768    0.10394   6.135 8.51e-10 ***\ntotal_reviews_sc   1.00843    0.10882   9.267  &lt; 2e-16 ***\nword_count_sc     -0.51049    0.10215  -4.997 5.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1031.17  on 749  degrees of freedom\nResidual deviance:  736.51  on 736  degrees of freedom\nAIC: 764.51\n\nNumber of Fisher Scoring iterations: 5\n\n# for later\ny_target_testing_bin = ifelse(testing_data$rating_good == \"good\", 1, 0)\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nmodel =  'rating_good ~ ' + \" + \".join(features)\n\nmodel_train_class = smf.glm(\n    formula = model,\n    data = training_data,\n    family = sm.families.Binomial()\n).fit()\n\n\nmodel_train_class.summary() \n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nrating_good\nNo. Observations:\n750\n\n\nModel:\nGLM\nDf Residuals:\n736\n\n\nModel Family:\nBinomial\nDf Model:\n13\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-367.76\n\n\nDate:\nMon, 26 Feb 2024\nDeviance:\n735.52\n\n\nTime:\n20:55:57\nPearson chi2:\n672.\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.3189\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-1.7239\n0.439\n-3.931\n0.000\n-2.583\n-0.864\n\n\ngenre[T.Comedy]\n2.4492\n0.448\n5.462\n0.000\n1.570\n3.328\n\n\ngenre[T.Drama]\n1.9952\n0.256\n7.789\n0.000\n1.493\n2.497\n\n\ngenre[T.Horror]\n-0.0215\n0.397\n-0.054\n0.957\n-0.799\n0.756\n\n\ngenre[T.Kids]\n-0.1908\n0.358\n-0.533\n0.594\n-0.893\n0.511\n\n\ngenre[T.Other]\n0.0147\n0.363\n0.041\n0.968\n-0.696\n0.726\n\n\ngenre[T.Romance]\n0.1751\n0.385\n0.455\n0.649\n-0.579\n0.929\n\n\ngenre[T.Sci-Fi]\n0.2399\n0.416\n0.577\n0.564\n-0.575\n1.055\n\n\nreview_year_0\n0.0272\n0.018\n1.529\n0.126\n-0.008\n0.062\n\n\nrelease_year_0\n0.0355\n0.011\n3.370\n0.001\n0.015\n0.056\n\n\nage_sc\n-0.2167\n0.094\n-2.296\n0.022\n-0.402\n-0.032\n\n\nlength_minutes_sc\n0.5661\n0.106\n5.360\n0.000\n0.359\n0.773\n\n\ntotal_reviews_sc\n0.9022\n0.109\n8.312\n0.000\n0.689\n1.115\n\n\nword_count_sc\n-0.5834\n0.106\n-5.522\n0.000\n-0.790\n-0.376\n\n\n\n\n\n\n\n\n\nNow that we have our model trained, we can use it to get the predicted probabilities for each observation.\n\nRPython\n\n\n\npredictions = predict(\n    model_train_class,\n    newdata = testing_data,\n    type = \"response\"\n)\n\n\n\n\npredictions = model_train_class.predict(testing_data)\n\n\n\n\nWe are going to take those probability values and make a decision to convert everything at or above .5 to the positive class (a “good” review). It is a bold assumption, but one that we will make at first!\n\nRPython\n\n\n\npredictions = ifelse(predictions &gt;= .5 , 1, 0)\n\n\n\n\npredictions = np.where(predictions &gt;= .5, 1, 0)\n\npredictions = pd.Series(predictions)\n\n\n\n\n\n2.2.2.1 Confusion Matrix\nThe confusion matrix is a table that shows the number of correct and incorrect predictions made by the model.\n\nfrom sklearn.metrics import confusion_matrix\n\nrating_cm = confusion_matrix(testing_data.rating_good, predictions)\n\nAnd here is our result:\n\n\n\n\n\n  \n    \n    \n       \n      True 0\n      True 1\n    \n  \n  \n    Predicted 0\nTN: 70\nFN: 35\n    Predicted 1\nFP: 32\nTP: 113\n  \n  \n  \n\n\n\n\n\nTN: A True Negative is an outcome where the model correctly predicts the negative class – the model correctly predicted that the review was not good.\nFN: A False Negative is an outcome where the model incorrectly predicts the negative class – the model incorrectly predicted that the review was not good.\nFP: A False Positive is an outcome where the model incorrectly predicts the positive class – the model incorrectly predicted that the review was good.\nTP: A True Positive is an outcome where the model correctly predicts the positive class – the model correctly predicted that the review was good.\n\nIn an ideal world, we would have all of our observations fitting nicely in the diagonal of that table. Unfortunately, we don’t live in the ideal world and we always have values in the off diagonal. The more values we have in the off diagonal (i.e., in the FN and FP spots), the worse our model is at classifying outcomes.\nLet’s look at some metrics that will help to see if we’ve got a suitable model or not.\nTODO: these are in the table that’s now in this chapter, so we can just use summary of confmat e.g. via yardstick and/or pycm\n\n\n2.2.2.2 Accuracy\nAccuracy is the first thing you see and the last thing that you trust! Of all the metrics to assess the quality of classification, accuracy is the easiest to cheat. If you have any class imbalance (i.e., one class within the target has far more observations than the other), you can get a high accuracy by simply predicting the majority class all of the time!\nAccuracy’s allure is in its simplicity. The accuracy is the proportion of correct predictions made by the model. It is calculated as follows:\n\\[\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\]\nFrom our table above, we can calculate the accuracy as follows- it’s just the sum of the values of the diagonal divided by the sum of all the values in the table.\n\nRPython\n\n\n\nTN = rating_cm[1]\nTP = rating_cm[4]\nFN = rating_cm[3]\nFP = rating_cm[2]\n\nsum(diag(rating_cm)) / sum(rating_cm)\n\n[1] 0.732\n\n\n\n\n\nTN = rating_cm[0][0]\nTP = rating_cm[1][1]\nFN = rating_cm[1][0]\nFP = rating_cm[0][1]\n\n(TN + TP) / (TN + TP + FN + FP)\n\n0.796\n\n\n\n\n\nTo get around the false sense of confidence that accuracy alone can promote, we can look at a few other metrics.\n\n\n\n\n\n\nAccuracy is not enough\n\n\n\nSeriously, accuracy alone should not be your sole performance metric unless you have a perfectly even split in the target! If you find yourself in a meeting where people are presenting their classification models and they only talk about accuracy, you should be very skeptical of their model; this is especially true when those accuracy values seem too good to be true. At the very least, always be ready to comapre it to the baseline rate, or prevalance of the majority class.\n\n\n\n\n2.2.2.3 Sensitivity/Recall/True Positive Rate\nSensitivity, also known as recall or the true positive rate, is the proportion of observed positives that are correctly predicted as such by the model. If you want to know how well your model predicts the positive class, sensitivity is the metric for you. It is calculated as follows:\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN}\\]\n\nRPython\n\n\n\nTP / (TP + FN)\n\n[1] 0.7635135\n\n\n\n\n\nTP / (TP + FN)\n\n0.8571428571428571\n\n\n\n\n\n\n\n2.2.2.4 Specificity/True Negative Rate\nSpecificity, also known as the true negative rate, is the proportion of actual negatives that are correctly identified as such. If you want to know how well your model will work with the negative class, specificity is a great metric. It is calculated as follows:\n\\[\\text{Specificity} = \\frac{TN}{TN + FP}\\]\n\nRPython\n\n\n\nTN / (TN + FP)\n\n[1] 0.6862745\n\n\n\n\n\nTN / (TN + FP)\n\n0.7264957264957265\n\n\n\n\n\n\n\n2.2.2.5 Precision/Positive Predictive Value\nThe precision is the proportion of positive predictions that are correct. It is calculated as follows:\n\\[\\text{Precision} = \\frac{TP}{TP + FP}\\]\n\nRPython\n\n\n\nTP / (TP + FP)\n\n[1] 0.7793103\n\n\n\n\n\nTP / (TP + FP)\n\n0.7808219178082192\n\n\n\n\n\n\n\n2.2.2.6 Negative Predictive Value\nThe negative predictive value is the proportion of negative predictions that are correct. It is calculated as follows:\n\\[\\text{NPV} = \\frac{TN}{TN + FN}\\]\n\nRPython\n\n\n\nTN / (TN + FN)\n\n[1] 0.6666667\n\n\n\n\n\nTN / (TN + FN)\n\n0.8173076923076923\n\n\n\n\n\nLet’s get a confusion matrix and stats using packages that will give us a lot of these metrics at once.\n\nRPython\n\n\n\ncm = yardstick::conf_mat(\n    tibble(\n        pred = factor(predictions), \n        y = factor(testing_data$rating_good), \n    ),\n    truth = y,\n    estimate = pred,\n)\n\nsummary(cm)\n\n\n\n\nfrom pycm import ConfusionMatrix\n\ncm = ConfusionMatrix(testing_data.rating_good.to_numpy(), predictions.to_numpy(), digit = 3)\n\npd.DataFrame(cm.overall_stat)\n\n\n\n\nSome additional measures we might look at include the following. Those with an asterisk are also in table Table 2.1.\n\nkappa: A measure of how much better the model is than random guessing.\nPrevalence: The proportion of actual positives in the data. If you don’t know this, accuracy is fairly meaningless.\nBalanced Accuracy: The average of the sensitivity (TPR) and specificity (TNR).\nF1*: The harmonic mean of precision and recall.\nAUC*: The area under the ROC curve.\n\n\n\n2.2.2.7 Ideal Decision Points\nEarlier, when we just obtained the default predicted class, we used a predicted probability value of 0.5 to establish our predicted class. That is a pretty bold assumption on our part and we should probably make sure that the cut-off value we choose is going to offer use the best performance.\nTo handle this task, we will start by creating a Receiver Operating Characteristic (ROC) curve. This curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The area under the curve (AUC) is a measure of how well the model is able to distinguish between the two classes. The closer the AUC is to 1, the better the model is at distinguishing between the two classes.\n\nRPython\n\n\n\nlibrary(pROC)\n\nprediction_prob = predict(\n    model_train_class,\n    testing_data,\n    type = \"response\"\n)\n\nroc = roc(\n  testing_data$rating_good, \n  prediction_prob\n)\n\nplot(roc)\nauc(roc)\n\n\n\n\nfrom sklearn.metrics import roc_curve, auc, RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(\n    testing_data.rating_good, \n    model_train_class.predict(testing_data)\n)\n\nRocCurveDisplay(fpr=fpr, tpr=tpr).plot()\nauc(fpr, tpr)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: ROC curve and AUC value\n\n\n\n\n\nWith ROC curves and AUC values, we can get a sense of how well our model is able to distinguish between the two classes. Now we can find the ideal cut-point for balancing the TPR and FPR.\n\nRPython\n\n\n\ncoords(roc, \"best\")\n\n  threshold specificity sensitivity\n1 0.6158066   0.8431373   0.7094595\n\n\n\n\n\ncut = thresholds[np.argmax(tpr - fpr)]\n\n\n\n\nThose coordinates are going to give us the “best” decision cut-point, though know that there are different and equally valid ways of going about choosing the ideal cutpoint. Instead of being naive about setting our probability to .5, this will give a cut-point that will lead to better classifications for our testing data. We will leave it to you to take that ideal cut-point value and update your metrics to see how much of a difference it will make.\nWhether it is a meager, modest, or meaningful improvement is going to vary from situation to situation, as will how you determine if your model is “good” or “bad”. Is this a good model? Are you more interested in correclty identifying the positive class, or the negative class? Are you more interested in true/false positives or true/false negatives? These are all questions that you will need to answer depending on the modeling context.\n\n\n\n2.2.3 Model Comparison\nAnother common way to understand our model is by looking at how it compares to other models. We can do this by comparing the metric values of our choice, for example, with RMSE or ROC. Let’s see this in action for our regression model. Here we will compare three models: one with three features, one with additional features, and the three feature model with interactions with genrea. Our goal will be to see how these perform on the test set based on RMSE.\n\nRPython\n\n\n\n# create the models\nmodel_3 = lm(\n  rating ~ \n    review_year_0 \n    + release_year_0 \n    + age_sc \n    , \n    training_data\n)\n\nmodel_7 = lm(\n  rating ~ \n    review_year_0 \n    + release_year_0 \n    + age_sc \n    + length_minutes_sc \n    + total_reviews_sc \n    + word_count_sc \n    + genre \n    , \n    training_data\n)\n\nmodel_3_int = lm(\n  rating ~ \n    review_year_0 * genre\n    + release_year_0  * genre\n    + age_sc * genre\n    , \n    training_data\n)\n\n# get the predictions\n\nresult = map(\n  list(model_3, model_7, model_3_int), \n  ~ predict(.x, newdata = testing_data)\n) |&gt; \n  map_dbl(\n    ~ yardstick::rmse_vec(testing_data$rating, .)\n  )\n\n\n\n\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import root_mean_squared_error\n\n# create the models\nmodel_3 = smf.ols(\n    formula='rating ~ review_year_0 + release_year_0 + age_sc',\n    data=training_data\n).fit()\n\nmodel_7 = smf.ols(\n    formula='rating ~ review_year_0 + release_year_0 + age_sc + length_minutes_sc + total_reviews_sc + word_count_sc + genre',\n    data=training_data\n).fit()\n\nmodel_3_int = smf.ols(\n    formula='rating ~ review_year_0 * genre + release_year_0 * genre + age_sc * genre',\n    data=training_data\n).fit()\n\n# get the predictions\nmodels = [model_3, model_7, model_3_int]\n\nresult = [\n    root_mean_squared_error(\n        testing_data.rating, \n        model.predict(testing_data[features])\n    )\n    for model in models\n]\n\n\n\n\n\ntibble(\n  model = c(\"3 features\", \"7 features\", \"3 interactions\"),\n  rmse = result\n) |&gt; \n  gt()\n\n\n\nTable 2.2: RMSE for different models\n\n\n\n\n\n\n  \n    \n    \n      model\n      rmse\n    \n  \n  \n    3 features\n0.60\n    7 features\n0.43\n    3 interactions\n0.52\n  \n  \n  \n\n\n\n\n\n\n\nIn this case, we see that the model with 7 features has the lowest RMSE, indicating that it is the best model under these circumstances. This is a simple example, but it is a typical way to compare models that you would use frequently. The same approach would work for classification models, just using an appropriate metric.\n\n\n2.2.4 Model Visualization\nWe can also visualize our model to get a better understanding of how it is performing. We started out by look at the predicted values against the observed values to see if there was any correspondence, but another key way to understand our model is to look at the residuals. Here are a couple plots that can help us understand our model:\n\nResiduals vs. Fitted: This plot shows predicted values vs. the residuals (or some variant of the residuals, like their square root). If you see a pattern, that potentially means your model is not capturing something in the data. For example, if you see a funnel shape, that means your model is not capturing the variance in the data. If you see a curve, that means their may be some underlying non-linear relationship in the data.\nTraining/Test Performance: For iterative approaches, like deep learning, we may want to see how our model is performing across iterations, typically called epochs. We can look at the training and testing performance to see if our model is overfitting or underfitting. We can actually do this with standard models as well if the estimation approach is iterative.\nPosterior Predictive Check: This is an alternative to our predicted vs. observed. We simulate the target based on the model estimates and model uncertainty, and compare that distribution to the observed target target distribution. If the two distributions are similar, then the model is doing a good job of capturing the target distribution. This plot is ubiquitous in Bayesian modeling, but can be used for any model that has uncertainty estimates or is otherwise generative.\nOthers: Other plots may look at the distribution of residuals, check for extreme values, see if there is an overabundance of zero valeus, and other issues, some of which may be specific to the type of model you are using.\n\nLet’s see this in action for our regression model. In the following we show a posterior predictive check for our regression model, and then look at the residuals vs. prediction plot.\n\nRPython\n\n\n\nperformance::check_model(model_train_reg, check = c('linearity', 'pp_check'))\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.residplot(\n    x = model_train_reg.fittedvalues, \n    y = training_data.rating, \n    lowess = True, \n    line_kws={'color': 'red', 'lw': 1}\n)\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs. Fitted')\nplt.show()\n\n\n# get the model parameters\npp = model_train_reg.model.get_distribution(\n    params = model_train_reg.params, \n    scale  = model_train_reg.scale, \n    exog   = model_train_reg.model.exog\n)\n\n# Generate 10 simulated predictive distributions\npp_samples = [pp.rvs() for _ in range(10)]\n\n# Plot the distribution of pp_samples\nfor sample in pp_samples:\n    sns.kdeplot(sample, label='pp.rvs()', alpha=0.25)\n\n# Overlay the density plot of training_data.rating\nsns.kdeplot(\n    training_data.rating.to_numpy(), \n    label='training_data.rating', \n    linewidth=2\n)\n\nplt.xlabel('Rating')\nplt.ylabel('Density')\nplt.title('Distribution of predictions vs. observed rating')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Residuals vs. Fitted and PP Check plots for our regression model\n\n\n\n\n\nIn this case we’re looking pretty good, our predictions match the target distribution well and we don’t see any patterns in the residuals vs. fitted plot on the right, which would indicate that the model is missing something important.\n\n\n\n\n\n\nTests of Assumptions\n\n\n\nFor standard GLM models there are an abundance of statistical tests available for some of these checks, for example heterogeneity of variance, or whether your residuals are normally distributed. These are not usually helpful, and often misguided. For example, if you have a large sample size, you will almost always reject the null hypothesis that your residuals are normally distributed. It also starts the turtles all the way down problem of whether you need to check the assumptions of your test of assumptions! We prefer the ‘seeing is believing’ approach. It is often pretty clear when there are model and data issues.\n\n\nAnother way we might visualize our model under certain circumstances is to look at how performance improves across iterations or epochs. This is a common approach for iterative models, like deep learning, but can also be used for standard models if the estimation approach is iterative. We can visualize performance across resamples, such as in cross-validation.\n\n\n\n\n\n\n\n\nFigure 2.4",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#sec-feature-metrics",
    "href": "knowing_models.html#sec-feature-metrics",
    "title": "2  Knowing Your Model",
    "section": "2.3 Feature Metrics",
    "text": "2.3 Feature Metrics\n\n2.3.1 Basic Model Parameters\nWe saw in the linear model chapter (1.7.1) that we can get a lot out of the basic output from standard linear models. Our starting point should be the coefficients or weights, which can give us a sense of the direction and magnitude of the relationship between the feature and the target given their respective scales. We can also look at the standard errors and confidence intervals to get a sense of the uncertainty in those estimates.\nWe get a bit more relative comparison by using standardized coefficients, or some other scaling of the coefficients that allows for a bit of a more apples-to-apples comparison. But as we’ll see, in the real world even if we have just apples, there are fuji, gala, granny smith, honeycrisp, and many other types of apples, and some may be good for snacks, others for baking pies, some are good for cider, etc. In other words, there is no one size fits all approach to understanding how a feature contributes to understanding the target, and the sooner you grasp that, the better.\n\n\n2.3.2 Feature Contributions\nWe can also look at the contribution of a feature to the model’s explanatory power, namely through its predictions. To start our discussion, we don’t want to lean very heavily on the phrase feature importance yet, because as we’ll see later, trying to rank features by an importance metric is difficult at best, and a misguided endeavor at worst. We can however look at the feature contribution to the model’s predictions, and we can come to a conclusion about whether we think a feature is practically important, but just we need to be careful about how we do it.\nTruly understanding feature contribution is a bit more complicated than just looking at the coefficient if using any model that isn’t a linear regression, and there are many ways to go about it. For example, we can’t compare raw coefficients across features, because they are on different scales. But even when we put them on the same scale, it may be very easy for some features to move, e.g., one standard deviation, and very hard for others. Binary variables can only be on or off, while numeric variables can move around more, but numeric variables may also be highly skewed. We also can’t use statistical significance based on p-values, because they reflect sample size as much or more than effect size.\nSo what are we to do? What you need to know to get started looking at a feature’s contribution includes the following:\n\nfeature scales\nfeature distributions\nrepresentative values of the feature\ntarget scale\nfeature interactions and correlations\n\nWe can’t necessarily do a whole lot about these aspects, but we can at least be aware of them, and just as importantly, we can be aware of the limitations of our understanding of these effects. In any case, let’s try to get a sense of how we can understand the contribution of a feature to our model.\n\n\n2.3.3 Marginal Effects\nOne way to understand the contribution of a feature to the model is to look at the marginal effect of the feature, which conceptually attempts to boil a feature effect to something simple. Unfortunately, not everyone means the same thing when they use this term and it can be a bit confusing. Marginal effects typically refer to a partial derivative of the target with respect to the feature. This becomes very simple for standard linear models with no interactions and all linear effects as in linear regression. The derivative of our coefficient with respect to the feature is just the coefficient itself! But for more complicated models, even just GLM’s like our logistic regression, we need to do a bit more work to get the marginal effect, or other so called ‘average’ effects. Let’s think about a couple common versions:\n\nAverage slope, Average Marginal Effect\nMarginal effect at the mean\nMarginal Means (for categorical variables)\nCounterfactuals and other predictions at key feature values\n\n\n2.3.3.1 Marginal Effects at the Mean\nFirst let’s think about an average slope. This is the average of the slopes across it’s own values or values of another feature it interacts with. But let’s just look at the effect of word count first. A good question is, how do we visualize that? Here are two plots, and both are useful, neither is inherently wrong, and yet they both tell us something different. The first plot shows the predicted probability of a good review as word count changes, with all other features at their mean (or mode for categorical). The second plot shows what is called a partial dependence plot, which shows the average predicted probability of a good review as word count changes. In both cases we make predictions with imputed values, the left plot imputes the other features to be their mean or mode, while the right plot leaves the other features at their actual values, and then, using a range for word count, gets a prediction as if every observation had that value for word count. We then average the predictions for each value in the range.\n\n\n\n\n\n\n\n\nFigure 2.5: Marginal Effect at the Mean vs. Partial Dependence Plot\n\n\n\n\n\nWhen word count is zero, i.e. its mean and everything else is at its mean/mode, we’d predict a probability of a good review of about 85%. As such, we interpret this as ‘when everything is typical’, we have a pretty good chance of getting a good review. The average prediction we’d get if we predicted every observation as if it were the mean word count is more like 55%, which is notably less. Which is correct? Both, or neither! If it’s doubtful that the feature values are realistic (e.g. everything at its mean at the same time, or an average word count when length of a movie is at its minimum) then they may both be misleading. You have to know your features and your target to know what’s realistic.\n\n\n2.3.3.2 Average Marginal Effects\nLet’s say we want to boil our understanding of the effect to a single number. In this case, the coefficient is fine if we’re dealing with an entirely linear model. In this classification case, this means that the raw coefficient tells us what we need to know, but on the log odds scale, which provides pretty much no intuition for most folks. We can understand the probability scale, but this means things get nonlinear. As an example, a .1 to .2 change in the probability is doubling, while a .8 to .9 change is a 12.5% increase. But is there anyway we can stick with probabilities and get a single value to understand the change in the probability of a good review as word count changes by 1 unit?\nYes, we can look at the average marginal effect of word count. This is the average of the slope of the predicted probability of a good review as word count changes. This is a bit more complicated than just looking at the coefficient, but it’s a bit more intuitive. How do we get it? By a neat little trick where we predict the target with the feature at two values, a very small amount, and then take the difference. This results in the the same thing as taking the derivative of the target with respect to the feature.\n\nRPython\n\n\n\nfudge_factor = 1e-3\n\nfudge_plus = predict(\n    model_train_class, \n    newdata = training_data  |&gt; mutate(word_count_sc = word_count_sc + fudge_factor/2),\n    type = \"response\"\n)\nfudge_minus = predict(\n    model_train_class, \n    newdata = training_data  |&gt; mutate(word_count_sc = word_count_sc - fudge_factor/2),\n    type = \"response\"\n)\n\n# compare\n# mean(fudge_plus - fudge_minus) / fudge_factor\n\nmarginaleffects::avg_slopes(\n    model_train_class, \n    variables = \"word_count_sc\", \n    type = 'response'\n)\n\n\n          Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 %  97.5 %\n word_count_sc  -0.0828     0.0156 -5.3   &lt;0.001 23.0 -0.113 -0.0522\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n\n\nfudge_factor = 1e-3\n\nfudge_plus = model_train_class.predict(\n    training_data.assign(word_count_sc = training_data.word_count_sc + fudge_factor/2)\n)\n\nfudge_minus = model_train_class.predict(\n    training_data.assign(word_count_sc = training_data.word_count_sc - fudge_factor/2)\n)\n\n# note that the marginaleffects is available in Python, but still very fresh!\n# we'll add a comparison in the future\n\nnp.mean(fudge_plus - fudge_minus) / fudge_factor\n\n-0.09447284318194568\n\n\n# import marginaleffects as me\n# me.avg_slopes(model_train_class, variables = \"word_count_sc\")\n\n\n\n\nOur result above suggests we’re getting about a .09 drop in the expected probability of a good review for a 1 unit increase in word count. This is a bit more intuitive than the coefficient or odds ratio based on it.\n\n\n2.3.3.3 Marginal Means\nMarginal means are just getting\n\n\n\n2.3.4 Counterfactual Predictions\nThe nice thing about having a model is that we can make predictions for any set of feature values we want. This is a great way to understand the contribution of a feature to the model. We can make predictions for a range of feature values, and then compare the predictions to see how much the feature contributes to the model. Countefactual predictions allow us to ask what if? questions, and see how the model responds. As an example, we can get a prediction as if every review was being for a drama, and then see what we’d expect if every review pertained to a comedy. This is a very powerful approach, and often utilized in causal inference, but it’s also a great way to understand the contribution of a feature to a model in general.\nPDP, SHAP values and counterfactual predictions all are ways to look at predictions for features held at key values. In an experimental setting, ideally we’d be able to look at the same instances under when everything about them was identical, but in one case, the instance was part of the control group, and in another, part of the treatment group. Not only is it impossible to have everything be identical, but it’s also impossible to have the same instance be in two groups at once. Counterfactual predictions are the next best thing though, because once we have a model, we can predict an observation as if it was in the treatment and then when it is a control. If we do this for all observations, we can get a sense of the average treatment effect, one of the main points of interest in causal inference.\nBut you don’t need an experiment for this. Let’s try a new data set to really drive the point home. We’ll use some data at the global stage- the world happiness data set. For our model we’ll predict the happiness score, considering freedom to make life choices, GDP and other things. We’ll then switch the freedom to make life choices and GDP values for the US and Russia, and see how the predictions change!\n\nRPython\n\n\n\ndf_happiness_2018 = read_csv(\"data/world_happiness_2018.csv\")\n\nmodel_happiness = lm(\n    happiness_score ~ \n    log_gdp_per_capita \n    + healthy_life_expectancy_at_birth\n    + generosity \n    + freedom_to_make_life_choices\n    + confidence_in_national_government, \n    data = df_happiness_2018\n)\n\nhappiness_gdp_freedom_values = df_happiness_2018 |&gt; \n    filter(country %in% c(\"United States\", \"Russia\"))  |&gt; \n    arrange(country) |&gt; \n    select(log_gdp_per_capita, freedom_to_make_life_choices)\n\nbase_predictions = predict(\n    model_happiness, \n    newdata = df_happiness_2018 |&gt; \n    arrange(country) |&gt;\n    filter(country %in% c(\"United States\", \"Russia\")) \n)\n\n# switch up their GDP and freedom!\n\ndf_switch = df_happiness_2018 |&gt; \n    filter(country %in% c(\"United States\", \"Russia\")) |&gt; \n    arrange(country) |&gt; # alpha so russia is first\n    mutate(\n        log_gdp_per_capita = rev(log_gdp_per_capita),\n        freedom_to_make_life_choices = rev(freedom_to_make_life_choices)\n    )\n\nswitch_predictions = predict(\n    model_happiness, \n    newdata = df_switch\n)\n\n# tibble(\n#     country = c(\"Russia\", \"USA\"),\n#     base_predictions,\n#     switch_predictions\n# ) |&gt; \n#     mutate(\n#         diff_in_happiness = switch_predictions - base_predictions\n#     )\n\n\n\n\ndf_happiness_2018 = pd.read_csv('data/world_happiness_2018.csv')\n\nmodel_happiness = smf.ols(\n    formula = 'happiness_score ~ \\\n        log_gdp_per_capita \\\n        + healthy_life_expectancy_at_birth \\\n        + generosity \\\n        + freedom_to_make_life_choices \\\n        + confidence_in_national_government',\n    data = df_happiness_2018\n).fit()\n\nmodel_happiness.summary(slim = True)\n\nhappiness_gdp_freedom_values = df_happiness_2018[\n    df_happiness_2018.country.isin([\"United States\", \"Russia\"])\n][['log_gdp_per_capita', 'freedom_to_make_life_choices']]\n\nbase_predictions = model_happiness.predict(\n    df_happiness_2018[\n        df_happiness_2018.country.isin([\"United States\", \"Russia\"])\n    ]\n)\n\n# switch up their GDP and freedom!\ndf_switch = df_happiness_2018[\n    df_happiness_2018.country.isin([\"United States\", \"Russia\"])\n].copy()\n\ndf_switch[['log_gdp_per_capita', 'freedom_to_make_life_choices']] = (\n    df_switch[['log_gdp_per_capita', 'freedom_to_make_life_choices']].values[::-1]\n)    \n\nswitch_predictions = model_happiness.predict(df_switch)\n\npd.DataFrame({\n    \"country\": [\"Russia\", \"USA\"],\n    \"base_predictions\": base_predictions,\n    \"switch_predictions\": switch_predictions,\n    \"diff_in_happiness\": switch_predictions - base_predictions\n}).round(3)\n\n\n\n\n\n\n\n\nTable 2.3: Predictions for happiness score for Russia and the US with switched freedom and GDP\n\n\n\n\n\n\n  \n    \n    \n      country\n      base_predictions\n      switch_predictions\n      diff_in_happiness\n    \n  \n  \n    Russia\n5.7\n6.4\n0.7\n    United States\n6.8\n6.1\n−0.7\n  \n  \n  \n\n\n\n\n\n\n\nIn this case, we see that the happiness score is expected to be very lopsided in favor of the US, which our base prediction would suggest the US to be almost a full standard deviation higher in happiness than Russia given their current values. But if the US was just a bit more like Russia, we’d see a significant drop even if it maintained its life expectancy, generosity, and faith in government. Likewise, if Russia was a bit more like the US, we’d expect to see a significant increase in their happiness score.\nIt’s very easy with base package functions to see some very interesting things about our data and model. Counterfactual predictions get us thinking more explicitly about what the situation would be if things were much different, but in the end, we’re just playing around with prediction and thinking about possibilities!\n\n\n2.3.5 SHAP Values\nMost models are more complicated than can be explained by a simple coefficient, e.g. nonlinear effects in generalized additive models, or there may not even be feature-specific coefficients available, like gradient boosting models, or we may even have many parameters associated with a feature, as in deep learning. Such models typically won’t come with statistical output like standard errors and confidence intervals either. But we’ll still have some tricks up our sleeve to help us figure things out!\nA very common interpretation tool is called a SHAP value. SHAP stands for SHapley Additive exPlanations, and it provides a means to understand how much each feature contributes to a specific prediction. It’s based on a concept from game theory called the Shapley value, which is a way to understand how much each player contributes to the outcome of a game. For our modeling context, SHAP values break down a prediction to show the impact of each feature. The reason we bring it up here is that it is has a nice intuition in the linear model case, and seeing it in that context is a good way to get a sense of how it works. Furthermore, it builds on what we’ve been talking about with our various prediction approaches.\nWhile the actual computations behind the scenes can be tedious, the basic idea is relatively straightforward- for a given prediction at a specific observation with set feature values, we can calculate the difference between the prediction at that observation versus the average prediction for the model as a whole. We can break this down by feature, and see how much each feature contributes to the difference, and this provides us the local effect of the feature. The SHAP approach also has the benefit of being able to be applied to any model, whether a simple linear or deep learning model. Very cool! To demonstrate we’ll use a simple model with a couple features to predict the rating of a movie review.\n\nRPython\n\n\n\nmodel_reviews_3feat = lm(\n    rating ~\n     age\n    + release_year\n    + length_minutes,\n    data = df_reviews\n)\n\n# inspect if desired\n# summary(model_reviews_3feat)\n\n\n\n\nimport statsmodels.formula.api as smf\nfrom statsmodels.formula.api import ols\n\ndf_reviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\nmodel_reviews_3feat = smf.ols(\n    formula = 'rating ~ \\\n     age \\\n    + release_year \\\n    + length_minutes',\n    data = df_reviews\n).fit()\n\n# inspect if desired\n# model_reviews_3feat.summary(slim = True)\n\n\n\n\nWith our model in place let’s look at the SHAP values for our model. We’ll start with a single feature value/observation. Here we’ll use the first observation where the release year is 2020, age of reviewer is 30, a movie length of 110 minutes. To aid our understanding, we calculate the SHAP value related to word count at that observation by hand, and using a package. The by hand approach consists of the following steps.\n\nGet the average prediction for the model\nGet the prediction for the feature at the value of interest for all observations, and average the predictions\nCalculate the SHAP value as the difference between the average prediction and the average prediction for the feature value of interest\n\nNote that this only works for our simple linear regression case, and we’d need to use a package incorporating an appropriate approach for more complicated settings. Also be aware that our focus is a marginal contribution at a single observation. Our coefficient already tells us the average contribution of a feature across all observations for this linear regression setting, i.e the AME discussed previously.\n\nRPython with SHAPPython with Dalex\n\n\n\n# first we need to get the average prediction\navg_pred = mean(predict(model_reviews_3feat))\n\n# then we need to get the prediction for the feature value of interest\n# for all observations, and average them\npred_age_30 = predict(\n    model_reviews_3feat,\n    newdata = df_reviews |&gt; mutate(age = 30) \n)\n\npred_year_2022 = predict(\n    model_reviews_3feat,\n    newdata = df_reviews |&gt; mutate(release_year = 2020) \n)\n\npred_length_110 = predict(\n    model_reviews_3feat,\n    newdata = df_reviews |&gt; mutate(length_minutes = 110) \n)\n\n\n# then we can calculate the shap values\nshap_value_ours = tibble(\n    age    = mean(pred_age_30) - avg_pred,\n    release_year   = mean(pred_year_2022) - avg_pred,\n    length_minutes = mean(pred_length_110) - avg_pred\n)\n\n# we can also use the DALEX package to do this for us\nexplainer = DALEX::explain(model_reviews_3feat, verbose = FALSE)\n\n# observation of interest we want shap values for\nobs_of_interest = tibble(\n    age = 30,\n    length_minutes = 110,\n    release_year = 2020\n)\n\nshap_value_package = DALEX::predict_parts(\n    explainer,\n    obs_of_interest,\n    type = 'shap'\n)\n\n# rbind(\n#     shap_value_ours,\n#     shap_value_package[c('age', 'release_year', 'length_minutes'), 'contribution']\n# )\n\n\n\n\n# first we need to get the average prediction\navg_pred = model_reviews_3feat.predict(df_reviews).mean()\n\n# then we need to get the prediction for the feature value of interest\n# for all observations, and average them\n\npred_age_30 = model_reviews_3feat.predict(\n    df_reviews.assign(\n        age = 30\n    )\n)\n\npred_year_2022 = model_reviews_3feat.predict(\n    df_reviews.assign(\n        release_year = 2020\n    )\n)\n\npred_length_110 = model_reviews_3feat.predict(\n    df_reviews.assign(\n        length_minutes = 110\n    )\n)\n\n# then we can calculate the shap values\nshap_value_ours = pd.DataFrame({\n    'age': pred_age_30.mean() - avg_pred,\n    'release_year': pred_year_2022.mean() - avg_pred,\n    'length_minutes': pred_length_110.mean() - avg_pred\n}, index = ['new_observation'])\n\n\n# now use the shap package for this; it does not work with statsmodels though,\n# and single feature models are a bit cumbersome, \n# but we still get there in the end!\nimport shap\nfrom sklearn.linear_model import LinearRegression\n\n# set data up for shap and sklearn\nfnames = [\n    'age', \n    'release_year', \n    'length_minutes'\n]\n\nX = df_reviews[fnames]\ny = df_reviews['rating']\n\n# use a linear model that works with shap\nmodel_reviews = LinearRegression().fit(X, y)\n\n# 1000 instances for use as the 'background distribution'\nX_sample = shap.maskers.Independent(data = X, max_samples = 1000)  \n\n# # compute the SHAP values for the linear model\nexplainer = shap.Explainer(\n    model_reviews.predict, \n    X_sample   \n)\n\n# find an index where word_count is 12\nobs_of_interest = pd.DataFrame({\n    'age': 30,\n    'release_year': 2020,\n    'length_minutes': 110\n}, index = ['new_observation'])\n\nshap_values = explainer(obs_of_interest)\n\nshap_value_package = pd.DataFrame(\n    shap_values.values[0, :], \n    index = fnames, \n    columns = ['new_observation']\n).T\n\n# pd.concat([shap_value_ours, shap_value_package])\n\n\n\nFor consistency with the R presentation and because it works with the statsmodels object, we also use Dalex here, but it is notably unweildy for doing much of anything besides the default plot, and shap is by far more popularly used in general.\n\nimport dalex as dx\n\nfnames = [\n    'age', \n    'release_year', \n    'length_minutes'\n]\n\nexplainer = dx.Explainer(\n    model_reviews_3feat, \n    data = df_reviews[fnames], \n    y = df_reviews['rating'],\n    verbose = False\n)\n\nobs_of_interest = pd.DataFrame({\n    'age': 30,\n    'release_year': 2020,\n    'length_minutes': 110\n}, index = ['new_observation'])\n\nshap_values = explainer.predict_parts(\n    new_observation = obs_of_interest,\n    type = \"shap\"\n)\n\nshap_value_package = (\n    shap_values\n    .result.iloc[:3]\n    [['variable_name', 'contribution']]\n    .set_index('variable_name')\n    .T\n)\n\npd.concat([shap_value_ours, shap_value_package])\n\n\n\n\n\n\n\n\nTable 2.4: SHAP Value Comparison\n\n\n\n\n\n\n  \n    \n    \n      source\n      age\n      release_year\n      length_minutes\n    \n  \n  \n    By Hand\n0.063\n0.206\n−0.141\n    Package\n0.063\n0.206\n−0.141\n  \n  \n  \n\n\n\n\n\n\n\nThese values are useful because they tell us how much each feature contributes to the prediction for the observation under consideration. We can visualize these as well, via a force plot or waterfall plot, the latter of which is shown below. The dotted line at E[f(x)] represents the average prediction from our model (~3.05), and the prediction we have for the observation at f(x), which is about 3.18.\nWith the average prediction as our starting point, we add the SHAP values for each feature to get the prediction for the observation. First we add the SHAP value for age, which bumps the value by 0.063, then the SHAP value for movie length, which decreases the prediction -0.141, and finally the SHAP value for release year, which brings us to the final predicted value by increasing the prediction 0.206.\n\n\n\n\n\nSHAP Visualizations\n\n\n\n\nPretty neat huh? So for any observation we want to inspect, and more importantly, for any model we might use, we can get a sense of how features contribute to that prediction. We also can get a sense of how much each feature contributes to the model as a whole by aggregating these values across all observations in our data, and this potentially provides a measure of feature importance, but we’ll come back to that in a bit.\n\n\n2.3.6 Related Visualizations\nWe’ve seen how we can get some plots for predictions in different ways previously with what’s called a partial dependence plot (Figure 2.5). A PDP shows the average prediction of a feature on the target across the feature values, which is in fact what we were just doing to calculate our SHAP value, and for the linear case, the PDP has a direct correspondence to the SHAP. As we saw, the SHAP value is the value the difference between the average prediction and the point on the PDP for a feature at a specific feature value. With regard to the PDP, this is the difference the point on the PDP and the average prediction for the model at that feature value, shown in the red line below.\n\n\n\n\n\n\n\n\nFigure 2.6: PDP, ICE, and ALE Plots\n\n\n\n\n\nWe can also look at the individual conditional expectation (ICE) plot, which is a PDP plot for a single observation, but across all values of a select feature. By looking at several observations, as in the second plot above, we can get a sense of the variability in the feature’s effect. As we can see, there is not much to tell beyond a PDP when we have a simple linear model, but it becomes more interesting when we have interactions or other nonlinearities in our model.\nIn addition, there are other plots that are similar to the PDP and ICE, such as the accumulated local effect (ALE) plot, shown last, which is a bit more robust to correlated features than the PDP plot, while also showing the general feature-target relationship. Where the PDP and ICE plots show the average effect of a feature on the target, the ALE plot focuses on average differences in predictions for the feature at a specific value, versus predictions at feature values nearby, and then centers the result so that the average difference is zero. In general, all our plots reflect the positive linear relationship between movie length and rating.\n\n\n\n\n\n\nVisualization Tools\n\n\n\nThe waterfall plot was created using DALEX, but the shap python package will also provide this. For PDP, ICE and ALE plots in R, you can look to the iml package, and in Python, the shap or scikit-learn package. Many others are available though, so feel free to explore!\n\n\n\n\n2.3.7 Global Assessment of Feature Importance\nHow important is a feature? It’s a common question, and one that is often asked of models, but the answer ranges from ‘it depends’ and ‘it doesn’t matter’. Let’s start with some hard facts:\n\nThere is no single definition of importance for any given model.\nThere is no single metric for any model that will definitively tell you how important a feature is relative to others in all data/model contexts.\nThere are multiple metrics of importance for a given model that are equally valid, but which may come to different conclusions.\nAny non-zero feature contribution is potentially ‘important’, however small.\nMany metrics of importance fail to adequately capture interactions and/or deal with correlated features.\nAll measures of importance are measured with uncertainty, and the uncertainty can be large.\nA question for feature importance is relative to… what? A poor model will still have relatively ‘important’ features, but they still may not be useful since the model itself isn’t.\nIt rarely makes sense to drop features based on importance alone, and doing so will typically drop performance as well.\nIn the end, what will you do with the information?\n\nAs we noted previously, if I want to know how a feature relates to a target, I have to know how a feature moves, and I need to know what types of feature values are more likely than others, and what a typical movement in its range of values would be. If a feature is skewed, then the even the mean may not be the best value to use for prediction, and basing ‘typical’ movement on its standard deviation may be misguided. If a unit movement in a feature results in a movement in the target of 2 units, what does that mean? Is it a large movement? If I don’t know the target very well I can’t answer that. As an example, if the target is in dollars, a $2 movement is nothing for salary, but might be large for a stock price. We have to know the target as well as we do the feature predicting it.\nOn top of all this, we need to know how the feature interacts with other features. If a feature is highly correlated with another feature, then it may not be adding much to the model even if we’d otherwise have some metrics indicating a notable contribution. In addition, penalized approaches will either spread the contribution of correlated features across them, or just pick one of them to include in the model. It may be mostly arbitrary which one is included, or you might miss both.\nIf a feature interacts with another feature, then there really is no way to say how much it contributes to the model without knowing the value of the other feature. Full stop. Synergistic effects cannot be understood by pretending they don’t exist. A number of metrics will still be provided for a single feature, either by trying to include its overall contribution or averaging over the values of the other feature, but this is a problematic approach because it ignores the other feature. As an example, if a drug doesn’t work for your age group or for someone with your health conditions, do you really care if it works ‘in general’ or ‘on average’?\nTo help us further understand this issue, consider the following two plots. On the left we show an interaction between two binary features. If we were to look at the contribution of each feature without the interaction, their respective coefficients would be estimated as essentially zero2. On the right we show a feature that has a strong relationship with the target, but only for a certain range of values. If we were to look at a single ‘effect’ of the feature, we would likely underestimate how strong it is with smaller values and overestimate the relationship at the upper range.\n\n\n\n\n\n\n\n\nFigure 2.7: Two plots showing the importance of understanding feature interactions and non-linear relationships\n\n\n\n\n\nAll this is to say as we get into measures of feature importance, we need to be very careful about how we interpret and use them!\n\n2.3.7.1 Example: Feature Importance in a Linear Model\nTo show just how difficult measuring feature importance is, we only have to stick with our simple linear regression. Think again about R2: it tells us the proportion of the target explained by our features. An ideal measure of importance would be able to tell us how much each feature contributes to that proportion, or in other words, one that decomposes R2 into the relative contributions of each feature. One of the most common measures of importance in linear models is the standardized coefficient we have demonstrated previously. You know what it doesn’t do? It doesn’t decompose R2 into relative contributions. Even the more complicated SHAP approach will not do this.\nThe easiest situation we could hope for with regard to feature importance is the basic linear regression model we’ve been using. Everything is linear, with no interactions or other things going on. And yet there are many logical ways to determine feature importance, and some even break down R2 into relative contributions, but they won’t necessarily agree with each other in ranking or relative differences. If you can get a measure of statistical difference between whatever metric you choose, it’s often the case that ‘top’ features will not be statistically different from other features. So what do we do? We’ll show a few methods here, but the main point is that there is no single answer, and it’s important to understand what you’re trying to do with the information.\nLet’s start things off by using one of our previous linear regression models with several features, but which has no interactions or other complexity (Section 1.8.1). It’s just a model with simple linear relationships and nothing else.\n\nRPython\n\n\n\nmodel_reviews_extra = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_reviews\n)\n\n\n\n\nmodel_reviews_extra = smf.ols(\n    formula = 'rating ~ \\\n        word_count \\\n        + age \\\n        + review_year \\\n        + release_year \\\n        + length_minutes \\\n        + children_in_home \\\n        + total_reviews',\n    data = df_reviews\n).fit()\n\n\n\n\nOur first metric available for us to use is just the raw coefficient value, but they aren’t comparable because the features are on very different scales- moving a unit in length is not the same as moving a unit in age. We can standardize them which helps in this regard, and you might start there despite its limitations. Another we can usecomes from the SHAP value, which provides a measure of contribution of a feature to the prediction. These can be positive or negative and are specific to the observation. But If we take the average absolute SHAP value for each feature, we get a sense of the typical contribution size for the features. We can then rank order them as accordingly. Here we see that the most important features here are the number of reviews and the length of the movie. Note that we can’t speak to direction here, only magnitude. We can also see that word count is relatively less important.\n\n\n\n\n\nSHAP Importance\n\n\n\n\nNow here are some additional methods3, some which decompose R2 (car, lmg, and pratt), and those that do not (SHAP, permutation-based, standardized coefficient squared). On the left, values represent the proportion of the R2 value that is attributable to the feature- their sum is equal to the overall R2 = 0.32. These are in agreement for the most part and seem to think more highly of word count as a feature. The others on the right are a little more varied, and only SHAP devalues word count, but possibly for good reason. Which is best? Which is correct? None. But by looking at a few of these, we can get a sense at least that total reviews, word count, release year, and length in minutes are likely useful features to our model, while age, review year, and children in home are less so, at least in the context of the model we have.\n\n\n\n\n\n\n\n\nFigure 2.8: Feature Importance by Various Methods\n\n\n\n\n\nAll of the metrics shown have uncertainty in their estimate, and some packages make it easy to plot or extract. As an example one could bootstrap a metric, or use the permutations as a a means to get at the uncertainty. However, the behavior and distribution of these metrics is not always well understood, and in some cases, the computation would often be quite a bit (e.g. with SHAP). You could also look at the range of the ranks created by bootstrapping or permuting, and take the lower bound as worst case for a given feature. Although possibly conservative, the usual problem is that people are too optimistic about their feature importance, so this might be a good thing.\nThe take home message is that in the best of circumstances, there is no automatic way of saying one feature is more important than another. It’s nice that we can use approachs like SHAP and permuation methods for more complicated models like boosting and deep learning models, but they’re not perfect, and they still suffer from most of the same issues as the linear model. In the end, understanding a feature’s role within a model is ultimately a matter of context and highly dependent what you’re trying to do with the information.\n\n\n\n\n\n\nSHAP values can be useful for observational level interpretation under the right circumstances, but they really shouldn’t be used for importance. The mean of its absolute value is not a good measure of importance except in the unlikely case you have purely balanced/symmetric features of the exact same scale, and which do not correlate with each other (or have any interactions).\n\n\n\n\n\n\n2.3.8 Feature Metrics for Classification\nAll of what’s been demonstrated for feature metrics applies to classification models. Counterfactual predictions, average marginal effects, SHAP, and permutation-based methods for feature importance would be done in the exact same way. The only real difference is of course the outcome- we’d be talking in terms of probabilities and using a different loss metric to determine importance, that sort of thing.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#wrapping-up",
    "href": "knowing_models.html#wrapping-up",
    "title": "2  Knowing Your Model",
    "section": "2.4 Wrapping Up",
    "text": "2.4 Wrapping Up\nIt is easy to get caught up in the excitement of creating a model and then using it to make predictions. It is also easy to get caught up in the excitement of seeing a model perform well on a test set. It is much harder to take a step back and ask yourself, “Is this model really doing what I want it to do?” You should always be looking at which variables are pulling the most weight in your model and how predictions are being made.\n\n2.4.1 Where to go from here\nIf you haven’t alreayd, feel free to take your linear models further in Chapter 5 and Chapter 4, where you’ll see how to handle different distributions for your target, add interactions, nonlinear effects, and more. Otherwise, you’ve got enough at this point to try your hand at the ?sec-ml section, where you can dive into machine learning!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#exercises",
    "href": "knowing_models.html#exercises",
    "title": "2  Knowing Your Model",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#additional-resources",
    "href": "knowing_models.html#additional-resources",
    "title": "2  Knowing Your Model",
    "section": "2.6 Additional Resources",
    "text": "2.6 Additional Resources\nIf this chapter has piqued your curiosity, we would encourage you to check out the following resources.\nEven though we did not use the mlr3 package in this chapter, the Evaluation and Benchmarking chapter of the companion book, Applied Machine Learning Using mlr3 in R, offers a great conceptual take on model metrics and evaluation.\nFor a more Pythonic look at model evaluation, we would highly recommend going through the sci-kit learn documentation on Model Evaluation. It has you absolutely covered on code examples and concepts.\nTo get the most out of DaLEX visualizations, check out the authors’ book Explanatory Model Analysis.\nWe also recommend checking out Christoph Molnar’s book, Interpretable Machine Learning. It is a great resource for learning more about model explainers and how to use them, and provides a nice package that has a lot of the functionality we’ve shown here.\nThe marginal effects zoo, written by the marginaleffects package author, is your goto for getting started with marginal effects, but we also recommend the excellent blog post by Andrew Heiss as a very nifty overview and demonstration.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#exercise",
    "href": "knowing_models.html#exercise",
    "title": "2  Knowing Your Model",
    "section": "2.7 Exercise",
    "text": "2.7 Exercise",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#footnotes",
    "href": "knowing_models.html#footnotes",
    "title": "2  Knowing Your Model",
    "section": "",
    "text": "For anyone comparing Python to R results, the data splits are not the same so outputs likewise will not be identical, though they should be very similar.↩︎\nTo understand why, for the effect of X1, just take the mean of the two points on the left vs. the mean of the two points on the right. It would basically be a straight line of no effect as you move from group 0 to group 1. For the effect of X2, the two group means for A and B would be at the intersection of the two lines.↩︎\nThe car, lmg, pratt, and beta-squared values were provided by the relaimpo package in R. See the documentation there for details. Permutation based importance was provided by the iml package, though we supplied a custom function to base it on the drop in R-squared. SHAP values were calculated using the fastshap package.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What Is This Book?\nThis book is a practical resource, something we hope you can refer to for a quick overview of a specific modeling technique, a reminder of something you’ve seen before, or perhaps a sneak peak into some modeling details. The text is focused on a few statistical and machine learning approaches that are widely employed, and specifically those which form the basis for most other models in use. Believe it or not, whether a lowly t-test or a complex neural network, there is a tie that binds. We hope to help you understand some of the core modeling principles, and how the simpler models can be extended and applied to a wide variety of data scenarios.\nOur approach here is first and foremost a practical one, as models themselves are just tools to help us reach a goal. If a model doesn’t work in the world, it’s not very useful. But modeling is often a delicate balance of interpretation and prediction, and each data situation is unique in some way, requiring a bespoke approach. What works well in one setting may be poor in another, and what may be the state of the art may only be marginally better than a notably simpler approach that is far more interpretable. In addition, complexities arise even in an otherwise deceptively simple application. However, if you have the core understanding of the techniques that lie at the heart of many models, you’ll automatically have many more tools at your disposal to tackle the problems you face, and be more comfortable with choosing the best for your needs.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#who-should-use-this-book",
    "href": "introduction.html#who-should-use-this-book",
    "title": "Introduction",
    "section": "Who Should Use This Book?",
    "text": "Who Should Use This Book?\nThis book is intended for every type of data dabbler, no matter what part of the data world you call home. If you consider yourself a data scientist, a business analyst, or a statistical hobbyist, you already know that the best part of a good dive into data is the modeling. But whatever your data persuasion, models give us the possibility to answer questions, make predictions, and understand what we’re interested in a little bit better. And no matter who you are, it isn’t always easy to understand how the models work. Even when you do get a good grasp of a modeling approach, it can still get complicated, and there are a lot of details to keep track of. In other cases, maybe you just have other things going on in your life and have forgotten a few things. We find that it’s always good to remind yourself of the basics! So if you’re just interested in data and hoping to understand it a little better, then it’s likely you’ll find something useful in this book!\nYour humble authors have struggled mightily themselves throughout the course of their data history, and still do! We were initially taught by those that weren’t exactly experts, and often found it difficult to get a good grasp of statistical modeling and machine learning. We’ve had to learn how to use the tools, how to interpret the results, and possibly the most difficult, how to explain what we’re doing to others! We’ve forgotten a lot, confused ourselves, and made some happy accidents in the process. That’s okay! Our goal here is to help you avoid some of those pitfalls, help you understand the basics of how models work, and get a sense of how most modeling endeavors have a lot of things in common.\nWhether you enthusiastically pour over formulas and code, or prefer to skip over them, we promise that you don’t need to memorize a formula to get a good understanding of modeling and related issues. We are the first to admit that we have long dumped the ability to pull formulas out of our brain folds1; however, knowing how those individual pieces work together only helps to deepen your understanding of the model. Typically using code puts the formula into more concrete terms that you can then use in different ways to solidify and expand your knowledge. Sometimes you just need a reminder or want to see what function you’d use. And often, the visualization will reveal even more about what’s going than the formula or the code. In short, there are a lot of tools at your disposal to help learn modeling in a way that works for you. We hope that anyone that would be interested in the book will find a way to learn things in a manner that suits them best.\nThere is bit of a caveat. We aren’t going to teach you basic statistics or how to program in R or Python. Although there is a good chance you will learn some of it here, you’ll have an easier time if you have a very basic understanding of statistics and some familiarity with coding. We will provide some resources for you to learn more about these topics, but we won’t be covering them in detail. The (appendix?) will provide some more information about prerequisites or just stuff that would be good to know. However, we really aren’t assuming a lot of conceptual knowledge, and are, if anything, assuming that whatever knowledge you have may be a bit loose or fuzzy. That’s okay!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#what-can-you-expect",
    "href": "introduction.html#what-can-you-expect",
    "title": "Introduction",
    "section": "What Can You Expect?",
    "text": "What Can You Expect?\nFor each model that we cover, you can expect the following in terms of content:\n\nOverview\n\nWhy it’s useful\nConceptual example and interpretation\nWhere the model lies in the grand scheme of topics that we will cover\n\nKey ideas and concepts\n\nBrief summary and definition list of concepts or terms\n\nDemonstration with data, code, results, and visualizations\n\nThe data will often be simulations as that opens doors for further understanding, or a dataset that hopefully is a little more interesting than mtcars or iris.\nThe demonstrations will provide you the opportunity to get your hands as dirty as you wish. We will present the code in two ways:\n\nstandard functions (e.g., lm in R, ols in statsmodels for Python)\nthe steps to recreate the estimation process on your own (or at least something a little more hands-on)\n\n\nCommentary, cautions, and where to explore next\n\nWe are taking this approach for one reason: so that you can go as deep as you wish. If you are looking for a quick tutorial on helpful models, then you might not find yourself going any deeper than the standard functions (or even getting into the code at all). If you want to really dive into these models, then you might find yourself working through the complete steps. Another approach is to allow yourself some time between the standard functions and complete steps. You could work through the standard functions of every chapter, give it some time to marinate, and then work back through the complete steps. While we certainly recommend working through the chapters in order, we want to give you the flexibility to choose your own depth within each.\nWe hope that this book can serve as a “choose your own adventure” statistical reference. Whether you want a surface-level understanding, a deeper dive, or just want to be able to understand what the analysts in your organization are talking about, you will find value in this book. While we assume that you have a basic familiarity with coding, that doesn’t mean that you need to work through every line of code to understand the fundamental principles and use cases of every model.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#which-language",
    "href": "introduction.html#which-language",
    "title": "Introduction",
    "section": "Which Language?",
    "text": "Which Language?\nYou’ve probably noticed most books, blogs, and courses on data science choose R or Python. While many individuals often take an opinionated approach towards teaching and using one over the other, we eschew dogmatic approaches and language flame wars. R and Python are both great languages (and equally flawed in unique ways), and it is advantageous to be knowledgeable of both, even if you focus on one specifically, as they are the most popular languages for statistical modeling and machine learning. We use both extensively in our own work for teaching, personal use, and production level code, and have found both are up to whatever task you have in mind. Throughout this book, we will be presenting demonstrations in both R and Python, and you can use both or take your pick, but we want to leave that choice up to you. Our goal isn’t to obscure the ideas behind packages and specialty functions or tell you why one languages is superior to the other (they aren’t), but to show you the most basic functions behind big model ideas.\nWhile we want to provide you choice, we truly hope that displaying both languages can help people to “convert” from one to the other. We have spent countless hours, slumped over our computers, debugging errors and figuring things out. If we can take away one source of pain for you, that would be great! We’d like to consider this as a resource for the R user, who knows exactly what they want to do in R, but could use a little help translating their R knowledge to Python; we’d also like this book to be a resource for the Python user, who sees the value in R’s statistical modeling abilities.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#choose-your-own-adventure",
    "href": "introduction.html#choose-your-own-adventure",
    "title": "Introduction",
    "section": "Choose Your Own Adventure",
    "text": "Choose Your Own Adventure\nAs an example of how things will go, let’s look at the different ways we might express a relationship between two variables. If you want just a little bit of background, you can read through the Overview and Key Ideas. If you want to see the easy way to use the model, you can work through the Standard Function section. If you want to see the method from start to finish, feel free to work through the Roll Your Own section. If all of that sounds like a lot of work and you’d rather look at the pretty pictures, that’s fine too2, just jump to the Visualization section.\n\nOverview\nCorrelations provide a means of understanding how, and if, two or more things are related. For any two variables or features, we can estimate a single value that signals the strength and direction of the relationship between them. Despite limitations, correlations can be a great way to get a quick understanding of the relationship between two features. One variable could be temperature while another variable is the number of ice cream cones sold, or one is the number of hours spent studying and another variable is the grade on a test, maybe the number of hours spent watching TV versus the number of hours spent exercising. The correlation value will give us information that is similarly interpretable in each case.\n\n\nKey Ideas\nHere are key ideas to consider for understanding correlation.\n\nVariance: Two variables must vary if they are to co-vary\nCovariance: Joint variability of two variables, i.e. how they vary together\nInterpretation: Covariance is hard to interpret because the variables are typically on different scales\nCorrelation: Correlation is a standardized covariance, so it is easier to interpret\n\nAnother way to think about correlation is through the lens of variance covariance. Covariance is a measure of the joint variability of two variables, i.e. how they vary together. If two variables are highly correlated, then they have a high degree of covariance, i.e. they vary together in a meaningful way.\n\n\nDemonstration\nTo demonstrate correlation, we could start by formally defining it. Here is a formula for the Pearson-Product-Moment correlation coefficient. What everyone typically just calls “correlation” or Pearson’s r is actually the Pearson Product-Moment Correlation – that is a lot of words, though, so we will just go with correlation. As we mentioned earlier, we want to give you the choice to dive in as deep as you want.\n\\[\n\\rho = \\frac{\\Sigma(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\Sigma(x_i-\\bar{x})^2\\Sigma(y_i-\\bar{y})^2}}\n\\tag{1}\\]\nIn words, starting with the numerator, we are going to subtract the mean of x from every observation of x, do the same for y, multiply them together, and then sum them to produce a single value. If you’ve taken a statistics class before, you might have seen this part of the formula when talking about covariance – the joint variability of two variables. In fact, if you just divide the numerator by n-1 you’d get the covariance value. Covariance values, though, are not standardized; therefore, they are a bit tough to interpret. You will get a sign indicating relationship (just like a correlation) and the magnitude tells you the strength of the relationship, but it is hard to get your head around how much stronger a covariance of 4.00 is compared to a covariance of 0.28.\nThis is where the denominator comes into play. To put it into real terms, we will subtract the mean of x from every observation of x (i.e. get the deviations of x), square those values, and sum those values to produce a single value. We will do the same for y, multiply those results, and then take the square root of that product. The denominator might also look familiar to some. The individual pieces- \\(\\Sigma(x_i-\\bar{x})^2\\) and \\(\\Sigma(y_i-\\bar{y})^2\\) - are computing the variance of x and y. Taking the square root of variance gives us the standard deviation. Basically, we’re getting a combined standard deviation to scale the covariance. Using the standard deviation will mitigate the effects of x and y potentially being on different scales; in other words, we want to know how much two variables move together, even if the means and typical movement (deviation) might be very different.\nHere is a simplification, depicting correlation as a standardized covariance.\n\\[\n\\rho = \\frac{cov(x,y)}{\\sigma_x\\sigma_y}\n\\tag{2}\\]\nwhere \\(cov(x,y)\\) is the covariance between \\(x\\) and \\(y\\), and \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively. So putting it all together, we are simply looking at the ratio of raw covariance (i.e., how much two variables “move” together) and standard deviation (i.e., the amount of dispersion within a variable). Just as we mentioned earlier, simpler concepts often tie models together! What might look like a tricky formula, really just starts with means and deviations, variance and covariance!\nAs a reminder, this correlation will give us an idea about the linear relationship between two continuous variables; we will get a value between -1 and 1, with values closer to 0 indicating that there is no linear relationship between the two variables. As values get closer to 1, we have a positive correlation – as values for one variable increase, values for the other variable tend to increase along with it. As the correlation gets closer to -1, we have a negative correlation – as values for one variable increase, values for the other variable typically decrease. Correlations can be useful for quickly exploring linear relationships, but let’s not get too excited about it – they aren’t going to help you answer any big questions! We also don’t want to get too carried away with “statistical significance” yet – once samples get large, even small correlations become “significant”. Instead, just use correlations to explore the patterns within your data, start getting ideas about interesting relationships that you might find, and leave worries about significance for people with more time on their hands.\n\nCode\nIt’s often easier to understand a concept by seeing it in action. So let’s start by creating some data. We’ll create a variable x, then make a y that will have a linear relationship with it, but also have some random noise. We’ll then plot the two variables to see what they look like. To help your understanding, fiddle with the knobs noted.\n\nRPython\n\n\n\nset.seed(seed = 1001)\nN = 500\nx = rnorm(n = N, mean = 0, sd = 1)\n\n# Fiddle with the .5 and .75.\n# The first can also be negative if you like!\ny = .5 * x + rnorm(n = N, mean = 0, sd = .75)\n\n\n\n\nimport numpy as np\n\nnp.random.seed(seed = 1001)\nN = 500\nx = np.random.normal(loc = 0, scale = 1, size = N)\n\n# Fiddle with the .5 and .75. \n# The first can also be negative if you like!\ny = .5*x + np.random.normal(loc = 0, scale = .75, size = N) \n\n\n\n\nNow check out the plot of those two values:\n\n\n\n\n\nScatterplot of two variables.\n\n\n\n\nRemember that correlation is testing for the presence of a linear relationship, with -1 indicating a perfect negative relationship, 1 indicating a perfect positive relationship, and 0 indicating no relationship. Before we see the actual correlation value for these two variables, take a guess as to what value we are going to get!\nBefore we create our own function, we can use R’s cor function or numpy’s corrcoef function. You should get something around 0.6.\n\n# Results for R and Python will be slightly\n# different due to different random number generators\ncor(x, y)\nnp.corrcoef(x, y)\n\n\n\n[1] 0.557\n\n\nWhen you guessed the value, were you close? If so, congrats! If not, try fiddling with those knobs noted until things get a little clearer. But now that we already know the answer, let’s make sure that we can get the same answer by working through the formula via code. The following takes that initial formula approach and turns it into a function that we can use to compute the correlation between any two variables. We’ll then use that function to compute the correlation between our x and y variables.\n\nRPython\n\n\n\nmy_cor = function(x, y) {\n    # First, we need to compute the averages for x and y.\n    # The rest follows the formula.\n    x_bar = mean(x)\n    y_bar = mean(y)\n\n    numerator = sum((x - x_bar) * (y - y_bar))\n\n    denominator = sqrt(\n        sum((x - x_bar)^2) * sum((y - y_bar)^2)\n    )\n\n    numerator / denominator\n}\n\n# using the builtin functions\nmy_cor2 = function(x, y) {\n    cov(x, y) / (sd(x) * sd(y))\n}\n\nmy_cor(x, y)\n\n[1] 0.557\n\n\n\n\n\ndef my_cor(x, y):\n  # First, we need to compute the averages for x and y.\n  # The rest follows the formula.\n    x_bar = np.mean(x)\n    y_bar = np.mean(y)\n    \n    numerator = np.sum((x - x_bar) * (y - y_bar))\n    \n    denominator = np.sqrt(\n      np.sum((x - x_bar)**2) * np.sum((y - y_bar)**2)\n    )\n\n    # We will finish by dividing the numerator by the \n    # denominator.\n    # This will ensure that we have a value between -1 and 1.\n    return(numerator / denominator)\n\nmy_cor(x, y)\n\n0.5390318454354402\n\n\n\n\n\nIt doesn’t matter which language we use, the steps are largely the same when we break it down into the individual pieces!\n\n\n\nVisualization\nA long time ago, in a land far away, the authors of this book worked together to help clients traverse the forests of data to reach their modeling goals. While there were many great learning opportunities along the way, working with clients showed us the kinds of help that people really needed in adventuring with data and models. Even so, there were many requests that made us grimace, and one stood atop Mount Ridiculous: to produce a correlation matrix with 115 variables and export that matrix to a spreadsheet. We still don’t recommend such shenanigans, but there are ways to try and understand correlation matrices. Since we were in the business of helping people do their work better, one way we often did so was via a corrplot.\nWe’ll start with a something manageable. We create a data set with six variables of two sets: a, b , c, and x, y, z, and then we can take a quick look at the correlation matrix.\n\n\n\n\n\n\n\nCorrelation matrix\n\n\nfeature\na\nb\nc\nx\ny\nz\n\n\n\n\na\n1.00\n0.46\n0.45\n−0.16\n−0.21\n−0.19\n\n\nb\n0.46\n1.00\n0.49\n−0.07\n−0.18\n−0.14\n\n\nc\n0.45\n0.49\n1.00\n−0.16\n−0.23\n−0.20\n\n\nx\n−0.16\n−0.07\n−0.16\n1.00\n0.49\n0.48\n\n\ny\n−0.21\n−0.18\n−0.23\n0.49\n1.00\n0.52\n\n\nz\n−0.19\n−0.14\n−0.20\n0.48\n0.52\n1.00\n\n\n\n\n\n\n\n\nNow we have the pairwise correlations between all six of our variables, with 1’s on the diagonals (naturally, a variable has a perfect correlation with itself). You can check out the lower diagonal or the upper diagonal, because they contain the exact same information. Quickly, though, find the interesting pattern in that matrix!\nProducing the correlations between just 6 variables gives us 15 correlation coefficients to examine! You can see that you’ll need to spend more than a few seconds on finding the interesting patterns within the data (or if there are any patterns at all). Our brains are oriented towards vision, so we can use preattentive processing elements, like hue, saturation, and size, to make finding interesting patterns easier.\nSince we already have a correlation matrix, we can use various means to find those patterns, which include visualizing the matrix itself, network graphs and others. Here is one way to do it.\n\n\n\n\n\n\nSome recommended R packages for visualizing a correlation matrix include corrr, ggcor, and corrplot. For python, one has options for seaborn, pandas, biokit, and others.\n\n\n\n\n\n\n\n\nCorrelation matrix visualization\n\n\n\n\nLet’s break down what we’re seeing just a little bit. The lower triangle has the correlation values. It adds information, though, by changing the hue by correlation value – red for negative values and blue for positive values – and increasing the saturation as the correlation value becomes stronger. The upper triangle contains the same information, but the size of the circle is tied directly to the strength of the correlation. You’ll also notice that the weaker correlations are more hidden in the visualization, allowing us to focus only on those interesting relationships.\nWhat do you think? Was it easier to spot the points of interest? It looks like there is an a-b-c group and x-y-z group that are similarly correlated within their respective groups. We can also see that the a-b-c group is negatively correlated with the x-y-z group. Visualizing the correlation matrix can usually make it easier to find those interesting patterns.\n\n\nCommentary\nThe correlation is a starting point for understanding linear models, which serve as the foundation for modeling in general. It is very limited by only assessing linear relationships between variables, as well as only pairwise relationships. Other metrics can overcome these, but they have their own limitations. The basic Pearson-Product-Moment correlation coefficient is still the most widely used and a typical starting point in many data adventures.\nThings to explore further next:\n\nRank correlation (e.g., Spearman’s rho, Kendall’s tau)\nDistance metrics (e.g., euclidean, manhattan, cosine)\nNon-linear relationships and interactions (e.g., distance correlation, polynomial, splines)\nMultivariate relationships (e.g., partial correlations, r-squared)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#moving-towards-an-excellent-adventure",
    "href": "introduction.html#moving-towards-an-excellent-adventure",
    "title": "Introduction",
    "section": "Moving Towards An Excellent Adventure",
    "text": "Moving Towards An Excellent Adventure\nRemember the point we made about “choosing your own adventure”? Statistical modeling and programming is an adventure, even if you never leave your desk! Every situation calls for choices to be made and every choice you make will lead you down a different path. You will run into errors, dead ends, and you might even find that you’ve spent considerable time to conclude that nothing interesting is happening in your data. This, no doubt, is part of the fun and all of those struggles make success that much sweeter. Like every adventure, things might not be immediately clear and you might find yourself in perilous situations! If you find that something isn’t making sense upon your first read, that is okay. Both authors have spent considerable time mulling over models and foggy ideas during our assorted (mis)adventures; nobody should expect to master complex concepts on a single read through! In any arena where you strive to develop skills, distributed practice and repetition are essential. When concepts get tough, step away from the book, and come back with a fresh mind.\nThanks for coming on this adventure with us and welcome to our Book of Models.\nThis is a test reference (hastie_elements_2009?).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "We actually never had this ability.↩︎\nWe love pretty pictures too! Sometimes it’s the best way to bring a lot of ideas together.↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "linear_models.html",
    "href": "linear_models.html",
    "title": "1  The Foundation",
    "section": "",
    "text": "1.1 Introducing the Greatest Of All Time\nNow that you’re here, it’s time to dive in! We’ll start things off by covering the building block of all modeling, and a solid understanding here will provide you the basis for just about anything that comes after, no matter how complex it gets. The linear model is our starting point. At first glance, it may seem like a very simple model, and it is in some ways, relatively speaking. But it’s also quite powerful and flexible, able to take in different types of inputs, handle nonlinear relationships, temporal and spatial relations, clustering, and more. Linear models have a long history, with even the formal and scientific idea behind correlation and linear regression being well over a century old1! And in that time, the linear model is far and away the most used model out there. But before we start talking about the linear model, we need to talk about what a model is in general.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-key-ideas",
    "href": "linear_models.html#sec-lm-key-ideas",
    "title": "1  The Foundation",
    "section": "1.2 Key Ideas",
    "text": "1.2 Key Ideas\nTo get us started, we can pose a few concepts key to understanding models. We’ll cover each of these as we go along.\n\nWhat a model is: The model as an idea\nFeatures, targets, and input-output mappings: how do we get from input to output?\nPrediction: how do we use a model?\nInterpretation: what does a model tell us?\n\nPrediction underlies all interpretation\nWe can interpret a model at the feature level and as a whole\n\n\nAs we go along and cover these concepts, be sure that you feel you have the ‘gist’ of what we’re talking about. Almost everything of what comes after linear models builds on these ideas, so it’s important to have a firm grasp before climbing to new heights.\n\n\n1.2.1 Why this matters\nThe basic linear model and how it comes about underpins so many models, from the simplest t-test to the most complex neural network. It takes a bit to get used to the important aspects of it, but it provides a powerful foundation, and one that you’ll see in many different contexts. It’s also a model that is relatively easy to understand, and one that you can use to understand other models. So it’s a great place to start!\n\n\n1.2.2 Good to know\nWe’re just starting out here, but we’re kind of assuming you’ve had some exposure to to the idea of statistical or other models, even if only from an interpretation standpoint. We assume you have understanding of basic stats like central tendency (e.g., a mean or median) and correlation, stuff like that. And if you intend to get into the code examples, you’ll need a basic familiarity to with Python or R.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-what-is-a-model",
    "href": "linear_models.html#sec-lm-what-is-a-model",
    "title": "1  The Foundation",
    "section": "1.3 What is a Model?",
    "text": "1.3 What is a Model?\nAt its core, a model is just an idea. It’s a way of thinking about the world, about how things work, how things change over time, how things are different from each other, and how they are similar. The underlying thread is that a model expresses relationships about things in the world around us. One can also think of a model as a tool, one that allows us to take in information, derive some meaning from it, and act on it in some way. Just like other ideas and tools, models have consequences in the real world, and they can be used wisely or foolishly.\nOn a practical level, a model is expressed through a particular language, math, but don’t let that worry you if you’re not so inclined. As it’s still just an idea at its core, the idea is the most important thing to understand about a model. The math is just a formal way of expressing the idea in a manner that can be communicated and understood by others in a standard way, and math can help make the idea precise. But in everyday terms, we’re trying to understand things like how the amount of sleep relates to cognitive functioning, how the weather affects the number of people who visit a park, how much money to spend on advertising to increase sales, how to detect fraud, and so on. Any of these could form the basis of a model, as they stem from scientifically testable ideas, and they all express relationships between things we are interested in, possibly even with an implication of causal relations.\nActually applying models to data can be simple. For example, if you wanted to run a linear model to understand the relationship between sleep and cognitive functioning, you might express it in code as:\n\nRPython\n\n\n\nlm(cognitive_functioning ~ sleep, data = df)\n\n\n\n\nfrom statsmodels.formula.api import ols\n\nmodel = ols('cognitive_functioning ~ sleep', data = df).fit()\n\n\n\n\nThe first part with the ~ is the model formula, which is how math comes into play to help us express relationships. Beyond that we just specify where, for example, the scores for cognitive functioning and the amount of sleep is, in this case, in the same data frame called df, which may have been imported from a spreadsheet somewhere. Very easy! But that’s all it takes to express a straightforward idea. More conceptually, we’re saying that cognitive functioning is a linear function of sleep. You can probably already guess why R’s function is lm,but you’ll eventually also learn why statsmodels function is ols, but for now just know that both are doing the same thing.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-in-a-model",
    "href": "linear_models.html#sec-lm-in-a-model",
    "title": "1  The Foundation",
    "section": "1.4 What goes into a model?",
    "text": "1.4 What goes into a model?\n\n1.4.1 Features and Targets\nIn the context of a model, how we specify the nature of the relationship depends on the context. In the interest of generality, we’ll refer to the target as what we want to explain, and features as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. The table below shows some of the common terms used to refer to features and targets.\n\n\n\n\nTable 1.1: Common Terms for Features and Targets\n\n\n\n\n\n\n  \n    \n    \n      Feature\n      Target\n    \n  \n  \n    independent variable\ndependent variable\n    predictor variable\nresponse\n    explanatory variable\noutcome\n    covariate\nlabel\n    x\ny\n    input\noutput\n    right-hand side\nleft-hand side\n  \n  \n  \n\n\n\n\n\n\n\nSome of these actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but we’ll typically avoid those terms if we can. In the end, we may use any of these words to describe things so that you are comfortable with the terminology, but typically we’ll stick with features and targets for the most part. In our opinion, this terminology has the least hidden assumptions/implications, and just implies ‘features of the data’ and the ‘target’ we’re trying to explain or predict\n\n\n1.4.2 Expressing Relationships\nAs noted, a model is a way of expressing a relationship between a set of features and a target, and one way of thinking about this is in terms of inputs and outputs. But how can we go from input to output? Well to begin, we assume that the features and target are correlated, that there is some relationship between the feature x and target y. If so, then we can ultimately use the features to predict the target. In the simplest setting, a correlation implies a relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down (right plot).\n\n\n\n\n\n\n\n\nFigure 1.1: Correlation\n\n\n\n\n\nIn addition, the typical correlation suggests a linear relationship, one that is adequately captured by a straight line. There are many types of correlation metrics, but the most common one, the Pearson correlation, is explicitly a measure of the linear relationship between two variables. It’s expressed as a number between -1 and 1, where 0 means there is no linear relationship. As we move closer to a 1.0 correlation value, we would see a tighter scatterplot like the one on the left, until it became a straight line. The same happens for the negative relationship as we get closer to a value of -1. If we have only one feature and target, the Pearson correlation reflects the exact result of the linear model we’d conduct in a more complicated fashion. But even with multiple features, we often use a version of the Pearson R to help us understand how the features account for the target’s variability.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-linear-model",
    "href": "linear_models.html#sec-lm-linear-model",
    "title": "1  The Foundation",
    "section": "1.5 THE Linear Model",
    "text": "1.5 THE Linear Model\nThe linear model is perhaps the simplest functional model we can use to express a relationship between features and targets. And because of that, it’s possibly still the most common model used in practice, and it is the basis for many types of other models. Why don’t we run one now?\nThe following dataset has individual movie reviews containing the movie rating (1-5 stars scale), along with features pertaining to the review (e.g., word count, etc.), those that regard the reviewer (e.g., age) and features about the movie (e.g., genre, release year).\nFor our first linear model, we’ll keep things simple. Let’s predict the rating from the length of the review in terms of word count. We’ll use the lm() function in R and the ols() function in Python2 to fit the model. Both functions take a formula as the first argument, which is a way of expressing the relationship between the features and target. The formula is expressed as y ~ x1 + x2 + ..., where y is the target name and x are the feature names. We also need to specify what the data object is, typically a data frame.\n\nRPython\n\n\n\ndf_reviews = read_csv(\"data/movie_reviews.csv\")\n\nmodel_reviews = lm(rating ~ word_count, data = df_reviews)\n\nsummary(model_reviews)\n\n\nCall:\nlm(formula = rating ~ word_count, data = df_reviews)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0648 -0.3502  0.0206  0.3352  1.8498 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.49164    0.04236    82.4   &lt;2e-16 ***\nword_count  -0.04268    0.00369   -11.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.591 on 998 degrees of freedom\nMultiple R-squared:  0.118, Adjusted R-squared:  0.118 \nF-statistic:  134 on 1 and 998 DF,  p-value: &lt;2e-16\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ndf_reviews = pd.read_csv('data/movie_reviews.csv')\n\nmodel_reviews = smf.ols('rating ~ word_count', data = df_reviews).fit()\n\nmodel_reviews.summary(slim = True)\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.118\n\n\nModel:\nOLS\nAdj. R-squared:\n0.118\n\n\nNo. Observations:\n1000\nF-statistic:\n134.1\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n3.47e-29\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.4916\n0.042\n82.431\n0.000\n3.409\n3.575\n\n\nword_count\n-0.0427\n0.004\n-11.580\n0.000\n-0.050\n-0.035\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nFor such a simple model, we certainly have a lot to unpack here! Don’t worry, you’ll eventually come to know what it all means. But it’s nice to know how easy it is to get the results! For now we can just say that there’s a negative relationship between the word count and the rating, and that the value regarding relationship is statistically significant.\nGetting more into the details, we’ll start with the fact that the linear model posits a linear combination of the features. This is an important concept to understand, but really, a linear combination is just a sum of the features, each of which has been multiplied by some specific value. That value is often called a coefficient, or possibly weight, depending on the context. The linear model is expressed as (math incoming!):\n\\[\ny = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n\n\\tag{1.1}\\]\n\n\\(y\\) is the target.\n\\(x_1, x_2, ... x_n\\) are the features.\n\\(b_0\\) is the intercept, which is kind of like a baseline value or offset. If we had no features at all it would just be the mean of the target.\n\\(b_1, b_2, ... b_n\\) are the coefficients or weights for each feature.\n\nBut lets start with something simpler, let’s say you want to take a sum of several features. In math you would write it as:\n\\[\nx_1 + x_2 + ... + x_n\n\\]\nIn the previous equation, x is the feature and n is the number identifier for the features, so \\(x_1\\) is the first feature, \\(x_2\\) the second, and so on. \\(x\\) is an arbitrary designation, you could use any letter, symbol you want, or even better, would be the actual feature name. Now look at the linear model.\n\\[\ny = x_1 + x_2 + ... + x_n\n\\]\nIn this case, the function is just a sum, something so simple we do it all the time. In the linear model sense though, we’re actually saying a bit more. Another way to understand that equation is that y is a function of x. We don’t show any coefficients here, i.e. the bs in our initial equation, but technically it’s as if each coefficient was a value of 1. In other words, for this simple linear model, we’re saying that each feature contributes in an identical fashion to the target.\nIn practice, features will not contribute in the same ways, because they correlate with the target differently, or are on different scales. So if we want to relate some feature, \\(x_1\\), and some other feature, \\(x_2\\), to target \\(y\\), we probably would not assume that they both contribute in the same way from the beginning. We might give relatively more weight to \\(x_1\\) than \\(x_2\\). In the linear model, we express this by multiplying each feature by a different coefficient. So the linear model is really just a sum of the features multiplied by their coefficients, i.e. a weighted sum. In fact, we’re saying that each feature contributes to the target in proportion to the coefficient. So if we have a feature \\(x_1\\) and a coefficient \\(b_1\\), then the contribution of \\(x_1\\) to the target is \\(b_1*x_1\\). If we have a feature \\(x_2\\) and a coefficient \\(b_2\\), then the contribution of \\(x_2\\) to the target is \\(b_2 * x_2\\). And so on. So the linear model is really just a sum of the features multiplied by their respective weights.\nFor our model, here is the mathematical representation:\n\\[\n\\textrm{rating} = b_0 + b_1 \\cdot \\textrm{word\\_count}\n\\]\nAnd with the actual results of our model:\n\\[\n\\textrm{rating} = 3.49 + -0.04 \\cdot \\textrm{word\\_count}\n\\]\nNot too complicated we hope! But let’s make sure we see what’s going on here just a little bit more.\n\nOur idea is that the length of the review is in some way related to the eventual rating given to the movie.\nOur target is rating, and the feature is the word count\nWe map the feature to the target via the linear model, which provides an initial understanding of how the feature is related to the target. In this case, we start with a baseline of 3.49. This value makes sense only in the case of a rating with no review, and is what we would guess if the word count was 0. But we know there are reviews for every observation, so it’s not very meaningful as is. We’ll talk about ways to get a more meaningful intercept later, but for now, that is our starting point. Moving on, if we add a single word to the review, we expect the rating to go down by -0.04 stars. So if we had a review that was 10 words long, i.e., the mean word count, we would predict a rating of 3.49 + 10*-0.04 = 3.1 stars.\n\n\n1.5.1 The Linear Model as a Graph\nWe can also express the linear model as a graph, which can be a very useful way to think about models in a visual fashion, and as we see other models, can help us literally see how different models relate to one another and are actually very similar. In the following, we have three features predicting a single target, so we have three nodes for the features, and a single node for the target. The feature nodes are combined into a linear combination, or, linear predictor, with each ‘edge’ signifying the connection, and labeled with the coefficient or weight. This connection between our linear predictor, and the ultimate target is direct, without any additional change. Later on, we’ll see more about that, but for our standard linear model, we’re all set.\n\n\n\n\n\n\nFigure 1.2: A linear regression as a graphical model\n\n\n\nSo at this point you have the basics of what a linear model is and how it works, and a couple ways to think about it, whether through programming, math, or just visually. But there is a lot more to it than that. Just getting the model is easy enough, but we need to be able to use it and understand the details better, so we’ll get into that now!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-what-do-we-do",
    "href": "linear_models.html#sec-lm-what-do-we-do",
    "title": "1  The Foundation",
    "section": "1.6 What do we do with a model?",
    "text": "1.6 What do we do with a model?\nOnce we have a working model, there are two primary ways we can use it. One way to use a model is to help us understand the relationships between the features and our outcome of interest. In this way, the focus can be said to be on explanation, or interpreting the model results. The other way to use a model is to make estimates about the outcome for specific observations, often ones we haven’t seen in our data. In this way the focus is on prediction. In practice, we often do both, but the focus is usually on one or the other. We’ll cover both in detail eventually, but let’s start with prediction.\n\n1.6.1 Prediction\nIt may not seem like much at first, but a model is of no use if it can’t be used to make predictions about what we can expect in the world around us. Once our model has been fit to the data, we can obtain our predictions by plugging in values for the features that we are interested in, and, using the corresponding weights and other parameters that have been estimated, come to a guess about a specific observation. Let’s go back to our results, starting with a simpler depiction.\n\n\n\n\n\n  \n    \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n3.49\n0.04\n82.43\n0.00\n3.41\n3.57\n    word_count\n−0.04\n0.00\n−11.58\n0.00\n−0.05\n−0.04\n  \n  \n  \n\n\n\n\nThe table shows the coefficient for each feature including the intercept, which is our starting point. In this case, the coefficient for word count is -0.04, which means that for every additional word in the review, the rating goes down by -0.04 stars. So if we had a review that was 10 words long, we would predict a rating of 3.49 + 10*-0.04 = 3.1 stars.\nWhen we’re talking about predictions for a linear model, we usually will see this as the following mathematically:\n\\[\n\\hat{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n\n\\]\nWhat is \\(\\hat{y}\\)? The hat over the \\(y\\) just means that it’s a predicted value of the model, rather than the one we actually observe. Our first equations that just used \\(y\\) implicilty suggested that we would get a perfect rating value given the model, but that’s not the case. We can only get an estimate. The \\(\\hat{y}\\) is also the linear predictor in our graphical version (Figure 1.2), which makes clear it is not the actual target, but a combination of the features that is related to the target.\nTo make our first equation accurately reflect the relationship between the target and our features, we need to add what is usually referred to as an error term, \\(\\epsilon\\), to account for the fact that our predictions will not be perfect3. So the full linear (regression) model is:\n\\[\ny = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n + \\epsilon\n\\]\nThe error term is a random variable that represents the difference between the actual value and the predicted value, which comes from the weighted combination of features. We can’t know what the error term is, but we can estimate it, just like we can the coefficients. We’ll talk more about that in the section on estimation (Chapter 3).\nTODO: think about which of the following parts of of prediction can move to the ‘knowing your model’ chap\n\n\n1.6.2 What kinds of predictions can we get?\nWhat predictions we get depends on the type of model we are using. For the linear model, we can get predictions for the target, which is a continuous variable. Very commonly, we also can get predictions for a categorical target, such as whether the rating is ‘good’ or ‘bad’. This simple breakdown pretty much covers everything, as we typically would be predicting a continuous variable or a categorical variable, or more of them, like multiple continuous variables, or a target with multiple categories, or sequences of categories (e.g. words). In our case, we can get predictions for the rating, which is a number between 1 and 5. Had our target been a binary good vs. bad rating, our predictions would still be numeric, and usually expressed as a probability between 0 and 1, say, for the ‘good’ category. Higher probabilities would mean we’d more likely predict the movie is good. We then would convert that probability to a class of good or bad depending on a chosen probability cutoff. We’ll talk about how to get predictions for categorical targets later.\nWe previously saw a prediction for a single observation, but we can also get predictions for multiple observations at once. In fact, we can get predictions for all observations in our dataset. Besides that, we can also get predictions for observations that we don’t even have data for! Fun! The following shows how we can get predictions for all data, and for a single observation with a word count of 5.\n\nRPython\n\n\n\nall_predictions   = predict(model_reviews)\n\ndf_prediction = tibble(word_count = 5)\nsingle_prediction = predict(model_reviews, newdata = df_prediction)\n\n\n\n\nall_predictions   = model_reviews.predict()\n\ndf_prediction = pd.DataFrame({'word_count': [5]})\nsingle_prediction = model_reviews.predict(df_prediction)\n\n\n\n\nHere is a plot of our predictions for the observed data versus the actual ratings4. The reference line is where the points would fall if we had perfect prediction. We can see that the predictions are definitely not perfect, but we don’t expect this. They are not completely off base either, in that generally higher predicted scores are associated with higher observed values. We’ll talk about how to assess the quality of our predictions later, but we can at least get a sense that we have a correspondence relationship between our predictions and target, which is definitely better than not having a relationship at all!\n\n\n\n\n\n\n\n\nFigure 1.3: Predictions vs. Actual Ratings\n\n\n\n\n\nNow let’s look at what our prediction looks like for a single observation, and we’ll add in a few more- one for 10 words, and one for a 50 word review, which is beyond the length of any review in this dataset, and one for 12.3 words, which isn’t even possible for this data, since words are only counted as whole values. To get these values we just use the same prediction approach we showed above ((my-first-model-predictions-r?)), and we specify the word count value we want to predict for.\n\n\n\n\nTable 1.2: Predictions for Specific Observations\n\n\n\n\n\n\n  \n    \n    \n      Word Count\n      Predicted Rating\n    \n  \n  \n    5.0\n3.3\n    10.0\n3.1\n    12.3\n3.0\n    50.0\n1.4\n  \n  \n  \n\n\n\n\n\n\n\nThe values reflect the negative coefficient from our model, reflecting decreasing ratings with increasing word counts. Furthermore, we see the power of the model’s ability to make predictions for what we don’t see in the data. Maybe we limited our data review size, but we know there are 50 word reviews out there, and we can still make a guess as to what the rating would be for such a review. Maybe in another case, we know a group of people who have on average 12.3 word reviews, and we can make a guess as to what the average rating would be for that group. Our model doesn’t know anything about the context of the data, but we can use our knowledge to make predictions about the world around us. This is a very powerful capability, and it’s one of the main reasons we use models in the first place.\nTODO: summarize some thoughts but move bulk to ‘knowing your model’ chapter\n\n\n1.6.3 Prediction Error\nAs we have seen, predictions are not perfect, and an essential part of the modeling endeavor is to better understand these errors and why they occur. In addition, error assessment is the fundamental way in which we assess a model’s performance, and, by extension, compare that performance to other models. In general, prediction error is the difference between the actual value and the predicted value or some function of it, and in statistical models, is also often called the residual. We can look at these individually, or we can look at them in aggregate with a single metric.\nLet’s start with looking at the residuals visually. Often the modeling package you use will have this as a default plotting method when doing a standard linear regression, so it’s wise to take advantage of it. We plot both the distribution of raw error scores and the cumulative distribution of absolute prediction error. Here we see a couple things. First, the distribution is roughly normal, which is a good thing, since statistical linear regression assumes our error is normally distributed, and the prediction error serves as an estimate of that. Second, we see that the mean of the errors is zero, which is a consequence of linear regression, and the reason we look at other metrics when assessing model performance. We can also see that most of our predictions are within 1 star rating.\n\n\n\n\n\n\n\n\nFigure 1.4: Distribution of Prediction Errors\n\n\n\n\n\nOf more practical concern is that we don’t see extreme values or clustering, which might indicate a failure on the part of the model to pick up certain segments of the data. It can still be a good idea to look at the extremes just in case we can pick up on some aspect of the data that we could potentially incorporate into the model.\nLooking at our worst prediction in absolute terms, we see the observation has a typical word count, and so our simple model will just predict a fairly typical rating. But the actual rating is 1, which is 2.1 away from our prediction, a very noticeable difference. Further data inspection may be required to figure out whiy this came about.\n\n\n\n\nTable 1.3: Worst Prediction\n\n\n\n\n\n\n  \n    \n    \n      rating\n      prediction\n      word_count\n    \n  \n  \n    1.0\n3.1\n10\n  \n  \n  \n\n\n\n\n\n\n\nWe can also get an overall assessment of the prediction error. In the case of the linear model we’ve been looking at, we can express this in a single metric as the sum or mean of our (squared) errors, the latter of which is a very commonly used modeling metric- MSE or mean squared error, or also, its square root - RMSE or root mean squared error.\nIf we look back at our results, we can see this expressed as the part of the output or as an attribute of the model5. The RMSE is more interpretable, as it gives us a sense that our typical errors bounce around by about 0.59. Given that the rating is on a 1-5 scale, this maybe isn’t bad, but we could definitely hope to do better than get within roughly half a point on this scale. We’ll talk about ways to improve this later.\n\nRPython\n\n\n\nsummary(model_reviews) # 'Residual standard error' is approx RMSE\n\n\nCall:\nlm(formula = rating ~ word_count, data = df_reviews)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0648 -0.3502  0.0206  0.3352  1.8498 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.49164    0.04236    82.4   &lt;2e-16 ***\nword_count  -0.04268    0.00369   -11.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.591 on 998 degrees of freedom\nMultiple R-squared:  0.118, Adjusted R-squared:  0.118 \nF-statistic:  134 on 1 and 998 DF,  p-value: &lt;2e-16\n\n\n\n\n\nnp.sqrt(model_reviews.scale)   # RMSE\n\n0.590728780660127\n\n\n\n\n\nAt this point you have the gist of prediction and prediction error, but there is a lot more to it. More detail can be found in the estimation chapter (Chapter 3), since we often estimate the parameters of our model by picking those that will reduce the prediction error the most. Makes sense right? For now though, let’s move on to the other main use of models, which is to help us understand the relationships between the features and the target, or explanation.\nTODO: This could possibly be moved to ‘knowing your model’ chapter. Instead, we could just focus on the coefficient/uncertainty/r2 stuff here, and then move to the ‘knowing your model’ chapter for the rest. We could also use other models that would better demonstrate SHAP and other metrics.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-interpretation",
    "href": "linear_models.html#sec-lm-interpretation",
    "title": "1  The Foundation",
    "section": "1.7 How do we interpret the model?",
    "text": "1.7 How do we interpret the model?\nWhen it comes to interpreting the results of our model, there are a lot of tools at our disposal, though many of the tools we can ultimately use will depend on the specifics of the model we have employed. In general though, we can group our approach to understanding results at the feature level and the model level. A feature level understanding regards the relationship between a single feature and the target. Beyond that, we also attempt comparisons of feature contributions to prediction, i.e., relative importance. Model level interpretation is focused on assessments of how well the model ‘fits’ the data, or more generally, predictive performance. We’ll start with the feature level, and then move on to the model level.\n\n1.7.1 Feature Level\nAs mentioned, at the feature level, we are primarily concerned with the relationship between a single feature and the target. More specifically, we are interested in the direction and magnitude of the relationship, but in general, it all boils down to how a feature induces change in the target. For numeric features, we are curious about the change in the target given some amount of change in the feature values. It’s conceptually the same for categorical features, but often we like to express the change in terms of group mean differences or something similar, since the order of categories is not usually meaningful. An important aspect of feature level interpretation is the specific predictions we can get by holding the data at key feature values.\n\n1.7.1.1 Basics\nLet’s start with the basics by looking again at our coefficient table from the model output.\n\n\n\n\n\n  \n    \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n3.49\n0.04\n82.43\n0.00\n3.41\n3.57\n    word_count\n−0.04\n0.00\n−11.58\n0.00\n−0.05\n−0.04\n  \n  \n  \n\n\n\n\nHere, the main thing to look at are the actual feature coefficients and the direction of their relationship, positive or negative. We saw before that the coefficient for word count is -0.04, and this means that for every additional word in the review, the rating goes down by -0.04. So if we had a review that was 10 words long, we would predict a rating of 3.49 + 10*-0.04 = 3.1 stars.\nThis interpretation gives us directional information, but how can we interpret the magnitude of the coefficient? Let’s try and use some context to help us. The value for the coefficient is -0.04, and the standard deviation of the rating score, i.e., how much it moves around naturally on its own, is 0.63. So the coefficient is about 6% of the standard deviation of the target. In other words, the addition of a single word to a review results in an expected decrease of 6% of what the review would normally bounce around in value. We might not consider this large, but also, a single word change isn’t much. What would be a significant change in word count? Let’s consider the standard deviation of the feature. In this case, it’s 5.07 for word count. So if we increase the word count by one standard deviation, we expect the rating to decrease by -0.04 * 5.07 = -0.2. That decrease then translates to a change of -0.2/0.63 = -0.32 standard deviation units of the target. Without additional context, many would think that’s a significant change6, or at the very least, that the coefficient is not negligible, and that the feature is indeed related to the target. But we can also see that the coefficient is not so large that it’s not believable.\n\n\n\n\n\n\nStandardized Coefficients\n\n\n\n\n\nThe calculation we just did results in what’s often called a ‘standardized’ or ‘normalized’ coefficient. In the case of the simplest model with only one feature like this, it is identical to the Pearson r correlation metric, which we invite you to check and confirm on your own, which should roughly equal our calculation using rounded values. In the case of multiple features, it represents a (partial) correlation between the target and the feature, after adjusting for the other features. But before you start thinking of it as a measure of importance, it is not. It provides some measure of the feature-target linear relationship, but that doesn’t not entail practical importance, nor is it useful in the presence of nonlinear relationships, interactions, and a host of other interesting things that are typical to data and models.\n\n\n\nAfter assessing the coefficients, next up in our table is the standard error. The standard error is a measure of how much the coefficient varies from sample to sample. If we collected the data multiple times, even under practically identical circumstances, we wouldn’t get the same value each time - it would bounce around a bit, and the standard error is an estimate of how much it would bounce around. In other words, the standard error is a measure of uncertainty, and along with the coefficients, it’s used to calculate everything else in the table. The statistic, here a t-statistic from the student t distribution7, is the ratio of the coefficient to the standard error. This gives us a sense of the effect relative to its variability, but the statistic’s primary use is to calculate the p-value related to its distribution8, which is the probability of seeing a coefficient as large as the one we have, if we assume from the outset that the true value of the coefficient is zero. In this case, the p-value is 3.47e-29, which is very small. We can conclude that the coefficient is statistically different from zero, and that the feature is related to the target, at least statistically speaking. However, the interpretation we used regarding the coefficient previously is far more useful than the p-value, as the p-value can be affected by many things not necessarily related to the feature-target relationship, such as sample size, and is often misinterpreted.\nAside from the coefficients, the most important output is the confidence interval (CI). The CI is a range of values that encapsulates the uncertainty we have in our guess about the coefficients. While our best guess for the effect of word count on rating is -0.04, we know it’s not exactly that, and the CI gives us a range of reasonable values we might expect the effect to be based on the data at hand and the model we’ve employed. In this case, the default is a 95% confidence interval, and we can think of this particular confidence interval like throwing horseshoes. If we kept collecting data and running models, 95% of our CIs would capture the true value, and this is one of the many possibile CIs we could have gotten. That’s the technical definition, which is a bit abstract9, but we can also think of it more simply as a range of values that are good guesses for the true value. In this case, the CI is -0.05 to -0.035, and we can be 95% confident that a good range for the coefficient is between those values. We can also see that the CI is relatively narrow, which is good, as it implies that we have a good idea of what the coefficient is. If it was very wide, we would have a lot of uncertainty about the coefficient, and we would not likely not want to base important decisions regarding it.\n\nKeep in mind that your model has a lot to say about what you’ll be able to say at the feature level. As an example, as we get into machine learning models, you won’t have as easy a time with coefficients and their confidence intervals. For now we’ll stop here, but there is a lot more to the story when it comes to feature level interpretation, and we’ll continue to return to the topic. But first, let’s take a look at interpreting things in another way.\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\nThe confidence interval and p-value will for coefficients in typical statistical linear models will coincide with one another in that, if for a given alpha signficance level, if a 1-alpha% CI includes zero, then your p-value will be greater than alpha, and vice versa. This is because the same standard error is used to calculate both. However, the framework of using a CI vs. using the p-value for claiming statistical significance actually came from individuals that were philosophically opposed. Modern day usage of both is a bit of a mess that would upset both Fisher (p-value guy) and Neyman (CI guy), but your authors find that this incorrect practical usage doesn’t make much practical difference in the end.\n\n\n\n\n\n\n1.7.2 Is it a Good Model?\nThus far, we’ve focused on interpretation at the feature level. But knowing the interpretation of a feature doesn’t do you much good if the model itself is poor! In that case, we also need to assess the model as a whole, and as with the feature level, we can go about this in a few ways. Before getting too carried away with asking whether your model is any good or not, you always need to ask your self relative to what? Many models claim top performance under various circumstances, but which are statistically indistinguishable from many other models. So we need to be careful about how we assess our model, and what we compare it to.\nFirst, we can start with the predictions of our model. As noted previously, how well the predictions and target line up is a measure of how well the model fits the data. Most model-level interpretation involves assessing and comparing model fit and variations on this theme. One of the better ways to assess model fit is visually, so let’s look at our predictions vs. the target.\n\nRPython\n\n\n\npredictions = predict(model_reviews)\ny = df_reviews$rating\n\n\n\n\npredictions = model_reviews.predict()\ny = df_reviews.rating\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: Predictions vs. Observed Ratings\n\n\n\n\n\nThe one on the left is using the raw target and predictions, and they appear very grid-like. The reason is that our ratings are only at the single decimal place precision, and our word count is at the integer level precision, so we have a lot of ties. The right side jitters the data randomly a bit so we can see a better pattern, but is otherwise the same. In general, the closer to a line this plot becomes the better, so we can tell already there is still a lot of noise left to explain beyond our model.\n\n1.7.2.1 Model Metrics\nWe’ve already discussed mean-squared error10, but there are other metrics we can use to assess model fit. As we noted, (R)MSE is a very popular measure for continuous targets, telling us the standard deviation of errors, or how much they bounce around on average. In our case, the value was 0.59. Another metric we can use in this particular situation is the mean absolute error, which is similar to the mean squared error, but instead of squaring the errors, we just take the absolute value. Conceptually it attempts to get at the same idea, how much our predictions miss on average, and here the value is 0.46, which we actually showed in our initial residual plot (Figure 1.4). With either metric, the closer to zero the better, since as we get closer, we are reducing error.\nWe can also look at the R-squared (R2) value of the model. R2 is possibly the most popular measure of model performance with linear regression and linear models in general. Before squaring, it’s just the correlation of the values that we saw in the previous plot (Figure 1.5). When we square it, we can interpret it as a measure of how much of the variance in the target is explained by the model. In this case, our model shows the R2 is 0.12, which is not bad for a single feature model in this type of setting. We interpret it that 12% of the target is explained by our model. In addition, we can also interpret R2 as 1 - the prorportion of error varaince in the target, which we can calculate as \\(1 - \\frac{\\textrm{MSE}}{var(y)}\\). In other words the complement of R2 is the proportion of the variance in the target that is not explained by the model. Either way, since -11% is not explained by the model, our result suggests there is plenty of work left to do!\nNote also, that with R2 we get a sense of the variance shared between all features in the model and the target, however complex the model gets. As long as we use it descriptively as a simple correspondence assessment of our predictions and target, it’s a fine metric. For various reasons, it’s not a great metric for comparing models to each other, but again, as long as you don’t get carried away, it’s okay to use.\n\n\n\n1.7.3 Prediction vs. Explanation\nIn your humble authors’ views, one can’t stress enough the importance of a model’s ability to predict the target. It can be a poor model, maybe because the data is not great, or perhaps we’re exploring a new area of research, but we’ll always be interested in how well a model fits the observed data, and predicts new data.\nEven to this day, statistical significance is focused on a great deal, even to the point that a much hullabaloo is made about models that have no predictive power at all. As strange as it may sound, you can read whole journal articles, news features, and business reports in many fields with hardly any mention of prediction. The focus is almost entirely on the explanation of the model, and usually the statistical significance of the features. In those settings, statistical significance is often used as a proxy for importance, which it never should be. Unfortunately, statistical significance is affected by other things besides the size of the coefficient, and without an understanding of the context of the features, in this case, like how long typical reviews are, what their range is, what variability of ratings is, etc., the information it provides is extremely limited, and many would argue, not even useful at all. If we are very interested in the coefficient or weight value specifically, it is better to focus on the range of possible values, which is provided by the confidence interval. While a confidence interval is also a loaded description of a feature’s relationship to the target, we can use it in a very practical way as a range of possible values for that weight, and more importantly, think of possibilities rather than certainties.\nSuffice it to say at this point that how much one focuses on prediction vs. explanation depends on the context and goals of the data endeavor. There are cases where predictive capability is of utmost importance, and we care less about about explanatory details, but not to the point of ignoring it. For example, even with deep learning models for image classification, where the inputs are just RGB values, we’d still like to know what the (notably complex) model is picking up on, otherwise we may be classifying images based on something like image backgrounds (e.g. outdoors vs. indoors) instead of the objects of actual interest (dogs vs. cats). In some business or other organizational settings, we are very or even mostly interested in the coefficients/weights, which might indicate how to allocate monetary resources in some fashion. But if those weights come from a model with no predictive power, placing much importance on them may be a fruitless endeavor.\nIn the end we’ll need to balance our efforts to suit the task at hand. Prediction and explanation are both fundamental to the modeling endeavor.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-complexity",
    "href": "linear_models.html#sec-lm-complexity",
    "title": "1  The Foundation",
    "section": "1.8 Adding Complexity",
    "text": "1.8 Adding Complexity\nWe’ve seen how to fit a model with a single feature and interpret the results, and that helps us to get oriented to the process. However, we’ll always have more than one feature for a model except under some very specific circumstances, such as exploratory data analysis. So let’s see how we can do that with a model that makes more sense.\n\n1.8.1 Multiple Features\nWe can add more features to our model very simply. Using the standard functions we’ve already demonstrated, we just add them to the formula (both R and statsmodels) as follows.\n\n'y ~ feature_1 + feature_2 + feature_3'\n\nIn other cases where we use matrix inputs, additional features will just be the additional input columns, and nothing about the model code actually changes. We might have a lot of features, and even for relatively simple linear models this could be dozens in some scenarios. A compact depiction of our model uses the matrix representation, which we’ll show in the callout below, and you can find more detail in the matrix section Appendix C overview. For our purposes, all you really need to know is that this:\n\\[\ny = X\\beta\\qquad  \\textrm{or}\\qquad y = \\alpha + X\\beta\n\\tag{1.2}\\]\nis the same as this:\n\\[\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 \\dots\n\\]\nwhere \\(X\\) is a matrix of features11, and \\(\\beta\\) is a vector of coefficients. Matrix multiplication provides us an efficient way to get our expected value/prediction.\n\n\n\n\n\n\nMatrix Representation of a Linear Model\n\n\n\n\n\nHere we’ll show the matrix representation form of the linear model in more detail. In the following, \\(y\\) is a vector of all target observations, and \\(X\\) is a matrix of features. The \\(\\beta\\) vector is the vector of coefficients. The column of 1s serves as a means to incorporate the intercept, as it’s just mulitplied by whatever the estimated intercept value is. The matrix multiplication form is just a compact way of expressing the sum of the features multiplied by their coefficients.\nHere is y as a vector of observations, n x 1.\n\\[\n\\textbf{y} = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\tag{1.3}\\]\nHere is the n x p matrix of features, including the intercept:\n\\[\n\\textbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{np}\n\\end{bmatrix}\n\\tag{1.4}\\]\nAnd finally, here is the p x 1 vector of coefficients:\n\\[\n\\bf{\\beta} = \\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\tag{1.5}\\]\nPutting it all together, we get the linear model in matrix form:\n\\[\n\\bf{y = X\\beta }\n\\tag{1.6}\\]\nYou will also see it depicted in a transposed fashion, such that \\(y = \\beta X\\), which is just a matter of preference, except that it assumes your data is formatted where the features are the rows and the observations are the columns. You’ll rarely if ever see data stored this way in practice for tabular data, but you should be aware that other data settings will force you to think of multi-dimensional arrays12 instead of 2-d matrices, for example, with image processing. So it’s good to be flexible.\n\n\n\nWith that in mind, let’s get to our model! In what follows, we keep the word count, but now we add some aspects of the reviewer, such as age and the number of children in the household, and features related to the movie, like the release year, the length of the movie in minutes, and the total reviews received. We’ll also add another review level feature- the year the review was written. We’ll use the same approach as before, and literally just add them as we depicted in our linear model formula (Equation 1.1).\n\nRPython\n\n\n\nmodel_reviews_extra = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_reviews\n)\n\nsummary(model_reviews_extra)\n\n\nCall:\nlm(formula = rating ~ word_count + age + review_year + release_year + \n    length_minutes + children_in_home + total_reviews, data = df_reviews)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8231 -0.3399  0.0107  0.3566  1.5144 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -4.56e+01   7.46e+00   -6.11  1.5e-09 ***\nword_count       -3.03e-02   3.33e-03   -9.10  &lt; 2e-16 ***\nage              -1.69e-03   9.24e-04   -1.83   0.0683 .  \nreview_year       9.88e-03   3.23e-03    3.05   0.0023 ** \nrelease_year      1.33e-02   1.79e-03    7.43  2.3e-13 ***\nlength_minutes    1.67e-02   1.53e-03   10.90  &lt; 2e-16 ***\nchildren_in_home  1.03e-01   2.54e-02    4.05  5.5e-05 ***\ntotal_reviews     7.62e-05   6.16e-06   12.36  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.52 on 992 degrees of freedom\nMultiple R-squared:  0.321, Adjusted R-squared:  0.316 \nF-statistic:   67 on 7 and 992 DF,  p-value: &lt;2e-16\n\n\n\n\n\nmodel_reviews_extra = smf.ols(\n    formula = 'rating ~ word_count \\\n        + age \\\n        + review_year \\\n        + release_year \\\n        + length_minutes \\\n        + children_in_home \\\n        + total_reviews',\n    data = df_reviews\n).fit()\n\nmodel_reviews_extra.summary(slim = True)\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.321\n\n\nModel:\nOLS\nAdj. R-squared:\n0.316\n\n\nNo. Observations:\n1000\nF-statistic:\n67.02\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n3.73e-79\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-45.5688\n7.463\n-6.106\n0.000\n-60.215\n-30.923\n\n\nword_count\n-0.0303\n0.003\n-9.102\n0.000\n-0.037\n-0.024\n\n\nage\n-0.0017\n0.001\n-1.825\n0.068\n-0.004\n0.000\n\n\nreview_year\n0.0099\n0.003\n3.055\n0.002\n0.004\n0.016\n\n\nrelease_year\n0.0133\n0.002\n7.434\n0.000\n0.010\n0.017\n\n\nlength_minutes\n0.0167\n0.002\n10.897\n0.000\n0.014\n0.020\n\n\nchildren_in_home\n0.1028\n0.025\n4.051\n0.000\n0.053\n0.153\n\n\ntotal_reviews\n7.616e-05\n6.16e-06\n12.362\n0.000\n6.41e-05\n8.83e-05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.82e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\n\nThere is definitely more to unpack here, but it’s important to note that it’s just more stuff, not different stuff. The model-level components are the same in that we still see R2 etc., although they are all ‘better’ (higher R2, lower error) because we have a more predictive model. Our coefficents look the same also, and we’d interpret the in the same way. Starting with word count, we see that it’s still statistically significant, but it has been reduced just slightly from our previous model where it was the only feature (-0.04 vs. -0.03). Why? This suggests that word count has some non-zero correlation, sometimes called collinearity, with other features that are also explaining the target to some extent. Our linear model shows the effect of each feature controlling for other features, or, holding other features constant13. Conceptually this means that the effect of word count is the effect of word count after we’ve accounted for the other features in the model. In this case, an increase of a single word results in a -0.03 drop, even after adjusting for the effect of other features. Looking at another feature, the addition of a child to the home is associated with 0.1 bump in rating, accounting for the other features.\nThinking about prediction, how would we get a prediction for a movie rating with a review that is 12 words long, written in 2020, by a 30 year old with one child, for a movie that is 100 minutes long, released in 2015, with 10000 total reviews? Exactly the same as we did before (Section 1.6.2)! We just create a data frame with the values we want, and predict accordingly.\n\nRPython\n\n\n\npredict_observation = tibble(\n    word_count = 12,\n    age = 30,\n    children_in_home = 1,\n    review_year = 2020,\n    release_year = 2015,\n    length_minutes = 100,\n    total_reviews = 10000\n)\n\npredict(\n    model_reviews_extra,\n    newdata = predict_observation\n)\n\n   1 \n3.26 \n\n\n\n\n\npredict_observation = pd.DataFrame(\n    {\n        'word_count': 12,\n        'age': 30,\n        'children_in_home': 1,\n        'review_year': 2020,\n        'release_year': 2015,\n        'length_minutes': 100,\n        'total_reviews': 10000\n    },\n    index = ['new_observation']\n)\n\nmodel_reviews_extra.predict(predict_observation)\n\nnew_observation    3.2595\ndtype: float64\n\n\n\n\n\nIn our example we’re just getting a single prediction, but don’t let that hold you back! You can predict an entire data set if you want, and use any values for the features you want. We’ll do this explicitly later on, but for now, try getting a prediction for a different set of values.\nTODO: Reconcile with Data chapter, possibly remove from this chapter entirely. If we do, maybe add this model example to that chapter.\n\n\n1.8.2 Categorical Features\nCategorical features can be added to a model just like any other feature. The main issue is that they have to be represented numerically, because models only work on numerically coded features and targets. The simplest and most common encoding is called a one-hot encoding scheme, which creates a new feature for each category, and assigns a 1 if the observation is in that category, and a 0 otherwise. This is also called a dummy coding when used for statistical models. Here is an example of what the coding looks like for the season feature. This is really all there is to it.\n\n\n\n\n\n  \n    \n    \n      rating\n      season\n      Fall\n      Summer\n      Winter\n      Spring\n    \n  \n  \n    2.70\nFall\n1\n0\n0\n0\n    4.20\nFall\n1\n0\n0\n0\n    3.70\nFall\n1\n0\n0\n0\n    2.70\nFall\n1\n0\n0\n0\n    2.40\nSummer\n0\n1\n0\n0\n    4.00\nSummer\n0\n1\n0\n0\n    1.80\nFall\n1\n0\n0\n0\n    2.40\nSummer\n0\n1\n0\n0\n    2.50\nWinter\n0\n0\n1\n0\n    4.30\nSummer\n0\n1\n0\n0\n  \n  \n  \n\n\n\n\nWhen using statistical models we don’t have to do this ourselves. Even other tools for machine learning models will typically have a way to identify and appropriately handle categorical features, even in very complex ways when it comes to deep learning models. What is important is to be aware that they require special handling, but often this is done behind the scenes. Now let’s do a quick example using a categorical feature with our data, and we’ll keep a numeric feature as well just for consistency.\n\nRPython\n\n\n\nmodel_cat = lm(\n    rating ~ word_count + season,\n    data = df_reviews\n)\n\nsummary(model_cat)\n\n\nCall:\nlm(formula = rating ~ word_count + season, data = df_reviews)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9184 -0.3622  0.0133  0.3589  1.8372 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.3429     0.0530   63.11  &lt; 2e-16 ***\nword_count    -0.0394     0.0036  -10.96  &lt; 2e-16 ***\nseasonSpring  -0.0301     0.0622   -0.48     0.63    \nseasonSummer   0.2743     0.0445    6.17  9.8e-10 ***\nseasonWinter  -0.0700     0.0595   -1.18     0.24    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.572 on 995 degrees of freedom\nMultiple R-squared:  0.176, Adjusted R-squared:  0.173 \nF-statistic: 53.1 on 4 and 995 DF,  p-value: &lt;2e-16\n\n\n\n\n\nmodel_cat = smf.ols(\n    formula = \"rating ~ word_count + season\",\n    data = df_reviews\n).fit()\n\nmodel_cat.summary(slim = True)\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.176\n\n\nModel:\nOLS\nAdj. R-squared:\n0.173\n\n\nNo. Observations:\n1000\nF-statistic:\n53.09\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n1.41e-40\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.3429\n0.053\n63.109\n0.000\n3.239\n3.447\n\n\nseason[T.Spring]\n-0.0301\n0.062\n-0.483\n0.629\n-0.152\n0.092\n\n\nseason[T.Summer]\n0.2743\n0.044\n6.171\n0.000\n0.187\n0.362\n\n\nseason[T.Winter]\n-0.0700\n0.059\n-1.177\n0.239\n-0.187\n0.047\n\n\nword_count\n-0.0394\n0.004\n-10.963\n0.000\n-0.047\n-0.032\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nWe now see the usual output. There is word count again, with its slightly negative assocation with rating. And we have an effect for each season as well… except, wait a second, where is the fall effect? The coefficients are interepreted the same way - as we move one unit on x, we see a corresponding change in y. But moving from one category to another requires starting at some category in the first place! So one is chosen arbitrarily, but you would have control over this. In our model, fall is chosen because its first alphabetically. So if we look at say, the effect of summer, we see an increase in the rating of 0.27 relative to fall.\nA better approach to understanding categorical features for standard linear models is through what are called marginal effects, which can provide a kind of average prediction for each category while accounting for the other features in the model. Better still is to visualize these, and we can use something like our PDP approach from before to do so14. It’s actually tricky to define ‘average’ when there are multiple features and interactions involved, so be careful, but we’d interpret the result similarly in those cases as best we can. In this case, we expect higher ratings for summer releases.\n\n\n\n\n\nMarginal Effects of Season on Rating\n\n\n\n\n\n\n1.8.3 Other Complexity",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-assumptions",
    "href": "linear_models.html#sec-lm-assumptions",
    "title": "1  The Foundation",
    "section": "1.9 Assumptions and More",
    "text": "1.9 Assumptions and More\nTODO: MOVE BULK OF THIS TO MODEL CRITICISM?? ALSO NEEDS CLEANUP\nEvery model you use has underlying assumptions which, if not met, could potentially result in incorrect inferences about the effects, performance, or predictive capabilities of the model. The standard linear regression model we’ve shown is no different, and it has a number of assumptions that must be met for it to be statistically valid. Briefly they are:\n\nThat your model is not grossly misspecified (e.g., you’ve included the right features and not left out important ones)\nThe data that you’re modeling reflects the population you want to make generalizations about\nThe model is linear in the parameters (i.e. no \\(\\beta_1\\cdot e^x\\) type stuff)\nThe features are not correlated with the error (prediction errors, unobserved causes)\nYour data observations are independent of each other\nThe prediction errors are homoscedastic (don’t have large errors with certain predictions vs low with others)\nNormality of the errors (i.e. your prediction errors). Another way to put it is that your target variable is normally distributed conditional on the features.\n\nThings a linear regression model does not assume:\n\nThat the features are normally distributed\n\nFor example, using categorical features is fine\n\nThat the relationship between the features and target is linear\n\nInteractions, polynomial terms, etc. are all fine\n\nThat the features are not correlated with each other\n\nThey usually are\n\n\nIf you do meet these assumptions, it doesn’t mean:\n\nYou have large effects\nYou have a good model\nYou have causal effects\nYou (necessarily) have less uncertainty about your coefficients or predictions than other methods\n\nIf you don’t meet these assumptions, it doesn’t mean:\n\nThat your model will have poor predictions\nThat your conclusions will necessarily be incorrect\n\nSo basically whether or not you meet the assumptions of your model doesn’t actually say much about whether the model is great or terrible, it just means that you have to be careful about what you can say about it. For the linear regression model, if you do meet those assumptions, your coefficient estimates are unbiased15, and in general, your statistical inferences are correct ones. If you don’t meet them, there are alternative versions of the linear model you could use that would get around the problem. For example, data that runs over a sequence of time (time series data) violates the independence assumption since observations closer in time are more likely to be similar than those farther apart. But we would use a time series or similar model instead to account for this. If normality is difficult to meet, you could assume a different data generating distribution. We’ll discuss some of these in Chapter 5, but it’s also important to note that not meeting the assumptions may only mean you’ll prefer a different type of linear or other model to use for the data. It’s often the case taht not meeting the assumptions is often the result of a poor model, e.g., using poor features in an underspecified way, like not including interactions or other complexity.\nOn top of not meeting the assumptions, we may in fact intentionally introduce bias to get better prediction! For example, we might use a penalized regression model to reduce the variance in our predictions, at the cost of introducing bias in the coefficients. We’ll talk more of this in the Chapter 7, but suffice it to say for now, if you are more interested in prediction, you may be less interested in the statistical assumptions of the basic linear model.\n\n1.9.1 More Complex Models\nLet’s say your running some XGBoost or Deep Linear Model and getting outstanding predictions. Assumptions smumptions you say! And you might even be right! But if you want to talk confidently about feature contributions, or know something about the uncertainty in the predictions (which you’re assessing right?), well, maybe you might want to know if you’re meeting your assumptions. Some of them are:\n\nYou have enough data to make the model generalizable\nYour data isn’t biased (e.g., you don’t have 90% of your data from one region when you want to talk about a whole area)\nYou adequately sampled the hyperparameter space (e.g. you didn’t just use the defaults or a small grid search)\nYour observations are independent or at least exchangeable and don’t have data leakage, or you are explicitly modeling observation dependence\nThat all the parameter settings you set are correct or at least viable (e.g. you let the model run for a long enough set of iterations, your batch size was adequate, you had enough hidden layers, etc.)\n\nAnd if you want to talk about specific feature contributions, you are assuming:\n\nThe features are largely uncorrelated\nThe features largely do not interact (but then why are you doing a complex model that is inherently interactive), or that your understanding of feature contribution deals with the interactions\n\nYeah… so, sorry to say, using non-statistical models doesn’t mean you don’t have to worry about assumptions, you still have some of the old stuff and some new ones to boot.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-classification",
    "href": "linear_models.html#sec-lm-classification",
    "title": "1  The Foundation",
    "section": "1.10 Classification",
    "text": "1.10 Classification\nWe’ve been using a continuous target, but what about a categorical target? For example, what if we just had a binary target of whether a movie was good or bad? We will dive much more into classification models in our upcoming chapters, but it turns out that we can still formulate it as a linear model problem. The main difference is that we use a transformation of our linear combination of features, using what is sometimes called a link function, and we’ll need to use a different objective function rather than least squares, such as the binomial likelihood, to deal with the binary target. This also means we’ll move away from R2 as a measure of model fit, and look at something something else, like accuracy.\nGraphically we can see it in the following way, which when compared with our linear model (Figure 1.2), doesn’t look much different. In what follows, we create our linear combination and put it through the sigmoid function, which is a common link function for binary targets16. The result is a probability, which we can then use to classify the observation as good or bad based on a chosen threshold.\n\n\n\n\n\n\nFigure 1.6: A Linear Model with Transformation Can Be a Logistic Regression\n\n\n\nAs soon as we move away from the standard linear model and use transformations of our linear predictor, simple coefficient interpretation becomes difficult, sometimes exceedingly so. We will explore more of these types of models in Chapter 4.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-more",
    "href": "linear_models.html#sec-lm-more",
    "title": "1  The Foundation",
    "section": "1.11 More linear models",
    "text": "1.11 More linear models\nBefore we leave our humble linear model, let’s look at some others. Here is a brief overview of some of the more common linear models you might encounter.\nGeneralize Linear Models and related\n\nTrue GLM e.g. logistic, poisson\nOther distributions: beta regression, tweedie, t (so-called robust), truncated\nPenalized regression: ridge, lasso, elastic net\nCensored outcomes: Survival models, tobit\n\nMultivariate/multiclass/multipart\n\nMultivariate regression (multiple targets)\nMultinomial/Categorical/Ordinal regression (&gt;2 classes)\nZero (or some number) -inflated/hurdle/altered\nMixture models and Cluster analysis\n\nRandom Effects\n\nMixed effects models (random intercepts/coefficients)\nGeneralized additive models (GAMMs)\nSpatial models (CAR)\nTime series models (ARIMA)\nFactor analysis\n\nLatent Linear Models\n\nPCA, Factor Analysis\nMixture models\nStructural Equation Modeling, Graphical models generally\n\nAll of these are explicitly linear models or can be framed as such, and most are either identical in description to what you’ve already seen or require only a tweak or two - e.g. a different distribution, a different link function, penalizing the coefficients, etc. In other cases, we can bounce from one to the another. For example we can reshape our multivariate outcome to be amenable to a mixed model approach and get the exact same results. We can potentially add a random effect to any model, and that random effect can be based on time, spatial or other considerations. The important thing to know is that the linear model is a very flexible tool that expands easily, and allows you to model most of the types of outcomes were interested in. As such, it’s a very powerful approach to modeling.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-wrap",
    "href": "linear_models.html#sec-lm-wrap",
    "title": "1  The Foundation",
    "section": "1.12 Wrapping Up",
    "text": "1.12 Wrapping Up\nLinear models such as the linear regression demonstrated in this chapter are a very popular tool for data analysis, and for good reason. They are relatively easy to understand, and they are very flexible. They can be used for prediction, explanation, and inference, and they can be used for a wide variety of data types. They are also many tools at our disposal to help us implement and explore them. But they are not without their limitations, and you’ll want to have more in your toolbox than just the approach we’ve seen so far.\n\n1.12.1 Choose your own adventure\nNow that you’ve got the basics, where do you want to go?\n\nIf you want a deeper dive into how we get the results from our model: head to Chapter 3\nIf you want to know more about how to understand the model: Chapter 2\nIf you want to do some more modeling: Chapter 4, Chapter 5 or Chapter 6\nGot more data questions? Go to the Chapter 9\n\nIf you are interested in a deeper dive into the theory and assumptions behind linear models, you can check out more statistical/econometric treatments such as:\n\nGelman, Hill, and Vehtari (2020)\nGelman (2013)\nHarrell (2015)\nFahrmeir et al. (2013)\nFaraway (2014)\nWooldridge (2012)\nGreene (2017)\nGrolemund (2023)\nKuhn and Silge (2023)\n\nBut there are many, many books on statistical analysis, linear models, and linear regression specifically. There are those that show nothing but applied results, and those that are very theoretical. Texts tend to get more mathy and theoretical as you go back in time, to the mostly applied and code-based treatments today. You will likely need to do a bit of exploration to find one you like best.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-exercise",
    "href": "linear_models.html#sec-lm-exercise",
    "title": "1  The Foundation",
    "section": "1.13 Exercise",
    "text": "1.13 Exercise\n\nImport some data. Stick with the current data if you want and just try out other features, or maybe try the world happiness data 2018 data. You can find details about it in the appendix Appendix B.\n\nTODO: ADD DATA LINK\n\nFit a linear model, try to keep it to no more than three features.\nGet all the predictions for the data, and try at least\nInterpret the coefficients\nAssess the model fit",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#refs",
    "href": "linear_models.html#refs",
    "title": "1  The Foundation",
    "section": "1.14 refs",
    "text": "1.14 refs\nMolnar (2023)\n\n\n\n\nCohen, Jacob. 2009. Statistical Power Analysis for the Behavioral Sciences. 2. ed., reprint. New York, NY: Psychology Press.\n\n\nFahrmeir, Ludwig, Thomas Kneib, Stefan Lang, and Brian Marx. 2013. Regression: Models, Methods and Applications. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-34333-9.\n\n\nFaraway, Julian. 2014. “Linear Models with R.” Routledge & CRC Press. https://www.routledge.com/Linear-Models-with-R/Faraway/p/book/9781439887332.\n\n\nGelman, Andrew. 2013. “What Are the Key Assumptions of Linear Regression?  Statistical Modeling, Causal Inference, and Social Science.” https://statmodeling.stat.columbia.edu/2013/08/04/19470/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nGreene, William. 2017. Econometric Analysis - 8th Edition. https://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm.\n\n\nGrolemund, Hadley Wickham and Garrett. 2023. Welcome  R for Data Science. https://r4ds.hadley.nz/.\n\n\nHarrell, Frank E. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Springer Series in Statistics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nKuhn, Max, and Julia Silge. 2023. Tidy Modeling with R. https://www.tmwr.org/.\n\n\nMolnar, Christoph. 2023. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.\n\n\nRovine, Michael J, and Douglas R Anderson. 2004. “Peirce and Bowditch.” The American Statistician 58 (3): 232–36. https://doi.org/10.1198/000313004X964.\n\n\nWooldridge, Jeffrey M. 2012. Introductory Econometrics: A Modern Approach. 5th edition. Mason, OH: Cengage Learning.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#footnotes",
    "href": "linear_models.html#footnotes",
    "title": "1  The Foundation",
    "section": "",
    "text": "Regression in general is typically attributed to Galton, and correlation to Pearson, whose coefficient bearing his name is still the mostly widely used measure of association. Peirce & Bowditch were actually ahead of both (Rovine and Anderson 2004).↩︎\nWe actually are using the smf.ols approach because it is modeled on the R approach.↩︎\nIn most circumstances, if you ever have perfect prediction, or even near perfect prediction, the usual issues are that you have either asked a rather obvious/easy question of your data (e.g., predicting whether an image is of a human or a car), or have accidentally included the target in your features (or a combination of them) in some way.↩︎\nWord count is discrete- it can only take whole numbers like 3 or 20, and it is our only feature. Because of this, we can only make very limited predicted rating values, while the observed rating can take on many other values. Because of this, the true plot would show a more banded result with many points overlapping, so we use a technique called jittering to move the points around a little bit so we can see them all. The points are still roughly in the same place, but they are moved around a little bit so we can see them all.↩︎\nThe actual divisor for linear regression output depends on the complexity of the model, and in this case the sum of the squared errors is divided by N-2 (due to estimating the intercept and coefficient) instead of N. This is a technical detail that would only matter for data too small to make much of in the first place, and not important for our purposes here.↩︎\nHistorically, people cite Cohen (2009) for effect size guidelines for simple models, but such guidelines are notoriously problematic. Rely on your own knowledge of the data, provide reasons for your conclusions, and let others draw their own. If you cannot tell what would constitute a notable change in your outcome of interest, you probably shouldn’t be modeling it in the first place.↩︎\nMost statistical tables of this sort will use a t (student t distribution), Z (normal distribution), or F (F distribution) statistic. It doesn’t really matter for your purposes which is used by default, they provide the p-value of interest to claim statistical significance.↩︎\nYou can calculate this as pt(stat, df = model degrees of freedom, lower=FALSE)*2 in R or stats.t.cdf in Python. The model degrees of freedom are provided in the summary output (a.k.a. residual degrees of freedom) lower=FALSE and *2 are to get the two-sided p-value, which is what we want in this case. When it comes to t and Z statistics, anything over 2 is statistically significant by the common standard of a p-value of .05 or less. Note also that even though output will round it to zero, the true p-value can never be zero.↩︎\nThe interpretation regarding the CI is even more nuanced than this, but we’ll leave that for another time. For now, we’ll just say that the CI is a range of values that are good guesses for the true value. Your authors have used frequentist and Bayesian statistics for many years, and we are fine with both of them, because they both work well enough in the real world. Despite where this ranged estimate comes from, the vast majority use CIs in the same way, and they are a useful tool for understanding the uncertainty in our estimates.↩︎\nAny time we’re talking about MSE, we’re also talking about RMSE as it’s just the square root of MSE, so which you choose is mostly arbitrary.↩︎\nIn the first depiction without \\(\\alpha\\), there is an additional column at the beginning of the matrix that is all ones, which is a way to incorporate the intercept into the model. However, most models that use a matrix as input will not have the intercept column, as it’s not part of the model estimation or is estimated separately.↩︎\nIn deep learning, models arrays are referred to as the more abstract representation of tensors, but for practical purposes the distinction doesn’t really matter for modeling, as the tensors are always represented as some n-dimensional array.↩︎\nA lot of statisticians and causal modeling folks get very hung up on the terminology here, but we’ll leave that to them as we’d like to get on with things. For our purposes, we’ll just say that we’re interested in the effect of a feature after we’ve accounted for the other features in the model.↩︎\nAt the time of this writing, there seems to be very little for this sort of thing in Python. statsmodels provides limited functionality, but only for logistic regression models. In R you have various tools like marginaleffects, emmeans, ggeffects and more.↩︎\nThis means they are correct on average, not the true value. And if they were biased, this is statistical bias, and has nothing to do with the moral or ethical implications of the data, or whether the features themselves are biased in measurement. Culturally biased data is a different problem than statistical/prediction bias or measurement error, though they are not mutually exclusive. The latter can more readily be tested, while the former is usually more difficult to assess. For example, if our movie reviews only came from a website with a paywall, they would be biased if we wanted to use them to refer to general public opinion. Even then, our model results are perfectly reasonable, as long as they are used to generalize only to that population of people who pay for the website. If we wanted to generalize to the general public, we would need to account for this bias in some way, or use a different data source. MOVE THIS TO SOME OTHER CHAPTER (DATA?)↩︎\nThe sigmoid function in this case is the inverse logistic function, and the resulting statistical model is called logistic regression. In other contexts the model would not be a logistic regression, but this is still a very commmonly used activation function. But many others could potentially be used e.g. using a normal instead of logistic distribution, resulting in the so-called probit model.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "estimation.html",
    "href": "estimation.html",
    "title": "3  How Did We Get Here?",
    "section": "",
    "text": "3.1 Key Ideas\nA few concepts we’ll keep using here are fundamental to understanding estimation and optimization. We’ll return to these throughout the book, so it’s good to get some basic familiarity.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-key-ideas",
    "href": "estimation.html#sec-estim-key-ideas",
    "title": "3  How Did We Get Here?",
    "section": "",
    "text": "Parameters are the values associated with a model, that we have to estimate.\nEstimation is the process of finding the parameters associated with a model.\nThe objective function produces a value that we want to, for example, maximize or minimize.\nPrediction error is the difference between the actual value of the target and the predicted value of the target, and is often used to calculate the objective function.\nOptimization is the process of finding the parameters that maximize or minimize some objective function.\nModel Selection is the process of choosing the best model from a set of models.\n\n\n3.1.1 Why this matters\nWhen it comes to modeling, even knowing just a little bit about what goes on beyond the scenes is a great demystifier. And if models are less of a mystery, you’ll feel more confident in using them. Parts of what you see here are used in almost every common model used for statistics and machine learning, providing you even more of a foundation for understanding what’s going on.\n\n\n3.1.2 Good to know\nThis chapter is more involved than most of the others, and is really for those who like to get their hands dirty. It’s all about ‘rolling your own’, and so we’ll be doing a lot of the work ourselves. If you’re not one of those types of people that gets much out of that, that’s ok, you can skip this chapter and still get a lot out of the rest of the book. But if you’re curious about how things work, or you want to be able to do more than just run a function, then we think you’ll find the following useful. You’d want to at least have your linear model basics down.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-data-setup",
    "href": "estimation.html#sec-estim-data-setup",
    "title": "3  How Did We Get Here?",
    "section": "3.2 Data Setup",
    "text": "3.2 Data Setup\nFor the examples here, we’ll use the world happiness dataset for the year 2018. We’ll use the happiness score as our target, and we’ll use the GDP per capita as our primary feature, though we may throw in some others. Let’s take a look at the data here, but for more information see the appendix.\nTODO: Link data description\n\n\n\nWorld happiness data summary\n\n\nOur happiness score has values from around 3-7, life expectancy and gdp appear to have some notable variability, and corruption perception is skewed toward lower values. We can also see that the features and target are correlated with each other, which is not surprising.\nTODO: check the width of this table in pdf\n\n\n\n\nCorrelation matrix for world happiness data\n  \n    \n    \n      term\n      happiness\n      life_exp\n      log_gdp_pc\n      corrupt\n    \n  \n  \n    happiness\nNA\n0.78\n0.82\n−0.47\n    life_exp\n0.78\nNA\n0.86\n−0.34\n    log_gdp_pc\n0.82\n0.86\nNA\n−0.34\n    corrupt\n−0.47\n−0.34\n−0.34\nNA\n  \n  \n  \n\n\n\n\n\n\n\nWe’ll do some minor cleaning and renaming of columns, and we’ll drop any rows with missing values. We’ll also scale the features so that they are on the same scale, which as noted in the data chapter, can help make estimation easier.\n\nRPython\n\n\n\ndf_happiness = read_csv(\"data/world_happiness_2018.csv\") |&gt;\n    drop_na() |&gt;\n    select(\n        country,\n        happiness_score,\n        healthy_life_expectancy_at_birth,\n        log_gdp_per_capita,\n        perceptions_of_corruption\n    ) |&gt;\n    rename(\n        happiness  = happiness_score,\n        life_exp   = healthy_life_expectancy_at_birth,\n        log_gdp_pc = log_gdp_per_capita,\n        corrupt    = perceptions_of_corruption\n    ) |&gt;\n    # put gdp back on original scale before scaling\n    mutate(\n        gdp_pc = exp(log_gdp_pc), \n        across(life_exp:gdp_pc, \\(x) scale(x)[,1])\n    ) |&gt;\n    select(-log_gdp_pc) # drop the log version\n\n\n\n\ndf_happiness = (\n    pd.read_csv('data/world_happiness_2018.csv')\n    .dropna()\n    .rename(\n        columns = {\n            'happiness_score': 'happiness',\n            'healthy_life_expectancy_at_birth': 'life_exp',\n            'log_gdp_per_capita': 'log_gdp_pc',\n            'perceptions_of_corruption': 'corrupt'\n        }\n    )\n    .assign(\n        gdp_pc = lambda x: np.exp(x['log_gdp_pc']),\n    )    \n    [['country', 'happiness','life_exp', 'gdp_pc', 'corrupt']]\n)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf_happiness[['life_exp', 'gdp_pc', 'corrupt']] = scaler.fit_transform(\n    df_happiness[['life_exp', 'gdp_pc', 'corrupt']]\n)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-guessing",
    "href": "estimation.html#sec-estim-guessing",
    "title": "3  How Did We Get Here?",
    "section": "3.3 Starting Out by Guessing",
    "text": "3.3 Starting Out by Guessing\nSo we’ll start with a model in which we predict a country’s level of happiness by their life expectancy, where if you can expect to live longer, maybe you’re probably in a country with better health care, higher incomes, and other important stuff. We’ll stick with our simple linear model as well.\nAs a starting point we can just guess what the parameter should be, but how would we know what to guess? How would we know which guesses are better than others? Let’s try a few and see how they do. Let’s say that we don’t think life expectancy matters, and that most countries are at a happiness value of 4. We can plug this into the model and see what we get:\n\\[\n\\textrm{prediction} = 4 + 0\\cdot\\textrm{life\\_exp}\n\\]\nAlternatively we could use the data to inform our guess. We start with a mean of happiness score, but moving up a standard deviation of life expectancy (roughly ~1 years) would move us up a whole point of happiness. \\[\n\\textrm{prediction} = \\overline{\\textrm{happiness}} + 1\\cdot\\textrm{life\\_exp}\n\\]\nIn this case, our offset (or intercept) is the mean of the target, and our coefficient for the scaled life expectancy is 1. This is probably a better guess, since it is at least data driven, but it’s still not great. But how do we know it’s better?",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-prediction-error",
    "href": "estimation.html#sec-estim-prediction-error",
    "title": "3  How Did We Get Here?",
    "section": "3.4 Prediction Error",
    "text": "3.4 Prediction Error\nWe can compare the predictions from each guess to the actual values of the target, and observe how far off our predictions are from the observed target. This difference is the prediction error, or in the context of a linear model, they are also called residuals. We can express this as:\n\\[\n\\epsilon = y - \\hat{y}\n\\] \\[\n\\textrm{error} = \\textrm{target} - \\textrm{(model based) guess}\n\\]\nNot only does this tell us how far off our model prediction is, it gives us a way to compare models. With out measure of prediction error, we can calculate a metric for total error for all observations/predictions, or similarlym the average error. If one model or parameter set has less total or average error, we can say it’s a better model than one that has more. Ideally we’d like to choose a model with the least error, but we’ll see that this is not always possible1. For now, let’s calculate the error for our two guesses. One thing though, if we miss the mark above or below our target, we still want it to count the same in terms of prediction error. In other words, if the true happiness score is 5 and our model predicts 5.5 or 4.5, we want those to count as the same kind of error when we total up our error2.\nHowever, if we just take the average, you’ll see that it is roughly zero! This is by design for many common models, were we even will explicitly write the formula for the error as coming from a normal distribution with mean of zero. So we need to do something else to get a meaningful error metric. One way we can get around this is to use the squared error value, or maybe the absolute value. We’ll use squared error here, and we’ll calculate the mean of the squared errors for all our predictions. We’ll do this for our two models above.\n\nRPython\n\n\n\ny = df_happiness$happiness\n\n# Calculate the error for the guess of 4\nprediction = 4\nmse_four   = mean((y - prediction)^2)\n\n# Calculate the error for our other guess\nprediction = mean(y) + 1 * df_happiness$life_exp\nmse_other  = mean((y - prediction)^2)\n\n\n\n\ny = df_happiness['happiness']\n\n# Calculate the error for the guess of four\nprediction = 4\nmse_four   = np.mean((y - prediction)**2)\n\n# Calculate the error for our other guess\nprediction = y.mean() + 1 * df_happiness['life_exp']\nmse_other  = np.mean((y - prediction)**2)\n\n\n\n\nNow let’s look at our Mean Squared Error (MSE), and we’ll also inspect the square root of it, or the Root Mean Squared Error, as that puts things back on the original target scale, and tells us the standard deviation of our prediction errors. We also add the Mean Absolute Error (MAE) as another metric with straightforward interpretation. Inspecting the metrics, we can see that we are off on average by over a point for our ‘#4’ model, but notably less when guessing the mean.\n\n\n\n\nComparison of error metrics for two models\n  \n    \n    \n      Model\n      MSE\n      RMSE\n      MAE\n      RMSE % drop\n      MAE % drop\n    \n  \n  \n    #4\n3.36\n1.83\n1.52\n\n\n    Other\n0.50\n0.71\n0.58\n61%\n62%\n  \n  \n  \n\n\n\n\nWe can see that the ‘other’ model is not only better, but results in a 61% drop in RMSE, and similar for MAE. We’d definitely prefer that model over the ‘#4’ model. Furthermore, we can see how we can compare models in a general fashion.\nNow all of this is useful, and at least we can say one model is better than another. But you’re probably hoping there is an easier way to do get a good guess for our model parameters, especially when we have possibly dozens of features and/or parameters to keep track of, and there is!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-ols",
    "href": "estimation.html#sec-estim-ols",
    "title": "3  How Did We Get Here?",
    "section": "3.5 Ordinary Least Squares",
    "text": "3.5 Ordinary Least Squares\nFor a simple linear model, we can estimate the parameters in several ways, but the most common is to use the Ordinary Least Squares (OLS) method. OLS is a method of estimating the coefficients that minimizes the sum of the squared errors, which we’ve just been doing in the previous section3. In other words, it finds the coefficients that minimize the sum of the squared differences between the predicted values and the actual values. We can express this as:\n\\[\n\\textrm{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n\\tag{3.1}\\]\nWhere \\(y_i\\) is the actual value of the target for observation \\(i\\), and \\(\\hat{y_i}\\) is the predicted value from the model. The sum of the squared errors is also called the residual sum of squares (RSS), as opposed to the total sums of squares (i.e. the variance of the target), and the part explained by the model (model or explained sums of squares). The OLS method finds the coefficients that minimize the sum of the squared differences between the predicted values and the actual values. It’s called ordinary least squares because there are other least squares methods - generalized least squares, weighted least squares, and others, but we don’t need to worry about that for now. What matters is that we have a way to estimate the coefficients that minimizes the sum of the squared errors.\nThe resulting value - the sum or mean of the squared errors - can be referred to as our objective value, while the objective function is just the process of taking the predictions and observed target values as inputs and totaling up their squared differences to be returned as an output. We can use this value to find the best parameters for a specific model, as well as compare models with different parameters, such as a model with additional features versus one with fewer. We can also use this value to compare different types of models that are using the same objective function, such as a linear model and a decision tree model.\nLet’s calculate the OLS estimate for our model. From our steps above, we need guesses and a way to update them. For now, we can just provide a bunch of guesses, and just move along from one set to the next, and ultimately just choose whichever has the lowest value.\n\nRPython\n\n\n\n# for later comparison\nmodel_happy = lm(happiness ~ life_exp, data = df_happiness)\n\nols = function(X, y, par, sum_sq = FALSE) {\n    # add a column of 1s for the intercept\n    X = cbind(1, X)\n\n    # Calculate the predicted values\n    y_hat = X %*% par # %*% is matrix multiplication\n\n    # Calculate the error\n    error = y - y_hat\n\n    # Calculate the value as sum or mean squared error\n    value = crossprod(error) # crossprod is matrix multiplication\n\n    if (!sum_sq) {\n        value = value / nrow(X)\n    }\n\n    # Return the value\n    return(value)\n}\n\n# create a grid of guesses\nguesses = crossing(\n    b0 = seq(1, 7, 0.1),\n    b1 = seq(-1, 1, 0.1)\n)\n\n# Example for one guess\nols(\n    X = df_happiness$life_exp,\n    y = df_happiness$happiness,\n    par = unlist(guesses[1, ])\n)\n\n       [,1]\n[1,] 23.777\n\n\n\n\n\n# for later comparison\nmodel_happy = smf.ols('happiness ~ life_exp', data = df_happiness)\nmodel_happy = model_happy.fit()\n\ndef ols(par, X, y, sum = False):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    y_hat = X @ par\n    \n    # Calculate the error\n    value = np.sum((y - y_hat)**2)\n    \n    # Calculate the value as sum or average\n    if not sum:\n        value = value / X.shape[0]\n    \n    # Return the value\n    return(value)\n\n# create a grid of guesses\nfrom itertools import product\n\nguesses = pd.DataFrame(\n    product(\n        np.arange(1, 7, 0.1),\n        np.arange(-1, 1, 0.1)\n    ),\n    columns = ['b0', 'b1']\n)\n\n# Example for one guess\nols(\n    par = guesses.iloc[0,:],\n    X = df_happiness['life_exp'],\n    y = df_happiness['happiness']\n)\n\n23.793842044979073\n\n\n\n\n\nNow we want to calculate the loss for each guess and find which one gives us the minimum function value. Note that above, we could get the total or mean squared error by setting the sum parameter to TRUE or FALSE. Either is fine, but it’s more common to use the mean, which is a little more understandable - how far do our guess deviate from the true value on average? In the following darker suggests a better mean squared error result from our approach.\n\n\n\nResults of parameter search\n\n\nIf we inspect our results from the built-in functions, we had estimates of 5.44 and 0.89 for our coefficients versus the best guess from our approach of 5.4 and 0.9. These are very similar but not exactly the same, but this is mostly due to the granularity of our guesses. Even so, in the end we can see that we get pretty dang close to what our basic lm or statsmodels functions would get us. Pretty neat!\n\n\n\n\n\n\nEstimation as ‘Learning’\n\n\n\nEstimation, and/or optimization, can be seen as the process of a model learning which parameters will best allow the predictions to match the observed data, and hopefully, predict as-yet-unseen future data. This is a very common way to think about estimation in machine learning, and it is a useful way to think about our simple linear model also.\nOne thing to keep in mind is that it is not a magical process. It takes good data, a good idea (model), and an appropriate estimation method to get good results.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-opt",
    "href": "estimation.html#sec-estim-opt",
    "title": "3  How Did We Get Here?",
    "section": "3.6 Optimization",
    "text": "3.6 Optimization\nBefore we get into other objective functions, let’s think about a better way to find the best parameters for our model. Rather than just guessing, we can use a more systematic approach, and thankfully, there are tools out there to help us. We just use a function like our OLS function, give it a starting point, and let the algorithms do the rest! Thanks to some nifty approaches to making better guesses, these tools eventually arrive at a pretty good set of parameters. Well, they usually do, but not always- nothing’s perfect! But they are pretty good, and they are a lot better than guessing. Let’s see how we can use one of these tools to find the best parameters for our model.\nPreviously we created a set of guesses to search over to see which set of parameters resulted in prediction that matched the data best. What we did is called a grid search, and it is a bit of a brute force approach to finding the best fitting model. You can imagine a couple of unfortunate or problematic scenarios, such as having a very large number of parameters, or that our specified range doesn’t allow us to get to the right sets of parameters, or we specify a very large range, but the best fitting model is within a very narrow part of that range. In any of these cases we waste a lot of time, and may not find an optimal solution.\nIn general, we can think of optimization as a way to find the best parameters for our model. We start with an initial guess, see how well it does in terms of our objective function, and then try to improve it with a new guess. We continue to do so until a stopping point is reached. Here is an example.\n\nStart with an initial guess for the parameters\nCalculate the objective function given the parameters\nUpdate the parameters to a new guess (that hopefully improves the objective function)\nCalculate the objective function given the new parameters\nRepeat until the improvement is small enough, or we reach the desired number of iterations we want to attempt\n\nThis is what we described before with estimation in general. The key idea now is how we update the old parameters with a new guess at each iteration. Different optimization algorithms use different approaches to find the updated parameters. At some point, either the improvement is no longer practical, often refered to as our tolerance, or we reach a maximum number of iterations we want to attempt, and either of these is something we can set ourselves. If we meet the terms of our objective, we say that our model has converged. Sometimes, the number of iterations is not enough for us to reach convergence in terms of tolerance, and we have to try again with a different set of parameters, a different algorithm, maybe use some data transformations, or something else.\nSo let’s try it out! Both R and Python offer a function where we can specify the objective function, and it will try to find the best parameters for us. It needs several inputs:\n\nthe objective function\nthe initial guess for the parameters to get things going\ninputs to the objective function\noptions for the optimization process, e.g. algorithm, maximum number of iterations, etc.\n\nWith these inputs, we’ll let the optimization functions do the rest of the work. We’ll also compare our results to the built-in functions to make sure we’re on the right track.\n\nRPython\n\n\nWe’ll use the optim function in R.\n\nour_result = optim(\n    par     = c(1, 0),\n    fn      = ols,\n    X       = df_happiness$life_exp,\n    y       = df_happiness$happiness,\n    method  = \"BFGS\", # optimization algorithm\n    control = list(   # specify tolerance, max iter, etc. here\n        tol   = 1e-6,\n        maxit = 500\n    )  \n)\n\n# our_result\n\n\n\nWe’ll use the minimize function in Python.\n\nfrom scipy.optimize import minimize\n\nour_result = minimize(\n    fun    = ols,\n    x0     = np.array([1., 0.]),\n    args   = (\n        np.array(df_happiness['life_exp']), \n        np.array(df_happiness['happiness'])\n    ),\n    method = 'BFGS', # optimization algorithm\n    tol    = 1e-6,   # tolerance\n    options = {\n        'maxiter': 500\n    }\n)\n\n# our_result\n\n\n\n\n\n\n\n\nTable 3.1: Comparison of our results to built-in function\n\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Built-in\n      Our Result\n    \n  \n  \n    Intercept\n5.4450\n5.4450\n    Life Exp. Coef.\n0.8880\n0.8880\n    Objective/MSE\n0.4890\n0.4890\n  \n  \n  \n\n\n\n\n\n\n\nSo our little function and the right tool allows us to come up with the same thing as base R and statsmodels! I hope you’re feeling pretty good at this point because you should! You just proved you could do what seemed before to be like magic, but really all it took is just a little knowledge about some key concepts. Let’s try some more!\n\n\n\n\n\n\nA Note on Terminology\n\n\n\nThe objective function is often called the loss function, and sometimes the cost function. However, these both imply that we are trying to minimize the function, which is not always the case4, and it’s arbitrary whether you want to minimize or maximize the function. In fact, some people will minimize the negative likelihood when using maximum likelihood! As such we’ll try to stick to the more neutral term ‘objective function’, but you may see the other terms used interchangeably in this text. In addition, some packages will use the term metric to refer a value that you might want to examine as well, or even use to compare models. For example, the MSE is a metric, but it may also be the objective function we are trying to minimize. Other metrics we could calculate without being the objective might be Adjusted R-squared and mean absolute error. We could also use MSE as the objective, but use percentage drop in error from baseline when selecting among several models that minimized MSE. This can be very confusing when starting out! We’ll try to stick to the term metric for additional values that we might want to examine separate from the objective function value.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-maxlike",
    "href": "estimation.html#sec-estim-maxlike",
    "title": "3  How Did We Get Here?",
    "section": "3.7 Maximum Likelihood",
    "text": "3.7 Maximum Likelihood\nIn our example thus far, we have been minimizing the specific objective (or loss) function, which basically takes our parameter estimates, produces a prediction, and returns the sum or mean of the squared errors. But this is just one approach we could take. Now we’d like you to think about the data generating process. We have a model that says happiness is a function of life expectancy, but more specifically, let’s think about how the observed value of the happiness score is generated in a statistical sense. In particular, what kind of probability distribution might be involved? Ignoring the model, we might think that each happiness value is generated by some random process, and that the process is the same for each observation. Let’s assume that random process is a normal distribution. So something like this would describe it mathematically:\n\\[\n\\textrm{happiness} \\sim N(\\mu, \\sigma)\n\\]\nwhere \\(\\mu\\) is the mean of the happiness and \\(\\sigma\\) is the standard deviation, or in other words, we can think of happiness as a random variable that is drawn from a normal distribution with \\(\\mu\\) and \\(\\sigma\\) as the parameters of that distribution.\nLet’s apply this idea to our linear model setting. In this case, the mean is a function of life expectancy, and we’re not sure what the standard deviation is, but we can go ahead and write our model as follows.\n\\[\n\\mu = \\beta_0 + \\beta_1 * \\textrm{life\\_exp}\n\\] \\[\n\\textrm{happiness} \\sim N(\\mu, \\sigma)\n\\]\nNow, we can think of the model as a way to estimate the parameters of the normal distribution, but we have an additional parameter to estimate. We still have our previous coefficients, but now we need to estimate \\(\\sigma\\), which is basically our RMSE, as well. But we still have to think of things a little differently. When we compare our prediction to the observed value, we don’t look at the simple difference, but we are still interested in the discrepancy between the two. So now we think about the likelihood of observing the happiness score given our prediction, which is based on the estimated parameters, i.e. given the \\(\\mu\\) and \\(\\sigma\\), and \\(\\mu\\) is a function of the coefficients and life expectancy. We can write this as:\n\\[\n\\textrm{Pr}(\\textrm{happiness} \\mid \\textrm{life\\_exp}, \\beta_0, \\beta_1, \\sigma)\n\\]\n\\[\n\\textrm{Pr}(\\textrm{happiness} \\mid \\mu, \\sigma)\n\\]\nEven more generally, the likelihood gives us a sense of the probability of the observed data given the parameter estimates \\(\\theta\\). \\[\n\\textrm{Pr}(\\textrm{Data} \\mid \\theta)\n\\]\nHere is a simple code demo to get a likelihood in the context of our model. The values you see are referred to statistically as probability density values, and they are technically not probabilities, but rather the probability density, or relative likelihood, at that observation5. For your conceptual understanding, if it makes it easier, you can think of them in the same was as you do probabilities, but just know that technically they are not.\nTODO: UPDATE VALUES WHEN DEMO IS SETTLED\n\nRPython\n\n\n\n# two example life expectancy scores, mean and 1 sd above\nlife_expectancy = c(0, 1)\n\n# observed happiness scores\nhappiness = c(4, 5.2)\n\n# predicted happiness with rounded coefs\nmu = 5 + 1 * life_expectancy\n\n# just a guess for sigma\nsigma = .5\n\n# likelihood for each observation\nL = dnorm(happiness, mean = mu, sd = sigma)\nL\n\n[1] 0.1079819 0.2218417\n\n\n\n\n\nfrom scipy.stats import norm\n\n# two example life expectancy scores, mean and 1 sd above\nlife_expectancy = np.array([0, 1])\n\n# observed happiness scores\nhappiness = np.array([4, 5.2])\n\n# predicted happiness with rounded coefs\nmu = 5 + 1 * life_expectancy\n\n# just a guess for sigma\nsigma = .5\n\n# likelihood for each observation\nL = norm.pdf(happiness, loc = mu, scale = sigma)\nL\n\narray([0.1080, 0.2218])\n\n\n\n\n\nSo, given a guess at the parameters, and an assumption about the distribution of the data, we can calculate the likelihood of observing each data point, and sum those up, just like we did with our squared errors. In theory, we’d deal with the product of each likelihood, but in practice we sum the log of the likelihood, otherwise values would get too small for our computers to handle. Here is a corresponding function we can use to calculate the likelihood of the data given our parameters. Note that the actual likelihood value returned isn’t really interpretable, just that higher is better from a maximization standpoint. But we can use it to compare models with different sets of parameter guesses. Even if our total likelihoods under comparison are negative, we prefer the model with the relatively higher likelihood. As we just demonstrated, we’ll use optim to help us get good guesses6.\n\nRPython\n\n\n\nlikelihood = function(par, X, y) {\n    X = cbind(1, X)\n    # setup\n    beta = par[-1] # coefficients\n    sigma = exp(par[1]) # error sd, exp keeps positive\n\n    N = nrow(X)\n\n    LP = X %*% beta # linear predictor\n    mu = LP # identity link in the glm sense\n\n    # calculate (log) likelihood\n    ll = dnorm(y, mean = mu, sd = sigma, log = TRUE)\n    -sum(ll) # for minimization\n}\n\n\nour_result = optim(\n    par = c(1, 0, 0),\n    fn  = likelihood,\n    X   = df_happiness$life_exp,\n    y   = df_happiness$happiness\n)\n\n# our_result\n\n\n\n\ndef likelihood(par, X, y):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # setup\n    beta   = par[1:]         # coefficients\n    sigma  = np.exp(par[0])  # error sd, exp keeps positive\n\n    N = X.shape[0]\n\n    LP = X @ beta          # linear predictor\n    mu = LP                # identity link in the glm sense\n\n    # calculate (log) likelihood\n    ll = norm.logpdf(y, loc = mu, scale = sigma) \n    return(-np.sum(ll))\n\nour_result = minimize(\n    fun  = likelihood,\n    x0   = np.array([1, 0, 0]),\n    args = (\n        np.array(df_happiness['life_exp']), \n        np.array(df_happiness['happiness'])\n    )\n)\n\n\n\n\nHow would we switch to a maximum likelihood approach using readily available functions? In both R and Python you can switch to using glm and GLM respectively as a start. We can use different likelihoods corresponding to the binomial, poisson and other distributions. Still other packages would allow even more distributions for consideration. In general, we choose a distribution that we feel best reflects the data generating process. For binary targets for example, we typically would feel a bernoulli or binomial distribution is appropriate. For count data, we might choose a poisson or negative binomial distribution. For targets that fall between 0 and 1, we might go for a beta distribution. There are many distributions, and even when some might feel more appropriate, we might choose another for convenience. Some distributions tend toward a normal (a.k.a. gaussian) distribution depending on various factors, while others are special cases of more general distributions. For example, the exponential distribution is a special case of the gamma distribution, and a cauchy is equivalent to a t distribution with 1 degree of freedom, and the t tends toward a normal with increasing degrees of freedom. Here is a visualization of the relationships among some of the more common distributions Wikipedia (2023).\nTODO: fig needs work for pdf\n\n\n\n\nRelationships among some of probability distributions\n\n\n\nHere are examples of standard GLM functions in R and Python\n\nRPython\n\n\n\nglm(happiness ~ life_exp, data = df_happiness, family = gaussian)\nglm(binary_target ~ x1 + x2, data = some_data, family = binomial)\nglm(count ~ x1 + x2, data = some_data, family = poisson)\n\n\n\n\nimport statsmodels.formula.api as smf\n\nsmf.glm(\n    'happiness ~ life_exp', \n    data = df_happiness, \n    family = sm.families.Gaussian()\n)\n\nsmf.glm(\n    'binary_target ~ x1 + x2', \n    data = some_data, \n    family = sm.families.Binomial()\n)\n\nsmf.glm(\n    'count ~ x1 + x2', \n    data = some_data, \n    family = sm.families.Poisson()\n)\n\n\n\n\nWith that in mind, we can compare our result to a built-in function that has capabilities beyond OLS. As before, we’re duplicating the basic glm result. We show more decimal places on the log likelihood estimate to prove we aren’t getting exactly the same result\n\n\n\n\nTable 3.2: Comparison of our results to built-in function\n\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Built-in\n      Our Result\n    \n  \n  \n    Intercept\n5.44\n5.44\n    Life Exp. Coef.\n0.89\n0.89\n    Sigma\n0.71\n0.701\n    LogLik (neg)\n118.80\n118.80\n  \n  \n  \n    \n      1 Parameter estimate is exponentiated\n    \n  \n\n\n\n\n\n\n\nLet’s think more about what’s going on here. It turns out that our objective function defines a space or surface. We can think of it as a landscape, and we are trying to find the lowest point on that landscape. We can then think of our guesses as points on that landscape, and we are trying to find the lowest point. Let’s start get a sense of this with the following visualization, based on a single parameter. The data is drawn from Poisson distributed variable with true mean \\(\\theta=5\\). We note the calculated likelihood increases as we estimate values for \\(\\theta\\) closer to \\(5\\), or more precisely, whatever the mean observed value is for the data. However, with more and more data, the final ML estimate will converge on the true value. Model estimation finds that maximum on the curve, and optimization algorithms are the means to find it.\n\n\n\n\n\n\n\n\nFigure 3.1: Likelihood function one parameter\n\n\n\n\n\nNow let’s add a parameter. If we have more than one parameter, we now have a surfaace to deal with. Given some starting point, an optimization procedure then travels along the surface looking for a minimum/maximum point. For simpler settings such as this, we can visualize the likelihood surface and its minimum point. However, even our simple demo model has three parameters plus the likelihood, so would be difficult to visualize without additional complexity. To get around this, we show the results for an alternate model where happiness is standardized also, which means the intercept is zero7, and we don’t have to show that.\nTODO: CANT DO INTERACTIVE WITH PDF/LATEX. NEED WORKAROUND.\n\n\n\n\n\n\n\n\nFigure 3.2: Likelihood surface for two parameters\n\n\n\n\n\n\nWe can also see the path our estimates take, starting at a rather poor point, but quickly updating to estimates that result in a better likelihood value. We also see little exploratory jumps creating a star like pattern, before things ultimately settle to the best values. In general, these updates and paths are dependent on the optimization algorithm one uses.\n\n\n\n\n\n\n\n\nFigure 3.3: Optimization path for two parameters\n\n\n\n\n\n\nIt turns out that in the case of a normal distribution, the maximum likelihood estimate of the standard deviation is the estimate as the standard deviation of the residuals. Furthermore, the maximum likelihood estimates and OLS estimates converge to the same estimates as the sample size increases. For any data of significance, these estimates are indistinguishable, and the OLS estimate is the maximum likelihood estimate for linear regression.\n\n\n3.7.0.1 Additional Thoughts on Maximum Likelihood\nTODO: Remove?\nOne of the key things to note is that maximum likelihood is an estimation technique that relies on specifying the probability distribution that serves as the data generating process. Maximum likelihood allows us to be explicit about why we think those target values are the way they are. The likelihood also serves as a fundamental part of Bayesian analysis, which we’ll discuss more later. In general, maximum likelihood is a powerful technique that can be used in many contexts, and likelihoods can be used as the objective for many machine learning algorithms as well.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-review",
    "href": "estimation.html#sec-estim-review",
    "title": "3  How Did We Get Here?",
    "section": "3.8 Estimation: Quick Review",
    "text": "3.8 Estimation: Quick Review\nTODO: MOVE WHERE? NEEDED?\nAt this point we understand a few things:\n\nParameters are the values associated with a model\nObjective functions specify a modeling goal with which to estimate the parameters.\nEstimation is a way of finding the best model, i.e. parameters that help us achieve a goal.\nOptimization is the process of finding the parameters that maximize or minimize some objective function\nThe likelihood is an alternate way to assess the match of data and model, and allows us to compare the relative fits of models",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-penalty",
    "href": "estimation.html#sec-estim-penalty",
    "title": "3  How Did We Get Here?",
    "section": "3.9 Penalized Objectives",
    "text": "3.9 Penalized Objectives\nTODO: MOVE TO AFTER CLASSIFICATION?\nOne thing we may want to take into account of with our models is their complexity, especially in the context of overfitting. We talk about this with machine learning also, but the basic idea is that we can get too close to the data we have, such that when we try to predict on new data, our performance suffers or even gets worse than a simpler model. In other words, we are not generalizing well. One way to deal with this is to penalize the objective function value for complexity, or at least favor simpler models that might do as well. In some contexts this is called regularization, and in other contexts shrinkage, since the parameter estimates are typically shrunk toward some specific value (e.g., zero).\nAs a starting point, in our basic linear model we can add a penalty that is applied to the size of coefficients. This is called ridge regression, or, more mathily, as L2 regularization. The penalty is just the sum of the squared coefficients multiplied by a some value, which we call \\(\\lambda\\). We can write this formally as:\n\\[\n\\textrm{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n\\tag{3.2}\\]\nThe first part is the same as before (Equation 3.1), but the second part is the penalty for \\(p\\) features. The penalty is the sum of the squared coefficients multiplied by some value, which we call \\(\\lambda\\). This is an additional model parameter that we typically want to estimate in some fashion, e.g. through cross-validation. This kind of parameter is often called a hyperparameter, mostly just to distinguish it from those that may be of actual interest. For example, we could probably care less what the actual value for \\(\\lambda\\) is, but we would still be interested in the coefficients.\nInterestingly, as you’ll notice that this is just OLS+, you might be wondering how our results or interpretation might change. Well for starters, L2 regularization is not limited to linear regression, so just keep that in mind. But also, if we know that OLS produces unbiased estimates if assumptions of linear regression are met, that means, if these aren’t the same estimates, they must be biased, right? Your are correct! As we talk about with machine learning (Section 6.4), the bias-variance tradeoff is a key concept in machine learning, and this is a good example of that. We are introducing some bias in order to reduce the variance. In other words, we are willing to accept some bias in order to get a model that generalizes better.\nAnother common penalty that is the sum of the absolute value of the coefficients, which is called lasso regression or L1 regularization. An interersting property of the lasso is that in typical implementations, it will potentially zero out coefficients, which is the same as dropping the feature from the model altogether. This is a form of feature selection or variable selection. The true values are never zero, but if we want to use a ‘best subset’ of features, this is one way we could do so. We can write the lasso objective as:\n\\[\n\\textrm{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\tag{3.3}\\]\nBut let’s get to a code example to make sure we understand this better! Here is an example of a function that calculates the ridge objective. To make things interesting, let’s add the other features we talked about regarding GDP per capita and perceptions of corruption.\n\nRPython\n\n\n\nridge = function(par, X, y, lambda = 0) {\n    # add a column of 1s for the intercept\n    X = cbind(1, X)\n\n    # Calculate the predicted values\n    mu = X %*% par # %*% is matrix multiplication\n\n    # Calculate the value as sum squared error\n    error = crossprod(y - mu)\n\n    # Add the penalty\n    value = error + lambda * crossprod(par)\n\n    return(value)\n}\n\nour_result = optim(\n    par = c(0, 0, 0, 0),\n    fn = ridge,\n    X = df_happiness |&gt; select(-happiness, -country) |&gt; as.matrix(),\n    y = df_happiness$happiness,\n    lambda = 0.1,\n    method = \"BFGS\"\n)\n\n\n\n\n# we use lambda_ because lambda is a reserved word in python\ndef ridge(par, X, y, lambda_ = 0):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    mu = X @ par\n    \n    # Calculate the error\n    value = np.sum((y - mu)**2)\n    \n    # Add the penalty\n    value = value + lambda_ * np.sum(par**2)\n    \n    return(value)\n\nour_result = minimize(\n    fun  = ridge,\n    x0   = np.array([0, 0, 0, 0]),\n    args = (\n        np.array(df_happiness.drop(columns=['happiness', 'country'])),\n        np.array(df_happiness['happiness']), \n        0.1\n    )\n)\n\n\n\n\nWe can compare this to built-in functions as we have before, and can see that the results are very similar, but not exactly the same. We would not worry about such differences in practice, but the main point is again, we can use simple functions that do just about as well as any what we’d get from package output.\n\n\n\n\nTable 3.3: Comparison of ridge regression results\n\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Built-in1\n      Our Result\n    \n  \n  \n    Intercept\n5.44\n5.44\n    Life Exp. Coef.\n0.49\n0.52\n    Corrupt\n−0.12\n−0.11\n    GDP_PC\n0.42\n0.44\n  \n  \n  \n    \n      1 Showing results from R glmnet package with alpha = 0, lambda = .1\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nIt turns out that, given a set \\(\\lambda\\) penalty, ridge regression estimates need not be estimated, as there is an analytical result. See a demo for more.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-classification",
    "href": "estimation.html#sec-estim-classification",
    "title": "3  How Did We Get Here?",
    "section": "3.10 Classification",
    "text": "3.10 Classification\nSo far we’ve been assuming a continuous target, but what if we have a categorical target? Now we have to learn a bunch of new stuff for that situation, right? Actually, no! When we want to model categorical targets, conceptually nothing changes - we can still have an objective function that maximizes or minimizes some goal, use the same algorithms to estimate parameters, etc. However, we need to think about how we can do this in a way that makes sense for the target.\n\n3.10.1 Misclassification\nA straightforward correspondence to MSE is a function that minimizes classification error (or maximizes accuracy). In other words, we can think of the objective function as the proportion of incorrect classifications. This is called the misclassification rate. We can write this as:\n\\[\n\\textrm{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i \\neq \\hat{y_i})\n\\]\nWhere \\(y_i\\) is the actual value of the target for observation \\(i\\), arbitrarily coded as 1 or 0, and \\(\\hat{y_i}\\) is the predicted class from the model. The \\(\\mathbb{1}\\) is an indicator function that returns 1 if the condition is true, and 0 otherwise. In other words, we are counting the number of times the predicted value is not equal to the actual value, and dividing by the number of observations. Very straightforward, so let’s do this ourselves!\n\nRPython\n\n\n\n# misclassification rate\nmisclassification = function(par, X, y, class_threshold = .5) {\n    X = cbind(1, X)\n    # Calculate the predicted values\n    mu = X %*% par # %*% is matrix multiplication\n\n    # Convert to a probability ('sigmoid' function)\n    p = 1 / (1 + exp(-mu))\n\n    # Convert to a class\n    predicted_class = as.integer(\n        ifelse(p &gt; class_threshold, \"good\", \"bad\")\n    )\n\n    # Calculate the error\n    error = y - predicted_class\n\n    return(mean(error))\n}\n\n\n\n\ndef misclassification_rate(par, X, y, class_threshold = .5):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    mu = X @ par\n    \n    # Convert to a probability ('sigmoid' function)\n    p = 1 / (1 + np.exp(-mu))\n    \n    # Convert to a class\n    predicted_class = np.where(p &gt; class_threshold, 1, 0)\n    \n    # Calculate the error\n    error = y - predicted_class \n    \n    return(np.mean(error))\n\n\n\n\nWe’ll leave it as an exercise to the reader to play around with this, as the next objective function is more commonly used. But at least you can see how easy it can be to switch to the classification case.\n\n\n3.10.2 Log loss\nAnother approach is to use the log loss, sometimes called logistic loss or cross-entropy. If we have just the binary case it is:\n\\[\n\\textrm{Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})\n\\]\nWhere \\(y_i\\) is the actual value of the target for observation \\(i\\), and \\(\\hat{y_i}\\) is the predicted value from the model (essentially a probability). It turns out that this is the same as log-likelihood used in a maximum likelihood approach for logistic regression, made negative for minimization. We typically prefer this objective function to classification error because it results in a smooth optimization surface, like in the visualization we showed before for maximum likelihood (Figure 3.2), which means it is differentiable in mathemetical sense. This is important because it allows us to use optimization algorithms that rely on derivatives in updating the parameter estimates. You don’t really need to get into that too much, but just know that it is a good thing. Here’s some code to try out.\n\nRPython\n\n\n\nobjective = function(par, X, y) {\n    X = cbind(1, X)\n\n    # Calculate the predicted values on the raw scale\n    y_hat = X %*% par\n\n    # Convert to a probability ('sigmoid' function)\n    y_hat = 1 / (1 + exp(-y_hat))\n\n    # likelihood (or dbinom(y, size = 1, prob = y_hat, log = TRUE))\n    ll = y * log(y_hat) + (1 - y) * log(1 - y_hat)\n\n    return(sum(-ll))\n}\n\n\n\n\ndef objective(par, X, y):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    y_hat = X @ par\n    \n    # Convert to a probability ('sigmoid' function)\n    y_hat = 1 / (1 + np.exp(-y_hat))\n    \n    # likelihood\n    ll = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n    \n    return(-np.sum(ll))\n\n\n\n\nLet’s go ahead and demonstrate this. Let’s go back to our movie review data, but we’ll use a version of our rating where a movie is ‘good’ if the rating is 3 or greater, and ‘bad’ otherwise, which we have in our processed version of the data. Our features will be the review year (starting at zero), reviewer age, and word count. Let’s use our previous optimization functions, and compare our results to the built-in complements.\n\nRPython\n\n\n\ndf_reviews_pr = read_csv(\"data/movie_reviews_processed.csv\")\n\nmod_logloss = optim(\n    par = c(0, 0, 0, 0),\n    fn = objective,\n    X = df_reviews_pr |&gt;\n        select(review_year_0, age_sc, word_count_sc) |&gt;\n        as.matrix(),\n    y = df_reviews_pr$rating_good\n)\n\nmod_glm = glm(\n    rating_good ~ review_year_0 + age_sc + word_count_sc,\n    data   = df_reviews_pr,\n    family = binomial\n)\n\n\n\n\nfrom scipy.optimize import minimize\n\nmod_logloss = minimize(\n    objective,\n    x0 = np.array([0, 0, 0, 0]),\n    args = (\n        df_reviews_pr[['review_year_0', 'age_sc', 'word_count_sc']], \n        df_reviews_pr['rating_good']\n    )\n)\n\nmod_glm = smf.glm(\n    'rating_good ~ review_year_0 + age_sc + word_count_sc',\n    data   = df_reviews_pr,\n    family = sm.families.Binomial()\n).fit(method = 'lbfgs')\n\n\n\n\nOnce again we can see that the results are very similar, but not exactly the same, though actually have to go out several decimal places before we start seeing differences between our result and the built-in function.\n\n\n\n\nTable 3.4: Comparison of log loss results\n\n\n\n\n\n\n  \n    \n    \n      name\n      Ours\n      GLM\n    \n  \n  \n    LogLike\n622.5935\n622.5935\n    int\n−0.0819\n−0.0818\n    review_year_0\n0.0213\n0.0213\n    age_sc\n−0.2213\n−0.2213\n    word_count_sc\n−0.7344\n−0.7343\n  \n  \n  \n\n\n\n\n\n\n\nSo when it comes to classification, you should feel confident in what’s going on under the hood, just like you did with a numeric target. Conceptually it really is the same approach.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-opt-algos",
    "href": "estimation.html#sec-estim-opt-algos",
    "title": "3  How Did We Get Here?",
    "section": "3.11 Optimization Algorithms",
    "text": "3.11 Optimization Algorithms\nWhen it comes to optimization, there are a number of algorithms that have been developed over time. We’ll demonstate one of the most popular ones used in machine learning, but there many variants of this one even! The main thing to keep in mind is that these are all just ways to find the best fitting parameters for a model. The algorithms differ in how they do this, and some may be better suited for certain data tasks, or provide computatational advantages.\n\n3.11.1 Gradient Descent\nOne of the most common approaches in optimization is called gradient descent. The idea behind it is that we can use the gradient of the objective function to guide us to the best fitting parameters. We still use estimation approaches like maximum likelihood - gradient descent is just a way to find that path along the objective surface. More formally, the gradient is the vector of partial derivatives of the objective function with respect to each parameter. That may not mean much to you, but the basic idea is that the gradient is a vector that points in the direction of steepest ascent in terms of the objective function. So if we want to maximize the objective function, we can take a step in the direction of the gradient, and if we want to minimize it, we can take a step in the opposite direction of the gradient. The size of the step is called the learning rate, and, like our penalty parameter we saw with penalized regression, it is a hyperparameter that we can tune. If the learning rate is too small, it will take a longer time to converge. If the learning rate is too large, we might overshoot the objective and never converge. There are a number of variations on gradient descent that have been developed over time. Here is a function to illustrate the process. Let’s see this in action with the happiness data model we used previously.\n\nRPython\n\n\n\ngradient_descent = function(\n    par,\n    X,\n    y,\n    tolerance = 1e-3,\n    maxit = 1000,\n    learning_rate = 1e-3,\n    adapt = FALSE,\n    verbose = TRUE\n) {\n    # add a column of 1s for the intercept\n    X = cbind(1, X)\n    N = nrow(X)\n\n    # initialize\n    beta = par\n    names(beta) = colnames(X)\n    mse = crossprod(X %*% beta - y) / N\n    tol = 1\n    iter = 1\n\n    while (tol &gt; tolerance && iter &lt; maxit) {\n        LP = X %*% beta\n        grad = t(X) %*% (LP - y)\n        betaCurrent = beta - learning_rate * grad\n        tol = max(abs(betaCurrent - beta))\n        beta = betaCurrent\n        mse = append(mse, crossprod(LP - y) / N)\n        iter = iter + 1\n\n        if (adapt) {\n            stepsize = ifelse(\n                mse[iter] &lt; mse[iter - 1],\n                stepsize * 1.2,\n                stepsize * .8\n            )\n        }\n\n        if (verbose && iter %% 10 == 0) {\n            message(paste(\"Iteration:\", iter))\n        }\n    }\n\n    list(\n        par    = beta,\n        loss   = mse,\n        MSE    = crossprod(LP - y) / nrow(X),\n        iter   = iter,\n        fitted = LP\n    )\n}\n\nour_result = gradient_descent(\n    par = c(0, 0, 0, 0),\n    X = df_happiness |&gt; select(life_exp, gdp_pc, corrupt) |&gt; as.matrix(),\n    y = df_happiness$happiness,\n    learning_rate = 1e-3,\n    verbose = FALSE\n)\n\n\n\n\ndef gradient_descent(\n    par, \n    X, \n    y, \n    tolerance = 1e-3, \n    maxit = 1000, \n    learning_rate = 1e-3, \n    adapt = False, \n    verbose = True\n):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n    \n    # initialize\n    beta = par\n    loss = np.sum((X @ beta - y)**2)\n    tol = 1\n    iter = 1\n\n    while (tol &gt; tolerance and iter &lt; maxit):\n        LP = X @ beta\n        grad = X.T @ (LP - y)\n        betaCurrent = beta - learning_rate * grad\n        tol = np.max(np.abs(betaCurrent - beta))\n        beta = betaCurrent\n        loss = np.append(loss, np.sum((LP - y)**2))\n        iter = iter + 1\n\n        if (adapt):\n            stepsize = np.where(\n                loss[iter] &lt; loss[iter - 1], \n                stepsize * 1.2, \n                stepsize * .8\n            )\n\n        if (verbose and iter % 10 == 0):\n            print(\"Iteration:\", iter)\n\n    return({\n        \"par\": beta,\n        \"loss\": loss,\n        \"RSE\": np.sqrt(np.sum((LP - y)**2) / (X.shape[0] - X.shape[1])),\n        \"iter\": iter,\n        \"fitted\": LP\n    })\n\nour_result = gradient_descent(\n    par = np.array([0, 0, 0, 0]),\n    X = df_happiness[['life_exp', 'gdp_pc', 'corrupt']].to_numpy(),\n    y = df_happiness['happiness'].to_numpy(),\n    learning_rate = 1e-3,\n    verbose  = False\n)\n\n\n\n\nComparing our results, we have the following table. In what has become the norm, we see that the results are very similar.\n\n\n\n\nTable 3.5: Comparison of gradient descent results\n\n\n\n\n\n\n  \n    \n    \n      Value\n      Built-in\n      Our Result\n    \n  \n  \n    Intercept\n5.445\n5.437\n    Life Exp. Coef.\n0.525\n0.521\n    GDP_PC\n0.438\n0.439\n    Corrupt\n−0.105\n−0.107\n    MSE\n0.367\n0.367\n  \n  \n  \n\n\n\n\n\n\n\nIn addition, when we visualize the loss function across iterations, we see smooth decline in the MSE value as we go along each iteration. This is a good sign that we are converging to a good solution.\n\n\n\n\n\n\n\n\nFigure 3.4: Gradient descent path\n\n\n\n\n\n\n\n3.11.2 Stochastic Gradient Descent\nStochastic gradient descent (SGD) is a variation on gradient descent that uses a random sample of the data to estimate the gradient, while the ‘true’ gradient is the gradient of the objective function with respect to all of the data. As such, it’s less accurate than the ‘batch’ gradient descent in some sense, but the advantage of SGD is that it is faster. In practice, SGD is often used in machine learning applications where the data is large, and the tradeoff between accuracy and speed is worth it.\nLet’s see this in action with the happiness data model we used previously. The following is a conceptual version of the AdaGrad approach8, which is a variation of stochastic gradient descent that adjusts the learning rate for each parameter. We will also add a variation that averages the parameter estimates across iterations, which is a common approach to improve the performance of stochastic gradient descent, but by default it is not used, just something you can play with. We are going to use a ‘batch size’ of one, which is similar to a ‘streaming’ or ‘online’ version where we update the model with each observation. Since our data are alphabetically ordered, we’ll shuffle the data first. We’ll also use a stepsize_tau parameter, which is a way to adjust the learning rate at early iterations. We’ll set it to zero for now, but you can play with it to see how it affects the results. The values for the learning rate and stepsize_tau are arbitrary, selected after some initial playing around, but you can play with them to see how they affect the results.\nTODO: SHOULD MAYBE CLEAN UP/ALTER TO LESS VERBOSE VERSION\n\nRPython\n\n\n\nstochastic_gradient_descent = function(\n    par, # parameter estimates\n    X, # model matrix\n    y, # target variable\n    learning_rate = 1, # the learning rate\n    stepsize_tau = 0, # if &gt; 0, a check on the LR at early iterations\n    average = FALSE # a variation of the approach\n    ) {\n    # initialize\n    X = cbind(1, X)\n    beta = par\n\n    # Collect all estimates\n    betamat = matrix(0, nrow(X), ncol = length(beta))\n\n    # Collect fitted values at each point))\n    fits = NA\n\n    # Collect loss at each point\n    loss = NA\n\n    # adagrad per parameter learning rate adjustment\n    s = 0\n\n    # a smoothing term to avoid division by zero\n    eps = 1e-8\n\n    for (i in 1:nrow(X)) {\n        Xi = X[i, , drop = FALSE]\n        yi = y[i]\n\n        # matrix operations not necessary here,\n        # but makes consistent with standard gd func\n        LP = Xi %*% beta\n        grad = t(Xi) %*% (LP - yi)\n        s = s + grad^2 # adagrad approach\n\n        # update\n        beta = beta - learning_rate / (stepsize_tau + sqrt(s + eps)) * grad\n\n        # a variation\n        if (average & i &gt; 1) {\n            beta = beta - 1 / i * (betamat[i - 1, ] - beta)\n        }\n\n        betamat[i, ] = beta\n        fits[i] = LP\n        loss[i] = crossprod(LP - yi)\n    }\n\n    LP = X %*% beta\n    lastloss = crossprod(LP - y)\n\n    list(\n        par = beta, # final estimates\n        par_chain = betamat, # estimates at each iteration\n        MSE = sum(lastloss) / nrow(X),\n        fitted = LP\n    )\n}\n\n# setting a seed ensures replicability\nset.seed(123)\n\n# generate random sample indices (could also have done within the function)\nidx = sample(1:nrow(df_happiness), nrow(df_happiness))\n\nX_train = df_happiness |&gt;\n    select(life_exp, gdp_pc, corrupt) |&gt;\n    dplyr::slice(idx) |&gt;\n    as.matrix()\n\ny_train = df_happiness$happiness[idx]\n\nour_result = stochastic_gradient_descent(\n    par = c(mean(df_happiness$happiness), 0, 0, 0),\n    X = X_train,\n    y = y_train,\n    learning_rate = .15,\n    stepsize_tau = .1\n)\n\n\n\n\ndef stochastic_gradient_descent(\n    par, # parameter estimates\n    X, # model matrix\n    y, # target variable\n    learning_rate = 1, # the learning rate\n    stepsize_tau = 0, # if &gt; 0, a check on the LR at early iterations\n    average = False # a variation of the approach\n):\n    # initialize\n    X = np.c_[np.ones(X.shape[0]), X]\n    beta = par\n\n    # Collect all estimates\n    betamat = np.zeros((X.shape[0], beta.shape[0]))\n\n    # Collect fitted values at each point))\n    fits = np.zeros(X.shape[0])\n\n    # Collect loss at each point\n    loss = np.zeros(X.shape[0])\n\n    # adagrad per parameter learning rate adjustment\n    s = 0\n\n    # a smoothing term to avoid division by zero\n    eps = 1e-8\n\n    for i in range(X.shape[0]):\n        Xi = X[None, i, :]\n        yi = y[i]\n\n        # matrix operations not necessary here,\n        # but makes consistent with standard gd func\n        LP = Xi @ beta\n        grad = Xi.T @ (LP - yi)\n        s = s + grad**2 # adagrad approach\n\n        # update\n        beta = beta - learning_rate / \\\n            (stepsize_tau + np.sqrt(s + eps)) * grad\n\n        # a variation\n        if (average & i &gt; 1):\n            beta = beta - 1 / i * (betamat[i - 1, :] - beta)\n\n        betamat[i, :] = beta\n        fits[i] = LP\n        loss[i] = np.sum((LP - yi)**2)\n\n    LP = X @ beta\n    lastloss = np.sum((LP - y)**2)\n\n    return({\n        \"par\": beta, # final estimates\n        \"par_chain\": betamat, # estimates at each iteration\n        \"MSE\": lastloss / X.shape[0],\n        \"fitted\": LP\n    })\n\n# setting a seed ensures replicability\nnp.random.seed(1234)\n\n# generate random sample indices (could also have done within the function)\nidx = np.random.choice(\n    df_happiness.shape[0], \n    df_happiness.shape[0], \n    replace = False\n)\n\nX_train = df_happiness[['life_exp', 'gdp_pc', 'corrupt']].to_numpy()[idx, :]\ny_train = df_happiness['happiness'].to_numpy()[idx]\n\nour_result = stochastic_gradient_descent(\n    par = np.array([np.mean(df_happiness['happiness']), 0, 0, 0]),\n    X = X_train,\n    y = y_train,\n    learning_rate = .15,\n    stepsize_tau = .1\n)\n\n\n\n\nNext we’ll compare it to OLS estimates. Very similar even though SGD normally would not be used for such a small dataset. We also show our previous ‘batch’ gradient descent results for comparison.\n\n\n\n\nTable 3.6: Comparison of stochastic gradient descent results\n\n\n\n\n\n\n  \n    \n    \n      Value\n      Built-in\n      Our Result\n      Batch SGD\n    \n  \n  \n    Intercept\n5.445\n5.469\n5.437\n    Life Exp. Coef.\n0.525\n0.514\n0.521\n    GDP_PC\n0.438\n0.390\n0.439\n    Corrupt\n−0.105\n−0.111\n−0.107\n    MSE\n0.367\n0.370\n0.367\n  \n  \n  \n\n\n\n\n\n\n\nAnd here’s a plot of the estimates as they moved along the data. For this plot we don’t include the intercept as it’s on a notably different scale. We can see that the estimates are moving around a bit, but they appear to be converging to a solution.\n\n\n\n\n\n\n\n\nFigure 3.5: Stochastic gradient descent path\n\n\n\n\n\n\n\n\n3.11.3 Other Optimization Algorithms\nThere are lots of other approaches to optimization. For example, here are some of the options available in R’s optim or scipy’s minimize function:\n\nNelder-Mead\nBFGS\nL-BFGS-B (provides constraints)\nConjugate gradient\nSimulated annealing\nNewton’s method\nGenetic algorithms\n\nThe main reason to choose one method over another usually is some sort of computational gain, e.g. memory or speed, or it may just work better for some types of models in practice. For statistical problems, many GLM-type functions appear to use Newton’s as a default, but more complicated models may implement a different default for better convergence. In general, we can always try a few different methods to see which works best, and often there would be little differences in the results. For example, here are the results for the happiness model using different algorithms, with a comparison to the standard linear regression model function. We can see that the results are very similar, and for simpler modeling endeavors they should converge on the same result.\n\n\n\n\nTable 3.7: Comparison of optimization results\n\n\n\n\n\n\n  \n    \n    \n      parameter\n      NM1\n      BFGS2\n      CG3\n      GD4\n      Built-in5\n    \n  \n  \n    Intercept\n5.445\n5.445\n5.445\n5.437\n5.445\n    Life Exp. Coef.\n0.525\n0.525\n0.525\n0.521\n0.525\n    GDP_PC\n0.437\n0.438\n0.438\n0.439\n0.438\n    Corrupt\n−0.105\n−0.105\n−0.105\n−0.107\n−0.105\n    MSE\n0.367\n0.367\n0.367\n0.367\n0.367\n  \n  \n  \n    \n      1 NM = Nelder-Mead\n    \n    \n      2 BFGS = Broyden–Fletcher–Goldfarb–Shanno\n    \n    \n      3 CG = Conjugate gradient\n    \n    \n      4 GD = Gradient descent\n    \n    \n      5 Built-In = Standard OLS function",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-other",
    "href": "estimation.html#sec-estim-other",
    "title": "3  How Did We Get Here?",
    "section": "3.12 Other Estimation Approaches",
    "text": "3.12 Other Estimation Approaches\nBefore leaving our estimation discussion, we should mention that there are other approaches to estimation that are out there, some quite common. These include variaions on least squares, method of moments, generalized estimating equations, robust estimation, and more. The above that we’ve focused on will generally be sufficient for most applications, but it’s good to be aware of others. But there are two we want to discuss in a little bit detail before we leave model estimation formally given their widespread usage, and that is the bootstrap and Bayesian estimation.\n\n3.12.1 Bootstrap\nThe bootstrap is a resampling approach to estimation. We sample with replacement from the data observations, generating an entirely new data set of the same size, and then estimate the model. We repeat this process many times, collecting parameter estimates, predictions, or any thing we want to calculate along the way. Ultimately we end up with a distribution of possible parameter estimates, metrics, and whatever else we calculated.\nThis distribution is useful for inference9, as we can use the distribution to calculate confidence intervals, prediction intervals or intervals for anything we happen to calculate. The average estimate will typically be the same as whatever the underlying model used would produced, but the bootstrap provides a way to get at a measure of uncertainty with fewer assumptions about how that distribution should take shape. The bootstrap is very flexible, and it can be used with any estimation approach, let’s see this in action with the happiness data model we used previously.\n\nRPython\n\n\n\nbootstrap = function(X, y, nboot = 100, seed = 123) {\n    # add a column of 1s for the intercept\n    N = nrow(X)\n\n    # initialize\n    beta = matrix(NA, (1+ncol(X))*nboot, nrow = nboot, ncol = 1+ncol(X))\n    colnames(beta) = c('Intercept', colnames(X))\n    mse = rep(NA, nboot)\n\n    # set seed\n    set.seed(seed)\n\n    for (i in 1:nboot) {\n        # sample with replacement\n        idx = sample(1:N, N, replace = TRUE)\n        Xi = X |&gt; slice(idx)\n        yi = y[idx]\n\n        # estimate model\n        mod = lm(yi ~., data = Xi)\n\n        # save results\n        beta[i, ] = coef(mod)\n        mse[i] = sum((mod$fitted - yi)^2) / N\n    }\n\n    # given mean estimates, calculate MSE\n    y_hat = cbind(1, as.matrix(X)) %*% colMeans(beta)\n    final_mse = sum((y - y_hat)^2) / N\n\n    list(\n        beta = as_tibble(beta),\n        MSE = mse,\n        final_mse = final_mse\n    )\n}\n\nour_result = bootstrap(\n    X = df_happiness |&gt; select(life_exp, gdp_pc, corrupt),\n    y = df_happiness$happiness,\n    nboot = 250\n)\n\n\n\n\ndef bootstrap(X, y, nboot=100, seed=123):\n    cn = X.columns\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n    N = X.shape[0]\n\n    # initialize\n    beta = np.empty((nboot, X.shape[1]))\n    \n    # beta = pd.DataFrame(beta, columns=['Intercept'] + list(cn))\n    mse = np.empty(nboot)    \n\n    # set seed\n    np.random.seed(seed)\n\n    for i in range(nboot):\n        # sample with replacement\n        idx = np.random.randint(0, N, N)\n        Xi = X[idx, :]\n        yi = y[idx]\n\n        # estimate model\n        model = LinearRegression(fit_intercept=False)\n        mod = model.fit(Xi, yi)\n\n        # save results\n        beta[i, :] = mod.coef_\n        mse[i] = np.sum((mod.predict(Xi) - yi)**2) / N\n\n    # given mean estimates, calculate MSE\n    y_hat = X @ beta.mean(axis=0)\n    final_mse = np.sum((y - y_hat)**2) / N\n\n    return dict(beta = beta, mse = mse, final_mse = final_mse)\n\nour_result = bootstrap(\n    X = df_happiness[['life_exp', 'gdp_pc', 'corrupt']],\n    y = df_happiness['happiness'],\n    nboot = 250\n)\n\n\n\n\nHere are the results of the interval estimates for the coefficients. For each parameter, we have the mean estimate, the lower and upper bounds of the 95% confidence interval, and the width of the interval. We can see that the bootstrap intervals are wider than the OLS intervals, possibly better capturing the uncertainty in this model based on not too many observations.\n\n\n\n\nTable 3.8: Bootstrap parameter estimates\n\n\n\n\n\n\n  \n    \n    \n      Parameter\n      mean\n      Lower BS\n      Upper BS\n      Lower OLS\n      Upper OLS\n      Diff Width1\n    \n  \n  \n    Intercept\n5.44\n5.33\n5.55\n5.33\n5.56\n−0.02\n    life_exp\n0.51\n0.30\n0.70\n0.35\n0.70\n0.04\n    gdp_pc\n0.46\n0.18\n0.76\n0.24\n0.64\n0.18\n    corrupt\n−0.10\n−0.29\n0.09\n−0.25\n0.04\n0.09\n  \n  \n  \n    \n      1 Width of bootstrap estimate minus width of OLS estimate\n    \n  \n\n\n\n\n\n\n\nLet’s look more closely at the bootstrap distributions for each coefficient. With standard statistical estimates, we are assuming a distribution like the normal, which is a very specific shape. With the bootstrap, we can be more flexible, though often it may tend toward the distribution that would otherwise be assumed anyway. These aren’t perfectly symmetrical, but they suit our needs in that we can extract the lower and upper quantiles to create an interval estimate.\n\n\n\n\n\n\n\n\nFigure 3.6: Bootstrap distributions of parameter estimates\n\n\n\n\n\nThe bootstrap is a commonly used for predictions and other metrics, but it is computationally inefficient, and can become prohibitive with large data sizes. Also, the simple bootstrap will likely not estimate the appropriate uncertainty for some types of statistics (e.g. extreme values) or in some data contexts (e.g. correlated observations). Overcoming the limitations may typically require an even more computationally intensive approach, further limiting its utility. But it is a useful tool to have in your toolbox, and it can be used in conjunction with other approaches to get at uncertainty in a model.\n\n\n3.12.2 Bayesian Estimation\nThe Bayesian approach to modeling is a philosophical viewpoint, an entirely different way to think about probability, a different way to measure uncertainty, and on a practical level, just another way to get model parameter estimates. It can be as frustrating as it is fun to use, and one of the really nice things about using Bayesian estimation is that it can handle model complexities that other approaches don’t do well.\nThe basis of Bayesian estimation is the likelihood, the same as used with maximum likelihood, and everything we did there follows to here. However, here we can incorporate domain knowledge about the parameters, in the form of prior distributions, which we specify in addition to the likelihood. For example, we may say that the coefficients for a linear model come from a normal distribution centered on zero with some variance. The combination prior distributions with the likelihood ultimately results in the posterior distribution. And this is the key difference when comparing Bayesian estimation to the others we’ve talked about, and something it shares in common with the bootstrap- the end result is not a point estimate of the parameters, but rather a distribution of possible parameter values.\n\nDealing with distributions instead of single estimates is a different way to think about modeling, but it can be very useful. For example, as we did with the bootstrap, the Bayesian posterior distribution is useful for inference. With these distributions, we can look at any range in between for our credible interval, which is the Bayesian equivalent of a confidence interval10. Here is an example of the posterior distribution for the parameters of our happiness model, along with 95% intervals.\n\n\n\n\n\n\n\n\nFigure 3.7: Posterior distribution of parameters\n\n\n\n\n\nWith Bayesian modeling, we use the algorithm of our choosing, give it starting values and proceed much in the same way as other optimization procedures. However, in this approach, we always specify a number of iterations as the stopping rule, i.e. when the model should terminate. These iterations are single draws from the posterior distribution for each parameter. So if we specified 1000 iterations, we would have 1000 draws from the posterior distribution for each parameter. Typically we don’t use the first few hundred draws, as these are considered burn-in or warmup draws, and we use the remaining draws for inference. The number of burn-in draws is a bit of an art, but it’s not too important as long as it’s not too small. The more iterations we set, the longer it will take to run. We also specify multiple chains, which are each doing the exact same thing, but do to the random nature of the Bayesian approach, would take different estimation paths. We can then compare the chains to see if they are converging to the same result, which is a check on the model. If they are not converging, we may need to run the model longer, or we may need to change something else. Here is an example of the chains for our happiness model for the life expectancy coefficient. We can see that they are converging to the same result, so we are good to go. Nowadays we have simple metrics that allow us to check whether the chains are converging, making it easier to assess many parameters quickly.\n\n\n\n\n\n\n\n\nFigure 3.8: Bayesian chains for life expectancy coefficient\n\n\n\n\n\nWhen we are interested in making predictions, we can use the results to generate a distribution of possible predictions for each observation, which can be very useful when we want to quantify uncertainty in for complex models. This is referred to as posterior predictive distribution. Here is a plot of several draws of predicted values against the true happiness scores.\n\n\n\n\n\n\n\n\nFigure 3.9: Posterior predictive distribution of happiness values\n\n\n\n\n\nNote that any metric we can calculate from a model will also have a distribution. For example, you have a classification model and you want to know the accuracy or true positive rate of the model. Instead of a single number, you now have access to a whole distribution of values for that metric. Why? For every sample of the distribution of parameters, you generate a prediction, convert it a class and compare it to the true class. So now you have a posterior predictive distribution for the predicted probabilities and class, and you can then calculate the accuracy, area under a receiver operating curve, true positive rate, etc., for each sample, and you have a distribution of possible values. As an example, we did this for our happiness model and show the interval estimate for R-squared. Pretty neat!\n\n\n\n\nTable 3.9: Bayesian R2\n\n\n\n\n\n\n  \n    \n    \n      Bayes R2\n      Lower\n      Upper\n    \n  \n  \n    0.71\n0.65\n0.75\n  \n  \n  \n    \n       95% Credible interval for R-squared\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is nothing keeping you from doing posterior predictive checks with other estimation approaches, and it’s a good idea to do so. For example, in a GLM you have the beta estimates and the covariance matrix for them, and can simulate from a normal distribution with those estimates. But it’s a bit more straightforward with the Bayesian approach, and some packages will allow you to do this automatically even.\n\n\n\n\n3.12.2.1 Additional Thoughts\nIt turns out that any standard (frequentist) statistical model can be seen as a Bayesian one from a particular point of view. Here are a couple:\n\nGLM and related estimated via maximum likelihood: Bayesian estimation with a flat/uniform prior on the parameters.\nRidge Regression: Bayesian estimation with a normal prior on the coefficients, penalty parameter is related to the variance of the prior\nLasso Regression: Bayesian estimation with a Laplace prior on the coefficients, penalty parameter is related to the variance of the prior\n\nSo in many modeling contexts, you’re actually doing a restrictive form of Bayesian estimation already. Hopefully this helps to demystify the Bayesian approach a bit, and you feel more comfortable switching to it. R has excellent tools here for modeling and post-processing, like brms and tidybayes, and Python has pymc3, numpyro, and arviz, which are also useful11.\nWe can see that the Bayesian approach is very flexible, and can be used for many different types of models, and can be used to get at uncertainty in a model in ways that other approaches can’t. It’s not a panacea, and it’s not always the best approach, but it’s a good one to have in your toolbox.\nTODO: WHERE TO PUT THIS PRIOR STUFF? DELETE!?\nThe tough part about the Bayesian approach is specifying priors, but even when you don’t have a great idea, many have offered solutions, and there are ways to check whether what you’ve chosen makes sense for your data before trying the model itself.\n\n\n\n\n\n\nSpecification of priors can be done in different ways, and nowadays, there is a lot of information on how to do so, and with some tools, it’s also pretty straightforward to check whether the priors are sensible without even running a model. When you do have actual prior knowledge, either domain knowledge (e.g. a prior study found the beta values to be positive), statistical knowledge, (e.g. only the largest standard coefficients go near or beyond 1), data from time periods, there’s typically at least something to help you specify your priors with sensible values. This takes away most of the luster of the primary argument against the Bayesian approach, which is the subjective nature of priors. But there is likewise so much subjective decision making in other approaches, that it’s not really a useful argument to begin with. The Bayesian approach just makes it more explicit. And if you don’t have any prior knowledge, you can use non- or weakly- informative priors, which will likely have little influence and let the data do the talking, producing a result that is not that different from maximum likelihood estimation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-wrap",
    "href": "estimation.html#sec-estim-wrap",
    "title": "3  How Did We Get Here?",
    "section": "3.13 Wrapping Up",
    "text": "3.13 Wrapping Up\nWow, we covered a lot here! But this is the sort of stuff that can take you from just having some fun with data, to doing that and also understanding how things are actually happening. Just having the gist of how modeling actually is done ‘under the hood’ makes so many other things make sense, and can give you a lot of confidence, even in less familiar modeling domains.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-where-to-go",
    "href": "estimation.html#sec-estim-where-to-go",
    "title": "3  How Did We Get Here?",
    "section": "3.14 Where to Go From Here",
    "text": "3.14 Where to Go From Here\nReally, after this chapter, you should feel fine with any of the others, so dive in! Here are some additional resources to consider.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#exercise",
    "href": "estimation.html#exercise",
    "title": "3  How Did We Get Here?",
    "section": "3.15 Exercise",
    "text": "3.15 Exercise\nTry creating an objective function for a continuous target that uses the mean absolute error, and compare your estimated parameters to the previous results.\nTODO: refs need work!\nOLS and Maximum Likelihood Estimation:\nFor OLS and maximum likelihood estimation, there are so many resources out there, we recommend just taking a look and seeing which one suits you best. Practically any more technical statistical book will cover these topics in detail.\n\nA list of classical references\nTODO: recent?\n\nGradient Descent:\n\nGradient Descent, Step-by-Step StatQuest with Josh Starmer (2019a)\nStochastic Gradient Descent, Clearly Explained StatQuest with Josh Starmer (2019b)\n\nThe simple AdaGrad algorithm used above:\n\nBrownlee (2021)\nDataBricks (2019)\n\nBootstrap: ?\nBayesian:\n\nBDA Gelman et al. (2013)\nStatistical Rethinking McElreath (2020)\nChoosing priors\n\n\n\n\n\nBrownlee, Jason. 2021. “Gradient Descent With AdaGrad From Scratch.” MachineLearningMastery.com. https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/.\n\n\nDataBricks. 2019. “What Is AdaGrad?” Databricks. https://www.databricks.com/glossary/adagrad.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. CRC Press.\n\n\nMcElreath, Richard. 2020. “Statistical Rethinking: A Bayesian Course with Examples in R and STAN.” Routledge & CRC Press. https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919.\n\n\nMurphy, Kevin P. 2012. “Machine Learning: A Probabilistic Perspective.” MIT Press. https://mitpress.mit.edu/9780262018029/machine-learning/.\n\n\nStatQuest with Josh Starmer. 2019a. “Gradient Descent, Step-by-Step.” https://www.youtube.com/watch?v=sDv4f4s2SB8.\n\n\n———. 2019b. “Stochastic Gradient Descent, Clearly Explained!!!” https://www.youtube.com/watch?v=vMh0zPT0tLI.\n\n\nWikipedia. 2023. “Relationships Among Probability Distributions.” Wikipedia. https://en.wikipedia.org/w/index.php?title=Relationships_among_probability_distributions&oldid=1180084573.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#footnotes",
    "href": "estimation.html#footnotes",
    "title": "3  How Did We Get Here?",
    "section": "",
    "text": "It turns out that our error metric is itself an estimate of the true error. We’ll get more into this later, but for now this means that we can’t ever know the true error, and so we can’t ever really know the best or true model. However, we can still choose a good or better model relative to others based on our estimate.↩︎\nWe don’t have to do it this way, but it’s the default in most scenarios. As an example, maybe for your situation overshooting is worse than undershooting, and so you might want to use an approach that would weight those errors more heavily.↩︎\nSome disciplines seem confuse models with estimation methods and link functions. It doesn’t really make sense, nor is informative, to call something an OLS model or a logit model. Many models are estimated using a least squares approach, and different types of models use a logit link.↩︎\nYou may find that some packages will only minimize (or maximize) a function, possibly one that you come up with yourself. It’d be nice if they did this internally, or allowed the user to specify the direction like most packages, but you’ll need to take care when implementing your own metrics.↩︎\nThe actual probability of a specific value is 0, but the probability of a range of values is not 0. You can find out more about likelihoods and probabilities at the discussion here, but in general many traditional statistical texts will cover this also.↩︎\nThose who have experience here will notice we aren’t putting a lower bound on sigma. You typically want to do this otherwise you may get nonsensical results. You can do this by using the lower parameter in optim with an algorithm that uses boundaries, or even more simply by exponentiating the parameter, i.e. exp(par[1]). Just remember that the returned value will be on the log scale, so you’ll have to exponentiate it to get to the correct scale. We leave this detail out of the code for now to keep things simple.↩︎\nLinear regression will settle on a line that cuts through the means, and when standardizing the mean of the features and target are both zero, so the line goes through the origin.↩︎\nMC does not recall exactly where this origin of his function came from except that Murphy’s PML book was a key reference (Murphy (2012)).↩︎\nWe’re using inference here in the statistical/philosophical sense, not as a synonym for prediction or generalization, which is how it is often used in machine learning. We’re not exactly sure how that terminological muddling arose in ML, but be on the lookout for it.↩︎\nYour default interpretation of a standard confidence interval is almost certaintly, and incorrectly, the actual interpretation of a Bayesian confidence interval, because the Bayesian interpretation of confidence intervals and p-values is how we tend to naturally think about them. But that’s okay, everyone else is in the same boat. We also don’t care if you want to call the Bayesian version a credible interval or a confidence interval.↩︎\nHonestly R has way more going on here, with many packages devoted to Bayesian estimation of specific models even, but if you want to stick with Python for it you at least have some options. Stan, a probabilistic language underlying many of the packages in R, has tools there as well, but they are not nearly as well developed or test in Python.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html",
    "href": "generalized_linear_models.html",
    "title": "4  Generalized Linear Models",
    "section": "",
    "text": "4.1 Key Ideas",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-key",
    "href": "generalized_linear_models.html#sec-glm-key",
    "title": "4  Generalized Linear Models",
    "section": "",
    "text": "A simple tweak to our previous approach allows us to generalize are linear model to account for other settings.\nCommon distributions such as binomial, poisson, and others can often do better for us both in terms of model fit and interpretability.\nGetting familiar with just a couple distributions will allow you to really expand your modeling repertoire.\n\n\n4.1.1 Why this matters\nThe linear model is powerful on its own, but even more so when you realize you can extend many other data settings, some of which are implicitly nonlinear! When we want to classify observations, count them, or deal with proportions and other things, simple tweaks of our standard linear model allow us to handle such situations.\n\n\n4.1.2 Good to know\nGeneralized linear models are a broad class of models that extend the linear model to different distributions of the target variable. In general, you’d need to have a pretty good grasp of linear regression before getting too carried away here.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-distributions",
    "href": "generalized_linear_models.html#sec-glm-distributions",
    "title": "4  Generalized Linear Models",
    "section": "4.2 Distributions & Link Functions",
    "text": "4.2 Distributions & Link Functions\nRemember how linear models really enjoy the whole Gaussian distribution scene? The essential form of the linear model can be expressed as follows:\n\\[\n\\mu = \\alpha + X\\beta\n\\] \\[\ny \\sim \\textrm{Normal}(\\mu,\\sigma)\n\\]\nNot all data follows a Gaussian distribution. Instead, we often find some other form of an exponential distribution. So, we need a way to incorporate different distributions of the target into our model. Distributions cannot do it alone! We also need a link function to connect the linear model to the distribution.\nFrom a theoretical perspective, link functions are tricky to get your head around.\n\nFind the exponential of the response’s density function and derive the canonical link function…\n\nFrom a conceptual perspective, all they are doing is allowing the linear feature to “link” to a distribution function’s mean. If you know a distribution’s canonical link function, that is all the deeper you will probably every need.\nAt the end of the day, these link functions will convert the target to an unbounded continuous variable. The take-away here is that the link function describes how the mean is generated from the predictors.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-logistic",
    "href": "generalized_linear_models.html#sec-glm-logistic",
    "title": "4  Generalized Linear Models",
    "section": "4.3 Logistic Regression",
    "text": "4.3 Logistic Regression\n\n4.3.1 Why Should You Care\nYou will often have a binary variable that you might want to use as a target – it could be dead/alive, lose/win, quit/retain, etc. You might be tempted to use a linear regression, but you will quickly find that it is not the best option. You are going to be figuring out the probability of moving from “failure” to “success”, given the features in your model.\nTODO: can probably move distribution part of this section to appendix as part of general discussion of some distributions worth knowing, esp. as this is focused on binomial as a count, and the model focus is on the binary/bernoulli special case.\n\n\n4.3.2 The Binomial Distribution\nLogistic regression is substantially different than linear regression. It is also a bit confusing, because it is named after its link function (logit) instead of its distribution (binomial). Instead of that nice continuous target, we are dealing with a binomially-distributed target and the target takes the form of a binary variable.\nWe don’t have a \\(\\mu\\) or \\(\\sigma^2\\) to identify the shape of the binomial distribution; instead we have p and n, where p is a probability and n is the number of trials. We tend to talk about p with regard to the probability of a specific event happening (heads, wins, defaulting, etc.).\nLet’s see how the binomial distribution looks with 100 trials and probabilities of “success” at p =  .25, .5, and .75:\n\n\n\n\n\n\n\n\nFigure 4.1: Binomial distributions for different probabilities\n\n\n\n\n\nIf we examine the distribution for a probability of .5 in Figure 4.1, we will see that it is centered over 50 – this would suggest that we have the highest probability of encountering 50 successes if we ran 100 trials. If we run 100 trials 100 times and the outcome is 50/50, the most common outcome from those 100 trials would be 50 successes. with a decreasing probability of observing more or less successes as we move away from 50. Shifting our attention to a .75 probability of success, we see that our density is sitting over 75. Again running 100 trials, would give us the highest probability of observing 75 successes. Some of those 100 trials produce more or less than 75 successes, but with lower probabilities as you get further away from 75.\nSince we are dealing with a number of trials, it is worth noting that the binomial distribution is a discrete distribution. If you have any interest in knowing the probability for a number of success under the binomial distribution, we can use the following formula:\n\\[P(x) = \\frac{n!}{(n-x)!x!}p^xq^{n-x}\\]\nWhile we don’t need to dive into finding those specific values for the binomial distribution, we can spend our time exploring how it looks in linear model space:\n\\[\n\\textrm{logit}(p) = \\alpha + X\\beta\n\\]\n\\[y \\sim \\textrm{Binomial}(n, p) \\\\ \\]\nThe logit function is defined as:\n\\[\\textrm{log}\\frac{p}{1-p}\\]\nWe are literally just taking the log of the odds (the log odds becomes important later).\nNow we can map this back to our model:\n\\[\\textrm{log}\\frac{p}{1-p} = \\alpha + X\\beta\\]\nAnd finally we can take that logistic function and invert it (the inverse-logit) to produce the probabilities.\n\\[p = \\frac{\\textrm{exp}(\\alpha + X\\beta)}{1 + \\textrm{exp}(\\alpha + X\\beta)}\\]\nWhenever we get coefficients for the logistic regression model, we are always going to get them as log odds. We can exponentiate them to get the odds ratio, but we can also exponentiate them and divide by 1 + that value to get the probability.\n\n\n4.3.3 Probability, Odds, and Log Odds\nProbability lies at the heart of all of this. We can look at the relationship between the probability, odds, and log odds. We can start with a set of probability values where \\(0 &lt; p &gt; 1\\)\nWith that list of probability values, we can convert them to odds with \\(\\\\p\\, / 1 - p\\).\n\n\n\n\n\n\n\n\nFigure 4.2: Log odds and odds values for a range of probabilities\n\n\n\n\n\nWe can see how those probability values map to odds in Figure 4.2.\nNow, we can take those odds values and convert them to log odds.\n\n\n\n\n\n\n\n\nFigure 4.3: Log odds and probability values\n\n\n\n\n\nIf you’ve ever seen the sigmoid featured in Figure 4.3 before, it is the classic logistic function!\nWe can clearly go back and forth between the 3, but the main message here is that we took a bounded variable in probability and transformed it to continuous space.\nWe will see more about how this happens after playing with the model.\n\n\n4.3.4 Data Import and Preparation\nWe are going to return to our movie reviews data and we are going to use rating_good as our target. Before we get to modeling, see if you can find out the frequency of “good” and “bad” reviews. We will use word_count and gender as our predictors. Before we move on, though, find the probability of getting a “good” review.\nTODO: change import to df_reviews and proceed accordingly. Leave model fitting via optim to estimation chapter\n\nRPython\n\n\n\nreviews = read.csv(\"data/movie_reviews_processed.csv\")\n\n\nX = reviews[, c(\"word_count\", \"gender\")]\n\nX = cbind(1, X)\n\nX$gender = ifelse(X$gender == \"male\", 1, 0)\n\nX = as.matrix(X)\n\ny = reviews$rating_good\n\n\n\n\nimport pandas as pd\n\n\nreviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\n\nX = reviews[['word_count', 'gender']]\n\ny = reviews[\"rating_good\"]\n\n\n\n\n\n\n4.3.5 Standard Functions\nTo get started with our first logistic regression model, let’s use the glm function from R and Python’s statsmodels function.\n\nRPython\n\n\n\nmodel_logistic = glm(\n    rating_good ~ word_count + gender, \n    data = reviews,\n    family = binomial\n)\n\nsummary(model_logistic)\n\n\nCall:\nglm(formula = rating_good ~ word_count + gender, family = binomial, \n    data = reviews)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.71240    0.18136   9.442   &lt;2e-16 ***\nword_count  -0.14639    0.01551  -9.436   &lt;2e-16 ***\ngendermale   0.11891    0.13751   0.865    0.387    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1370.4  on 999  degrees of freedom\nResidual deviance: 1257.4  on 997  degrees of freedom\nAIC: 1263.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nimport statsmodels.api as sm\n\nX = sm.add_constant(X)\n\nX = pd.get_dummies(X, drop_first = True)\n\nmodel_logistic = sm.Logit(y, X.astype(float)).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.628697\n         Iterations 5\n\nmodel_logistic.summary()\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nrating_good\nNo. Observations:\n1000\n\n\nModel:\nLogit\nDf Residuals:\n997\n\n\nMethod:\nMLE\nDf Model:\n2\n\n\nDate:\nMon, 26 Feb 2024\nPseudo R-squ.:\n0.08245\n\n\nTime:\n20:56:18\nLog-Likelihood:\n-628.70\n\n\nconverged:\nTrue\nLL-Null:\n-685.19\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n2.925e-25\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.7124\n0.181\n9.442\n0.000\n1.357\n2.068\n\n\nword_count\n-0.1464\n0.016\n-9.436\n0.000\n-0.177\n-0.116\n\n\ngender_male\n0.1189\n0.138\n0.865\n0.387\n-0.151\n0.388\n\n\n\n\n\n\n\n\n\n\n\n4.3.6 Interpretation and Visualization\nWe need to know what those results mean. The coefficients that we get from our model are in log odds. We can exponentiate them to get the odds ratio, but we can also exponentiate them and divide by 1 + that value to get the probability. Interpretting log odds is a fool’s errand, but we can at least get a feeling for them directionally. A log odds of 0 would indicate no relationship between the feature and target. A positive log odds would indicate that an increase in the feature will increase the log odds of moving from “bad” to “good”, whereas a negative log odds would indicate that a decrease in the feature will decrease the log odds of moving from “bad” to “good”. We can convert those log odds to help make some more sense from them.\nWhen we exponentiate the log odds coefficients, we are given the odds ratio. This is the ratio of the odds of the outcome (i.e., success from our binomial distribution) occurring for a one unit increase in the predictor.\n\n\n(Intercept)  word_count  gendermale \n  5.5422663   0.8638229   1.1262687 \n\n\nFortunately, the intercept is easy – it is the odds of a “good” review when word count is 0 and gender is “female”. We see that we’ve got an odds ratio of .86 for the word_count variable and 1.12 for the male variable. An odds ratio of 1 means that there is no change in the odds of the outcome occurring – essentially that the predictor does not influence the target. An odds ratio of less than 1 means that the odds of the outcome occurring decrease as the predictor increases (while a bit more complicated to wrap your head around, it captures the idea of the odds of moving from a “bad” review to a “good” review decreasing). An odds ratio of greater than 1 means that the odds of the outcome occurring increase as the predictor increases (again, the odds of moving from a “bad” review to a “good” review increasing).\nIt is far more intuitive to interpret the probability. We can do this by exponentiating the coefficients and dividing by 1 + that value. This will give us the probability of the outcome occurring for a one unit increase in the predictor.\n\n\n(Intercept)  word_count  gendermale \n  0.8471478   0.4634683   0.5296925 \n\n\nWe would say that our probability of moving from a “bad” review to a “good” review is .84 when there are 0 words in the review and the gender is female. Since word_count is below .5, we know that it will have a negative relationship with the probability of moving from “bad” to “good”; being a male reviewer will have a positive relationship with the probability of moving from “bad” to “good”.\nAnd visualizing those probabilities is absolutely the best way to see how the features influence the target:\nTODO: Use output to make a better visual, also use see over sjPlot\n\n\n\n\n\n\n\n\nFigure 4.4: Logistic regression predictions for word count feature\n\n\n\n\n\nIn Figure 4.4, we can see a clear negative relationship between the number of words in a review and the probability of being considered a “good” movie. As we get over 20 words, the predicted probability of being a “good” movie is less than .2.\nTODO: Use output to make a better visual\n\n\n\n\n\n\n\n\nFigure 4.5: Logistic regression predictions for gender feature\n\n\n\n\n\nIn Figure 4.4, we can see a clear negative relationship between the number of words in a review and the probability of being considered a “good” movie. As we get over 20 words, the predicted probability of being a “good” movie is less than .2. It does not appear that gender has much of an effect on the probability of being a “good” movie, since the curves are very similar to each other.\nThere are interesting issues at play here with regard to our predictor coefficients (what can be considered a relative effect) and the model’s effect as a whole on the probability (the absolute effect). In circumstances where the intercept is very large (essentially promising a success), the relative effect of a coefficient is practically meaningless. Similarly, very negative coefficients render the relative effects useless.\n\n\n4.3.7 Objective Function\nLet’s see how we can pick that work apart to create our own functions. We can use maximum likelihood estimation to estimate the parameters of our model.\n\nRPython\n\n\n\nlogreg_ml = function(par, X, y) {\n  beta = par\n  N = nrow(X)\n  LP = X %*% beta                           \n  mu = plogis(LP)                           \n  L = dbinom(y, size = 1, prob = mu, log = TRUE)   \n  -sum(L)                                   \n}\n\n\n\n\ndef logreg_ml(par, X, y):\n    beta = par\n    N = X.shape[0]\n    LP = X.dot(beta).to_numpy()  \n    mu = [1 / (1 + np.exp(-x)) for x in LP]\n    mu_minus_1 = [1 - x for x in mu]\n    L = y*np.log(mu) + (1 - y)*np.log(mu_minus_1)   \n    return -np.sum(L)   \n  \n\n\n\n\n\n\n4.3.8 Model Fitting\nNow that we have our objective function, we can fit our model. We will use the optim function in R and the minimize function in Python.\n\nRPython\n\n\n\ninit = rep(0, ncol(X))\n\nnames(init) = c('intercept', 'b1', 'b2')\n\nfit_ml = optim(\n  par = init,\n  fn  = logreg_ml,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\npars_ml = fit_ml$par\n\npars_ml\n\n intercept         b1         b2 \n 1.7121816 -0.1463750  0.1189308 \n\n\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ninit = np.zeros(X.shape[1])\n\nfit_ml = minimize(\n    fun = logreg_ml,\n    x0 = init,\n    args = (X, y),\n    method = 'BFGS',\n    options = {'disp': True}\n)\n\nOptimization terminated successfully.\n         Current function value: 628.696593\n         Iterations: 11\n         Function evaluations: 68\n         Gradient evaluations: 17\n\nfit_ml.x\n\narray([ 1.71240414, -0.14638763,  0.11891015])\n\n\n\n\n\nTODO: move one of these so we don’t have back-to-back\n\n\n\n\n\n\nIn theory, there is no such thing as 0 or 1 probability. When your model encounters such a value, you may receive a warning, but not an error. The most likely cause of this warning is separation: a variable is perfectly separating the target. In other words, once a feature gets below/above a certain value, the target is always 0/1. This can often be caused by very extreme feature values, interaction groups with very small sample sizes, or even accidentally including a function of your target as a feature. More evidence of separation comes when you see your log odds coefficients return something comically large.\n\n\n\n\n\n\n\n\n\nLogistic regression does not have an \\(R^2\\) value in the way that a linear regression model does. Instead, there are pseudo-\\(R^2\\) values, but they are not the same as the \\(R^2\\) value that you are used to seeing. Here is a great breakdown of different pseudo methods.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-poisson",
    "href": "generalized_linear_models.html#sec-glm-poisson",
    "title": "4  Generalized Linear Models",
    "section": "4.4 Poisson Regression",
    "text": "4.4 Poisson Regression\n\n4.4.1 Why Should You Care\nLike logistic regression, poisson regression belongs to a broad class of generalized linear models. Poisson regression is used when you have a count variable as your target. The nature of a count variable is very different, since it starts at 0 and can only be a whole number. We need a model that will not produce negative predictions and poisson regression will do that for us.\n\n\n4.4.2 The Poisson Distribution\nThe Poisson distribution is very similar to the binomial distribution, but has some key differences. The biggest difference is in its parameter: Poisson has a single parameter noted as \\(\\lambda\\). This rate parameter is going to estimate the expected number of events during a time interval. This can be accidents in a year, pieces produced in a day, or hits during the course of a baseball season. We can find the rate by determining the number of events per interval, multiplied by the interval length.\n\\[\\frac{\\text{event}}{\\text{interval}}*\\text{interval length} \\]\nTo put some numbers to that, if we have 1 accident per week in a factory and we are observing a whole year, we would have a rate of \\((1 / 7) * 28 = 4\\) accidents per month.\nLet’s see what that particular distribution might look like in Figure 4.6:\n\n\n\n\n\n\n\n\nFigure 4.6: Poisson distribution for a rate of 4\n\n\n\n\n\nWe can also see what it looks like for different rates (some places might be safer than others) in Figure 4.7:\n\n\n\n\n\n\n\n\nFigure 4.7: Poisson distributions for different rates\n\n\n\n\n\n\n\n\n\n\n\nA cool thing about these distributions is that they can deal with different exposure rates. You don’t need observations recorded over the same interval length, because you can adjust for them appropriately. They can also be used to model inter-arrival times and time-until events.\n\n\n\nLet’s make a new variable that will count the number of times a person uses a personal pronoun word.\n\nRPython\n\n\n\nreviews$poss_pronoun = stringr::str_count(\n  reviews$review_text, \n  \"\\\\bI\\\\b|\\\\bme\\\\b|\\\\b[Mm]y\\\\b|\\\\bmine\\\\b|\\\\bmyself\\\\b\"\n)\n\n\n\n\nreviews['poss_pronoun'] = reviews['review_text'].str.count(\n  \"\\\\bI\\\\b|\\\\bme\\\\b|\\\\b[Mm]y\\\\b|\\\\bmine\\\\b|\\\\bmyself\\\\b\"\n  )\n\n\n\n\n\n\n4.4.3 The (Sometimes) Thin Line\nLet’s think long and hard about our target variable and what it actually might be. Since Poisson regression gets its name from the Poisson distribution, we should probably see if it follows the Poisson distribution.\n\n\n\n     Goodness-of-fit test for poisson distribution\n\n                      X^2 df  P(&gt; X^2)\nLikelihood Ratio 2.283728  3 0.5156453\n\n\nThis is a \\(\\chi^2\\) to test if the distribution deviates from a Poisson. If we see a statistically significant value, we would say that it deviates from the tested distribution. In this case, it is pretty clear that poss_pronoun could come from a Poisson distribution.\nWe can also plot that test using a hanging rootogram:\nTODO: convert to ggplot\n\n\n\n\n\n\n\n\nFigure 4.8: Hanging rootogram for Poisson distribution\n\n\n\n\n\nIn Figure 4.8, the bars are the observed counts and the red line/points are the fitted counts (i.e., how many would be expected). If a bar does not reach the 0 line, then the model would over-predict for that particular count; if the bar dips below the 0 line, the model under-predicts that count. It looks like we are pretty close for our counts.\n\n\n4.4.4 Standard Functions\nRecall that every distribution has a link function (or several) that tend to work well for it. The poisson distribution uses a log link function:\n\\[\\text{log}(\\lambda) = \\alpha + X\\beta\\] \\[y = \\textrm{Poisson}(\\lambda)\\]\nUsing the log link keeps the outcome positive (we cannot deal with negative counts). Logs, as they are prone to do, are going to tend towards an exponential relationship; just be sure that it makes sense over the entire range of your data.\n\nRPython\n\n\n\nmodel_poisson = glm(\n  poss_pronoun ~ word_count,\n  data = reviews,\n  family = poisson\n)\n\nsummary(model_poisson)\n\n\nCall:\nglm(formula = poss_pronoun ~ word_count, family = poisson, data = reviews)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.848982   0.099409  -18.60   &lt;2e-16 ***\nword_count   0.103126   0.006433   16.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 996.21  on 999  degrees of freedom\nResidual deviance: 776.19  on 998  degrees of freedom\nAIC: 1699.7\n\nNumber of Fisher Scoring iterations: 5\n\nexp(model_poisson$coefficients)\n\n(Intercept)  word_count \n  0.1573974   1.1086314 \n\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nmodel_poisson = smf.glm(\n  formula = \"poss_pronoun ~ word_count\",\n  data = reviews,\n  family = sm.families.Poisson()\n).fit()\n\nmodel_poisson.summary()        \n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nposs_pronoun\nNo. Observations:\n1000\n\n\nModel:\nGLM\nDf Residuals:\n998\n\n\nModel Family:\nPoisson\nDf Model:\n1\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-847.83\n\n\nDate:\nMon, 26 Feb 2024\nDeviance:\n776.19\n\n\nTime:\n20:56:19\nPearson chi2:\n717.\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1975\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-1.8490\n0.099\n-18.599\n0.000\n-2.044\n-1.654\n\n\nword_count\n0.1031\n0.006\n16.030\n0.000\n0.091\n0.116\n\n\n\n\n\nnp.exp(model_poisson.params)\n\nIntercept     0.157397\nword_count    1.108631\ndtype: float64\n\n\n\n\n\nWe are going to interpret this almost the same as a linear regression. The slight wrinkle here, though, is that we are looking at the log counts (remember that we specified the log link function). In other words, an increase in one one review word leads to an expected log count increase of ~.01. Just like our logisitc regression, we could exponentiate this to get 1.108 – every added word in a review gets us a ~1% increase in the number of possessive pronouns. Let’s see what this looks like in action in Figure 4.9:\n\n\n\n\n\n\n\n\nFigure 4.9: Poisson regression predictions for word count feature\n\n\n\n\n\nWith everything coupled together, we have a meaningful coefficient for word_count, a clear plot, and adequate model fit. Therefore, we might conclude that there is a positive relationship between number of words in a review on the number of times a person uses a personal possessive.\n\n\n\n    Overdispersion test\n\ndata:  model_poisson\nz = -8.0493, p-value = 1\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n 0.7606014 \n\n\nThe dispersion value that we see returned (0.7606014 in our case) should be under 1. A dispersion value over 1 means that we have overdispersion. Our dispersion value, coupled with our high p-value, indicates that we would fail to reject the null hypothesis of equidispersion.\nWe can also look back to our model results to compare our residual deviance to our residual deviance degrees of freedom; if our deviance is greater than our degrees of freedom, we might have an issue with overdispersion. Since we are just a bit over and our overdispersion tests do not indicate any huge issue, we can be relatively okay with our model. If we had some more extreme overdispersion, we would want to flip to a quasi-poisson distribution – our coefficients would not change, but we would have improved standard errors.\n\n\n4.4.5 Model Specification\nTODO: Need some text here\n\nRPython\n\n\n\npois_ll = function(y, X, par) {\n  beta = par\n  lambda = exp(beta%*%t(X))\n  loglik = -sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\n\n\n\nfrom scipy.stats import poisson\n\ndef pois_ll(par, X, y):\n    beta = par\n    lambda_ = np.exp(X.dot(beta))\n    loglik = -np.sum(poisson.logpmf(y, lambda_))\n    return loglik\n\n\n\n\n\n\n4.4.6 Model Fitting\n\nRPython\n\n\n\nform = as.formula(\"poss_pronoun ~ word_count\")\nmodel = model.frame(form, data = reviews)\nX = model.matrix(form, data = reviews)\ny = model.response(model)\n\nstarts = c(0, 0)\n\nfit = optim(\n  par = starts ,\n  fn  = pois_ll,\n  X   = X,\n  y   = y,\n  method  = \"BFGS\",\n  hessian = TRUE\n)\n\nfit$par\n\n[1] -1.8487431  0.1031103\n\n\n\n\n\nX = np.column_stack((np.ones(reviews.shape[0]), reviews[['word_count']]))\n\ny = reviews[\"poss_pronoun\"]\n\ninit = np.zeros(X.shape[1])\n\nfit = minimize(\n  fun = pois_ll,\n  x0 = init,\n  args = (X, y),\n  method = 'BFGS'\n)\n\n\nfit.x\n\narray([-1.84898106,  0.10312625])",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-wrap",
    "href": "generalized_linear_models.html#sec-glm-wrap",
    "title": "4  Generalized Linear Models",
    "section": "4.5 Wrapping Up",
    "text": "4.5 Wrapping Up\nThese are just two of the many models that fall under the broad umbrella of generalized linear models. Depending on your data situation, you might want to keep Table 4.1 in mind:\n\n\n\n\nTable 4.1: Targets and distributions for generalized linear models\n\n\n\n\n\n\n  \n    \n    \n      Target\n      Distribution\n    \n  \n  \n    Proportions\nbinomial/beta\n    Exponential response\ngamma\n    3+ categories\nmultinomial\n    Count\npoisson/negative binomial\n  \n  \n  \n\n\n\n\n\n\n\nThat is, however, just a tiny slice of the potential distributions that you might find yourself needing to use in a similar way. While not all are considered official ‘generalized linear models’, the approach is the same. While you could always use the general linear model, the key is to understand the distribution of your target and then find the appropriate link function to connect it to the linear model. Using the proper distribution will yield better results and get your model a little closer to the answer you seek.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-resources",
    "href": "generalized_linear_models.html#sec-glm-resources",
    "title": "4  Generalized Linear Models",
    "section": "4.6 Additional Resources",
    "text": "4.6 Additional Resources\nIn any given graduate coursework, you might find a whole semester dedicated to GLMs. We’ve only scratched the surface here, but there are some great resources out there to help you dig deeper. If you are itching for a text book, there isn’t any shortage of them out there and you can essentially take your pick. If you are looking for something a bit more applied to get you going, you might want to check out Roback and Legler’s Beyond Multiple Linear Regression, available for free at https://bookdown.org/roback/bookdown-BeyondMLR/.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html",
    "href": "linear_model_extensions.html",
    "title": "5  Extending the Linear Model",
    "section": "",
    "text": "5.1 Key Ideas",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-key-ideas",
    "href": "linear_model_extensions.html#sec-lm-extend-key-ideas",
    "title": "5  Extending the Linear Model",
    "section": "",
    "text": "The linear and generalized linear models are great and powerful starting points for modeling, but there’s even more we can do!\nLinear models can be used to model nonlinear feature-target relationships\nVarious technqiues are availabel allow us to model relationships that are not linear or monotonic, and can help us to better understand our data, even while still being linear models.\nWhile these seem like very different approaches, we can still use our linear model concepts and approach at the core, take similar estimation steps, and even have similar, albeit more, interpretation.\n\n\n5.1.1 Why this matters?\nThe linear model is a great starting point for modeling. It is a simple approach that can be used to model a wide variety of relationships between features and targets, and it’s also a great way to get a feel for how to think about modeling. But linear and generalized models are just the starting point, and the models depicted here are very common extensions used in a variety of disciplines and industries. More generally, the following techniques allow for nonlinear realationships will still employing a linear model approach. This is a very powerful combination, and it’s good to be aware of these tools.\n\n\n5.1.2 Good to know\nWhile these models are extensions of the linear model, they are not necessarily more complex, but it can take a bit more effort to interpret. You likely want to be fairly comfortable with standard linear models at least before you start to explore these extensions.\nTODO: This can be a chapter with a general focus on nonlinearities: intearctions in general and mixed model, gam (effects vary with self or other), quantile (effects vary with target). Also remind GLM as introducing nonlinearity.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-interactions",
    "href": "linear_model_extensions.html#sec-lm-interactions",
    "title": "5  Extending the Linear Model",
    "section": "5.2 Interactions",
    "text": "5.2 Interactions\nThings can be quite complex in a typical model with multiple features, but just adding features may not be enough to capture the complexity of the relationships between features and target. Sometimes, we need to consider how features interact with each other to better understand the relationships between features and target. A common way to add complexity in linear models is through interactions. This is where we allow the effect of a feature to vary depending on the values of another feature, or even itself!\nAs a conceptual example, we might expect that the effect of the number of children in the home on a movie’s rating is different for movies from different genres (much higher for kids movies, maybe lower for horror movies), or that genre and season work together in some way to affect rating (e.g. action movies get higher ratings in summer). We might also consider that the length of a movie might plateau or even have a negative effect on rating after a certain point, i.e., it would have a curvilinear effect. All of these are types of interactions we can explore. Interactions allow us to incorporate nonlinear relationships into the model, and so greatly extend the linear model’s capabilities - we basically get to use a linear model in a nonlinear way!\nWith that in mind, let’s explore how we can add interactions to our models. Going with our first example, let’s see how having kids impacts the relationship between genre and rating. We’ll start with a standard linear model, and then add an interaction term. Using a formula approach makes it very straightforward to add an interaction term. We just need to add a : between the two features we want to interact.\n\nRPython\n\n\n\ndf_reviews = read_csv(\"data/movie_reviews_processed.csv\")\n\nmodel_base = lm(rating ~ children_in_home + genre, data = df_reviews)\nmodel_interaction = lm(rating ~ children_in_home * genre, data = df_reviews)\n\n# summary(model_interaction)\n\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ndf_reviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\nmodel_base = smf.ols(\n  formula = 'rating ~ children_in_home + genre', \n  data = df_reviews\n).fit()\n\nmodel_interaction = smf.ols(\n  formula = 'rating ~ children_in_home * genre', \n  data = df_reviews\n).fit()\n\nmodel_interaction.summary()\n\n\n\n\nHere is a quick look at the model output for the interaction vs. no interaction interaction model. Starting with the base model, the coefficients look like what we’ve seen before, but we have several coefficients for genre. The reason is that genre is composed of several categories, and converted to a set of dummy variables (refer to Section 1.8.2 and Section 9.2.2). In the base model, the intercept tells us what the mean is for the reference group, in this case Action/Adventure, and the genre coefficients tell us the difference between the mean for that genre and the reference. For example, the mean rating for Action/Adventure is 2.76, and the difference between that genre rating for the drama genre is 0.55. Adding the two gives us the mean for drama movies 2.76 + 0.55 = 3.32. We also have the coefficient for the numbre of children in the home, and this does not vary by genre.\n\n\n\n\nTable 5.1: Model coefficients with interaction\n\n\n\n\n\n\n  Model coefficients with and without an interaction\n  \n    \n    \n      feature\n      coef_base\n      coef_inter\n    \n  \n  \n    (Intercept)\n2.7640\n2.7641\n    children_in_home\n0.1422\n0.1419\n    genreComedy\n0.6350\n0.6371\n    genreDrama\n0.5539\n0.5352\n    genreHorror\n0.1288\n0.1938\n    genreKids\n−0.1990\n−0.2759\n    genreOther\n0.0288\n0.0836\n    genreRomance\n0.2275\n0.2981\n    genreSci-Fi\n−0.1227\n−0.1090\n    children_in_home:genreComedy\n\n−0.0060\n    children_in_home:genreDrama\n\n0.0534\n    children_in_home:genreHorror\n\n−0.1274\n    children_in_home:genreKids\n\n0.2306\n    children_in_home:genreOther\n\n−0.1061\n    children_in_home:genreRomance\n\n−0.1235\n    children_in_home:genreSci-Fi\n\n−0.0285\n  \n  \n  \n\n\n\n\n\n\n\nBut we have an interaction in our other model, and an interaction basically tells us that the effects of feature A change depending on the values of feature B and vice versa. In this setting, feature A can be children in the home or genre, and B can be genre or children in the home. So let’s start with the coefficient for children in the home. It is 0.14, which means that for every additional child in the home, the rating increases by that amount. But! Due to our interaction, we now interpret that as just the effect of children in the home when genre is the reference group Action/Adventure. Now let’s look at the interaction effect for children in home and the kids genre. It is 0.23, which means that for the kids genre, the effect of having children in the home increases by that amount. So our actual effect for an additional child in the home for the kids genre is 0.14 + 0.23 = 0.37 increase in the review rating. It is also correct to say that the difference in rating between the kids genre and the reference group Action/Adventure is 0.23, but, when with an increase in children in the home, the difference in rating between the kids genre and the reference group Action/Adventure increases by 0.23. In other words, it is a difference in differences1.\nWhen we talk about differences in coefficients, across values of features, it can get a little bit hard to follow. In every case that you employ an interaction, you should look at the interaction visually. Here is a plot of the predictions from the interaction model. We hightlight the predictions for the kids genre, and we can see that the effect of children in the home is strongest for kids movies than for other genres, which makes a whole lot of sense! In other genres, the effect of having children seems to have little effect, and in others it still has a positive effect, but not as strong as for kids movies.\n\n\n\n\n\n\n\n\nFigure 5.1: Interaction plot\n\n\n\n\n\nSo we can see that interactions can allow a linear effect to vary depending on the values of another feature. But the real take home message from this is that the general effect is actually not linear! The effect changes depending on the setting. Furthermore, the coefficient for children in the home is only the effect of children in the home when genre is the reference group, or more generally, when other features are at their reference group or zero if they are numeric. Whenever you have interactions, you really can’t talk about a singular effect of a feature, but rather the effect of a feature at a particular setting of the other features. Some think this is a drawback, but it’s actually the reality of most feature-target relationships. Interactions allow us to model more complex relationships between features and target, and they are very common in practice.\n\n5.2.1 Average Effects\nSo what is the effect of children in the home? Or genre, for that matter? We can’t really say, because the effect of one feature depends on the setting of the other feature. We can say what the effect of a feature is on average across the settings of the other features. This is called the average marginal effect2. We can compute this by averaging the effect of a feature across the values of the other features.\n\n\n\n\nTable 5.2: Average Marginal Effects of Children in the Home\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    children_in_home\n0.152\n0.03\n5.68\n0.00\n0.10\n0.20\n  \n  \n  \n\n\n\n\n\n\n\nSo-called marginal effects and related approaches such as SHAP values (see ?sec-model-explore-shap-values) attempt to boil down the effect of a feature to a single number, but this is difficult even in the simpler GLM settings, and downright misleading in more complex settings like our interaction model. Here we see the average coefficient for children in the home is 0.15, but we saw in Table 5.1 that this is slightly larger than what we would estimate in the non-interaction model, and we saw in Figure 5.1 it’s actually near zero (flat) for most genres. So what is the average effect really telling us? Consider a more serious case of drug effects across demographic groups, where the effect of the drug is much stronger for some groups than others. Would you want your doctor to prescribe you a drug based on the average effect across all groups or the specific group to which you belong?\nIn the end, when it comes to interactions, it’s better to think about the effect of a feature in terms of the setting of the other features it interacts with. It’s even better to visualize the effect of a feature across a range of settings of the other features to get the best understanding of how the relationship changes. It’s also good to think about what the actual prediction for your outcome is at key values of the features, and how that changes depending on what the feature values are. This is what we’ve done here with interactions, and it’s a good approach to take in general.\n\n\n5.2.2 ANOVA\nA common method for summarizing categorical effects in linear models is through analysis of variance or ANOVA. ANOVA breaks down the variance in a target attributable to different features or their related effects such as interactions. It’s a bit beyond the scope here to get into all the details, but both base R and statsmodels have functions for this as demonstrated here.\n\nRPython\n\n\n\nanova(model_base)\n\n\n\n\nimport statsmodels.api as sm\n\nsmf.stats.anova_lm(model_base)\n\n\n\n\nIn this case, it doesn’t appear that the interaction effect is statistically significant if we use the typical .05 cut-off.\n\n\n\n\nTable 5.3: ANOVA table for interaction model\n\n\n\n\n\n\n  \n    \n    \n      feature\n      df\n      sum_sq\n      mean_sq\n      f\n      p\n    \n  \n  \n    children_in_home\n1.00\n6.45\n6.45\n21.25\n0.00\n    genre\n7.00\n86.17\n12.31\n40.55\n0.00\n    children_in_home:genre\n7.00\n3.75\n0.54\n1.76\n0.09\n    Residuals\n984.00\n298.69\n0.30\n\n\n  \n  \n  \n\n\n\n\n\n\n\nThe ANOVA approach can be generalized to provide a statistical test to compare models. For example, we can compare the base model to the interaction model to see if the interaction model is a better fit. However, it’s entirely consistent with just looking at the interaction result in the ANOVA for the interaction model, so doesn’t provide additional information, and the only models that can be compared in a meaningful way must be nested in this way, i.e., one model is a subset of the other.\nIt’s perhaps worth noting that ANOVA is often confused with being a model itself. When people use it as such, it is just a linear regression with only categorical features, something that can typically only happen within strict experimental designs that ignore interactions with continuous features. It’s pretty difficult to think of a linear regression setting where no continuous features would be of interest, but back when people were doing this stuff by hand, they just categorized everything to enable this approach. It’s a bit of a historical artifact, but still might be useful for exploratory purposes. Beyond that, ANOVA can be used to compare models more generally, but other approaches are a little more general or not confined to nested models- ones that can be seen as subsets of another.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-mixed-models",
    "href": "linear_model_extensions.html#sec-mixed-models",
    "title": "5  Extending the Linear Model",
    "section": "5.3 Mixed Models",
    "text": "5.3 Mixed Models\n\n\n5.3.1 Knowing Your Data\nAs much fun as modeling is, knowing your data is far more important. You can throw any model you want at your data, from simple to fancy, but you can count on disappointment if you don’t fundamentally know the structure that lies within your data. Let’s take a look at the following visualizations. In Figure 5.2, we see a positive relationship between the length of the movie and ratings.\n\n\n\n\n\n\n\n\nFigure 5.2: Linear relationship between length of movied and rating.\n\n\n\n\n\nWe could probably just stop there, but we might be ignoring something substantial within our data: genre. We might want to ask a question, “Does this relationship work the same way across the different genres?”\n\n\n\n\n\n\n\n\nFigure 5.3: Genre Effects on Length and Rating\n\n\n\n\n\nA very quick examination of Figure 5.3 might suggest that the rating varies by genre, and that the relationship between length and rating varies significantly over the different genres. The group means in the right panel show variability across genre. In addtion, on the left panel, some genres show a strong positive relationship, some show less of a positive relationship, a couple even show a negative relationship, and one even looks flat. We can also see that they would have different intercepts. This is a very important thing to know about your data! If we had just run a model with length as a feature and nothing else, we would have missed this important information.\n\n\n\n5.3.2 Overview of Mixed Models\nClearly genre is offering some type of additional information to the model, but how can we incorporate that into our model? An interaction might come to mind at first, and that’s the right way to think about it! A mixed model can be used to get at that type of relationship into our model, which we can think of as a group interaction, without much hassle and additional explanability.\nBefore going too much further, the term mixed model is as vanilla as we can possibly make it, but you might have heard of different flavors of them before. You might have heard of hierarchical linear models, or multilevel models, or maybe mixed-effects models tossed around before. Maybe you’ve even been exposed to ideas like random effects or random slopes. These are in fact all instances of what we’re calling a mixed model.\nWhat makes a model a mixed model? The mixed model is characterized by the idea that a model can have fixed effects and random effects. Fortunately, you’ve already encountered fixed effects – those are the features that we have been using in all of our models so far! We are assuming a single true parameter (coefficient/weight) for each of those features to estimate, and that parameter is fixed.\nIn the mixed model context, the random effect typically comes from some type of specific distribution, almost always a normal distribution, that contributes uniquely to the variance in the outcome. This distribution of effects can be characterized from something like a grouping variable (such as genre), such that we let those parameters, i.e. coefficients (or weights), vary across the groups, creating the observed distribution of values.\n\n\nFormally, we might specify something like this:\n\\[\n\\text{rating} = b_{\\text{0[genre]}} + b_\\text{length}*\\text{length}\n\\]\nWe are explicitly saying that genre has its own unique effect for this model in the form of specific intercepts for each genre. This means that whenever an observation belongs to a specific genre, it will have an intercept reflect that genre, and that means that two observations with the same length but from different genres would have different predictions.\nWe also posit that those come from a random distribution. We can specify that as:\n\\[b_{\\text{0[genre]}} \\sim \\text{N}(b_\\text{intercept}, \\sigma_\\text{int\\_genre})\\]\nThis means that the random intercepts will be normally distributed and the overall intercept is just the mean of those random intercepts, and with its own variance, an extra parameter we’ll eventually have to estimate as part of the model. Another very common depiction is:\n\\[\\text{re}_{[\\text{int\\_genre}]} \\sim \\text{N}(0, \\sigma_\\text{int\\_genre})\\]\n\\[b_{\\text{0[genre]}} = b_\\text{intercept} +\\text{re}_{[\\text{int\\_genre}]}\\]\nThe same approach would apply with a random slope, where we would have a random slope for each group, and that random slope would be normally distributed with its own variance.\n\\[b_{\\text{length[genre]}} \\sim \\text{N}(b_\\text{length}, \\sigma_\\text{length\\_genre})\\]\n\n\n\n5.3.3 Using a Mixed Model\nTo use mixed models, at a minimum we have to specify a group effect in some way, but that’s the primary difference from our approaches used for linear or generalized linear models previously. We can specify a random effect in a few different ways, but we’ll start with the simplest way, which is to just add a random effect to the model.\n\nRPython\n\n\nWe’ll use the lme4 package in R which is the most widely used package for mixed models.\n\nlibrary(lme4)\n\n# random intercepts are specified by a 1\nfit_ran_int = lmer(\n  rating ~ length_minutes_sc + (1 | genre), \n  df_reviews\n)\n\nfit_ran_slope = lmer(\n  rating ~ length_minutes_sc + (1 + length_minutes_sc | genre), \n  df_reviews\n)\n\nsummary(fit_ran_int)\nsummary(fit_ran_slope)\n\n\n\nAs with our recommendation with GAMs later, you really should just use R for mixed models. The functionality is overwhelmingly better there. However, you can use statsmodels in Python to fit mixed models3. But as an example, this doesn’t even converge with default settings even after scaling the data, so we had to switch the optimization method. These results correspond with the R results.\n\nimport statsmodels.api as sm\n\nfit_ran_int = sm.MixedLM.from_formula(\n  \"rating ~ length_minutes_sc\", \n  df_reviews, \n  re_formula= '1',\n  groups=df_reviews[\"genre\"]\n)\n\nfit_ran_slope = sm.MixedLM.from_formula(\n  \"rating ~ length_minutes_sc \", \n  df_reviews, \n  re_formula= 'length_minutes_sc',\n  groups=df_reviews[\"genre\"]  \n)\n\nfit_ran_int = fit_ran_int.fit()\nfit_ran_slope = fit_ran_slope.fit(maxiter=1000)\n\nfit_ran_int.summary()\nfit_ran_slope.summary()\n\n\n\n\nWith Table 5.4 we can see some typical output from a mixed model. The fixed effect part is your basic GLM result and interpreted as such. Nothing new there, and we can see a general positive relationship between length and rating, but maybe not a strong one. But the random effects are where the action is! We can see the standard devation (or variance) of the random effects, i.e., the intercepts and slopes. We can also see the standard deviation of the residual, which conceptually identical to your standard regression model’s residual standard deviation, but won’t be the same value. We can also see the correlation between the random intercepts and random slopes. Depending on your tool, the default may be in terms of variances and covariances rather than standard deviations and correlations, but you would not see anything fundamentally different.\n\n\n\n\nTable 5.4: Mixed model results\n\n\n\n\n\n\n  \n    \n    \n      group\n      term\n      estimate\n      std.error\n      statistic\n    \n  \n  \n    \n      Fixed\n    \n    \nIntercept\n2.97\n0.11\n27.73\n    \nlength_minutes_sc\n0.08\n0.04\n1.88\n    \n      Random\n    \n    genre\nsd__Intercept\n0.30\n\n\n    genre\ncor__Intercept.length_minutes_sc\n−0.24\n\n\n    genre\nsd__length_minutes_sc\n0.10\n\n\n    Residual\n\n0.55\n\n\n  \n  \n  \n\n\n\n\n\n\n\nIn this case, we can see notable variability attributable to the random effects. How do we know? Well, if if our rating is on a 1-5 scale, and we naturally have a standard deviation of 0.63 for rating before accounting for anything else, we mights surmise that having an effect of that size for just genre (roughly 0.3) is a relatively notable amount. We can also see that the correlation between the random intercepts and random slopes is negative, which means that the groups with higher intercepts have more negative slopes. Now let’s look at the estimates for the random effects for the model with both intercepts and slopes4.\n\nRPython\n\n\n\nranef = ranef(fit_ran_slope)\n# mixedup::extract_random_effects(fit_ran_slope) # prettier version\n\n\n\n\nranef = pd.DataFrame(fit_ran_slope.random_effects).T\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.4: Random effects for mixed model\n\n\n\n\n\nHow do we interpret these deviations? For the intercept plot, we see that Kids, Sci-Fi, and Action/Adventure have a lower default value for rating, while Drama and Comedy start off relatively higher. These values reflect what’s happening when the length is zero, which, since it’s standardized, means that we’re talking about what’s happening for an average length movie.\nComedy’s estimated trend over length suggests that it has a smaller slope, and relative to the global slope, this means a negative relationship for Comedy movies, i.e. longer is not better! Longer romance movies, on the other hand, have an even larger positive coefficient, and so seem to do better seem to do better than short ones- maybe all the short ones are mostly awful Rom-Coms!\n\n\n\n\n\n\nAlways scale features for mixed models\n\n\n\nYour authors have run a lot of these models. Save yourself some trouble and standardize or otherwise scale your features before fitting the model. Just trust us, or at least, don’t be surprised when your model doesn’t converge.\n\n\n\n\n5.3.4 Mixed Model Summary\nEven with just one feature, we certainly had a lot to talk about! This is just a glimpse of what mixed models have to offer, and the approach can be even richer than what we’ve just seen. But you might be asking- Why don’t I just put genre into the model like other categorical features? In the case of genre, that’s okay, but doing even just that would add several coefficients to the model before counting any interactions. Now consider thousands of United States county voting percentages for elections over time- would you just put 3000+ county indicator variables into the model as is? You can try, but you’ll likely run into estimation problems for typical GLM settings. In addition, as we saw, mixed models can correlate the random effects, which can be very useful for understanding the relationships between the groups. Furthermore, mixed models estimate the correlation of the observations within groups. Default mixed models assume this correlation is constant, but this can be modified to allow for different correlation structures. For example, in a longitudinal study you might want to assume that the correlation between observations within a group decreases as the time between observations increases. This is a very common approach for longitudinal data, where the correlation between observations decreases as the time between observations increases.\nIn general mixed models provide several advantages for the data scientist:\n\nAny coefficient can be allowed to vary by groups, including other random effects. It actually is just an interaction in the end as far as the linear predictor is concerned.\nThe group-specific effects are penalized, which shrinks them toward the overall mean, and makes this a different approach from just adding a ‘mere interaction’. This helps to avoid overfitting, and that penalty is related to the variance estimate of the random effect. In other words, you can think of it as running a penalized linear model where the penalty is applied to the group-specific effects.\nAlso unlike standard interaction approaches, we can estimate the covariance of the random effects, which can be useful for understanding the relationships between the groups. We can specify different covariance structures for observations within groups.\nStandard modeling approaches actually only estimate the variance part of the random effects, and get the estimated group-specific effects via a predictive method as part of model post-processing. This allows only the variances and covariances of the random effects to require estimation, rather than a weight or coefficient for every group.\nThe group effects are like a very simplified embedding, where we have taken a cateogrical feature and turned it into a numeric one, like those shown in Figure 5.4. This may help you understand other embedding techniques that are used in other places like deep learning if you think of this as the simplest embedding approach.\nWhen you start to think aobut random effects and/or distributions for effects, you’re already thinking like a Bayesian, who is always thinking about the distributions for various effects. Mixed models are a perfect segue from standard linear model estimation to Bayesian estimation, where everything is random.\nThe random effect is akin to a latent variable of ‘unspecified group causes’. This is a very powerful idea that can be used in many different ways, but importantly, you might want to start thinking about how you can figure out what those ‘unspecified’ causes may be!\nGroup effects will almost always improve your model’s performance relative to not having them, especially if you weren’t including those groups in your model because of how many there were.\n\nIn short, mixed models are a fun way to incorporate additional interpretive color to your model, while also getting several additional benefits to help you understand your data!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-gam",
    "href": "linear_model_extensions.html#sec-gam",
    "title": "5  Extending the Linear Model",
    "section": "5.4 Additive Models",
    "text": "5.4 Additive Models\n\nWiggle, wiggle, wiggle, yeah! – LMFAO\n\nTODO: GAM gets a bit deep, but we do want to keep discussion of penalized appoach, and ultimately a word on the random effect connection.\nBut what if we want to allow the effect of a feature to vary depending on its own values? This is called a curvilinear effect, and we can use a linear model to capture this as well. \n\n5.4.1 When Straight Lines Aren’t Enough\nFitting a line through your data is always going to be the best approach. While doing so often give us a wonderful ability to say important things about the relationships between variables and how one variable might influence another. What if we just want to dispense with the notion that we need to fit a straight line through some mass of the data? What if we relax the idea that we need a straight line and think in terms of fitting something curvy through the data?\nTODO: Maybe make a single plot.\nIn other words, we can go from the straight line here:\n\n\n\n\n\n\n\n\nFigure 5.5: A standard linear model\n\n\n\n\n\nTo the curve seen here:\n\n\n\n\n\n\n\n\nFigure 5.6: A generalized additive model\n\n\n\n\n\nThat curved line in Figure 5.6 is called a spline. It is created by a feature and expanding it to multiple columns, each of which is a function of the original feature. We then a fit a model to that data as usual. Oddly enough, the result is that we can use a linear model to fit a curve through the data. While this might not give us the same tidy explanation that a typical line would offer, we will certainly get better prediction, and a better understanding of the reality and complexity of the true relationship. But often it’s useful for exploratory purposes, and tools like ggplot, plotly5 and others make it easy to do so.\n\nRPython\n\n\n\nx = rnorm(1000)\ny = sin(x)\n\ntibble(x, y) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_smooth(method = 'gam', se = FALSE) \n\n\n\n\nimport plotly.graph_objects as go\nimport numpy as np\n\nx = np.random.normal(size = 1000)\ny = np.sin(x)\n\nfig = go.Figure()\nfig.add_trace(\n  go.Scatter(\n    x = x, \n    y = y,\n    line_shape = 'spline'\n  )\n)\n\n\n\n\nSuch models belong to a broad group of generalized additive models (GAMs). When we used an interations, we explored how the feature-target relationship varies with another feature. When we fit mixed models and interactions earlier, we focused on our feature and its relationship to the target at different values of other features. When we use a GAM, we are going to focus on our feature, and see how the relationship changes at different values for it. How are we going to do this, you might ask? Conceptually, we will have a model that looks like this:\n\\[\ny = f(x) + \\epsilon\n\\]\nThis isn’t much different than before, and technically, it really isn’t. It’s the same linear combination of features we have with a basic linear model. The difference is that we are going to let \\(f(x)\\) be a function of a particular feature \\(x\\) that allows us to capture other types of relationships by expanding the feature \\(x\\) in different ways. Some approaches can be quite complex, tackling spatial, temporal, or other aspects of the data. But on the practical side are just extra columns in the model matrix that find their way into the model fitting function like any other feature.\nThese additive features will allow us to capture nonlinearities in our data very nicely. At this point, you might be asking yourself, “Why couldn’t I just use some type of polynomial regression or even a nonlinear regression?”. Of course you could, but both have limitations relative to a GAM. If you are familiar with polynomial regression, where we add columns that are squares, cubes, etc. of the original feature, you can think of GAMs as a more general approach, and very similar in spirit. But with a lack of penalization, the typical polynomial regression tends to overfit the data you currently have, and you are forcing curves to fit through the data. To use a nonlinear model, you need to know what the underlying nonlinear form actually looks like before you can even specify the model, and without taking extra steps, such models likewise can tend to overfit. Furthermore, outside of well-known physical, chemical, or biological processes, it’s rarely clear what the underlying functional form should be.\nA GAM handles this situation a little better in that it will produce a curve that will provide a good fit to the data without the need to know the underlying functional form. Additionally, the default penalized approach will help prevent overfitting for smaller and/or more complex settings. Note also that we can do this for multiple features at once, and we can even include interactions between features. We can also use different types of splines to capture different types of nonlinearities. Here is another formal definition of a GAM that makes more clear we can deal with mulitple features.\n\\[\n\\hat{y} = \\sum \\mathbf{X_j\\beta_j}\n\\]\nIn this case, each \\(X_j\\) is a matrix of the feature and its basis expansion, and the \\(\\beta_j\\) are the coefficients for each of those basis expansion columns. But a specific X could also just be a single feature and it’s coefficient to model a linear relationship. The nice thing is that you don’t have to worry about the details of the basis expansion – the package you choose will take care of that for you. You will have different options, and often the default is fine, but sometimes you’ll want to play with both the technique, and how ‘wiggly’ you want the curve to be.\n\n\n5.4.2 A Standard GAM\nNow that you have some background, let’s give this a shot! In most respects, we can use the same sort of approach as we did with our other linear model examples. For our exmaple here, we’ll use model what was depicted in figure Figure 5.6, which looks at the relationship between the healthy_life_expectancy_at_birth and happiness_score variables from the world happiness data.\n\nRPython\n\n\nWe’ll use the very powerful mgcv package in R. The s function will allow us to use a spline approach to capture the nonlinearity.\n\nlibrary(mgcv)\n\ndf_happiness = read_csv('data/world_happiness_2018.csv')\n\ngam_model = gam(\n  happiness_score ~ s(healthy_life_expectancy_at_birth, bs = \"bs\"), \n  data = df_happiness\n)\n\nsummary(gam_model)\n\n\n\nWe can use the statsmodels package in Python to fit a GAM, or alternatively, pygam, and for consistency with previous models we’ll choose the former. Honestly though, you should use R’s mgcv, as both require notably more work without much of the functionality. In addition, there is an ecosystem of R packages to further extend mgcv’s capabilities.\n\nimport statsmodels.api as sm\n\nfrom statsmodels.gam.api import GLMGam, BSplines\nimport pandas as pd\n\ndf_happiness = pd.read_csv('data/world_happiness_2018.csv')\n\nbs = BSplines(df_happiness['healthy_life_expectancy_at_birth'], df=[9])\n\ngam_happiness = GLMGam.from_formula(\n  'happiness_score ~ healthy_life_expectancy_at_birth', \n  smoother = bs,\n  data = df_happiness\n)\n  \ngam_happiness_result = gam_happiness.fit()\n\ngam_happiness_result.summary()\n\n\n\n\n\n\n\n\nTable 5.5: GAM model output\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Term\n      Estimate\n      Std.Error\n      t.value\n      p.value\n    \n  \n  \n    parametric coefficients\nIntercept\n5.44\n0.06\n92.73\n0\n    \n\nEDF\nREF.DF\nF.VALUE\nP.VALUE\n    smooth terms\ns(healthy_life_expectancy_at_birth)\n5.55\n6.49\n40.11\n0\n  \n  \n  \n\n\n\n\n\n\n\nWhen you look at the model output, what you get will depend a lot on the tool you use, and the details are mostly beyond the scope we want to present here (check out this for more). But in general, the following information will be provided as part of the summary or as an attribute of the model object:\n\ncoefficients: The coefficients for each of the features in the model. For a GAM, these are the coefficients for the basis expansion columns, as well as standard linear feature effects. Typically, the total effect for a smooth term is displayed in the summary rather than the coefficients for each basis expansion column. Above we have the intercept and the summarized smooth term.\nglobal test of significance: Some tools will provide a test of the significance of the entire feature, as opposed to just the individual coefficients. This is a test of whether the feature is useful in the model at all.\nedf/EDoF: Effective degrees of freedom. This is a measure of wiggle in the relationship between the feature and the target. The higher the value, the more wiggle you have. If you have a value close to 1, then you have a linear relationship. With our current result, we can be pretty confident that a nonlinear relationship gives a better idea about the relationship between healthy_life_expectancy_at_birth and happiness_score than a linear one.\nR-squared: Adjusted/Pseudo \\(R^2\\) or deviance explained. This is a measure of how much of the variance in the target is explained by the model. The higher the value, the better the model. Deviance explained is an analog to the unadjusted \\(R^2\\) value for a Gaussian model that is used in the GLM setting. It’s fine as a general assessment of prediction-target correspondence, but don’t believe the actual value since we’re not in a basic OLS setting.\n\nFar more important than any of these is the visual interpration, and we can get plots from GAMs easily enough (results not shown).\n\n\nTODO: NEED VISUAL\n\nRPython\n\n\n\nplot(gam_model)\n\n\n\n\nres_bs.plot_partial(0, cpr=True)\n\n\n\n\nUnfortunately the default package plots are not pretty, and sadly aren’t provided in the same way we’d expect for interpretation. But they’re fine for a quick look at your wiggly result. We provide a better looking one her6. The main interpretation is that there is not much relationship between healthy_life_expectancy_at_birth and happiness_score until you get to about 60 years of life expectancy, and then it increases at a faster rate. Various tools are available to easily plot the derivatives for more understanding.\n\n\n\n\n\n\n\n\nFigure 5.7: Visualizing a GAM\n\n\n\n\n\n\n\n\nTo summarize, we can use a GAM to model nonlinear relationships between our features and target. We can use splines to capture those nonlinearities, and we can use a penalized approach to control the amount of wiggle in our model. What’s more we can interact the wiggle with other categorical and numeric features to capture even more complexity in our data. Because of this, GAMs are a very powerful modeling tool that take us a step toward more complex models, but without the need to go all the way to a neural network or other more complex model, and they can still provide statistical inference information as a default. A great tool to have in your modeling toolbox!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-quantile",
    "href": "linear_model_extensions.html#sec-lm-extend-quantile",
    "title": "5  Extending the Linear Model",
    "section": "5.5 Quantile Regression",
    "text": "5.5 Quantile Regression\n\nOh, you think the median is your ally. But you merely adopted the median; I was born in it, molded by it. I didn’t see anything interesting until I was already a man. And by then, it was nothing to me but illuminating. – Bane (probably)\n\nPeople generally understand the concept of the arithmetic mean. You see it some time during elementary school, it gets tossed around in daily language (usually using the word “average”), and it is statistically important. After all, where would the normal distribution be without a mean? Why, though, do we feel so tied to it from a regression modeling perspective? Yes, it has handy features, but it is also a bit restrictive to the types of relationships that it can actually model well.\nHere we’ll show you what to do when the mean betrays you – and trust us, the mean will betray you at some point!\n\n\n5.5.1 When The Mean Breaks Down\nIn a perfect data world, we like to assume the mean is equal to the middle observation of the data: the median. But that is only when things are symmetric though, and usually our data comes loaded with challenges. Skewness and even just a few extreme scores in your data may cause a rift between the median and the mean.\nLet’s say we take the integers between 1 and 10, and find the mean.\n\\[\\frac{1+2+3+4+5+6+7+8+9+10}{10} =  5.5\\]\nThe middle value in that vector of numbers would also be 5.5.\nWhat happens we replace the 1 with a more extreme value, like -10?\n\\[\\frac{-10+2+3+4+5+6+7+8+9+10}{10} =  4.5\\]\nWith just one dramatic change, our mean went down by a whole point. The median observation, though, is still 5.5. In short, the median is invariant to wild swings out in the tails of your numbers.\nYou might be saying to yourself, “Why should I care about this central tendency chicanery?” Let us tell you why you should care – the least squares approach to the standard linear model dictates that the regression line needs to be fit through the means of the variables. If you have extreme scores that influence the mean, then your regression line will also be influenced by those extreme scores.\nConsider the following regression line:\n\n\n\n\n\n\n\n\nFigure 5.8\n\n\n\n\n\nNow, what would happen if we replaced a few of our observations with extreme scores?\n\n\n\n\n\n\n\n\nFigure 5.9: Linear line with extreme scores\n\n\n\n\n\nWith just a casual glance, it doesn’t look like our two regression lines are that different. They both look like they have a similar positive slope, so all should be good. To offer a bit more clarity, though, let’s put those lines in the same space:\n\n\n\n\n\n\n\n\nFigure 5.10: Line lines with and without extreme scores\n\n\n\n\n\nWith 1000 observations, we see that having just 10 relatively extreme scores is enough to change the regression line, even if just a little. But that little bit can mean a huge difference for predictions or just the conclusions we come to.\nThere are a few approaches we could take here, with common approaches being dropping those observations or Windsorizing them. Throwing away data because you don’t like the way it behaves is nearing on statistical abuse, and Windsorization is just replacing those extreme values with numbers that you like a little bit better. Let’s not do that!\nA better answer to this challenge might be to not fit the regression line through the mean, but the median instead. This is where a model like quantile regression becomes handy. Formally, the objective function for the model can be expressed as: \n\\[\n\\text{Objective} =  \\Sigma \\left((\\tau - 1)\\sum_{y_{i}&lt;q}(y_{i}-q)+\\tau\\sum_{y_{i}\\geq q}(y_{i}-q) \\right)\n\\]\nWith quantile regression, we are given an extra parameter for the model: \\(\\tau\\) or tau. The tau parameter let’s us choose which quantile we want to use for our line fitting. Since the median splits the data in half, we can translate that to a quantile of .5. The objective function treats positive residuals differently than negative residuals. If the residual is positive, then we multiply it by the tau value. If the residual is negative, then we multiply it by -1 plus the tau value.\nWe can again use our movie reviews data. Let’s say that we are curious about the relationship between the word_count variable and the rating variable to keep things simple. To make it even more straightforward, we will use the standardized (scaled) version of the variable. In our default approach, we will start with a median regression, in other words, a quantile of .5.\n\n\nRPython\n\n\n\nlibrary(quantreg)\n\nmodel_median = rq(\n  rating ~ word_count_sc, \n  tau = .5,\n  data = df_reviews\n)\n\nsummary(model_median)\n\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ndf_reviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\nmodel_median = smf.quantreg('rating ~ word_count_sc',  data = df_reviews)\nmodel_median = model_median.fit(q = .5)\n                           \nmodel_median.summary()                           \n\n\n\nQuantReg Regression Results\n\n\nDep. Variable:\nrating\nPseudo R-squared:\n0.1247\n\n\nModel:\nQuantReg\nBandwidth:\n0.2375\n\n\nMethod:\nLeast Squares\nSparsity:\n1.221\n\n\nDate:\nMon, 26 Feb 2024\nNo. Observations:\n1000\n\n\nTime:\n20:56:48\nDf Residuals:\n998\n\n\n\n\nDf Model:\n1\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.0886\n0.019\n160.023\n0.000\n3.051\n3.126\n\n\nword_count_sc\n-0.2852\n0.019\n-14.770\n0.000\n-0.323\n-0.247\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5.6: Quantile regression model output\n\n\n\n\n\n\n  \n    \n    \n      feature\n      coef\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n3.09\n3.05\n3.26\n    word_count_sc\n−0.29\n−0.40\n−0.20\n  \n  \n  \n\n\n\n\n\n\n\nFortunately, our interpretation of this result isn’t all that different from a standard linear model – the rating should decrease by -0.29 for every bump in standard deviation for number of words, which in this case is about 5 words. However, this is concerns the rating median, not the mean, like the standard linear model.\nQuantile regression is not a one-trick-pony. Remember, it is called quantile regression – not median regression. Being able to compute a median regression is just the default. What we can do also is to model different quantiles of the same data. It gives us the ability to answer brand new questions – does the relationship between user age and their ratings change at different quantiles of rating? Very cool!\nInstead of a single model to capture the trend through the mean of the data, we can now examine the trends within 5 different quantiles of the data - .1, .3 .5, .7, and .9. We aren’t limited to just those quantiles though, and you can examine any of them that you might find interesting. Here is a plot of the results of these models.\n\n\n\n\n\n\n\n\nFigure 5.11: Quantile regression lines\n\n\n\n\n\nIf we had to put some words to our visualization, we could say that all of the quantiles show a negative relationship. The 10th and 90th quantiles show the weakest relationship, while those in the middle show a notably stronger relationship. We can also see that that the 90th percentile is better able to capture those values that would otherwise be deemed as outliers using other standard techniques.\n\n\n\n\nTable 5.7: Quantile regression model output\n\n\n\n\n\n\n  \n    \n    \n      feature\n      coef\n      conf.low\n      conf.high\n      quantile\n    \n  \n  \n    (Intercept)\n2.27\n2.20\n2.34\n0.10\n    word_count_sc\n−0.13\n−0.23\n−0.04\n0.10\n    (Intercept)\n2.79\n2.61\n2.93\n0.30\n    word_count_sc\n−0.23\n−0.45\n−0.14\n0.30\n    (Intercept)\n3.09\n3.05\n3.26\n0.50\n    word_count_sc\n−0.29\n−0.40\n−0.20\n0.50\n    (Intercept)\n3.32\n3.26\n3.35\n0.70\n    word_count_sc\n−0.30\n−0.36\n−0.23\n0.70\n    (Intercept)\n3.85\n3.74\n3.98\n0.90\n    word_count_sc\n−0.14\n−0.31\n−0.09\n0.90\n  \n  \n  \n\n\n\n\n\n\n\nTODO: MOVE TO ESTIMATION OR ONLINE ONLY\n\n\n5.5.2 Quantile Loss Function\nNow that we know how to use standard functions for quantile regression, let’s see one way that we can create a least squares loss function for fitting a linear regression model and compare it with a function for quantile loss.\n\nRPython\n\n\n\nquantile_loss = function(par, X, y, tau) {\n  \n  linear_parameters = X %*% par\n  \n  residual = y - linear_parameters\n  \n  loss = ifelse(\n    residual &lt; 0, \n    (tau-1)*residual, \n    tau*residual\n  )\n  \n  sum(loss)\n}\n\n\n\n\ndef quantile_loss(par, X, y, tau):\n  linear_parameters = X.dot(par)\n  \n  residual = y - linear_parameters\n  \n  loss = []\n  \n  loss = np.where(\n    residual &lt; 0, \n    (tau-1)*residual, \n    tau*residual\n  )\n\n  # for i in residual:\n  #   if i &lt; 0: loss.append((-1 + tau)*i)\n  #   else: loss.append(tau*i)\n  \n  return sum(loss)\n\n\n\n\nYou’ll notice right away that we have a few differences. Our quantile loss function includes the tau argument, which will let us set our quantile of interest; naturally, it can be any value between 0 and 1. The residual is multiplied by the tau value, only if the residual is greater than 0. If the residual is negative, we need to add tau to -1. Since we need a positive value for our loss values, we will multiply our negative residuals by the negative value produced from -1 plus our tau value. After that, we just sum all of those positive loss values and do our best to minimize that summed value.\n\n\n5.5.3 Model Fitting\nNow that we have our data and our loss function, we can fit the model almost exactly like our standard linear model. Again, note the difference here with our tau value, which we’ve set to .5 to represent the median.\n\nRPython\n\n\n\nX = cbind(1, df_reviews$word_count_sc)\ny = df_reviews$rating\n\noptim(\n  par = c(intercept = 0, word_count_sc = 0),\n  fn  = quantile_loss,\n  X   = X,\n  y   = y,\n  tau = .5\n)$par\n\n    intercept word_count_sc \n    3.0886074    -0.2852232 \n\n\n\n\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\nX = pd.DataFrame(\n  {'intercept': 1, \n  'word_count_sc': df_reviews['word_count_sc']}\n)\ny = df_reviews['rating']\n\nminimize(\n  quantile_loss, \n  x0 = np.array([0, 0]), \n  args = (X, y, .5)\n  ).x\n\narray([ 3.09011343, -0.28416408])\n\n\n\n\n\n\n\n\n\n\n\nAnother Interaction\n\n\n\n\n\nOne way to interpret this result is that we have a nonlinear relationship between the word count and the rating, in the same we we had an interaction previously. In this case, our effect of number of reviews interacts with the target! In other words, we have a different word count effect for different ratings. This is a bit of a mind bender, but it’s a good example of how a linear approach can be used to model quirky relationships!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#performance-comparisons",
    "href": "linear_model_extensions.html#performance-comparisons",
    "title": "5  Extending the Linear Model",
    "section": "5.6 Performance Comparisons",
    "text": "5.6 Performance Comparisons\nTODO: UPDATE WITH CURRENT MODELS\nJust for giggles, we should see how all of our models perform:\n\n\n\n\nTable 5.8: Comparing model performance with RMSE\n\n\n\n\n\n\n  \n    \n    \n      model\n      rmse\n    \n  \n  \n    standard\n0.59\n    median\n0.59\n    gam\n0.59\n    mixed\n0.50\n  \n  \n  \n\n\n\n\n\n\n\nLet’s check out the results in Table 5.8. Unsurprisingly, the standard linear model and the median regression were pretty close to each other. GAM offered a small bump in performance, but our best model came from the mixed model. This finding may or may not surprise you – as you spend more time with models, you often encounter situations where simple models outperform more complex models, or are on par with them. Here, we are seeing that the mixed model is offering us a better fit to the data than the other models. However, that doesn’t mean that you can just go right to the mixed model. You need to know your data and know what you are trying to accomplish.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-wrap",
    "href": "linear_model_extensions.html#sec-lm-extend-wrap",
    "title": "5  Extending the Linear Model",
    "section": "5.7 Wrapping Up",
    "text": "5.7 Wrapping Up\nThe standard linear model is useful across many different data situations. It does, unfortunately, have some issues when data becomes a little bit more “real”. When you have extreme scores or relationships that a standard model might miss, you don’t need to abandon your linear model in favor of something more exotic. Instead, you might just need to think about how you are actually fitting the line through your data.\nTODO: ADD Exercise",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-next-steps",
    "href": "linear_model_extensions.html#sec-lm-extend-next-steps",
    "title": "5  Extending the Linear Model",
    "section": "5.8 Next Steps",
    "text": "5.8 Next Steps\nNo matter how much we cover in this book, there is always more to learn. Here are some additional resources that you might find helpful related to this task. But if you’ve got a good grip on linear models and related topics, feel free to try out some machine learning Chapter 6!\nIf you want absolute depth on quantile regression, we will happily point you to the OG of quantile regression, Roger Koenker. His book, Quantile Regression is a must read for anyone wanting to dive deeper into quantile regression (2005), or just play around with his R package quantreg. Galton, Edgeworth, Frisch, and prospects for quantile regression in econometrics is another resource from him.\nIf you want to dive more into the GAM world, we would recommend that you start with the Moving Beyond Linearity chapter in An Introduction to Statistical Learning (James et al. 2021). Not only do they have versions for both R and Python, but both have been made available online. If you are wanting more after that, you can’t beat Simon Wood’s book, Generalized Additive Models: An Introduction with R (2017), or a more digestible covering of the same content by one of your own humble authors (Clark 2022).\nThere is no shortage of great references for mixed effects models. If you are looking for a great introduction to mixed models, we would recommend to start with yet another tutorial by one of your fearless authors! Michael Clark’s Mixed Models with R (2023), is a great introduction to mixed models and is freely available. If you want to dig just a little deeper, the lme4 vignette for Fitting Linear Mixed-Effects Models Using lme4 is a great resource.\n\n\n\n\nClark, Michael J. 2022. Generalized Additive Models. https://m-clark.github.io/generalized-additive-models/.\n\n\n———. 2023. Mixed Models with R. https://m-clark.github.io/mixed-models-with-R/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nKoenker, Roger. 2005. Quantile Regression. Vol. 38. Cambridge university press. https://books.google.com/books?hl=en&lr=&id=WjOdAgAAQBAJ&oi=fnd&pg=PT12&dq=info:E32s5Y3j4NMJ:scholar.google.com&ots=CQFHSt5qY_&sig=E8zXmNabc0nGmYJlSYjQqhzyVT8.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with R, Second Edition. 2nd ed. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781315370279.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#footnotes",
    "href": "linear_model_extensions.html#footnotes",
    "title": "5  Extending the Linear Model",
    "section": "",
    "text": "Some models that employ an interaction that investigates categorical group differences like this actually call their model a difference-in-difference model.↩︎\nThese results are provided by the marginaleffects package, which is great for this and has pretty much no equal in the Python realm.↩︎\nOne of your authors worked for several years with the key developer of the mixed models functionality in statsmodels. As such, we can say there is zero doubt about the expertise going into its development, as there are few in the world with such knowledge. Even so, the functionality is not as mature or as expansive as what you get in R.↩︎\nOne of your authors provides a package for mixed models in R called mixedup. It provides a nice way to extract random effects and summarize such models (link).↩︎\nPlotly is directly available in R and Python, and plotnine is the ggplot equivalent in Python.↩︎\nWe used the see in R for a quick plot. We also recommend its functionality via the gratia package to visualize the derivatives, which will show more of where the effect is changing most.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "6  Core Concepts",
    "section": "",
    "text": "6.1 Key ideas",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#key-ideas",
    "href": "machine_learning.html#key-ideas",
    "title": "6  Core Concepts",
    "section": "",
    "text": "Machine learning is not a set of modeling techniques, but rather a modeling focus on predictive performance, and a set of tools and methods to achieve that.\nModels used in machine learning are typically more complex and difficult to interpret than those used in standard statistical models, but any model, including classical statistical ones, can be used with ML.\nThere are many performance metrics used in machine learning, and care should be taken to choose the appropriate one for your situation.\nObjective functions likewise should be chosen for the situation, and are often different from the performance metric.\nMultiple performance metrics are able to be used for any given model assessment scenario.\nRegularization is a general approach to penalize complexity in a model, and is typically used to prevent overfitting in order to improve generalization.\nCross-validation is a method that allows us to select parameters and hyperparameters for our models, and to compare models to one another by assessing a model’s performance on data that was not used to fit the model. \n\n\n6.1.1 Why this matters\nMachine learning applications help define the modern world and how we interact with it. There are few aspects of modern society that have not been touched by it in some way. By understanding the basic ideas behind machine learning, you will be able to understand the models and techniques that are used in these applications, and be able to apply them to your own work. You’ll also be able to understand the limitations of these models.\n\n\n6.1.2 Good to know\nADD LINKS\nTo dive into applying machine learning models, you really only need a decent grasp of linear models as applied to regression and classification problems. It would also be good to have an idea behind how they are estimated, as the same basic logic serves as a starting point here.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-objective",
    "href": "machine_learning.html#sec-ml-objective",
    "title": "6  Core Concepts",
    "section": "6.2 Objective Functions",
    "text": "6.2 Objective Functions\nWe’ve implemented a variety of objective functions in other chapters, such as mean squared error for numeric targets and log loss for binary targets. As we have also noted elsewhere, the objective function is not necessarily the same as the performance metric we ultimately use to select a model. For example, we may use log loss as the objective function, but then use accuracy as the performance metric. In that setting, the log loss provides a ‘smooth’ objective function to search the parameter space over, while accuracy is a straightforward and more interpretable metric for stakeholders. In this case, the objective function is used to optimize the model, while the performance metric is used to evaluate the model. In some cases, the objective function and performance metric are the same (e.g. (R)MSE), and even if not, they might have selected the same ‘best’ model, but this is not always the case.\n\n\n\n\nTable 6.1: Commonly used objective functions in machine learning for standard regression and classification tasks.\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Objective Function\n      Description\n    \n  \n  \n    \n      Regression\n    \n    Mean Squared Error (MSE)\nCalculates the average of the squared differences between the predicted and actual values.\n    Mean Absolute Error (MAE)\nCalculates the average of the absolute differences between the predicted and actual values.\n    Huber Loss\nLess sensitive to outliers than MSE.\n    Log Likelihood\nUsed for models where the response variable follows a known distribution, and we want to maximize the likelihood of observing the data given the model parameters.\n    \n      Classification\n    \n    Binary Cross-Entropy / Log Likelihood (Loss)\nUsed for binary classification problems. Calculates the negative log-likelihood of the class labels given the predicted probabilities.\n    Categorical Cross-Entropy\nUsed for multi-class classification problems. Calculates the negative log-likelihood of the class labels given the predicted probabilities.\n  \n  \n  \n\n\n\n\n\n\n\nFor specific types of tasks, such as predicting ranks, you might use something else, but the above will apply in some of the most common settings. Even when dealing with different types of outcomes, such as counts, proportions, etc., one can typically use a likelihood objective. Recall that ‘maximum likelihood’ functions can be turned into a minimization problem by taking the negative (log) likelihood.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-metrics",
    "href": "machine_learning.html#sec-ml-metrics",
    "title": "6  Core Concepts",
    "section": "6.3 Performance Metrics",
    "text": "6.3 Performance Metrics\nThere are many performance metrics used in machine learning, and care should be taken to choose the appropriate one for your situation. Typically we have a standard set we might use for the type of predictive problem. For example, for numeric targets, we typically are interested in (R)MSE and MAE, but given a particular scenario we might want to switch to something else. For example, if we are predicting a count type of target, we might use Poisson deviance, which is akin to a Poisson GLM in statistical modeling.\nMost classification-based metrics are based on the confusion matrix, which is a table of the predicted classes versus the observed classes. Here is an example.\n\n\n\n\n\n\n\nExample Confusion Matrix\n\n\n\nObserved Negative\nObserved Positive\n\n\n\n\nPredicted Negative\n62\n10\n\n\nPredicted Positive\n10\n18\n\n\n\n\n\n\n\n\nThe diagonal of the confusion matrix is the number of correct predictions, and the off-diagonal is the number of incorrect predictions. In this particular example we have an accuracy of 80 correct out of 100 total, or 80%. However, there are many metrics we can calculate from this simple confusion matrix, and many of these can also be extended to the multiclass setting. For example, we can calculate the accuracy in general as well as for each class. For an overview of common metrics, refer to Table 2.1.\nAs an example, and as a reason to get our first taste of machine learning, let’s get some additional metrics for our movie review model. Depending on the tool used, getting one type of metric should be as straightforward as most others if we’re using common metrics. As we start our journey into machine learning, we’ll show Python code first, as it’s the dominant tool.\n\nPythonR\n\n\nIn Python, we can use the sklearn.metrics module to get a variety of metrics for both regression and classification problems.\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\n\nimport pandas as pd\n\ndf_movie_reviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\nX = df_movie_reviews[\n    [\n        'word_count',\n        'age',\n        'review_year',\n        'release_year',\n        'length_minutes',\n        'children_in_home',\n        'total_reviews',\n    ]\n]\n\ny = df_movie_reviews['rating']\ny_class = df_movie_reviews['rating_good']\n\nmodel_lin_reg = LinearRegression()\nmodel_lin_reg.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n# note that sklearn uses regularization by default for logistic regression\nmodel_log_reg = LogisticRegression() \nmodel_log_reg.fit(X, y_class)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\ny_pred_linreg = model_lin_reg.predict(X)\ny_pred_logreg = model_log_reg.predict(X)\n\n\n# regression metrics\nrmse = mean_squared_error(y, y_pred_linreg, squared=False)\nmae = mean_absolute_error(y, y_pred_linreg)\nr2 = r2_score(y, y_pred_linreg)\n\n\n# classification metrics\naccuracy = accuracy_score(y_class, y_pred_logreg)\nprecision = precision_score(y_class, y_pred_logreg)\nrecall = recall_score(y_class, y_pred_logreg)\n\n\n\nIn R, we can use the yardstick package, which has a consistent interface for a variety of metrics.\n\nlibrary(yardstick)\n\n# convert rating_good to factor for yardstick input\ndf_movie_reviews = read_csv(\"data/movie_reviews_processed.csv\") |&gt; \n  mutate(rating_good = factor(rating_good, levels = c(0, 1), labels = c(\"bad\", \"good\")))\n\nmodel_lin_reg = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_movie_reviews\n)\n\nmodel_log_reg = glm(\n    rating_good ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_movie_reviews,\n    family = binomial(link = \"logit\")\n)\n\ny_pred_linreg = predict(model_lin_reg)\ny_pred_logreg = predict(model_log_reg, type = \"response\")\ny_pred_logreg = factor(ifelse(y_pred_logreg &gt; .5, \"good\", \"bad\"))\n\n\n# regression metrics  \nrmse = rmse_vec(df_movie_reviews$rating, y_pred_linreg)\nmae  = mae_vec(df_movie_reviews$rating, y_pred_linreg)\nr2   = rsq_vec(df_movie_reviews$rating, y_pred_linreg)\n\n# classification metrics\naccuracy  = accuracy_vec(df_movie_reviews$rating_good, y_pred_logreg)\nprecision = precision_vec(df_movie_reviews$rating_good, y_pred_logreg)\nrecall    = recall_vec(df_movie_reviews$rating_good, y_pred_logreg)\n\n\n\n\nWe put them all together in the following table. Now we know how to get them, and it was easy! But as we’ll see later, there is a lot more to think about before we use these for model assesment.\n\n\n\n\nDemo Metrics\n  \n    \n      \n    \n    \n    \n      Metric\n      Value\n    \n  \n  \n    \n      Linear Regression\n    \n    RMSE\n0.52\n    MAE\n0.41\n    R-squared\n0.32\n    \n      Logistic Regression\n    \n    Accuracy\n0.71\n    Precision\n0.69\n    Recall\n0.60",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-generalization",
    "href": "machine_learning.html#sec-ml-generalization",
    "title": "6  Core Concepts",
    "section": "6.4 Generalization",
    "text": "6.4 Generalization\nOne of the key differences separating ML from traditional statistical modeling approaches is the assessment of performance on unseen or future data, a concept commonly referred to as generalization. The basic idea is that we want to build a model that will perform well on new data, and not just the data we used to fit the model, because ultimately data is ever evolving, and we don’t want to be beholden to a particular set of data we just happened to have at a particular time.\nBut how do we do this? For starters, we can simply split our data into two sets, a training set and a test set, often called a holdout set. The test set is typically a smaller subset, say 25%, but this amount is arbitrary, and will reflect the data situation. We fit the model on the training set, and then use the model to make predictions on, or score, the test set. This general approach is also known as the holdout method. Consider a simple linear regression. We can fit the linear regression model on the training set, which provides us coefficients, etc. We can then use that model result to predict on the test set, and then compare the predictions to the actual values in the test set. Here we demonstrate this with our simple linear model from before.\n\nPythonR\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\n\nX = df_movie_reviews[\n    [\n        'word_count',\n        'age',\n        'review_year',\n        'release_year',\n        'length_minutes',\n        'children_in_home',\n        'total_reviews',\n    ]\n]\n\ny = df_movie_reviews['rating']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=123\n)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n# get predictions\ny_pred = model.predict(X_test)\n\n# get RMSE on test\nmean_squared_error(y_test, y_pred, squared=False)\n\n\n\nRMSE on test: 0.53\n\n\n\n\n\n# create a train and test set\nlibrary(rsample)\n\nset.seed(123)\n\nsplit = initial_split(df_movie_reviews, prop = .75)\n\ndf_train = training(split)\ndf_test  = testing(split)\n\nmodel_reviews_extra = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_train\n)\n\n# get predictions\ntest_preds = predict(model_reviews_extra, newdata = df_test)\n\n# get RMSE on test\nyardstick::rmse_vec(df_test$rating, test_preds)\n\n\n\nRMSE on test: 0.54\n\n\n\n\n\nSo there you have it, we have a model that we can use to predict on new data without too much trouble. As we’ll soon see though, there are limitations to doing things this simply. But conceptually this is an important idea, and one we will continue to return to in our discussion of machine learning.\n\n\n6.4.1 Using Metrics for Model Evaluation and Selection\nAs we’ve seen, there are many performance metrics to choose from to assess model performance, and the choice of metric depends on the type of problem. For example, for a problem for numeric targets, we might use RMSE, while for a classification problem, we might use accuracy. As discussed, it turns out that assessing the metric on the data we used to fit the model does not give us the best assessment of that metric. This is because the model will do better on the data it was trained on than on new data it wasn’t trained on, and we can generally always improve that metric in training by making the model more complex. However, in many modeling situations, this complexity comes at the expense of generalization, and the model will not perform as well on new data, something we’ll discuss in more detail shortly. So what we really want to ultimately say about our model will regard performance on the test set with our chosen metric, and not the data we used to fit the model. At that point, we can also compare multiple models to one another given their performance on the test set, and select the one that performs best.\nYou should take a moment to compare our result with the holdout method to the intial model we used to get some metrics, where the model was fit on the entire dataset. Metrics are almost always better on the training set, and that’s because the model was fit on the entire dataset, and so it was able to capture more of the variability in the data. But that’s not what we’re interested in. We want to know how well the model will do on new data, and so we use the test set to get a sense of that.\n\n\n6.4.2 Understanding Test Error and Generalization\n\nThis part gets into the weeds a bit. If you are not so inclined, skip to the summary of this section.\n\nIn the following discussion, you can think of a standard linear model scenario just as we have been using, e.g. with squared-error loss function, and a data set where we split some of the observations in a random fashion into a training set, for initial model fitting, and a test set, which will be kept separate and independent, and used to measure generalization performance. We note training error as the average loss over all the training sets we could create in this process, and test error as the average prediction error obtained when a model fitted on the training data is used to make predictions on the test data. So, in addition to the previously noted goal of finding the ‘best’ model, model selection, we are interested further in estimating the prediction error with new data, model performance.\n\n6.4.2.1 Generalization in the Classical Regime\nSo consider a modeling situation where we have the usual situation of splitting data into training and test sets. We run the model on the training set, but we are more interested in generalization error, or how well it predicts on the test set. We can think of the test error as the average error over many such splits of the data into training and test sets. Given this scenario, let’s look at the following visualization inspired by Hastie, Tibshirani, and Friedman (2017).\n\n\n\n\n\n\nFigure 6.1: Bias Variance Tradeoff\n\n\n\nPrediction error on the test set, shown in red, is a function of several components, and the terms bias and variance generally refer to two of those components. One thing to note is that even if we had the ‘true’ model given the features specified correctly, there would still be prediction error due to the random data generating process.\nThe main idea here is that as the model complexity increases, we potentially capture more of the data variability. The so-called bias, which is the difference in our average prediction and the true model prediction, decreases, but this only continues for training error, shown in blue, where eventually our model can possibly fit the training data perfectly! For test error though, as the model complexity increases, the bias decreases, but the variance, which is the variability in prediction with changes in data, eventually increases. This is because we get too close to the training data and do poorly when we try to generalize beyond it. This is traditionally known as the bias-variance tradeoff - we can reduce one source of error in the test set at the expense of the other, but not both at the same time indefinitely. In other words, we can reduce bias by increasing model complexity, but this will eventually increase variance in our test predictions. We can reduce variance by reducing model complexity, but this will increase bias. The goal is to find the sweet spot where we have a model that is complex enough to capture the underlying process, but not so complex that it overfits to the training data. Recall that we’re not as interested in training error except to get a sense of how well the model fits the data- ideally it at least does well on training!\n\n\n6.4.2.2 Generalization in Deep Learning\nIt turns out that with lots of data and very complex models, or maybe just in most settings, our classical understanding doesn’t hold up like we’d think. In fact, we can get a model that fits the training data perfectly, and yet ultimately still generalizes well to new data! This phenomenon is encapsulated in the notion of double descent. The idea is that, with overly complex models such as those employed with deep learning, we get to the point of interpolating the data exactly (Figure 6.3). But as we continue to increase the complexity of the model, we actually start to generalize better again, and visually this displays as a double descent in terms of test error. We see an initial decrease in test error as the model gets better in general. After a while, it begins to rise as seen in the classical regime (Figure 6.1), to where we hit a peak at the point where we have as many parameters as data points. Beyond that however, as we go even more complex with our model, we can possibly see a decrease in test error again. Crazy!\nWe demonstrate this on the classic mtcars dataset3, which has only 32 observations! We repeatedly train a model to predict miles per gallon on only 10 of those observations, and assess test error on the rest. The model we use is a form of ridge regression, but implemented such that we can use splines for the car’s weight, horsepower, and displacement4. We fit increasingly complex models, and plot the test error and training error as a function of model complexity. We see that the test error dips as we get a better model, but eventually rises as expected. It eventually hits a peak, but then starts to decrease again! This is the double descent phenomenon with one of the simplest datasets around. Cool!\n\n\n\n\n\n\n\n\nFigure 6.2: Double Descent on the classic mtcars dataset\n\n\n\n\n\n\n\n6.4.2.3 Generalization Summary\nThe take home point is this: our primary concern is generalization error. We can reduce this error by increasing model complexity, but this may eventually cause test error to increase. However, with enough data and model complexity, we can get to the point where we can fit the training data perfectly, and yet still generalize well to new data. Unless you are doing deep learning, you can maybe assume the classical regime holds, but when doing deep learning, you can worry less about the model’s complexity. In any event, we still want to employ tools to help reduce generalization error, and we prefer smaller and simpler models that can do as well as more complex ones, even if those models are still billions of parameters!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-regularization",
    "href": "machine_learning.html#sec-ml-regularization",
    "title": "6  Core Concepts",
    "section": "6.5 Regularization",
    "text": "6.5 Regularization\nAs we’ve seen, a key aspect of the machine learning approach is to generalize to new data. One way to improve generalization is through the use of regularization, which is a general approach to penalize complexity in a model, and is typically used to prevent overfitting. Overfitting occurs when a model fits the training data very well, but does not generalize well to new data, and this is often due to the model being too complex for the data setting, and is fitting to noise in the training data that isn’t present in other data. Note that the converse can also happen, and is often the case with simpler models, where the model does not fit the training data well, and so does not generalize well to new data either, and this is known as underfitting5.\nWe demonstrate this in the following visualization. The first plot shows results from a model that is notably complex, and in doing so presents a very wiggly result. This is an example of overfitting, and is often seen in models that are too complex for the underlying data. The second plot shows a straight line fit as we’d get from linear regression, which is an example of underfitting. The third plot shows a model that is a better fit to the data, and is an example of a model that is complex enough to capture the nonlinear aspect of the data, but not so complex that it is trying to capitalize on noise in the data.\n\n\n\n\n\n\n\n\nFigure 6.3: Overfitting and Underfitting\n\n\n\n\n\nWhen we examine generalization performance6, we see that the overfit model does best on training data, but relatively very poorly on test- nearly a 20% increase in the RMSE value. The underfit model doesn’t change as much in performance because it was poor to begin with on training. Our ‘better’ model wasn’t best on training, but was best on the test set.\n\n\n\n\nTable 6.2: RMSE for each model on new data\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Model\n      RMSE\n      % change\n    \n  \n  \n    \n      Train\n    \n    Better\n2.18\n\n    Over\n1.97\n\n    Under\n3.05\n\n    \n      Test\n    \n    Better\n2.19\n0.6\n    Over\n2.34\n19.1\n    Under\n3.24\n6.1\n  \n  \n  \n\n\n\n\n\n\n\nWe have already seen one example of regularization in the ridge regression model (ADD CHAPTER LINK), where we add a penalty term to the objective function. This penalty term is a function of the coefficients, and is based on the sum of the squared values of the coefficients. It is also known as an L2 penalty, and is a very common type of regularization. Another common approach for linear models is the L1 penalty, which is the sum of the absolute values of the coefficients. This is used in the lasso model. There are other types of regularization as well, such as the elastic net, which is a combination of the L1 and L2 penalties. The relative size of the two penalties is controlled by a mixing parameter, and the optimal value of that parameter is determined by cross-validation.\nIt turns out that regularization is used in many modeling scenarios. Here is a quick rundown of some examples.\n\nGAMs also use penalized regression for estimation, where the coefficients used in the basis functions are penalized (typically with L2). This keeps the ‘wiggly’ part of the GAM from getting too wiggly, as in the overfit model above (Figure 6.3), tending toward a linear effect.\nSimilarly, the variance estimate of a random effect in mixed models, e.g. for the intercept or slope, is inversely related to an L2 penalty on the fixed effects estimates for that group effect. The more penalization applied, the less random effect variance, and the more the random effect is shrunk toward the overall mean7.\n\n\nStill another form of regularization occurs in the form of priors in Bayesian models. For example, the variance on the prior for regression coefficients could be very large, which amounts to a result where there is little influence of the prior on the posterior, or it could be very small, which amounts to a result where the prior has a lot of influence on the posterior, shrinking it toward the prior mean, which is typically zero. In fact, ridge regression is a frequentist form of standard Bayesian linear regression with a normal distribution prior for the coefficients, and the L2 penalty is related to the variance of that prior.\nAs a final example of regularization, dropout is a technique used in deep learning to prevent overfitting. It works by randomly dropping out some of the nodes in intervening/hidden layers in the network during training. This tends to force the network to learn more robust features, allowing for better generalization.\n\nIn short, regularization comes in many forms across the modeling landscape, and is a key aspect of machine learning and traditional statistical modeling alike. In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural networks, recommender systems etc. The primary goal again is to hopefully increase our ability to generalize the selected model to new data.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-cv",
    "href": "machine_learning.html#sec-ml-cv",
    "title": "6  Core Concepts",
    "section": "6.6 Cross-validation",
    "text": "6.6 Cross-validation\nSo we’ve talked a lot about generalization to unseen data, so now let’s think about some ways to go about a general process of selecting parameters for a model and assessing performance and generalization.\nAs noted previously, the simplest approach is to split the data into training and test sets, fit the model on the training set, and then assess performance on the test set. This is all well and good, but the test error has uncertainty, and would be slightly different with any training-test split we came up with. We’d also like to get a better assessment when searching the parameter space, because there are oftentimes parameters for which we have no way of guessing the value beforehand. In this case, we need to figure out the best parameters before assessing a final model’s performance. One way to do this is to split the data into multiple test sets, which we now call validation sets, because we still want a test set to be held out that is in no way used during the training process. We fit the model on the training set, and then assess performance on the validation set(s). We then repeat this process for many different splits of the data into training and validation sets, and average the results. This is known as K-fold cross-validation.\nHere is a visualization of 3-fold cross validation. We split the data such that 2/3 of it will be used for training, and 1/3 for validation. We then do this for a total of 3 times, such that the validation set is on a different part of the data each time, and all observations are used for both training and validation at some point. We then average the results of any metric across the validation sets. Note that in each case here, there is no overlap of data between the training and validation sets.\n\nThe idea is that we are trying to get a better estimate of the test error by averaging over many different test sets. The number of folds, or splits, is denoted by \\(K\\). The value of \\(K\\) can be any number, but typically is 10 or less. The larger the value of \\(K\\), the more accurate the estimate of the test error, but the more computationally expensive it is, and in application, you generally don’t need much to get a good estimate of the mean error. However, with smaller datasets, one can even employ a leave-one-out approach, where \\(K\\) is equal to the number of observations in the data.\nSo cross-validation provides a better measure of the test error. If we are interested when we look at models with different parameters we’re trying to figure out, we can pit their respective average errors against one another, and select the model with the lowest average error, a process known generally as model selection. This works for choosing a model within a potential set of hyperparameter settings, for example, with different penalty parameters for regularized regression, but can also aid in choosing a model from a set of different model types, for example, standard linear model approach vs. boosting.\nNow how might we go about this for modeling purposes? Very easily with modern packages. In the following we demonstrate this with a logistic regression model.\n\nPythonR\n\n\n\n# import necessary libraries\nfrom pandas import read_csv\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score\n\ndf_movies = read_csv(\"data/movie_reviews_processed.csv\")\n\nX = df_movies.filter(regex=\"_sc$\")\ny = df_movies[\"rating_good\"]\n\n# Cs is the (inverse) penalty parameter;\nclf = LogisticRegressionCV(penalty='l2', Cs=[1], cv=5, max_iter=1000)\nclf.fit(X, y)\n\nLogisticRegressionCV(Cs=[1], cv=5, max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegressionCV?Documentation for LogisticRegressionCViFittedLogisticRegressionCV(Cs=[1], cv=5, max_iter=1000) \n\n# clf.scores_  # show the accuracy score for each fold\n\n# print the average accuracy score\nclf.scores_[1].mean()\n\n0.671\n\n\n\n\nFor R, we prefer mlr3 for our machine learning demonstrations, as we feel it is more like sklearn in spirit, as well as offering computational advantages for when you want to actually do ML with R. The tidymodels ecosystem is also a good option.\n\n# Load necessary libraries\nlibrary(mlr3)\nlibrary(mlr3learners)\n\ndf_movies = read_csv(\n  \"data/movie_reviews_processed.csv\", \n  col_select = matches('_sc|rating_good')\n)\n\ndf_movies = df_movies %&gt;% \n  mutate(rating_good = as.factor(rating_good))\n\n\n# Define task\ntask_lr_ridge = TaskClassif$new(\"movie_reviews\", df_movies, target = \"rating_good\")\n\n# Define learner (alpha = 0 is ridge regression)\nlearner_lr_ridge = lrn(\"classif.cv_glmnet\", alpha = 0, predict_type = \"response\")\n\nlearner_lr_ridge$param_set$values$alpha = 1 # set the penalty parameter to some value\n\n# Define resampling strategy\nresult_lr_ridge = resample(\n    task       = task_lr_ridge,\n    learner    = learner_lr_ridge,\n    resampling = rsmp(\"cv\", folds = 5)\n)\n\nINFO  [20:57:01.047] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 1/5)\nINFO  [20:57:01.234] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 2/5)\nINFO  [20:57:01.285] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 3/5)\nINFO  [20:57:01.331] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 4/5)\nINFO  [20:57:01.507] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 5/5)\n\n# result_lr_ridge$score(msr('classif.acc')) # show the accuracy score for each fold\n\n# print the average accuracy score\nresult_lr_ridge$aggregate(msr('classif.acc'))\n\nclassif.acc \n      0.658 \n\n\n\n\n\nIn each case above, we end up with five separate accuracy values, one for each fold. Our final assessment of the model’s accuracy is the average of these five values. This is a better estimate of the model’s accuracy than if we had just used a single test set, and in the end it is based on the entire data.\n\n6.6.1 Methods of Cross-validation\nThere are different approaches we can take for cross-validation that we may need for different data scenarios. Here are some of the more common ones.\n\nShuffled: Shuffling prior to splitting can help avoid data ordering having undue effects.\nGrouped/stratified: In cases where we want to account for the grouping of the data, e.g. for data with a hierarchical structure. We may want groups to appear in training or test, but not both, as with grouped k-fold. Or we may want to ensure group proportions across training and test sets, as with stratified k-fold.\nTime-based: e.g. for time series data, where we only want to assess error on future values\nCombinations: e.g. grouped and time-based\n\nHere are images from the scikit-learn library documentation depicting some different cross-validation approaches.\n\n\n\n\n\n\n\n\n\nk-fold\n\n\n\n\n\n\n\nGrouped\n\n\n\n\n\n\n\n\n\nStratified\n\n\n\n\n\n\n\nTime series\n\n\n\n\n\nIn general, the form we employ will be based on our data needs.\n\n\n\n\n\n\nIt’s generally always useful to use a stratified approach to cross-validation, especially with classification problems, as it helps ensure a similar balance of the target classes across training and test sets. You can also employ this with numeric targets, enabling you to have a similar distribution of the target across training and test sets.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-tuning",
    "href": "machine_learning.html#sec-ml-tuning",
    "title": "6  Core Concepts",
    "section": "6.7 Tuning",
    "text": "6.7 Tuning\nOne problem with the previous ridge logistic model we just used is that we set the penalty parameter to a fixed value. We can do better by searching over a range of values instead, and picking a ‘best’ one. This is generally known as hyperparameter tuning, or simply tuning. We can do this with cross-validation as well where we will use k-fold cross-validation to assess the error for each value of the penalty parameter values. We then select the value of the penalty parameter that gives the lowest average error. This is a form of model selection.\nAnother potential point of concern is that we are using the same data to both select the model and assess its performance. This is a form of a more general phenomenon of data leakage, and may result in an overly optimistic assessment of performance. One solution is to do as we’ve discussed before, which is to split the data into three parts: training, validation, and test. We use the training set(s) to fit the model, the validation set(s) to select the model, and then finally use the test set to assess the model’s performance. The validation approach is used to select the model, and the test set is used to assess the model’s performance. The following visualizations from the scikit-learn documentation illustrates the process.\n\n\n\n\n\n\n\n\n\nTrain-Validation-Test Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the performance on test is not without uncertainty, we can actually nest the entire process within a validation approach, where we have an inner loop of k-fold cross-validation and an outer loop to assess the model’s performance on multiple hold out sets. This is known as nested cross-validation. This is a more computationally expensive approach, and generally would require more data, but it would result in a more robust assessment of performance\n\n\n\n\n6.7.1 A Tuning Example\nWhile this may start to sound complicated, it doesn’t have to be, as tools are available to make our generalization journey a lot easier. In the following we demonstrate this with a ridge based logistic regression model. The approach we use is called a grid search, where we explicitly step through potential values of the penalty parameter. While we only look at one parameter here, for a given modeling approach we could construct a ‘grid’ of sets of parameter values8 to search over as well.\nWe use the LogisticRegression function in sklearn to perform k-fold cross-validation to select the best penalty parameter. We then apply the best model to the test set and calculate accuracy. We do the same thing in R with the mlr3tuning package. We use the AutoTuner function to perform k-fold cross-validation to select the best penalty parameter. In both settings we are interested in the average accuracy score across the folds, and ultimately the test set9.\n\nPythonR\n\n\n\n# import necessary libraries\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nX = df_movie_reviews.filter(regex=\"_sc$\")\ny = df_movie_reviews[\"rating_good\"]\n\n# split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size=0.25, \n    random_state=42\n)\n\n# define the parameter values for GridSearchCV\nparam_grid = {\n    'C': [0.1, 1, 2, 5, 10, 20],\n}\n\n# perform k-fold cross-validation to select the best penalty parameter\n# Note that LogisticRegression by default is ridge regression for scikit-learn\ngrid_search = GridSearchCV(\n    LogisticRegression(), param_grid=param_grid, cv=5, scoring='accuracy'\n)\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={'C': [0.1, 1, 2, 5, 10, 20]}, scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={'C': [0.1, 1, 2, 5, 10, 20]}, scoring='accuracy') estimator: LogisticRegressionLogisticRegression()  LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\nbest_param = grid_search.best_params_['C']\n\n# apply the best model to the test set and calculate accuracy\nbest_model = grid_search.best_estimator_\nacc_train = best_model.score(X_train, y_train)\nacc_test = best_model.score(X_test, y_test)\n\n\n\n\nTable 6.3: Results of hyperparameter tuning\n\n\n\nBest C: 2\nAccuracy on train set: 0.661\nAccuracy on test set: 0.692\n\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(mlr3verse)\nlibrary(paradox) # for tuning\nlibrary(rsample) # for splitting data\n\ndf_movie_reviews_ = df_movie_reviews %&gt;% \n  mutate(rating_good = as.factor(rating_good)) |&gt; \n  select(matches('sc|rating_good'))\n\n# split the dataset into training and test sets\n\nsplits = initial_split(df_movie_reviews_, prop = 0.75)\n\ndf_train = training(splits)\ndf_test  = testing(splits)\n\n# Define task\ntask = TaskClassif$new(\"movie_reviews\", df_train, target = \"rating_good\")\n\n# Define learner\nlearner = lrn(\"classif.glmnet\", alpha = 0, predict_type = \"response\")\n\n# Define resampling strategy\nresampling = rsmp(\"cv\", folds = 5)\n\n# Define measure\nmeasure = msr(\"classif.acc\")\n\n# Define parameter space\nparam_set = ParamSet$new(\n  list(\n    ParamDbl$new(\"lambda\", lower = 1e-3, upper = 1)\n  )\n)\n\n# Define tuner\ntuner = AutoTuner$new(\n  learner = learner,\n  resampling = resampling,\n  measure = measure,\n  search_space = param_set,\n  tuner = tnr(\"grid_search\", resolution = 10),\n  terminator = trm(\"evals\", n_evals = 10)\n)\n\n# Tune hyperparameters\ntuner$train(task)\n\n# Get best hyperparameters\nbest_param = tuner$model$learner$param_set$values\n\n# Use the best model to predict and get metrics\nacc_train = tuner$predict(task)$score(msr(\"classif.acc\"))\nacc_test  = tuner$predict_newdata(df_test)$score(msr(\"classif.acc\"))\n\n\n\n\nTable 6.4: Results of hyperparameter tuning\n\n\n\nBest lambda: 0.223\nAccuracy on train set: 0.681333333333333\nAccuracy on test set: 0.684\n\n\n\n\n\n\n\nSo there you have it. We searched a parameter space, chose the best set of parameters via k-fold cross validation, and got an assessment of generalization error in just a couple lines of code. Neat!\n\n6.7.1.1 Search Spaces\nIn the previous example, we used a grid search to search over a range of values for the penalty parameter. This is a very simple approach, but it can be computationally expensive. We can do better by using a more sophisticated approach to search over the parameter space. For example, we can use a random search, where we randomly sample from the parameter space. This is generally faster than a grid search, and can be just as effective. Other methods are available that better explore the space and do so more efficiently.\n\n\n\n\n\n\nGrid search can work to some extent and is a quick and easy way to get started, but generally we want something that can search a true space rather than a limited grid. Typical options are random, bayesian optimization, hyperband, and genetic algorithms. Most of these are available in scikit-learn and mlr3.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-pipelines",
    "href": "machine_learning.html#sec-ml-pipelines",
    "title": "6  Core Concepts",
    "section": "6.8 Pipelines",
    "text": "6.8 Pipelines\nFor production-level work, or just for reproducibility, it is often useful to create a pipeline for your modeling work. A pipeline is a series of steps that are performed in sequence. For example, we might want to perform the following steps:\n\nImpute missing values\nTransform features\nCreate new features\nSplit the data into training and test sets\nFit the model on the training set\nAssess the model’s performance on the test set\nCompare the model with others\nSave the ‘best’ model\nUse the model for prediction on future data, sometimes called scoring\nRedo the whole thing from time to time\n\nWe can create a pipeline that performs all of these steps in sequence. This is useful for a number of reasons. First, doing so makes it far easier to reproduce the results as needed. Second, it is relatively easy to change the steps in the pipeline. For example, we might want to try a different imputation method, or add a new model. Third, it is relatively easy to apply the pipeline. For example, we might want to use the model on new data. We can just apply the pipeline to the new data, and it will perform all of the steps in sequence, including fitting the model. Fourth, having a pipeline facilitates model comparison, as we can ensure that the models are receiving the same data process. Finally, we can save the pipeline for later use- we just save the pipeline as a file, and then load it later when we want to use it again.\n\nPythonR\n\n\nHere is an example of a pipeline in Python. We use the make_pipeline function from the sklearn package. This function takes a series of steps as arguments, and then performs them in sequence. We can then use the pipeline to fit the model, assess its performance, and save it for later use.\n\n# import necessary libraries\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score\n\n# create pipeline\npipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    LogisticRegressionCV(penalty='l2', Cs=[1], cv=5, max_iter=1000),\n)\n\n# fit the pipeline\npipeline.fit(X_train, y_train)\n\n# assess the pipeline\ny_pred = pipeline.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n# save the pipeline\n# from joblib import dump, load\n# dump(pipeline, 'pipeline.joblib')\n\n\n\nWith R, mlr3 works in a very similar fashion to sklearn, which is why we use it for demonstration. We create a pipeline with the po, or pipe operator function, which takes a series of steps as arguments, and then performs them in sequence.\n\n# Load necessary libraries\nlibrary(mlr3verse)\n# library(mlr3learners)\n# library(mlr3pipelines)\n\n# Define task\ntask = TaskClassif$new(\"movie_reviews\", df_movie_reviews, target = \"rating_good\")\n\n# Define learner\nlearner = lrn(\"classif.cv_glmnet\", predict_type = \"response\")\n\n# Define pipeline\npipeline = po(\"scale\") %&gt;&gt;%\n  po(\"imputemean\") %&gt;&gt;%\n  po(\"learner\", learner)\n\n# Fit pipeline\npipeline$train(task)\n\n# Assess pipeline\npipeline$predict(task)[[1]]$score(msr(\"classif.acc\"))\n\n# Save pipeline\n# saveRDS(pipeline, \"pipeline.rds\")\n\n\n\n\nDevelopment and deployment of pipelines will depend on your specific use case, and can get notably complicated. Think of your model data being the culmination of features drawn from dozens of wildly different databases, and the model itself being a complex ensemble of models, each with their own hyperparameters. You can imagine the complexity of the pipeline that would be required to handle all of that, but it is possible. In any event, the basic idea is the same, and pipelines are a great way to organize your modeling work.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#commentary",
    "href": "machine_learning.html#commentary",
    "title": "6  Core Concepts",
    "section": "6.9 Commentary",
    "text": "6.9 Commentary\nWhen machine learning began to take off, it seemed many in the field of statistics sat on their laurels, and often scoffed at these techniques that didn’t bother to test their assumptions10! ML was, after all, mostly just a rehash of old ideas right? But the machine learning community, which actually comprised both computer scientists and statisticians, was able to make great strides in predictive performance, and the application of machine learning in myriad domains continues to enable us to push the boundaries of what is possible. Statistical analysis wasn’t going to provide ChatGPT or self-driving cars, but it remains vitally important whenever we need to understand the uncertainty of our predictions, or when we need to make inferences about the data world. A more general field of data science became the way people used statistics and machine learning to solve their data challenges. So the two fields are complementary and overlapping, and the best data scientists will be able to draw from both. In the end, use the best tool for the job, worry less about what it’s called or whether it’s the hot thing, or has a cool label, and importantly, just have fun!\nMOVE TO APPENDIX OR Part 3",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#using-r-and-python-in-ml",
    "href": "machine_learning.html#using-r-and-python-in-ml",
    "title": "6  Core Concepts",
    "section": "6.10 Using R and Python in ML",
    "text": "6.10 Using R and Python in ML\n\n6.10.1 Python\nPython is the king of ML. Many other languages can perform ML and maybe even well, but Python is the most popular, and has the most packages, and it’s where tools are typically implemented and developed first. Even if it isn’t your primary language, it should be for any implementation of machine learning.\nPros:\n\npowerful and widely used tools\ntypically very efficient on memory and fast\nmany modeling packages try to use the sklearn API for consistency11\neasy pipeline/reproducibility setup\n\nCons:\n\nEverything beyond getting a prediction can be difficult: e.g. good model summaries and visualizations, interpretability tools, extracting key estimated model features, etc. For example, getting features names as part of the output is a recent development for scikit-learn and other modeling packages.\nData processing beyond applying simple functions to columns can be notably tedious. Pandas, to put it simply, is not tidyverse.\nThe ML ecosystem is fragile, and one package’s update will often break another package’s functionality, meaning your work will often be frozen in time to whenever you first began model exploration. Many corporate modeling environments are still based on versions of Python that may be many years old, and the model packages will contain all the bugs from the time of release when the Python environment was created.\nPackage documentation is often quite poor, even for some important model aspects of the model, and there is no consistency from one package to another. Demos may work or not, and you may have to dig into the source code to figure out what’s actually going on. This hopefully will be alleviated in the future with modern AI tools that can write the documentation for you.\nInteractive model development with Jupyter has not been close to the level with alternatives like RMarkdown for years. However, Quarto has already shown great promise, as this book was written with it, so in the end, the R folks may bail out this issue for the Python folks.\n\n\n\n6.10.2 R\nSpeaking as folks who’ve used tools like mlr3, tidyverse, and more on millions of data points for very large and well-known companies, we can say definitively that R is actually great at ML and at production level. The tools are not as fast or memory efficient relative to Python, but they are typically more user friendly, and usually have good to even excellent documentation, as package development has been largely standardized for some time. As far as some drawbacks, some Python packages such as xgboost and lightgbm have concurrent development in R, but even then the R development typically lags with feature implementation. And when it comes to ML with deep learning models, R packages merely wrap the underlying Python packages. In general though, for everything before and after ML, from feature engineering to visualization to reporting, R has much more to offer.\nPros:\n\nvery user friendly and fast data processing\neasy to use objects that contain the things you’d need to use for further processing\npractically every tool you’d use works with data frames\nsaving models does not require any special effort\neasy post-processing of models with many packages designed to work with the output of other modeling packages (e.g. broom, tidybayes, etc.)\ndocumentation is standardized for any CRAN and most non-CRAN packages, and will only improve with AI tools. Unlike Python, examples are expected for documented functions, and the package will fail to build if any example fails, and warn if examples are empty. This is a great way to ensure that examples are present and actually work.\nML tools can be used on tabular data of millions of instances in memory and in production, and on data that is too large to fit in memory using disk-backed data structures.\n\nCons:\n\nrelatively slow\nmemory intensive\npipeline/reproducibility has only recently been of focus\n\ntidymodels is a great but fairly non-standard way of conducting machine learning\nmlr3 is much more sklearn-like- fast and memory efficient, but not as widely used\n\ndevelopers often don’t do enough testing\n\nIn summary, Python is the best tool for ML, but you can use R for pretty much everything else if you want, including ML if it’s not too computationally expensive or you don’t have to worry about that aspect. Quarto makes it easy to use both, including simultaneously, so the great thing is you don’t have to choose!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#where-to-go-from-here",
    "href": "machine_learning.html#where-to-go-from-here",
    "title": "6  Core Concepts",
    "section": "6.11 Where to go from here",
    "text": "6.11 Where to go from here\n\n6.11.1 refs\nESL for R/Python\nridge as Bayesian WIKILINK: https://en.wikipedia.org/wiki/Ridge_regression#Bayesian_interpretation\ndropout https://d2l.ai/chapter_multilayer-perceptrons/dropout.html\nbv tradeoff\nhttps://hastie.su.domains/Papers/ESLII.pdf\nhttps://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/\nRF/boosting https://developers.google.com/machine-learning/decision-forests\n“Reconciling modern machine-learning practice and the classical bias–variance trade-off”, 2019, by Belkin, Hsu, Ma, Mandal, https://www.pnas.org/doi/10.1073/pnas.1903070116.\nCV https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\nDL\nAnnotated History of Modern AI and Deep Learning, Juergen Schmidhuber\nInterpretation\nMolnar\nTechniques to Improve Ecological Interpretability of Black-Box Machine Learning Models https://link.springer.com/article/10.1007/s13253-021-00479-7\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd Edition. https://hastie.su.domains/ElemStatLearn/.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#footnotes",
    "href": "machine_learning.html#footnotes",
    "title": "6  Core Concepts",
    "section": "",
    "text": "Although this is implied by the name, the description of ML as ‘machines learning without human intervention’ can be misleading to the newcomer. In fact, many of the most common models in machine learning are not capable of learning ‘on their own’ at any level, and require human intervention to provide processed data, specify the model, its parameters, set up the search through that parameter space, analyze the results, update the model, etc. We only very recently, post 2020, have developed models that appear to be able to generalize to new tasks as if they have learned them without human intervention, but that would ignore all the hands-on work that went into the development of those models, which never could have such capabilities otherwise.↩︎\nGeneralization in statistical analysis is more about generalizing from our sample of data to the population from which it’s drawn. In order to do that well or precisely, one needs to meet certain assumptions about the model. In machine learning, generalization is more about how well the model will perform on new data, and is often referred to as ‘out-of-sample’ performance.↩︎\nIf not familiar, the mtcars object is a data frame that comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).↩︎\nIt’s actually called ridgeless regression.↩︎\nUnderfitting is a notable problem in many academic disciplines, where the models are often too simple to capture the complexity of the underlying process. Typically the models are often linear, and the underlying process may be anything but. These disciplines were slow to adopt machine learning techniques, as they are often more difficult to interpret, and so seen as not as useful for understanding the underlying process. However, one could make the obvious argument that ‘understanding’ an unrealistic result is not very useful either, and that the goal should be to understand the underlying process however we can, and not just the model.↩︎\nThe data is based on a simulation (using mgcv::gamSim), so the test data is just more simulated data points.↩︎\nOne more reason to prefer a random effects approach over so-called fixed effects models, as the latter are not penalized at all, and thus are more prone to overfitting.↩︎\nWe can use of expand.grid or crossing in R, or pandas’ version expand_grid to construct these values to iterate over. scikit-learn’s GridSearchCV function does this for us when we provide the dictionary of values for each parameter.↩︎\nIf you’re comparing the Python vs. R approaches, scikit-learn by default uses ridge regression, while in R we set the value alpha to enforce it, since glmnet by default uses the elastic net, a mixture of lasso and ridge. Also, scikit-learn uses the inverse of the penalty parameter, while mlr3 uses the penalty parameter directly, which is more straightforward. And obviously, no one will agree on what we should name the value (we have no idea where ‘C’ comes from, maybe ‘complexity’(?), though we have seen λ used in various statistical publications).↩︎\nTo paraphrase provocatively, ‘machine learning is statistics minus any checking of models and assumptions’. Brian D. Ripley useR! 2004, Vienna (May 2004) Want to know what’s even crazier than that statement? It was said by the guy that literally wrote the book on neural networks before anyone was even using them in any practical way! Also interesting to note is that techniques like random forests and others associated with machine learning actually came from established statisticians. In short, there never was a statistics vs. machine learning divide. Tools are tools, and the best data scientists will have many at their disposal for any project.↩︎\nNote to developers, just having a fit and predict method is not an API. But as scikit-learn is not internally consistent in using its own API, it’s not surprising that other packages don’t either.↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Core Concepts</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html",
    "href": "ml_common_models.html",
    "title": "7  Common Models",
    "section": "",
    "text": "7.1 Key Ideas\nThe take home messages from this section include the following:",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#key-ideas",
    "href": "ml_common_models.html#key-ideas",
    "title": "7  Common Models",
    "section": "",
    "text": "Any model can be used with machine learning\nA good and simple baseline is essential for interpreting your performance results\nOne only needs a small set of tools (models) to go very far with machine learning\n\n\n7.1.1 Why this matters\nHaving good choices in your data science toolbox means you don’t have to waste time with nuance and can get down to what matters- performance! Furthermore, using these common tools means you’ll know you’re in good company, and that you’ll be able to find many resources to help you along the way. Additionally, you’ll be able to focus on the data and the problem at hand, rather than the model, which in the end, is just a tool to help you understand the data. If you can get a good understanding of the data with a simple model, then that may be all you need for your situation. If you decide you need a more complex modeling approach, then using these models will still give you a good idea of what you should expect in terms of performance.\n\n\n7.1.2 Good to know\nTODO: ADD LINK TO ESTIMATION CHAPTER\nBefore diving in, it’d be helpful to be familiar with the following:\n\nLinear models, esp. linear and logistic regression\nBasic machine learning concepts as outlined in the the ML Concepts chapter (Chapter 6)\nModel estiamtion as outlined in the Estimation chapter (Chapter 3)",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-general-approach",
    "href": "ml_common_models.html#sec-ml-general-approach",
    "title": "7  Common Models",
    "section": "7.2 General Approach",
    "text": "7.2 General Approach\nLet’s start with a general approach to machine learning to help us get some bearings. Here is an example outline of the process we could take. This incorporates some of the ideas we’ve already discussed, and we’ll demonstrate most of this in the following sections.\n\nDefine the problem, including the target variable(s)\nSelect the model(s) to be used, including one baseline model\nDefine the performance objective and metric(s) used for model assessment\nDefine the search space (parameters, hyperparameters) for those models\nDefine the search method (optimization)\nImplement some sort of cross-validation technique and collect the corresponding performance metrics\nEvaluate the results on unseen data with the chosen model\nInterpret the results\n\nHere is a more concrete example:\n\nDefine the problem: predict the probability of heart disease given a set of features\nSelect the model(s) to be used: ridge regression, standard regression with no penalty as baseline\nDefine the objective and performance metric(s): (R)MSE, R-squared\nDefine the search space (parameters, hyperparameters) for those models: penalty parameter\nDefine the search method (optimization): grid search\nImplement some sort of cross-validation technique: 5-fold cross-validation\nEvaluate the results on unseen data: RMSE on test data\nInterpret the results: the ridge regression model performed better than the baseline model, and the coefficients tell us something about the nature of the relationship between the features and the target\n\nAs we go along in this chapter, we’ll most of this in action at various points. We’ll have a baseline model, an ultimately provide examples of several commonly used models in machine learning. In each case we will assess performance using cross-validation, and then evaluate the final models on unseen data. Separately, we’ll also demonstrate how to tune hyperparameters, which are parameters that are not estimated directly from the data, but rather are set by the modeler. We’",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#data-setup",
    "href": "ml_common_models.html#data-setup",
    "title": "7  Common Models",
    "section": "7.3 Data setup",
    "text": "7.3 Data setup\nFIXME: add appendix link for dataset! LINK to DATA Sections!\nFor our demonstration here, we’ll switch things up and use the heart disease dataset. This is a binary classification problem, where we want to predict whether a patient has heart disease, given information such as age, sex, resting heart rate etc. For more details see the appendix. We have done some initial data processing so that you can dive right in.\nThere are two forms of the data - one which is mostly as seen elsewhere, and one that is purely numeric, where the categorical features are dummy coded and where numeric variables have been standardized (Section 9.2). The purely numeric version will save any additional data processing for some model/package implementations. We also have to drop missing values, so that our 21 century packages don’t hurt themselves on them. When we get to the boosting demonstration, you can use the data as is, since the scale of the data doesn’t really matter, and missing values are treated in the same way as the rest of the data values.\nIn this data, roughly 54% suffered a death, so that is an initial baseline if we’re interested in accuracy- we could get 46% correct by just guessing the majority class.\n\nPythonR\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf_heart = pd.read_csv('data/heart_disease_processed.csv')\ndf_heart_num = pd.read_csv('data/heart_disease_processed_numeric_sc.csv')\n\n# convert appropriate features to categorical\nfor col in df_heart.select_dtypes(include='object').columns:\n    df_heart[col] = df_heart[col].astype('category')\n\nX = df_heart_num.drop(columns=['heart_disease']).to_numpy()\ny = df_heart_num['heart_disease'].to_numpy()\n\n# some models can't automatically handle missing data\ny_complete = df_heart_num.dropna()['heart_disease'].to_numpy().astype(int)\nX_complete = df_heart_num.dropna().drop(columns='heart_disease').to_numpy()\n\n\n\n\nlibrary(tidyverse)\n\ndf_heart = read_csv(\"data/heart_disease_processed.csv\") |&gt; \n    mutate(across(where(is.character), as.factor))\n\ndf_heart_num = read_csv(\"data/heart_disease_processed_numeric_sc.csv\")\n\n\n# as a data.frame for mlr3\nX_num_df = df_heart_num %&gt;%\n    as_tibble() |&gt; \n    mutate(heart_disease = factor(heart_disease)) |&gt; \n    janitor::clean_names() # remove some symbols",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-baseline",
    "href": "ml_common_models.html#sec-ml-baseline",
    "title": "7  Common Models",
    "section": "7.4 Do Better than the Baseline",
    "text": "7.4 Do Better than the Baseline\nCAN WE GET A VISUAL IN HERE SOMEWHERE?\nBefore getting carried away with models, we should try and get something that gives us a good reference point for performance - a baseline model. The baseline model should serve as a way to gauge how much better your model performs over one that is simpler, probably more computationally efficient, and more interpretable. Or maybe it’s one that is sufficiently complex to capture something about the data you are exploring, but not as complex as the models you’re also interested in. Take a classification model for example. We use a logistic regression as abseline, which is as simple as it gets, but is often too simple to be adequately performant for many situations. Even so, we should still be able to beat it with more complex models, or there is little justification for using them.\n\n7.4.1 Why do we do this?\nYou can actually find articles in which deep learning models do not even beat a logistic regression on some datasets, but the fact of which did not stop the authors writing several pages hyping the more complex technique. Probably the most important reason to have a baseline is so that you can avoid wasting time and resources implementing more complex tools, or simply getting excited for no good reason. It is probably rare, but sometimes relationships for the chosen features and target are mostly or nearly linear and have little interaction, and no amount of fancy modeling will make it come about. Furthermore, if our baseline is a complex linear model that actually incorporates nonlinear relationships and interactions (e.g. a GAMM), you’ll often find that the more complex models don’t significantly improve on the baseline by much, if at all. In addition, in time series settings, a moving average or last target value can often be a very good predictor. So in general, you may find that the initial baseline model is good enough for the time being, and you can then move on to other problems to solve, like acquiring data that is functionally predictive. This is especially true if you are working in a business setting where you have limited time and resources.\nA final note. In many (most?) settings, it often isn’t enough to merely beat the baseline model. You should look to do statistically better. For example, if your complex model accuracy is 75% and your baseline is 73%, that’s great, but you should check to see if that difference is statistically significant1, because those metrics are estimates, and they have uncertainty, which means you can get a range for them as well as test whether they are different from one another. If the difference is not notable, then you should probably stick with the baseline model or try something else, because the next time you run the model, the baseline may actually perform better, or at least you can’t be sure that it won’t.\nThat said, in some situations any performance increase is worth it, and even if we can’t be certain a result is statistically better, any sign of improvement is worth pursuing. For example, if you are trying to predict the next word in a sentence, and your baseline is 10% accurate, and your complex model is 11% accurate, that’s a 10% increase in accuracy, which may be a big deal for user experience. You should still work to show that this is a consistent increase and not a fluke.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-penalized",
    "href": "ml_common_models.html#sec-ml-penalized",
    "title": "7  Common Models",
    "section": "7.5 Penalized Linear Models",
    "text": "7.5 Penalized Linear Models\nTODO: ADD LINK TO ESTIMATION CHAPTER\nSo let’s get on with some models already! Let’s use the classic linear model as our starting point for ML, just because we can. We show explictly how to estimate models like lasso and ridge regression in Section 3.9. Those work well as a baseline, and so should be in your ML toolbox.\n\n7.5.1 Elastic Net\nAnother common linear model approach is elastic net, which is a combination of lasso and ridge. We will not show how to estimate elastic net by hand here, but all you have to know is that it combines two penalties, the same ones for lasso and one for ridge, along with the standard objective for a numeric or categorical target. The relative size of the two penalties is controlled by a mixing parameter, and the optimal value of that parameter is determined by cross-validation. So for example, you might end up with a 75% lasso penalty and 25% ridge penalty. In the end though, we’re just going to do a slightly fancier logistic regression!\nLet’s apply this to the heart disease data. We’ll used the ‘processed version’ which has dummy codes and has dropped the few observations with missing values. We are only doing simple cross-validation here to get a better performance assessment, but you are more than welcome to tune both the penalty parameter and the mixing ratio as we have demonstrated before. We’ll revist hyperparameter tuning towards the end of this chapter.\n\nPythonR\n\n\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import cross_validate, KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n\nmodel_elastic = LogisticRegression(\n    penalty='elasticnet',\n    solver='saga',\n    l1_ratio=0.5,\n    random_state=42,\n    max_iter=10000,\n    verbose=False,\n)\n\n# model_elastic.fit(X_complete, y_complete)\n\n # use cross-validation to estimate performance\ncv_elastic = cross_validate(\n    model_elastic,\n    X_complete,\n    y_complete,\n    cv=5,\n    scoring='accuracy',\n)\n\n# pd.DataFrame(cv_elastic) # default output\n\n\n\nTraining accuracy:  0.829 \nBaseline:  0.541\n\n\n\n\n\nlibrary(mlr3verse)\n\ntsk_elastic = as_task_classif(\n    X_num_df |&gt; drop_na(),\n    target = \"heart_disease\"\n)\n\nlrn_elastic = lrn(\n    \"classif.cv_glmnet\", \n    nfolds = 5, \n    type.measure = \"class\", \n    alpha = 0.5\n)\n\ncv_elastic = resample(\n    task       = tsk_elastic,\n    learner    = lrn_elastic,\n    resampling = rsmp(\"cv\", folds = 5)\n)\n\n# cv_elastic$aggregate(msr('classif.acc')) # default output\n\n\n\nTraining Accuracy: 0.839\nBaseline Prevalence: 0.541\n\n\n\n\n\nSo we’re starting off with what seems to be a good model. Our average accuracy across the validation sets is definitely doing better than guessing, an increase of almost 55%! Now let’s see if we can do better with other models!\n\n\n7.5.2 Strengths & Weaknesses\nStrengths\n\nIntuitive approach. In the end, it’s still just a standard regression model you’re already familiar with.\nWidely used for many problems. Lasso/Ridge/ElasticNet would be fine to use in any setting you would use linear or logistic regression.\n\nWeaknesses\n\nDoes not automatically seek out interactions and non-linearity, and as such will generally not be as predictive as other techniques.\nVariables have to be scaled or results will largely reflect data types.\nMay have issues with correlated predictors\n\n\n\n7.5.3 Additional Thoughts\nIncorporating regularization as done with penalized regression would be fine as your default linear model method, and is something to strongly consider for even statistical model settings. Furthermore, these approaches will have better prediction on new data than their standard, nonregularized complements. As such they are a nice balance between staying interpretable while enhancing predictive capability. However, in general they are not going to be as strong of a method as others in the ML universe, and possibly not even competitive without a lot of feature engineering. If prediction is all you care about for a particular modeling setting, you’ll likely want to try something else.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-trees",
    "href": "ml_common_models.html#sec-ml-trees",
    "title": "7  Common Models",
    "section": "7.6 Tree-based methods",
    "text": "7.6 Tree-based methods\nLet’s move beyond standard linear models and get into a notably different type of approach. Tree-based methods are a class of models that are very popular in machine learning, and for good reason, they work very well. To get a sense of how they are derived, consider the following classification example where we want to predict a binary target as ‘Yes’ or ‘No’. We have two numeric features, \\(X_1\\) and \\(X_2\\). At the start we take \\(X_1\\) and make a split at the value of 5. Any observation less than 5 on \\(X_1\\) goes to the right with a prediction of No. Any observation greater than or equal to 5 goes to the left, where we then split based on values of \\(X_2\\), and specifically at 3. Any observation less than 3 goes to the right with a prediction of Yes. Any observation greater than or equal to 3 (and greater than or equal to 5 on \\(X_1\\)) goes to the left with a prediction of No. So in the end, we see relatively lower on \\(X_1\\), or relatively higher on both, results in a prediction of No, and high on \\(X_1\\) and low on \\(X_2\\) results in a prediction of Yes. We can see this visually in the following graph.\n\n\n\n\n\n\n\n\n\n\n\n\nA simple classification tree\n\n\nThis is a simple example, but it illustrates the basic idea of a tree-based model, where the tree reflects the total process, and branches are represented by the splits going down, ultimately ending at leaves where predictions are made. We can also think of the tree as a series of if-then statements, where we start at the top and work our way down until we reach a leaf node, which is a prediction for all observations that qualify for that leaf.\nIf we just use a single tree, this would be the most interpretable model we could probably come up with, and it incorporates nonlinearities (multiple branches on a single feature), interactions (branches across features), and feature selection all in one (some features may not result in useful splits for the objective). However, a single tree is not a very stable model unfortunately, and so does not generalize well. For example, just a slight change in data, or even just starting with a different feature, might produce a very different tree2. The solution is straightforward though - by using the power of a bunch of trees, we can get predictions for each observation from each tree, and then average the predictions, result in a most stable estimate. This is the concept behind both random forests and gradient boosting, which can be seen as different algorithms to produce a bunch of trees, and then average the predictions. They also fall under the heading of ensemble models, which are models that combine the predictions of multiple models, in this case individual trees, to ultimately produce a single prediction for each observation.\nRandom forests and boosting methods are very easy to implement, to a point. However, there are typically a several hyperparameters to consider for tuning. Here are just a few to think about:\n\nNumber of trees\nLearning rate (GB)\nMaximum depth of each tree\nMinimum number of observations in each leaf\nNumber of features to consider at each tree/split\nRegularization parameters (GB)\nOut-of-bag sample size (RF)\n\nThose are the ones that you’ll usually be trying to figure out via cross-validation for boosting or random forests, but there are others. The number of trees and learning rate kind of play off of each other, where having more trees allows for a smaller rate3, which might work better but will take longer to train, and can lead to overfitting if other steps are not taken. The depth of each tree refers to the number of levels down the branches we allow the model to go, as well as how wide we let things get in some implementations. This is important because it controls the complexity of each tree, and thus the complexity of the overall model- less depth helps to avoid overfitting, but too little depth and you won’t be able to capture the nuances of the data. The minimum number of observations in each leaf is also important for the same reason. It’s also generally a good idea to take a random sample of features for each tree (or possibly even each branch), to also help reduce overfitting, but it’s not obvious what proportion to take. The regularization parameters are typically less important in practice, but in general you can use them to reduce overfitting as we would in other modeling circumstances.\n\nHere is an example of gradien boosting with the heart disease data. Although boosting methods are available in scikit-learn for Python, in general we recommend using lightgbm or xgboost packages directly for boosting implementation, which have a sklearn API anyway (as demonstrated). Also, they both provide R and Python implementations of the package, making it easy to not lose your place when switching between languages. We’ll use lightgbm here, but xgboost is also a very good option 4.\n\nPythonR\n\n\n\n# potential models you might use\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier, DMatrix\n\nfrom sklearn.metrics import accuracy_score\n\nmodel_boost = LGBMClassifier(\n    n_estimators=1000,\n    learning_rate=1e-3,\n    max_depth = 5,\n    verbose = -1\n)\n\ncv_boost = cross_validate(\n    model_boost,\n    df_heart.drop(columns='heart_disease'),\n    df_heart_num['heart_disease'],\n    cv=5,\n    scoring='accuracy',\n)\n\n\n\nTraining accuracy:  0.838 \nBaseline Prevalence:  0.541\n\n\n\n\nNote that as of writing, the mlr3 implementation of lightgbm doesn’t seem to handle factors even though the R package does. So we’ll use the numeric version of the data here.\n\nlibrary(mlr3verse)\n# for lightgbm, you need mlr3extralearners and lightgbm package installed\n# remotes::install_github(\"mlr-org/mlr3extralearners@*release\")\nlibrary(mlr3extralearners) \n\nset.seed(1234)\n\n# Define task\n# For consistency we use X_num_df, but lgbm can handle factors and missing data \n# and so we can use the original df_heart if desired\ntsk_boost = as_task_classif(\n    X_num_df,                 \n    target = \"heart_disease\"\n)\n\n# Define learner\nlearner_boost = lrn(\n  \"classif.lightgbm\",\n  num_iterations = 1000,\n  max_depth = 5,\n  learning_rate = 1e-3\n)\n\n\n# Cross-validation\ncv_boost = resample(\n    task       = tsk_boost,\n    learner    = learner_boost,\n    resampling = rsmp(\"cv\", folds = 5)\n)\n\n\n\nTraining Accuracy: 0.828\nBaseline Prevalence: 0.541\n\n\n\n\n\nSo here we have a model that is also performing well, though not significantly better or worse than our elastic net model. For most situations, we’d expect boosting to do better, but this shows why we want a good baseline or simpler model. We’ll revisit hyperparameter tuning using this model later. If you’d like to see an example of how we could implement a form of gradient boosting by hand, see the appendix.\nADD GBLINEAR BY HAND TO APPENDIX\n\n7.6.0.1 Strengths & Weaknesses\nRandom forests and boosting methods, though not new, are still ‘state of the art’ in terms of performance on tabular data like the type we’ve been using for our demos here. As of this writing, you’ll find that it will usually take considerable effort to beat them on tabular data.\nStrengths\n\nA single tree is highly interpretable.\nEasily incorporates features of different types (the scale of numeric features, or using categoricals, doesn’t matter).\nTolerance to irrelevant features.\nSome tolerance to correlated inputs.\nHandling of missing values. Missing values are just another value to potentially split on.\n\nWeaknesses\n\nHonestly few, but like all techniques, it might be relatively less predictive in certain situations. There is no free lunch.\nIt does take more effort to tune relative to linear model methods.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-dl-nn",
    "href": "ml_common_models.html#sec-ml-dl-nn",
    "title": "7  Common Models",
    "section": "7.7 Deep Learning and Neural Networks",
    "text": "7.7 Deep Learning and Neural Networks\n\n\n\nA neural network\n\n\nDeep learning has fundametally transformed the world of data science. It has been used to solve problems in image recognition, speech recognition, natural language processing, and more, from assisting with cancer diagnosis to summarizing entire novels. Deep learning has also been used to solve problems with tabular data of the kind we’ve been focusing on. As yet, it is not a panacea for every problem, and is not always the best tool for the job, but it is a tool that should be in your toolbox. Here we’ll provide brief overview of the key concepts behind neural networks, the underlying technology behind deep learning, and then demonstrate how to implement a simple neural network to get things started.\n\n7.7.1 What is a neural network?\nNeural networks have actually been around a while. Computationally, since the 80s, and conceptually even much further back. They were not very popular for a long time, but this was mostly a computing limitation, much the same reason Bayesian methods were slower to develop relative to related alternatives. But now neural networks have recently become the go-to method for many problems. They still can be very computationally expensive, but we at least have the hardware to pull it off now.\nAt its core, a neural network can be seen as complex series of matrix multiplications exactly as we’ve done with a basic linear model. One notable difference is that neural networks actually implement multiple combinations of features (often referred to as hidden nodes or units), and we add in nonlinear transformations between the matrix multiplications, typically referred to as activations. In fact, you can actually think of neural networks as nonlinear extensions of linear models5. The linear part is just like a standard linear model, where we have a set of features, each with a corresponding weight, and we multiply each feature by its weight and sum them up. The activation part is where things start to get more interesting, where we take the output of the linear part and apply a transformation to it, allowing the model to incoporate noninearities. Furthermore, by combining multiple linear parts and activations together, then repeating the whole process for yet another layer of the model but using the hidden nodes as inputs for the subsequent combinations, we can incorporate interactions between features.\nBefore getting carried away, let’s simplify things a bit. We have multiple options for our activation functions, the most common one being what’s called the rectified linear unit or ReLU. But, we could also use the sigmoid function, which is exactly the same as the logistic link function used in logistic regression. In logistic regression, we take the linear combination of features and weights, and then apply the sigmoid function to it. Because of this, we can actually think of logistic regression as a very simple neural network, with a the linear combination as a single hidden node and a sigmoid activation function adding the nonlinear transformation!\nThe following shows a logistic regression as a neural network. The input features are \\(X_1\\), \\(X_2\\), and \\(X_3\\), and the output is the probability of a positive outcome of a binary target. The weights are \\(w_1\\), \\(w_2\\), and \\(w_3\\), and the bias6 is \\(w_0\\). The hidden node is just our linear predictor which we can create via matrix multiplication of the input matrix and weights. The sigmoid function is the activation function, and the output is the probability of the chosen label.\n\n\n\n\n\n\n\n\n\n\n\n\nA logistic regression as a neural network\n\n\n\n\n7.7.2 Trying it out\nTODO: ADD LINK TO DATA CHAPTER re EMBEDDINGS\nFor simplicity we’ll use the same approach and tools as before, but do know this is probably the very bare minimimum approach for a neural network, and generally you’d prefer on alternative. Our model is a multi-layer perceptron (MLP), which consists of multiple hidden layers of varying sizes. Too begin with, you’d likely want to tune the architecture a bit in normal circumstances just as a starting point. Also, as noted in the data discussion, we’d usually want to use embeddings for categorical features as opposed to the one-hot approach used here, although it amounts to much the same thing, just with some additional computational load7.\nFor our example, we’ll use the processed data with one-hot encoded features. For our architecture, we’ll use three hidden layers with 200 nodes each. As noted, these and other settings are hyperparameters that you’d normally prefer to tune.\n\nPythonR\n\n\nFor our demonstration we’ll use sklearn’s builtin MLPClassifier. We set the learning rate to 0.001. We’ll also use a validation set of 20% of the data to help with early stopping. We set an adaptive learning rate, which is a way to automatically adjust the learning rate as the model trains. The relu activation function is default. We’ll also use the nesterov momentum approach, which is a way to help the model avoid local minima. We use a warm start, which allows us to train the model in stages, which is useful for early stopping. We’ll also set the validation fraction, which is the proportion of data to use for the validation set. And finally, we’ll use shuffle to randomly select observations for each batch.\n\nfrom sklearn.neural_network import MLPClassifier\n\nmodel_mlp = MLPClassifier(\n    hidden_layer_sizes=(200, 200, 200),  \n    learning_rate='adaptive',\n    learning_rate_init=0.001,\n    shuffle=True,\n    random_state=123,\n    warm_start=True,\n    nesterovs_momentum=True,\n    validation_fraction= .2,\n    verbose=False,\n)\n\n# with the above settings, this will take a few seconds\ncv_mlp = cross_validate(\n  model_mlp, \n  X_complete, \n  y_complete, \n  cv=5\n) \n\n# pd.DataFrame(cv_mlp) # default output\n\n\n\nTraining accuracy:  0.829 \nBaseline Prevalence:  0.541\n\n\n\n\nFor R, we’ll use mlr3torch, which calls pytorch directly under the hood. We’ll use the same architecture as was done with the Python example. It uses the relu activation function as a defualt. We’ll also use adam as the optimizer, which is a popular choice and the default for the sklearn approach also. We’ll also use cross entropy as the loss function, which is the same as the log loss objective function used in logistic regression and other ML classification models. We use a batch size of 16, which is the number of observations to use for each batch of training. We’ll also use epochs of 200, which is the number of times to train on the entire dataset. We’ll also use predict type of prob, which is the type of prediction to make. Finally, we’ll use both logloss and accuracy as the metrics to track. As specified, this took over a minute.\n\nlibrary(mlr3torch)\n\nlearner_mlp = lrn(\n    \"classif.mlp\",\n    # defining network parameters\n    layers = 3,\n    d_hidden = 200,\n    # training parameters\n    batch_size = 16,\n    epochs = 200,\n    # Defining the optimizer, loss, and callbacks\n    optimizer = t_opt(\"adam\", lr = 1e-3),\n    loss = t_loss(\"cross_entropy\"),\n    # # Measures to track\n    measures_train = msrs(c(\"classif.logloss\")),\n    measures_valid = msrs(c(\"classif.logloss\", \"classif.ce\")),\n    # predict type (required by logloss)\n    predict_type = \"prob\",\n    seed = 123\n)\n\ntsk_mlp = as_task_classif(\n    backend = X_num_df |&gt; drop_na(),\n    target = 'heart_disease'\n)\n\n# this will potentially take about a minute\ncv_mlp = resample(\n    task       = tsk_mlp,\n    learner    = learner_mlp,\n    resampling = rsmp(\"cv\", folds = 5),\n)\n\n# cv_mlp$aggregate(msr(\"classif.acc\")) # default output\n\n\n\nTraining Accuracy: 0.826\nBaseline Prevalence: 0.541\n\n\n\n\n\nThis neural network model actually did pretty well, and we’re on par with our accuracy as we were with the other two models. This is somewhat suprising given the nature of the data- small number of observations with different data types- a type of situation in which neural networks don’t usually do as well as others. Just goes to show, you never know until you try!\n\n7.7.2.1 Strengths & Weaknesses\nStrengths\n\nGood prediction generally.\nIncorporates the predictive power of different combinations of inputs.\nSome tolerance to correlated inputs.\n\nWeaknesses\n\nSusceptible to irrelevant features.\nDoesn’t outperform other methods that are easier to implement on tabular data.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-tuned-ex",
    "href": "ml_common_models.html#sec-ml-tuned-ex",
    "title": "7  Common Models",
    "section": "7.8 A Tuned Example",
    "text": "7.8 A Tuned Example\nAs we noted in the chapter on machine learning concepts, there are typically multiple hyperparameters we are concerned with. For the linear model, we might want to tune the penalty parameter and the mixing ratio and/or penalty value. For a boosting method, we might want to tune the number of trees, the learning rate, the maximum depth of each tree, the minimum number of observations in each leaf, and the number of features to consider at each tree/split. And for a neural network, we might want to tune the number of hidden layers, the number of nodes in each layer, the learning rate, the batch size, the number of epochs, and the activation function. And so on.\nHere is an example using the boosted model from before. We’ll use the same data and settings as before, but we’ll tune the number of trees, the learning rate, and the maximum depth of each tree. We’ll use a randomized search approach, which is a way to randomly sample from a set of hyperparameters, rather than searching every possible combination. This is a good approach when you have a lot of hyperparameters to tune, and/or when you have a lot of data.\n\nPythonR\n\n\n\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom lightgbm import LGBMClassifier\n\n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    df_heart.drop(columns='heart_disease'), \n    df_heart_num['heart_disease'],\n    test_size=0.2,\n    random_state=42\n)\n\nmodel_boost = LGBMClassifier(\n    verbose = -1\n)\n\nparam_grid = {\n    'n_estimators': [500, 1000],\n    'learning_rate': [1e-3, 1e-2, 1e-1],\n    'max_depth': [3, 5, 7, 9],\n    'min_child_samples': [1, 5, 10],\n}\n\n# this will take a few seconds\ncv_boost_tune = RandomizedSearchCV(\n    model_boost, \n    param_grid, \n    n_iter = 10,\n    cv=5, \n    scoring='accuracy', \n    n_jobs=-1\n)\n\ncv_boost_tune.fit(X_train, y_train)\n\n\n\n\nTest Accuracy 0.82 \nBaseline Prevalence:  0.541\n\n\n\n\n\n# train test split\n\nset.seed(123)\n\nlibrary(mlr3verse)\nlibrary(rsample)\n\nsplit = initial_split(df_heart, prop = .75)\n\ndf_train = training(split)\ndf_test  = testing(split)\n\ntsk_lgbm_tune = as_task_classif(\n    df_train,\n    target = \"heart_disease\"\n)\n\nlrn_lgbm_tune = lrn(\n    \"classif.lightgbm\",\n    num_iterations = to_tune(c(500, 1000)),\n    learning_rate = to_tune(1e-3, 1e-1),\n    max_depth = to_tune(c(2, 3, 5, 7, 9)),\n    min_data_in_leaf = to_tune(c(1, 5, 10))\n)\n\n# set up the validation process\ninstance_lgbm_tune = ti(\n    task = tsk_lgbm_tune,\n    learner = lrn_lgbm_tune,\n    resampling = rsmp(\"cv\", folds = 5),\n    measures = msr(\"classif.acc\"),\n    terminator = trm(\"evals\", n_evals = 10)\n)\n\n# instance\ntuner = tnr(\"random_search\")\n\ntuner$optimize(instance_lgbm_tune)\n\n\n\nTest Accuracy: 0.855\nBaseline Prevalence: 0.541",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-compare",
    "href": "ml_common_models.html#sec-ml-compare",
    "title": "7  Common Models",
    "section": "7.9 Comparing models",
    "text": "7.9 Comparing models\nLet’s compare our models head to head. We went back and restarted our process. We first split the data into training and test sets, the latter a 25% holdout. Then with training, we tuned each model over different settings:\n\nElastic net: penalty and mixing ratio\nBoosting: number of trees, learning rate, and maximum depth, etc.\nNeural network: number of hidden layers, number of nodes in each layer\n\nAfter this, we used the tuned values to retrain on the complete data set. At this stage it’s not necessary to investigate typically, but here we show the results of the 10-fold cross-validation for the already-tuned models, to give a sense of the uncertainty in error estimation.\n\n\n\n\n\n\n\n\nFigure 7.1: Cross-validation results for tuned models.\n\n\n\n\n\n\nWhen it came to to the holdout set with our best models, we see something you might be surprised about - the simplest model wins! It means we can use the simpler model and not worry about the more complex one, or just that we’d be fine using whichever one we prefer. However, none of these results are likely statistically different from each other. As an example, the elastic net model had an accuracy of 0.88, but the interval estimate for such a small sample is very wide - from 0.78 to 0.94. The interval estimate for the difference in accuracy between the elastic net and boosting models is from -0.07 to 0.188. This was a good example of the importance of having an adequate baseline, and where complexity didn’t really help much, though all our approaches did well.\n\n\n\n\nTable 7.1: Results for tuned models on holdout data.\n\n\n\n\n\n\n  \n    \n    \n      model\n      Acc.\n      TPR\n      TNR\n      F1\n      PPV\n      NPV\n    \n  \n  \n    Elastic Net\n0.88\n0.86\n0.90\n0.87\n0.88\n0.88\n    LGBM\n0.83\n0.80\n0.85\n0.81\n0.82\n0.83\n    MLP\n0.84\n0.77\n0.90\n0.82\n0.87\n0.82\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nSome may wonder why the holdout results are better than the cross-validation results. This can happen, and at least in this case may mostly reflect the small sample size. The holdout set is a random sample of 20% of the complete data, 75 observations. Just a couple different predictions could result in a several percentage points difference in accuracy. Also, the holdout set is a random sample that is not the same data, so this could happen just by chance. In general though, you’d expect the holdout results to be a bit, or even significantly, worse than the cross-validation results, but not always.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-interpret",
    "href": "ml_common_models.html#sec-ml-interpret",
    "title": "7  Common Models",
    "section": "7.10 Interpretation",
    "text": "7.10 Interpretation\nWhen it comes to machine learning, just because we have some models at our disposal that don’t readily lend themselves to interpretation with simple coefficients, it doesn’t mean we can’t still figure out what’s going on. Let’s use the boosting model as an example.\n\n7.10.1 Feature Importance\nThe default importance metric for a lightgbm model is the number of splits in which a feature is used across trees, and this will depend notably on your settings and the chosen parameters of the best model. You could also use the Shap approach for variable importance as well, where importance is determined by average absolute Shap value. For this data and the model, depending on the settings, you might see that the most important features are age, cholesterol, and max heart rate.\n\nPythonR\n\n\n\n# load the model\nimport joblib\n\ncv_boost_tune = joblib.load('ml/data/tune-boost-py-model.pkl')\n\n# Get feature importances\ncv_boost_tune.feature_importances_\n\n\n\nR shows the porportion of splits in which a feature is used across trees rather than the raw number.\n\n# load the tuned model\nload(\"ml/data/tune-boost-r-results.RData\")\n\n# Get feature importances\nlrn_lgbm_tuned$importance()\n\n\n\n\n\n\n  \n    \n    \n      Feature\n      value\n    \n  \n  \n    chest_pain_type_asymptomatic\n0.28\n    num_major_vessels\n0.16\n    thalassemia_normal\n0.14\n    st_depression\n0.09\n  \n  \n  \n\n\n\n\n\n\n\nNow let’s think about a visual display. Here we demonstrate a quick partial dependence plot to see the effects of cholesterol and being male. We can see that males are expected to have a higher probability of heart disease, and that cholesterol has a positive relationship with heart disease, such that a notable rise begins around the mean value for cholesterol. The plot shown is a prettier version of what you’d get with the following code.\n\nPythonR\n\n\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator(\n    cv_boost_tune, \n    df_heart.drop(columns='heart_disease'), \n    features=['cholesterol', 'male'], \n    categorical_features=['male'], \n    percentiles=(0, .9),\n    grid_resolution=75\n)\n\n\n\nFor R we’ll use the IML package.\n\nlibrary(iml)\n\nprediction = Predictor$new(\n    lrn_lgbm_tuned, \n    data = df_train, \n    type = 'prob', \n    class = '1'\n)\n\neffect_dat = FeatureEffect$new(\n    prediction, \n    feature = c('cholesterol', 'male'), \n    method = \"pdp\", \n)\n\neffect_dat$plot(show.data = TRUE)",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-other-models",
    "href": "ml_common_models.html#sec-ml-other-models",
    "title": "7  Common Models",
    "section": "7.11 Other ML Models for Tabular Data",
    "text": "7.11 Other ML Models for Tabular Data\nWhen you look up models used in classical machine learning applied to data of the type we’ve been exploring, you’ll potentially see a lot of different kinds. Popular methods from the past include k-nearest neighbors regression, support vector machines, and more. You don’t see these used in practice much though, as these have mostly been made obsolete due to not being as predictive as other options in general (k-nn regression), making strong assumptions about the data distribution (linear discriminant analysis), maybe only works well with ‘pretty’ data situations (SVM), are computationally infeasible for larger datasets (most of them), or just being less interpretable.\nWhile some of these models might still work well in unique situations, when you have tools that can handle a lot of data complexity and predict very well (and typically better) like tree-based methods, there’s not much reason to use the historical alternatives these days. If you’re interested in learning more about them or think one of them is just ‘neat’9, you could potentially use it as a baseline model. Alternatively, you could maybe employ them as part of an ensemble model, where you combine the predictions of multiple models to produce a single prediction. This is a common approach in machine learning, and is often used in Kaggle competitions. We won’t go into detail here, but it’s worth looking into if you’re interested. There are also many other methods that are more specialized, such as those for text, image, and audio data. We will provide an overview of these in another chapter.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#wrapping-up",
    "href": "ml_common_models.html#wrapping-up",
    "title": "7  Common Models",
    "section": "7.12 Wrapping Up",
    "text": "7.12 Wrapping Up\nIn this chapter we’ve provided a few common and successful models you can implement with much success in machine learning. You don’t really need much beyond these for tabular data unless your unique data condition somehow requires it. But a couple things are worth mentinoing before moving on…\n\nFeature engineering will typically pay off more in performance than the model choice.\n\n\nThinking hard about the problem and the data is more important than the model choice.\n\n\nThe best model is simply the one that works best.\n\nYou’ll always get more payoff by coming up with better features to use in the model, as well as just using better data that’s been ‘fixed’ because you’ve done some good exploratory data analysis. Thinking harder about the problem means you won’t waste time going down dead ends, and you typically can find better data to use to solve the problem by thinking more clearly about the question at hand. And finally, it’s good to not be stuck on one model, and be willing to use whatever it takes to get things done efficiently.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#exercise",
    "href": "ml_common_models.html#exercise",
    "title": "7  Common Models",
    "section": "7.13 Exercise",
    "text": "7.13 Exercise\nTune a model of your choice to predict whether a movie is good or bad with the movie review data. Use the processed data which has the categorical outcome, and use one-hot encoded features if needed. Make sure you use a good baseline model for comparison!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#where-to-go-from-here",
    "href": "ml_common_models.html#where-to-go-from-here",
    "title": "7  Common Models",
    "section": "7.14 Where to go from here",
    "text": "7.14 Where to go from here\n\n7.14.1 refs\nRadfordM.Neal.Priorsforinfinitenetworks(tech.rep.no.crg-tr-94-1).UniversityofToronto, 1994a. https://arxiv.org/abs/1711.00165\nhttps://en.wikipedia.org/wiki/Activation_function\nhttps://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#footnotes",
    "href": "ml_common_models.html#footnotes",
    "title": "7  Common Models",
    "section": "",
    "text": "There would be far less hype and wasted time if those in ML and DL research simply did this rather than just reporting the chosen metric of their model ‘winning’ against other models. It’s not that hard to do, yet most do not provide any ranged estimate for their metric, let alone test statistical difference from other models. You don’t even have to bootstrap the metric estimates for binary classification! It’d also be nice if they used a more meaningful baseline than logistic regression, but that’s a different story.↩︎\nA single regression/classification tree actually could serve as a decent baseline model, especially given the interepretability.↩︎\nThis is pretty much the same concept as in stochastic gradient boosting. Larger learning rates allow for quicker exploration, but may overshoot the optimal value, however defined. Smaller learning rates are more conservative, but may take longer to find the optimal value.↩︎\nSome also prefer catboost. The authors have not actually been able to practically implement catboost in a setting where it was more predictive or as efficient/speedy as xgboost or lightgbm, but some have had notable success with it.↩︎\nRegression approaches like GAMs and gaussian process regression can be seen as approximations to neural networks. This brings us back to having a good baseline. If you know some simpler tools that can approximate more complex ones, you can often get ‘good enough’ results with the simpler models.↩︎\nIt’s not exactly clear why computer scientists chose to call this the bias, but it’s the same as the intercept in a linear model, or concpetually as an offset or constant. It has nothing to do with the word bias as used in every other modeling context.↩︎\nA really good tool for a standard MLP type approach with automatic categorical embeddings is fastai’s tabular learner.↩︎\nWe just used the prop.test function in R for these values with the test being, what proportion of predictions are correct, and are these proportions different? A lot of the metrics people look at from confusion matrices are proportions.↩︎\nMathy folk should love SVMs.↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Common Models</span>"
    ]
  },
  {
    "objectID": "ml_more.html",
    "href": "ml_more.html",
    "title": "8  More ML",
    "section": "",
    "text": "8.1 Key Ideas\nSome things to keep in mind when thinking about ML as we wrap up our discussion:",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More ML</span>"
    ]
  },
  {
    "objectID": "ml_more.html#key-ideas",
    "href": "ml_more.html#key-ideas",
    "title": "8  More ML",
    "section": "",
    "text": "There is practically no modeling or data domain where ML cannot potentially be applied.\nOther widely used techniques include unsupervised settings, reinforcement learning, computer vision, natural language processing, and more generally, artificial intelligence.\nTabular data has historically been the most common data setting for modeling by far, but this may not always be the case moving forward.\n\n\n8.1.1 Why this matters\nIt’s very important to know just how unlimited the modeling universe is, but also how there is a tie that binds. Even when we get into other data situations and complex models, we can always fall back on the core approaches we’ve already seen and know well at this point, and know that those ideas can potentially be applied in any modeling situation.\n\n\n8.1.2 Good to know\nFor the stuff in this chapter, a basic idea of modeling and machine learning would be enough.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More ML</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-unsuper",
    "href": "ml_more.html#sec-ml-more-unsuper",
    "title": "8  More ML",
    "section": "8.2 Unsupervised Learning",
    "text": "8.2 Unsupervised Learning\nAll the models considered thus far would fall under the name of supervised learning. That is, we have a target variable that we are trying to predict, and we use the data to train a model to predict the target. However, there are settings in which we do not have a target variable, or we do not have a target variable for all of the data. In these cases, we can still use what’s often referred to as unsupervised learning to learn about the data. Unsupervised learning is a type of machine learning that involves training a model without an explicit target variable in the sense that we’ve seen. But to be clear, a model is still definitely there! Unsupervised learning attempts learn patterns in the data in a general sense, and can be used in a wide range of applications, including clustering, anomaly detection, and dimensionality reduction, though it’s best to think of these as different flavors of a more general approach.\nTraditionally, one of the more common applications of unsupervised learning falls under the heading of dimension reduction, or data compression, such that we reduce features to a smaller latent, or hidden, or unobserved, subset that accounts for most of the (co-)variance of the larger set. Alternatively, we reduce the rows to a small number of hidden, or unobserved, clusters. For example, we start with 100 features and reduce them to 10 features that still account for most of what’s important in the original set, or we classify each observation as belong to 2-3 clusters. Either way, the primary goal is to reduce the dimensionality of the data, not predict an explicit target.\n\n\n\n\n\n\n\n\nFigure 8.1: Two Variables with Three Overlapping Clusters\n\n\n\n\n\nClassical methods in this domain include principal components analysis (PCA), singular value decomposition (SVD), and factor analysis, which are geared toward reducing column dimensions, as well as cluster methods such as k-means and hierarchical clustering for reducing observations into clusters. Sometimes these methods are often used as preprocessing steps for supervised learning problems, or as a part of exploratory data analysis, but often they are end in themselves. Most of us our familiar with recommender systems, whether via Netflix or Amazon, which suggest products or movies, and we’re all now becoming extremely familiar with text analysis methods via chat bots. While the underlying models are notably more complex these days, they actually just started off as SVD (recommender systems) or a form of factor analysis (text analysis via latent semantic analysis/latent dirichlet allocation). Having a conceptual understanding of the simpler methods can aid in understanding the more complex ones.\n\n\n\n\n\n\nIn general, do not use a dimension reduction technique as a preprocessing step for a supervised learning problem. Instead, use a supervised learning technique that can handle high-dimensional data, has a built-in way to reduce features (e.g. lasso, boosting), or use a dimension reduction technique that is specifically designed for supervised learning (e.g. partial least squares). Creating a reduced set of features without regard to the target will generally be suboptimal for the supervised learning problem.\n\n\n\n\n8.2.1 Connections\n\n8.2.1.1 Clusters are categorical latent features\nIt turns out that whether we are clustering rows or reducing columns we’re actually just using different methods to reduce the features. For methods like PCA and factor analysis, we’re reducing the columns to a smaller set of numeric features. For example, we might take answers to dozens of questions of a personality inventory, and reduce them to five key features that represent general aspects of personality. These new features are on their own scale, often standardized, but still reflect the variability originally seen in the original items to some extent1. However, think about a case where we just reduce the features to a single variable, and that variable was categorical. Now you have cluster analysis! You can discretize anything, e.g. from a nicely continuous feature to a coarse couple of categories, and this goes for latent variables as well as those we actually see in our data. For example, if we do a factor analysis with one latent feature, we could either convert it to a probability of some class with an appropriate transformation, or just say that scores higher than some cutoff are in cluster A and the others are in cluster B. Indeed, there is a whole class of clustering models called mixture models that do just that, i.e. estimate the latent probability of class membership. The point is that the underlying approach can be conceptually similar, and the bigger difference is how we interpret the results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.2.1.2 PCA as a neural network\nConsider the following neural network, called an autoencoder. The goal of an autoencoder is to learn a representation of the data that is smaller than the original data, but can be used to reconstruct the original data. It’s trained by minimizing the error between the original data and the reconstructed data. The autoencoder is a special case of a neural network used as a component of many larger architectures, but can be used for dimension reduction in and of itself.\n\n\n\nPCA or Autoencoder\n\n\nConsider the following setup for such a situation:\n\nSingle hidden layer\nNumber of hidden nodes = number of inputs\nLinear activation function\n\nAn autoencoder in this case would be equivalent to PCA. In this approach, PCA perfectly reconstructs the original data when considering all components, and so the error would be zero. But that doesn’t give us any dimension reduction, so we often only retain a small number of components that capture the data variance by some arbitrary amount.\nNeural networks however are not bound to linear activation functions, the size of the inputs or even a single layer, and so they provide a much more flexible approach that can compress the data at a certain layer, but still have very good reconstruction error. Typical autoencoders, would have multiple layers with notably more nodes than inputs. It’s not as easily interpretable as typical factor analytic techniques, and we still have to sort out the architecture. However, it’s a good example of how the same underlying approach can be used for different purposes.\n\n\n\n\n\n\n\nFigure 8.2: Conceptual Diagram of an Autoencoder\n\n\n\nTODO: find a way to make graphviz allow for labels/backgrounds on subgraphs with same rank\n\n\n\n\n\n\n\nAutoencoders are special cases of encoder-decoder models, which are used in many applications, including machine translation, image captioning, and more. Autoencoders have the same inputs and outputs, but in other scenarios, a similar type of architecture might be used to classify or generate text, as with large language models.\n\n\n\n\n\n8.2.1.3 Latent Linear Models\nAnother thing to be aware of is that factor analytic techniques can be thought of latent linear models. Here is a factor analysis as a latent linear model. The ‘targets’ are the observed features, and we predict each one by some linear combination of latent variables.\n\\[\n\\begin{aligned}\nx_1 &= \\beta_{11} h_1 + \\beta_{12} h_2 + \\beta_{13} h_3 + \\beta_{14} h_4 + \\epsilon_1 \\\\\nx_2 &= \\beta_{21} h_1 + \\beta_{22} h_2 + \\beta_{23} h_3 + \\beta_{24} h_4 + \\epsilon_2 \\\\\nx_3 &= \\beta_{31} h_1 + \\beta_{32} h_2 + \\beta_{33} h_3 + \\beta_{34} h_4 + \\epsilon_3 \\\\\n\\end{aligned}\n\\]\nIn this scenario, the \\(h\\) are estimated latent variables, and \\(\\beta\\) are the coefficients, which in some contexts are called loadings. The \\(\\epsilon\\) are the residuals, which are assumed to be independent and normally distributed as with a standard linear model. The \\(\\beta\\) are usually estimated by maximum likelihood, and the model is fit by iterative methods. The latent variables are not observed, but are to be estimated as part of the modeling process, or are derived in post-processing depending on the estimation approach, and typically restricted to have a mean of zero, and possibly standard deviation of 1. The number of latent variables we use is a hyperparameter, and so can be determined by the usual means2. To tie some more common models together:\n\nPCA is a factor analysis with no (residual) variance, and the latent variables are orthogonal (independent).\nProbabilisitic PCA is a factor analysis with constant residual variance.\nFactor analysis is a factor analysis with varying residual variance.\nIndependent component analysis is a factor analysis that does not assume an underlying gaussian data generating process.\nNon-negative matrix factorization and latent dirichlet allocation are factor analyses applied to counts (think poisson and multinomial regression).\n\n\n\n\n8.2.2 Other classical unsupervised learning techniques\nThere are several techniques that are used to visualize high-dimensional data in a low-dimensional spaces, hopefully to identify clusters or aid with interpretability. These include methods like multidimensional scaling, t-SNE, and (H)DBSCAN. These are often used as a part of exploratory data analysis.\nCluster analysis generally speaking has a very long history and you’ll see many different approaches, including hierarchical clustering algorithms (agglomerative, divisive), k-means, and more. Distance matrices are often the first step for these clustering approaches, and there are many ways to calculate distances between observations. Converesely, adjacency matrices, which focus on similarity of observations rather than differences, are often used for graph-based approaches, which may also used for clustering.\nAnomaly/outlier detection is an approach to find data points of interest. This is often done by looking for data points that are far from the rest of the data, or that are not well explained by the model. This is often used for fraud detection, network intrusion detection, and more. Standard clustering or modeling techniques might be used to identify outliers, or specialized techniques might be used.\n\n\n\n\n\n\nFigure 8.3: Network Graph\n\n\n\nNetwork analysis is a type of unsupervised learning that involves analyzing the relationships between entities. It is a graph-based approach that involves identifying nodes (e.g. people) and edges (e.g. do they know each other) in a network. It is used in a wide range of applications, like identifying communities within a network, or to see how they evolve over time. It is also used to identify relationships between entities, such as people, products, or documents. One might be interested in such things as which nodes that have the most connections, or the general ‘connectedness’ of a network. Network analysis or similar graphical models typically have their own clustering techniques that are based on the edge (connection) weights between individuals, such as modularity, or the number of edges between individuals, such as k-clique.\nIn short, there’s a lot out there that might fall under the umbrella of unsupervised learning, but even when you don’t think you have a target variable, you can still understand or frame these as models similar or even identically to how we have been. One should be less hung up on trying to distinguish modeling approaches with somewhat arbitrary labels, and focus more on what their modeling goal is and how best to achieve it!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More ML</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-reinforcement",
    "href": "ml_more.html#sec-ml-more-reinforcement",
    "title": "8  More ML",
    "section": "8.3 Reinforcement Learning",
    "text": "8.3 Reinforcement Learning\nPLACEHOLDER IMAGE\n\n\n\n\nReinforcement Learning\n\n\nReinforcement learning (RL) is a type of modeling approach that involves training an ‘agent’ to make decisions in an environment. The agent learns by receiving feedback in the form of rewards or punishments for its actions. The goal of the agent is to maximize its rewards over time by learning which actions lead to positive or negative outcomes.\nIn reinforcement learning, the agent interacts with the environment by taking actions and receiving feedback in the form of rewards or punishments. The agent’s goal is to learn a policy, which is a set of rules that dictate which actions to take in different situations. The agent learns by trial and error, adjusting its policy based on the feedback it receives from the environment. The classic example is a game like chess or simple video games- the agent learns which actions lead to positive outcomes (e.g. winning the game, higher scores) and which actions lead to negative outcomes (e.g. losing the game). The agent then adjusts its policy based on the feedback it receives from the environment. A key aspect of RL is the balance between exploration and exploitation, i.e. trying new things that might lead to greater rewards vs. sticking with what works.\nReinforcement learning has many applications, including robotics, game playing, and autonomous driving, but there is little restriction on where it might be applied. It is often a key part of some deep learning models, where reinforcement is supplied via human feedback or other means. In general, RL is a powerful tool that might be useful where traditional programming approaches may not be as feasible.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More ML</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-non-tabular",
    "href": "ml_more.html#sec-ml-more-non-tabular",
    "title": "8  More ML",
    "section": "8.4 Non-Tabular Data Applications",
    "text": "8.4 Non-Tabular Data Applications\nWhile our focus in this book is on tabular data due to its ubiquity, there are many other types of data that can be used for modeling, some of which can still potentially be used in that manner, but which often start as a different format or must be considered in a special way. Here we’ll briefly discuss some of the other types of data you’ll potentially come across.\n\n8.4.1 Spatial\nSpatial data such as geographic information can sometimes be quite complex. Oftentimes it is housed in its own data format (e.g. shapefiles), and there are many specialized tools for working with it. Spatial specific features may include continuous types such as latitude and longitude, or the telemetry of a person’s movements recorded from a watch. Others are more discrete such as states within a country. In general, we’d used these features as we would others in the tabular setting, but we often want to take into account the uniqueness of a particular region or the correlation of spatially regions. Historically, most spatial data can be incorporated into models like mixed models or generalized additive models, but in certain applications, such as satellite imagery, deep learning models are more the norm, and the models often transition into image processing techniques.\n\n\n8.4.2 Audio\nAudio data is a type of time series data that is also the focus for many modeling applications. It is often represented as a waveform, which is a plot of the amplitude of the sound wave over time. The goal of modeling the data may include speech recognition, music generation, and more. This sort of data, like spatial data, is typically housed in specific formats, and is often of a very large size. Also like spatial data, the specific type and research question may allow for a tabular format, and the modeling approaches are similar to those for other time series data. As in other domains where the data is of a singular type at its core, deep learning has proved very useful, and can even create songs people actually like, even recently helping the Beatles to release one more song.\n\n\n8.4.3 Image Processing\nDL CONVNETS IMAGE PLACEHOLDER\n\n\n\n\n\n\n\nFigure 8.4: Convolutional Neural Network\n\n\n\nImage processing involves a range of models and techniques for analyzing images. These include image classification, object detection, image segmentation, tracking, and more. Image classification is the task of assigning a label to an image. Object detection involves identifying the location of objects in an image. Image segmentation is the task of identifying the boundaries of objects in an image. Tracking requires following objects over time.\nIn general, your base data is an image, which is represented as a matrix of pixel values. For example, each row of the matrix could be a grayscale value for a pixel, or it could be a vector of RGB values for a pixel, such that each row is an image, the matrix is a collection of images, while the third dimension is the color channel of red, green and blue. The modeling goal then is to extract features from the image that can be used for the task at hand. For example, you might extract features such as color, texture, and shape. You can then use these features to train a model to classify images or whatever your task may be.\nImage processing is a broad field with many applications. It is used in medical imaging, satellite imagery, self-driving cars, and more. And while it can be really fun to classify objects such as cats and dogs, or generate images from text and vice versa, it can be quite challenging due to the size of the data, issues specific to video/image quality, and the model complexity. Even if your base data is often the same or very similar, the model architecture and training process can vary widely depending on the task at hand.\n\n\n8.4.4 Natural Language Processing\nSOME SORT OF CHAT RELATED IMAGE\n\n\n\n\n\n\nFigure 8.5: Demo for GPT4\n\n\n\nIt’s safe to say that the hottest area of modeling development in recent times regards natural language processing, as evidenced by the runaway success of models like ChatGPT. Natural language processing (NLP) is a field of study that focuses on understanding human language, and can be seen as a very visible subfield of artificial intelligence. NLP is used in a wide range of applications, including machine translation, speech recognition, text classification, and more. NLP is behind some of the most exciting applications today, with tools that continues to amaze with their capabilities to generate summaries of articles, answering questions, write code, and even pass the bar exam with flying colors!\nEarly efforts in this field were based on statistical models, and then variations on things like PCA, but it took a lot of data pre-processing work to get much from those approaches, and results could still be unsatisfactory. However, more recently, deep learning models have become the standard application, and there is no looking back in that regard. Current state of the art models have been trained on massive amounts of data, even the entire internet, and can be used for a wide range of tasks. But you don’t have to train such a model yourself- now you can simply use a pre-trained model like GPT-4 for many NLP tasks, and in some cases much of the trouble comes with generating the best prompt to produce the desired results. However, the field and the models are evolving extremely rapidly, and things are getting easier all the time3.\n\n\n8.4.5 Pre-trained Models & Transfer Learning\nPre-trained models are models that have been trained on a large amount of data, and can be used for a wide range of tasks. They are widely employed in image and natural language processing. The basic idea is that, if you can use a model that was trained on the entire internet of text, why start from scratch? Image processing models already understand things like edges and colors, so there is little need to reinvent the wheel when you know those features would be useful for your own task. These are viable in tasks where the inputs are similar to the data the model was trained on, as is the case with images and text.\nYou can use a pre-trained model as a starting point for your own model, and then fine-tune it for your specific task, and this is more generally called transfer learning. The gist is that you only need to train part of the model on your specific data, or possibly even not at all. You can just feed your data in and get predictions from the ready-to-go model! This obvioulsy can save a lot of time and resources, assuming you don’t have to pay much to use the model in the first place, and can be especially useful when you don’t have a lot of data to train your model on.\nhttps://bbycroft.net/llm\n\n\n8.4.6 Combining Models\nIt’s also important to note that these types of data and their associated models are not mutually exclusive. For example, you might have a video that contains both audio and visual information pertinent to the task. Or you might want to produce images from text inputs. In these cases, you can use a combination of models to extract features from the data, which may just be more features in a tabular format, or be as complex as a multimodal deep learning architecture. Many vision, audio, natural language and othe modeling approaches incorporate transformers. They are based on the idea of attention, which is a mechanism that allows the model to focus on certain parts of the input sequence and less on others. Transformers are used in many state-of-the-art models with different data types such as those that combine text and images. The transformer architecture is a bit complex, but it’s worth knowing about as it’s used in many of the most advanced models today.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More ML</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-ai",
    "href": "ml_more.html#sec-ml-more-ai",
    "title": "8  More ML",
    "section": "8.5 Artificial Intelligence",
    "text": "8.5 Artificial Intelligence\n\n\n\nAI\n\n\nThe prospect of combining models for computer vision, natural language processing, audio processing, and other domains can produce tools that mimic many aspects of what we call intelligence4. Current efforts in AI produce models that can pass law and medical exams, create better explanations of images and text than average human effort, and produce conversation on par with humans. AI even helped to create this book!\nIn many discussions of ML and AI, many put ML as a subset of AI, but this is a bit off the mark from a modeling perspective in our opinion5. For example, model-wise, any aspect of what we’d call modern AI almost exclusively employs deep learning models (although it didn’t in the past, and may supplement a DL model with non-DL models), while the ML approach to training and evaluating models can be used for any underlying model, from simple linear models to the most complex deep learning models, whether the application falls under the heading of AI or not. Furthemore, statistical model applications have never seriously attempted what we might call AI. If AI is some ‘autonomous and general tools that attempt to engage the world in a human-like way or better’, it’s not clear why it’d be compared to ML in the first place. That’s kind of like saying the brain is a subset of cognition. The brain does the work, much like ML does the modeling work with data, and gives rise to what we call cognition, but generally we would not compare the brain to cognition. The point is that to not get too hung up on the labels, and focus on the modeling goal and how best to achieve it. Deep learning models, and machine learning in general, can be used for non-AI settings, as we have seen for ourselves. And models still employ the perspective of the ML approach when ultimately used for AI - the steps taken from data to model output are largely the same.\nMany of the non-AI settings we use modeling for may well be things we can eventually rely on AI to do, but the computational limits, the amount of data that would be required for AI models do well, or the ability of AI to be able to deal with situations in which there is only small bits of data, are still hinderances in current applications of AI. However, we feel it’s likely these will eventually be overcome. But even then, a statistical approach may still have a place when the data does become small, e.g., with model comparison.\nArtificial general intelligence (AGI) is the holy grail of AI, and like AI itself is not consistently defined. In general, the idea behind AGI is the creation of some autonomous agent that can perform any task that a human can perform, many that humans cannot, and generalize abilities to new problems that have not even been seen yet. It seems we are getting closer to AGI all the time, but it’s not yet clear when it will be achieved, or even what it will look like when it is achieved, especially since no one has an agreed upon definition of what intelligence is in the first place.\nThat said, to be frank, you may very likely be reading a history book. Given recent advancements just in the last year or so, it seems unlikely that the data science being performed five years from now will resemble much of how things are done today6. We are already capable of making faster and further advancements to do AI, and it’s likely that the next generation of data scientists will be able to do so even more easily. The future is here, and it is amazing. Buckle up!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More ML</span>"
    ]
  },
  {
    "objectID": "ml_more.html#where-to-go-from-here",
    "href": "ml_more.html#where-to-go-from-here",
    "title": "8  More ML",
    "section": "8.6 Where to go from here",
    "text": "8.6 Where to go from here\nThe sky is the limit with machine learning and modeling. Go where your heart leads you, and have some fun! But if you want some more guidance, here are some ideas:\nTODO: NEEDS MORE WORK\n\nCourses on ML and DL: fastai, coursera, etc.\nKaggle competitions\nRead papers\nDo more modeling!\n\n\n8.6.0.1 refs\nRashcka https://nostarch.com/machine-learning-and-ai-beyond-basics\nVaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). “Attention is All you Need” (PDF). Advances in Neural Information Processing Systems. Curran Associates, Inc. 30.\nUnsupervised: [https://cloud.google.com/discover/what-is-unsupervised-learning]\nVisuals and deeper understanding: https://colah.github.io/\nembeddings https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More ML</span>"
    ]
  },
  {
    "objectID": "ml_more.html#footnotes",
    "href": "ml_more.html#footnotes",
    "title": "8  More ML",
    "section": "",
    "text": "Ideally we’d capture all the variability, but that’s not going to happen, and some techniques or results may only capture a relatively small percentage. In our personality example, this could be because the questions don’t adequately capture the underlying personality constructs (i.e. an issue of the reliability of instrument), or because personality is just not that simple and we’d need more dimensions.↩︎\nActually, for typical uses of ‘factor analysis’ in a measurement context (including structural equation modeling), e.g. as typically seen in social sciences, cross-validation is pretty very rarely employed, and the number of latent variables is determined by some combination of theory, model comparison for training data only, or trial and error. As a result, one can imagine how reproducible the results are.↩︎\nIt seems unlikely the prompt engineering will still be something of interest in a couple years, at least, probably not enough to warrant whole courses for it.↩︎\nIt seems most discussions of AI in the public sphere never bother to define intelligence very clearly in the first place, and the academic realm has struggled with the concept for centuries.↩︎\nAlmost every instance of this we’ve seen also never goes into actual detail or specific enough definitions to make the comparison meaningful to begin with, so don’t take it too far.↩︎\nA good reference for this sentiment is a scene from Star Trek in which Scotty has to use a contemporary computer.↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More ML</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "9  Data Issues in Modeling",
    "section": "",
    "text": "9.1 Key Ideas",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#key-ideas",
    "href": "data.html#key-ideas",
    "title": "9  Data Issues in Modeling",
    "section": "",
    "text": "Data transformations can provide many modeling benefits.\nCategorical data still needs a numeric representation, and this can be done in a variety of ways.\nThe data type for the target may suggest a particular model, but does not necessitate one.\nThe data structure, e.g. temporal or structural, likewise may suggest a particular model.\nLatent variables are everywhere!",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-transfromations",
    "href": "data.html#sec-data-transfromations",
    "title": "9  Data Issues in Modeling",
    "section": "9.2 Standard Feature & Target Transformations",
    "text": "9.2 Standard Feature & Target Transformations\nTransforming variables can provide several benefits in modeling, whether applied to the target, covariates, or both, and should regularly be used for most model situations. Some of these benefits include:\n\nInterpretable intercepts\nMore comparable covariate effects\nFaster estimation\nEasier convergence\nHelp with heteroscedasticity\n\nFor example, merely centering predictor variables, i.e. subtracting the mean, provides a more interpretable intercept that will fall within the actual range of the target variable, telling us what the value of the target variable is when the covariates are at their means (or reference value if categorical). But even if easier interpretation isn’t a major concern, variable transformations can help with convergence and speed up estimation, so can always be of benefit.\n\n9.2.1 Numeric variables\nThe following table shows the interpretation of some very common transformations applied to numeric variables- logging and standardization, (i.e. standardizing to mean zero, standard deviation one).\n\n\n\n\nTable 9.1: Common numeric transformations\n\n\n\n\n\n\n  \n    \n    \n      Target\n      Feature\n      Change in X\n      Change in Y\n      Benefits\n    \n  \n  \n    y\nx\n1 unit\nB unit\nInterpretation\n    log(y)\nx\n1 unit\n100 * (exp(B) -1) \nHeteroscedasticity in y\n    log(y)\nlog(x)\n1% change\nB% change\nInterpretation, deal with feature extremes\n    y\nscale(x)\n1 standard deviation\nB unit\nInterpretation, estimation\n    scale(y)\nscale(x)\n1 standard deviation\nB standard deviation\nInterpretation, estimation\n  \n  \n  \n\n\n\n\n\n\n\nFor example, it is very common to use standardized variables, or simply ‘scaling’ them. Some also call this normalizing but this can mean a lot of things, so one should be clear in their communication. If \\(y\\) and \\(x\\) are both standardized, a one unit (i.e. one standard deviation) change in \\(x\\) leads to a \\(\\beta\\) standard deviation change in \\(y\\). Again, if \\(\\beta\\) was .5, a standard deviation change in \\(x\\) leads to a half standard deviation change in \\(y\\). In general, there is nothing to lose by standardizing, so you should employ it often.\nAnother common transformation, particularly in machine learning, is min-max scaling, changing variables to range from some minimum to some maximum, which is almost always zero to one. This can make numeric and categorical indicators more comparable, or at least put the on the same scale for estimation purposes, and so can help with convergence and speed up estimation.\n\nPythonR\n\n\nWhen using sklearn it’s a very verbose process to do a simple transformatin, but this is beneficial when you want to do more complicated things, especially when usind data pipelines.\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport numpy as np\n\n# Create a sample dataset\nimport numpy as np\n\n# Create a random sample of integers\ndata = np.random.randint(low=0, high=100, size=(5, 3))\n\n# Apply StandardScaler\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n\n# Apply MinMaxScaler\nminmax_scaler = MinMaxScaler()\nminmax_scaled_data = minmax_scaler.fit_transform(data)\n\n\n\nR being made for statistics, it’s much easier to do simple transformations, but you can also use tools like recipes to and mlr3 pipeline operations when needed to make sure your preprocessing is applied appropriately.\n\n# Create a sample dataset\ndata = matrix(sample(1:100, 15), nrow = 5)\n\n# Standardization\nscaled_data = scale(data)\n\n# Min-Max Scaling\nminmax_scaled_data = apply(data, 2, function(x) {\n    (x - min(x)) / (max(x) - min(x))\n})\n\n\n\n\nUsing a log transformation for numeric targets and features is straightforward, and comes with several benefits. For example, it can help with heteroscedasticity, i.e. when the variance of the target is not constant across the range of the predictions1 (demonstrated below), keeping predictions positive after transformation, allows for interpretability gains, and more. One issue with logging is that it is not a linear transformation, and so can make certain more complicated transformations in post-modeling more less straightforward. Also if you have a lot of zeros, log plus one transformations are not going to be enough to help you overcome that hurdle. It also won’t help much when the variables in question have few distinct values, like ordinal variables, which we’ll discuss later in Section 9.2.3.\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5323  1.8196  2.7330  3.1763  4.0295 12.1670 \n\n\n\n\n\n\n\n\nFigure 9.1: Log transformation and heteroscedasticity\n\n\n\n\n\n\n\n\n\n\n\nIt is rarely necessary or a good idea to transform a numeric feature to a categorical one. This is because you are potentially throwing away useful information by making the feature a less reliable measure of the underlying construct. For example, discretizing age to ‘young’ and ‘old’ does not help your model, and you can always get predictions for what you would consider ‘young’ and ‘old’ after the fact. The primary reasons for doing this are not statistically sound, and can actually hinder interpretation by creating arbitrary groups.\n\n\n\n\n\n9.2.2 Categorical variables\nA raw character string is not an analyzable unit, so character strings and labeled variables like factors must be converted for analysis to be conducted on them. For categorical variables, we can employ what is called effects coding to test for specific types of group differences. Far and away the most common approach is called dummy coding or one-hot encoding2. In these situations we create columns for each category, and the value of the column is 1 if the observation is in that category, and 0 otherwise. Here is a one-hot encoded version of the season feature.\n\n\n\n\nTable 9.2: One-hot encoding\n\n\n\n\n\n\n  \n    \n    \n      seasonFall\n      seasonSpring\n      seasonSummer\n      seasonWinter\n      season\n    \n  \n  \n    1.00\n0.00\n0.00\n0.00\nFall\n    1.00\n0.00\n0.00\n0.00\nFall\n    1.00\n0.00\n0.00\n0.00\nFall\n    1.00\n0.00\n0.00\n0.00\nFall\n    0.00\n0.00\n1.00\n0.00\nSummer\n    0.00\n0.00\n1.00\n0.00\nSummer\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen doing statistical models, when doing one-hot encoding all relevant information is incorporated in k-1 groups, so one category will be dropped. For example, in a linear model, the intercept is the mean of the target for the dropped category, and the coefficients for the other categories are the difference between the mean for the dropped, a.k.a. reference, category and the mean the category being considered. As an example, in the case of the season feature, if the dropped category is winter, the intercept tells us the mean rating for winter, and the coefficients for the other categories are the difference between the value for winter and the mean of the target for the included category. For other modeling approaches, all categories are included, and the model will learn the best way to use them, and may even only consider some or one of them at a particular iteration of estimation.\n\n\n\nTODO: anova for summarizing categorical variables (and interactions)\n\n\n\n9.2.3 Ordinal Variables\nSo far in our discussion, our categorical data has been assumed to have no order. However you may find yourself with orders labels like “low”, “medium”, and “high”, or “bad” … to “good”, or simply are a few numbers, like ratings from 1 to 5. Ordinal data is categorical data that has a known ordering, but which still has arbitrary labels. Let us repeat that, ordinal data is categorical data.\n\n9.2.3.1 Ordinal Features\nThe simplest way to treat ordinal features is as if they were numeric. If you do this, then you’re just pretending that it’s not categorical, and this is usually fine. Most of the transformations we mentioned probably aren’t going to be as useful, but you can still use them if you want. For example, logging five values of ratings 1-5 isn’t going to do anything for you, but it technically doesn’t hurt anything. But you should know that typical statistics like means and standard deviations don’t really make sense for ordinal data, so the main reason for treating them as numeric is for modeling convenience.\nIf you choose to treat it as categorical, you can ignore the ordering and do the same as you would with categorical data. There are some specific approaches to coding ordinal data for use in linear models, but they are not common, and they generally aren’t going to help the model or interpreting it, so we do not recommend them. You could however use old-school contrast encodings that you would in traditional ANOVA approaches, but again, you’d need a good reason to do so.\nTake home message: treat ordinal features as you would numeric or non-ordered categorical. Either is fine.\n\n\n9.2.3.2 Ordinal Targets\nOrdinal targets are a bit more complicated. If you treat them as numeric, you’re assuming that the difference between 1 and 2 is the same as the difference between 2 and 3, and so on. This is probably not true. If you treat them as categorical, you’re assuming that there is no connection between categories, e.g. that in order to get to category three you have to have gone through category 2. So what should you do?\nThere are a number of approaches to modeling ordinal targets, but the most common is the proportional odds model. This model can be seen as a generalization of the logistic regression model, and is very similar to it, and actually identical if you only had two categories. But others are also possible, and your results could return something that gives coefficients for the model for the 1-2 category change, the 2-3 category change, and so on.\nOrdinality of a categorical outcome is largely ignored in machine learning approaches. The outcome is either treated as numeric or multi-category classification. This is not necessarily a bad thing, especially if prediction is the primary goal.\n\n\n\n\n\n\nSome are a little too eager to jump to simultaneously modeling multiple target variables, e.g. in structural equation modeling or mutivariate regression. It’s not wrong to do so, but given the difficulty our brains have with interpreting results for a single target, you might think twice about doing so. However, for scenarios focused much more on prediction performance that involve method like deep learning, it makes more sense, and is actually required.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#sec-missing-data",
    "href": "data.html#sec-missing-data",
    "title": "9  Data Issues in Modeling",
    "section": "9.3 Missing Data",
    "text": "9.3 Missing Data\nMissing data is a common challenge in data science, and there are a number of ways to deal with it, usually by substituting, or imputing some value for the missing one. The most common approaches are:\n\nComplete case analysis: Only use observations that have no missing data.\nSingle value imputation: Replace missing values with the mean, median, mode or some other value of the feature.\nModel-based imputation: Use a model based on complete cases to predict the missing values, and use those predictions are the imputed values.\nMultiple imputation: Create multiple imputed datasets based on the predictive distribution of the model used in model-based imputation. Estimates of coefficients and variances are averaged in some fashion over the imputations.\nBayesian imputation: Treat the missing values as parameters to be estimated.\n\nThe first approach to drop missing data is the simplest, but can lead to a lot of lost data, and can lead to biased statistical results if the data is not missing completely at random. There are special cases of some models that by their nature can ignore the missingness under an assumption of missing at random, but even those models would likely benefit from some sort of imputation. If you don’t have much missing data, this would be fine. How much is too much? Unfortunately that depends on the context, but if you have more than 10% missing, you should probably be looking at alternatives.\nThe second approach where we just plug in a single value like a mean is also simple, but will probably rarely help your model. Consider a numeric feature that is 50% missing, and for which you replace the missing with the mean. How good do you think that feature will be when at least half the values are identical? Whatever variance it normally would have and share with the target is probably reduced, and possibly dramatically. Furthermore, you’ve also attenuated correlations it has with the other features, which could potentially further hamper interpretation or cause other issues depending on the type of model you’re implementing. Single value imputation makes perfect sense if you know that the missingness means a specific value, like a count feature where missing means a count of zero. If you don’t have much missing data, it’s unlikely this would have any real benefit over complete case analysis, except if it allows you to use all the other features that would otherwise be dropped. But then, why not just drop this feature and keep the others?\nModel-based imputation is more complicated, but can be very effective. In essence, you run a model for complete cases in which the target is now the feature with missing values, and the covariates are all the other features and target. You then use that model to predict the missing values, and use those predictions as the imputed values. After these predictions are made, you move on to the next feature and do the same. There are no restrictions on which model you use for which feature. If the other features in the imputation model also have missing data, you can use something like mean imputation to get more complete data if necessary as a first step, and then when their turn comes, impute those values.\nAlthough the implication is that you would have one model per feature and then be done, you can do this iteratively for several rounds, such that the initial imputed values are then used in subsequent models to reimpute the missing values. You can do this as many times as you want, but the returns will diminish.\nMultiple imputation (MI) is the most complicated, but can be the most effective under some situations, depending on what you’re willing to sacrifice for having better uncertainty estimates vs. a deeper dive into the model. The idea is that you create multiple imputed datasets, each of which is based on the predictive distribution of the model used in model-based imputation. Say we use a linear regression assuming a normal distribution to impute feature A. We would then draw from the predictive distribution of that model to create a dataset with imputed values for feature A, then do it a gain, say a total of 10 times.\nYou now have 10 imputed data sets. You then run your actual model of interest on each of these datasets, and your final model results are a kind of average of the parameters of interest (or exactly an average, say for regression coefficients). This main thing this approach provides is that it acknowledges that your single imputation methods have uncertainty in those model predictions being used as imputed values, and that uncertainty is incorporated into the final model results.\nMI can in theory handle the any source of missingness and as such is a very powerful approach. But it has several drawbacks. One is that you need statistical or generative model and distribution for all models used, and that distribution is something you have to assume is appropriate. Your final model presumably is also a probabilistic model with coefficients and variances you are trying to estimate and understand. MI isn’t really going to help an XGBoost or deep learning model for example, or at least offer little if anything over single value imputation. If you have very large data and a complicated model, you could be waiting a long time, and as modeling is an iterative process itself, this can be rather tedious to work through. Finally, few data or post-model processing tools that you commonly use will work with MI results, especially visualization ones, and so you will have to hope that whatever package you use for MI has what you need. As an example, you’d have to figure out how you’re going to impute interaction terms if you have them, practically nothing will work with cross-validation approaches,\nPractically speaking, MI takes a lot of effort to often come to the same conclusions you would have with a single imputation approach, or possibly fewer conclusions for anything beyond GLM coefficients and their standard errors. But if you want your uncertainty estimate for those models to be better, MI can be an option.\nOne final option is to run a Bayesian model where the missing values are treated as parameters to be estimated. MI basically is a poor man’s Bayesian imputation approach. A package like brms can do this, and it can be very effective, but it is also very computationally intensive, and can be very slow. At least it would be more fun than standard MI!",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#sec-class-imbalance",
    "href": "data.html#sec-class-imbalance",
    "title": "9  Data Issues in Modeling",
    "section": "9.4 Class Imbalance",
    "text": "9.4 Class Imbalance\n\n\n\n\n\n\n\n\nFigure 9.2: Class Imbalance\n\n\n\n\n\nClass imbalance refers to the situation where the target variable has a large difference in the number of observations in each class. For example, if you have a binary target, and 90% of the observations are in one class, and 10% in the other, you would have class imbalance. You’ll almost never see a 50/50 split in the real world, but the issue is that as we move further away from that point, we can start to see problems in model estimation, prediction, and interpretation. As a starting point, if we just predict the majority class in a binary classification problem, we’ll be right 90% of the time in terms of accuracy. So right off the bat one of our favorite metrics isn’t going to help us assess model performance as much as we’d like.\nIn classification problems, class imbalance is the rule, not the exception. This is because nature just doesn’t sort itself into nice and even bins. For example, the majority of people of a random sample do not have cancer, the vast majority of people have not had a heart attack in the past year, most people do not default on their loans, and so on.\nThere are a number of ways to help deal with class imbalance, and the best approach depends on the context. Some of the most common approaches are:\n\nUse different metrics: Use metrics that are less affected by class imbalance, such as area under a receiver operating characteristic curve (AUC), or those that balance the\nOversampling/Undersampling: Randomly sample from the minority (majority) class to increase (decrease) the number of observations in that class.\nWeighted objectives: Weight the loss function to give more weight to the minority class. Although commonly employed, and a simple thing to use with models like lightgbm and xgboost, it often fails to help, and can cause other issues.\nThresholding: Change the threshold for classification to be more sensitive to the minority class. Nothing says you have to use 0.5 as the threshold for classification, and you can change it to be more sensitive to the minority class. This is a very simple approach, and may be all you need.\n\nThese are not necessarily mutually exclusive. For example, it’s probably a good idea to switch to a metric besides accuracy as you employ other techniques.\n\n9.4.1 Calibration issues in classification\nProbability calibration is often an issue in classification problems, and is a bit more complicated than just class imbalance but is often discussed in the same setting. Having calibrated probabilities refers to the situation where the predicted probabilities of the target match up well to the actual probabilities. For example, if you predict that 10% of people will default on their loans, and 10% of people actually do default on their loans, one would say your model is well calibrated. Conversely, if you predict that 10% of people will default on their loans, but 20% of people actually do default on their loans, your model is not so well-calibrated.\nOne of the most common approaches to assessing calibration is to use a calibration curve, which is a plot of the predicted probabilities vs. the observed proportions. In the following, one model seems to align well with the observed proportions based on the chosen bins. The other model is not so well calibrated, and is overshooting with its predictions.\n\n\n\n\n\nCalibration Plot\n\n\n\n\nWhile the issue is an important one, it’s good to keep the issue of calibration and imbalance separate. As miscalibration implies bias, bias can happen irrespective of the class proportions and can be due to a variety of factors related to the model, target, or features, and miscalibration is not inherent to a particular model.\nFurthermore, the assessment of calibration with this approach has a few issues. For one, the observed ‘probabilities’ are proportions based on arbitrarily chosen bins and observed values that are measured with some measurement error as well as having natural variance with will partly reflect sample size3. These plots are often presented such that observed proportions are labeled as the “true” probabilities. However, you do not have the true probabilities outside of simulation settings, just the observed class labels, so whether your model’s predicted probabilities match observed proportions is a bit of a different question. The predictions obviously have uncertainty as well, and this will depend on modeling approach, sample size, etc. And finally, the number of bins chosen can also affect the appearance of the plot in a notable way.\nAll this is to say that each point in a calibration plot has some error bar around it, and the differences between models and the ‘best case scenario’ would need additional steps to suss out. Some methods are available to calibrate probabilities, but they are not commonly implemented in practice, and often involve a model-based technique, with all of its own assumptions and limitations. It’s also not exactly clear that forcing your probabilities to be on the line is helping solve the actual modeling goal in any way4. But if you are interested, you can read more here.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-censoring",
    "href": "data.html#sec-data-censoring",
    "title": "9  Data Issues in Modeling",
    "section": "9.5 Censoring and Truncation",
    "text": "9.5 Censoring and Truncation\n\n\n\n\n\n\n\n\nFigure 9.3: Potential Censoring\n\n\n\n\n\nSometimes, we just don’t see all the data there is to see. Censoring is a situation where the target variable is not fully observed. This is common in survival analysis5, where the target is the time to an event, but the event has not yet occurred for some observations. This is called right censoring, and is the most common type of censoring, and depicted in the above plot, where several individuals are only observed to a certain age and were still alive at that time. There is also left censoring, where the censoring happens from the other direction data before a certain point is unknown. Finally, there is interval censoring, where the event of interest occurs within some interval, but the exact value is unknown.\nSurvival analysis is a common modeling technique in this situation, but you may also be able to keep things even more simple via something like tobit regression. In this approach, you assume that the target is fully observed, but that the values are censored, and you model the probability of censoring. This is a very common approach in econometrics, and can keep you in a traditional linear model context.\n\n\n\n\n\n\n\n\nFigure 9.4: Truncation\n\n\n\n\n\nTruncation is a situation where the target variable is only observed if it is above or below some value. One of the issues is that default distributional methods, e.g., via maximum likelihood, assume a distribution that is not necessarily bounded. In our plot above, we restrict our data to 70 and below, but typical modeling methods with default distributions would not respect that.\nYou could truncate predictions after the fact, but this is a bit of a hack, and often results in lumpiness in the predictions at the boundary that isn’t realistic in most situations. Alternatively, Bayesian approaches allow you to model the target as a distribution with truncated distributions, and so you can model the probability of the target being above or below some value. This is a very flexible approach. There are also models such as hurdle models that might prove useful where the truncation is theoretically motivated, e.g. a zero-inflated Poisson model for count data where the zero counts are due to a separate process than the non-zero counts.\n\n\n\n\n\n\nOne way to distinguish censored and truncated data is that censored data is usually due to some external process such that the target is not observed but could be possible (capping reported income at $1 million), whereas truncated data is due to some internal process that prevents the target from being observed, and is often derived from sample selection (we only want to model non-millionaires). We would not want observations past the censored point to be unlikely, but we would want observations past the truncated point to be impossible. Trickier still is that for bounded or truncated distributions that might be applied to the truncated scenario, such as folded vs. truncated distributions, they would not result in the same probability distributions even if they can be applied to the same situation.\n[]\nImage from StackExchange",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-time",
    "href": "data.html#sec-data-time",
    "title": "9  Data Issues in Modeling",
    "section": "9.6 Time Series",
    "text": "9.6 Time Series\nTODO: ADD CHAPTER LINKS HERE\nTime series data is any data that incorporates values over time. This could be something like the a state’s population over years, or the max temperature of an area over days, etc. Time series data is very common, and there are a number of approaches to modeling it, and we usually take special approaches to account for the fact that observations are not independent of one another. The most common approach is to use a time series regression approach, where the target is some value that varies time, and the covariates are other features that can be time-varying or not. There are old school approaches like ARIMA that can still serve as decent [baseline models]LINK THE TO ML CHAPTER, and newer approaches like more sophisticated Bayesian ones that can get quite complex.\nLongitudinal data6 is a special case of time series data, where the target is a function of time, but it is typically grouped in some fashion. An example is would be school performance for students over several semesters, where values are clustered within students over time. In this case, you can use a time series regression approach, but you can also use a mixed model (LINK TO THE EXTENSIONS CHAPTER) approach, where you model the target as a function of time, but also include a random effect for the grouping variable, in this case, students. This is a very common approach in many areas, and can be very effective. It can also be used for time series data that is not longitudinal, where the random effects are based on autoregressive covariance matrices.\n\n\n\n\n\n\nThe primary distinguishing feature for referring data as ‘time-series’ and ‘longitudinal’ is the number of time points, where the latter typically has relatively few. This arbitrary though.\n\n\n\n\n9.6.1 Time-based Features\nWhen it comes to time-series features, we can apply time-based transformations. One PCA like approach is the fourier transform, which can be used to decompose a time series into its component frequencies. This can be useful for identifying periodicity in the data, and can be used as a feature in a model. In marketing contexts, some perform adstocking on features, which is a way of modeling the lagged of the effect of features over time, such that they may have their most important impact immediately, but still can impact the present target from past values. This avoids directly putting lags for each time point as additional features in the model, though that is an option. In that case you have a feature at present time t, the same feature representing the previous time point t-1, the feature at t-2, etc.\nAnother thing to note about feature transformations, if you have year as a feature, you can use it as a numeric feature or as a categorical feature. In the former case, if you are only considering a linear effect, you should make the zero meaningful, typically by starting the year values at zero. This makes the intercept in linear models reference the first year rather than year 0, which can actually make it harder to estimate along with the other coefficients in a model. The same goes if you are using months or days as a numeric feature, since there is no ‘zero’ month. It doesn’t really matter which year/month/day is zero, just that zero refers to one of the actual time points observed.\nDates and/or times can be it bit trickier. Often you can just split dates out into year, month, day, etc., and proceed as discussed. In other cases you’d want to track the day to assess potential seasonal effects, where it makes sense to use a cyclic approach (e.g. cyclic spline or sine/cosine transformation) to get at yearly or within-day seasonal effects.\n\n\n\n\n\n\nWeeks are not universal. Some start on Sunday, others Monday. Some data contexts only consider weekdays. Some systems may have 52 or 53 weeks in a year, and dates may not be in the same week from one year to the next, etc. So use caution when considering weeks as a feature.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-spatial",
    "href": "data.html#sec-data-spatial",
    "title": "9  Data Issues in Modeling",
    "section": "9.7 Spatial Data",
    "text": "9.7 Spatial Data\nTODO: ADD CHAPTER LINKS HERE\nWe also visit spatial data in a discussion on non-tabular data, but here we want to talk about it from a modeling perspective, especially within the tabular domain. Say you have a target that is a function of location, such as the proportion of people voting a certain way in a county, or the number of crimes in a city. You can use a spatial regression approach, where the target is a function of location among other features that may or may not be spatially oriented. Two approaches already discussed may be applied in the case of having continous spatial features, such as latitude and longitude, or discrete features like county. One model approach to the continuous case include a GAM, where we use a smooth interaction of latitude and longitude. For the discrete setting, we can use a mixed model approach, where we include a random effect for county.\nThere are other traditional approaches to spatial regression, especially in the continuous spatial domain, such as using a spatial lag approach, where we incorporate information about the neighborhood of a observation’s location into the model. Such models fall under names such as CAR (conditional autoregressive), SAR (spatial autoregressive), BYM, kriging, and so on. These models can be very effective, but are in general different forms of random effects models, and can be seen as special cases of gaussian processes. We feel it’s probably unnecessary to get into details of the traditional spatial models unless you know other standard techniques like GAMs or mixed models won’t work, or more general approach like gaussian process regression isn’t more feasible.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-latent",
    "href": "data.html#sec-data-latent",
    "title": "9  Data Issues in Modeling",
    "section": "9.8 Latent Variables",
    "text": "9.8 Latent Variables\nTODO: ADD CHAPTER LINKS HERE\nLatent variables are a fundamental aspect of modeling, and simply put, are variables that are not directly observed, but are inferred from other variables. Here are some examples of what might be called latent variables:\n\nThe linear combination of features in a linear regression model is a latent variable, but usually we only think of it as such before the link transformation in GLMs.\nThe error term in any model is a latent variable representing all the unknown/unobserved/unmodeled factors that influence the target.\nThe principal components in PCA.\nThe factor scores in a factor analysis model or structural equation.\nThe true target underlying the censored values.\nThe clusters in a cluster in cluster analysis.\nThe random effects in a mixed model.\nThe hidden layers in a deep learning model.\n\nIt’s easy to see from such a list that latent variables are very common in modeling, so it’s good to get comfortable with the concept. Whether they’re appropriate to your specific situation will depend on a variety of factors, but they can be very useful in many settings, if not a required part of the modeling approach.\nTODO: Measurement error?",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#commentary",
    "href": "data.html#commentary",
    "title": "9  Data Issues in Modeling",
    "section": "9.9 Commentary",
    "text": "9.9 Commentary\nThere’s a lot going on with data before you ever get to modeling, and which will affect every aspect of your modeling approach. This chapter outlines common data types, issues, and associated modeling aspects, but in the end, you’ll always have to make decisions based on your specific situation, and they will often not be easy. These are only some of the things to consider, so be ready for surprises, and, be ready to learn from them!",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#refs",
    "href": "data.html#refs",
    "title": "9  Data Issues in Modeling",
    "section": "9.10 refs",
    "text": "9.10 refs\nTransformations\n\nmin-max vs. standardization https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n\nclass imbalance - https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28 - Monroe & Clark - https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data - https://machinelearningmastery.com/what-is-imbalanced-classification/\ncalibration\nhttps://machinelearningmastery.com/probability-calibration-for-imbalanced-classification/ https://stats.stackexchange.com/questions/452483/why-some-algorithms-produce-calibrated-probabilities\nNiculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning (pp. 625-632).",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "data.html#footnotes",
    "href": "data.html#footnotes",
    "title": "9  Data Issues in Modeling",
    "section": "",
    "text": "For the bazillionth time, logging does not make data ‘normal’ so that you can meet your normality assumption in linear regression. The only time that would work is if your data is log-normally distributed to begin with.↩︎\nNote that one-hot encoding can refer to just the 1/0 coding for all categories, or to the specific case of dummy coding where one category is dropped. Make sure the context is clear.↩︎\nRecall that each bin will only be portion of the test set size.↩︎\nOftentimes we are only interested in the ordering of the predictions, and not the actual probabilities. For example, if we are trying to identify the top 10% of people most likely to default on their loans, we’ll just take the top 10% of predictions, and the actual probabilities are irrelevant for that goal.↩︎\nSurvival analysis is also called event history analysis, and is a very common approach in biostatistics, sociology, demography, and other disciplines where the target is the time to an event, such as death, marriage, divorce, etc.↩︎\nWe prefer not to call this panel data, though you may also hear of the term in these situations. In our view it’s not as informative of a description and exclusively used in econometrics-oriented disciplines.↩︎",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Issues in Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html",
    "href": "causal.html",
    "title": "10  Causal Modeling",
    "section": "",
    "text": "10.1 Key ideas",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#key-ideas",
    "href": "causal.html#key-ideas",
    "title": "10  Causal Modeling",
    "section": "",
    "text": "No model can tell you whether a relationship is causal or not. Causality is inferred, not proven, based on the available evidence.\nThe exact same models would be used for similar data settings to answer a causal question, or a predictive question. The difference is in the interpretation of the results.\nExperimental design, such as randomized control trials, are the gold standard for causal inference. But in this case, the gold standard is often not practical, and not without its shortcomings even when it is, and never perfectly implemented. More like a silver standard?\nCausal inference is often done with observational data, which is often the only option, and that’s okay.\nSeveral models exist which are typically employed to answer a more causal-oriented question. These include structural equation models, graphical models, uplift modeling, and more.\nInteractions are the norm, if not the reality. Causal inference generally regards a single effect. If the normal setting is that such an effect would always vary depending on other features, you should question why you want to aggregate your results to a single ‘effect’, since that effect would be potentially misleading.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#why-it-matters",
    "href": "causal.html#why-it-matters",
    "title": "10  Causal Modeling",
    "section": "10.2 Why it matters",
    "text": "10.2 Why it matters\nOften we need a precise statement about the feature-target relationship, not just whether there is some relationship. For example, we might want to know whether a drug works well, or whether showing an advertisement results in a certain amount of new sales. Whether or not random assignment was used, we generally need to know whether the effect is real, and the size of the effect, and often, the uncertainty in that estimate. Causal modeling is, like machine learning, more of an approach than a specific model, and that approach may involve the design or implementing models we’ve already seen in a different way to answer the key question. Without more precision in our understanding, we could miss the effect, or overstate it, and make bad decisions as a result.\n\n10.2.1 Good to know\nHonestly this section is pretty high level, and we are not going to go into much detail here so even just some understanding of correlation and modeling would likely be enough.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#classic-experimental-design",
    "href": "causal.html#classic-experimental-design",
    "title": "10  Causal Modeling",
    "section": "10.3 Classic Experimental Design",
    "text": "10.3 Classic Experimental Design\nMany of those who have taken a statistics course have been exposed to the simple t-test to determine whether two groups are different. While it can be applied to any binary group setting, for our purposes here we can assume the two groups result from some sort of treatment that is applied to one group, and not the other. The ‘treatment’ could regard a new drug, demographic groups, a marketing campaign, a new app’s feature, or anything else.\nThis is a very simple example of an experimental design, and it is a very powerful one. Ideally, we would randomly assign our observational units to the two groups, one which gets the treatment and one which doesn’t.Then we’d measure the difference between the two groups, using some metric to conclude that the two groups are different. This is the basic idea behind the t-test, which would compare the target means of the two groups.\nThe t-test tells us whether the difference in means between the two groups is statistically significant. It definitely does not tell us whether the treatment itself caused the difference, whether the effect is large, nor whether the effect is real, or even if the treatment is a good idea to do in the first place. It just tells us whether the two groups are statistically different.\nTurns out, a t-test is just a linear regression. It’s a special case of linear regression where there is only one independent variable, and it is a categorical variable with two levels. The coefficient from the linear regression would tell you the mean difference, i.e. as you go from one group to the other, how much does the target mean change? The t-test is just a special case of this, but under the same conditions, the t-statistic from the linear regression and t-test, and corresponding p-value would be identical.\nAnalysis of variance, or **ANOVA, allows the t-test to be extended to more than two groups, and multiple features, and is also commonly employed to analyze the results of experimental design settings. But ANOVA is still just a linear regression. Even when we get into more complicated design settings such as repeated measures and mixed design, it’s still just a linear regression, we’d just be using mixed models. TODO: LINK TO MIXED MODELS\nIf using a linear regression didn’t suggest any notion of causality to you before, it certainly shouldn’t now either. The model is identical whether there was an experimental design with random assignment or not. The only difference is that the data was collected in a different way, and the theoretical assumptions and motivations are different. Experimental design can give us more confidence in the causal explanation of model results, whatever model is used, and this is why we like to use random assignment when we can. It gives us control for the unobserved factors that might otherwise be influencing the results. If we can be fairly certain the observations are essentially the same except for the treatment, then we can be more confident that the treatment is the cause of the difference, and we can be more confident in the causal interpretation of the results. But it doesn’t change the model itself, and the results of a model do not in any way prove a causal relationship.\n\n\n\n\n\n\nA/B testing is just marketing-speak for a two-group setting where one could employ the same mindset they would if they were doing a t-test. It implies randomized assignment, but you’d have to know the context to know if that is actually the case.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#natural-experiments",
    "href": "causal.html#natural-experiments",
    "title": "10  Causal Modeling",
    "section": "10.4 Natural Experiments",
    "text": "10.4 Natural Experiments\nAs we noted, random assignment or a formal experiment is not always possible or practical to implement. But sometimes we get to do it anyway, or at least close enough! Sometimes, the world gives us a natural experiment, where the assignment to the groups is essentially random, or where there is clear break before and after some event occurs, such that we examine the change as we would in pre-post design.\nFor example, a certain recent pandemic allowed us to examine vaccination effects, policy effects, remote work, and more. This was not a tightly controlled experiment, but it’s something we can treat very similar to an experiment, and we can compare the differences in various outcomes before and after the pandemic to see what changes took place.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#causal-inference",
    "href": "causal.html#causal-inference",
    "title": "10  Causal Modeling",
    "section": "10.5 Causal Inference",
    "text": "10.5 Causal Inference\nReasoning about causality is a very old topic, philosophically dating back millenia, and more formally hundreds of years. Random assignment is a relatively new idea, say 150 years old, but was even posited before Wright, Fisher, and Neyman Pearson and the 20th century rise of statistics. But with stats and random assignment we had a way to start using models to help us reason about causal relationships. Pearl and others came along to provide a perspective from computer science, and things have been progressing along. We were actually using programming approaches to do causal inference before back in the 1970s even! Economists eventually got into the game too (e.g., Heckman), though largely reinventing the wheel.\nNow we can use recently developed modeling approaches to help us reason about causal relationships, which can be both a blessing and a curse. Our models can be more complex, and we can use more data, which can potentially give us more confidence in our conclusions. But we can still be easily fooled by our models, as well as by ourselves. So we’ll need to be careful in how we go about things, but let’s see what some of our options are!",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#models-for-causal-inference",
    "href": "causal.html#models-for-causal-inference",
    "title": "10  Causal Modeling",
    "section": "10.6 Models for Causal Inference",
    "text": "10.6 Models for Causal Inference\n\n10.6.1 Linear Regression\nYep, linear regression. Your old #1 is quite possibly the mostly widely used model for causal inference, historically speaking. We’ve even already seen linear regression as a graphical model Figure 1.2, and in that sense can serve as the starting point for structural equation models and related, or be used as a baseline for other approaches. Linear regression also tells us for any particular effect, what that effect is, accounting for all the other features constant, which kind of already gets into a causal mindset.\nHowever, your standard linear model doesn’t care where the data came from and will tell you about group differences whether they come from a randomized experiment or not. And if you don’t include features that would have a say in the treatment, you’ll potentially get a biased estimate of the effect. As such, linear regression by itself cannot save us from the difficulties of causal inference. But it can be used in conjunction with other approaches, and it can be used to help us reason about causal relationships. For example, we can use it to help us understand the effect of a treatment, or to help us understand the effect of a feature on the target, accounting for other features.\n\n\n10.6.2 Structural Equation Models\nTODO: LINK LATENT VARIABLES\nStructural Equation Models are basically multivariate models, as in multiple targets, used for regression and classification. They are widely employed in the social sciences, and are often used to model both observed and latent variables, with either serving as features or targets. They are also used to model causal relationships, to the point that historically they were called causal graphical models or causal structural models, and are a special case of graphical models more generally speaking. They have one of the longest histories of formal statistical modeling dating back over a century1. Economists later reinvented the approach under various guises, and computer scientists joined the party after that.\nUnfortunately for those looking for causal effects, the basic input for SEM is a correlation matrix, and the basic output is a correlation matrix. Insert your favorite modeling quote here - you know which one. Also, a linear regression and even deep-learning models like autoencoders can be depicted as graphical models, as we have seen. The point is that SEM, like linear regression, can no more tell you whether a relationship is causal than the linear regression, or for that matter, the t-test, could.\n\n\n10.6.3 Counterfactual Thinking\nWhen we think about causality, we really out to think about counterfactuals. What would have happened if I had done something different? What would have happened if I had not done something? What would have happened if I had done something sooner rather than later? What would have happened if I had done nothing at all? These questions are all examples of counterfactual thinking. And this is one of the best ideas to take aways from this…\n\nthe question is not whether there is a difference between A and B but whether there would still be a difference if A was B and B was A.\n\nThis is the essence of counterfactual thinking. It’s not about whether there is a difference between two groups, but whether there would still be a difference if those in one group had actually been treated differently. In this sense, we are concerned with the potential outcomes of the treatment, however defined.\nHere is a more concrete example:\n\nRoy is shown ad A, and buys the product.\nPris is shown ad B, and does not buy the product.\n\nWhat are we to make of this? Which ad is better? A seems to be, but maybe Pris wouldn’t have bought the product if shown that ad either, and maybe Roy would have bought the product if shown ad B too! With counterfactual thinking, we are concerned with the potential outcomes of the treatment, which in this case is whether or not to show the ad.\nLet’s say ad A is the new one, i.e., our treatment group, and B is the status quo ad, our control group. Our real question can’t be answered by a simple test of whether means or predictions are different among the two groups, as this estimate would be biased if the groups are already different in the first place. The real effect is whether, for those who saw ad A, what the difference in the target would be if they hadn’t seen it.\nFrom a prediction stand point, we can get an estimate straightforwardly. For those in the treatment, we can just plug in their feature values with treatment set to ad A. Then we just make a prediction with treatment set to ad B.\n\nPythonR\n\n\n\nmodel.predict(X.assign(treatment = 'A')) - \n    model.predict(X.assign(treatment = 'B'))\n\n\n\n\npredict(model, X |&gt; mutate(treatment = 'A')) - \n    predict(model, X |&gt; mutate(treatment = 'B'))\n\n\n\n\nWith counterfactual thinking explicitly in mind, we can see that the difference in predictions is the difference in the potential outcomes of the treatment.\n\n\n10.6.4 Uplift Modeling\nThe counterfactual prediction we just did can be called the uplift or gain from the treatment. Uplift modeling is a general term applied to models where counterfactual thinking is at the forefront, especially in a marketing context. Uplift modeling is not a model, but any model that is used to answer a question about the potential outcomes of a treatment. The key question is what is the gain, or uplift, in applying a treatment vs. not? Typically any statistical model can be used to answer this question, and often the model is a classification model, whether Roy both the product or not.\nSome in uplift modeling distinguish:\n\nSure things: those who would buy the product whether or not shown the ad.\nLost causes: those who would not buy the product whether or not shown the ad.\nSleeping dogs: those who would buy the product if not shown the ad, but not if shown the ad. ‘Do not disturb’!\nPersuadables: those who would buy the product if shown the ad, but not if not shown the ad.\n\nOne of additional goals in uplift modeling is to identify persuadables for additional marketing efforts, and to avoid wasting money on the lost causes. But to get there, we have to think causally first!\n\n\n\n\n\n\nThere appear to be more widely used tools for uplift modeling and meta-learners in Python than in R, but there are some options in R as well. In Python you can check out causalml and sci-kit uplift for some nice tutorials and documentation.\n\n\n\n\n\n10.6.5 Meta-Learning\nMeta-learners are used in uplift modeling and other contexts to assess potentially causal relationships between some treatment and outcome.\n\nS-learner - single model for both groups; predict the difference as when all observations are treated vs when all are not, similar to our code demo above.\nT-learner - two models, one for each treatment group; predict the difference as when all observations are treated vs when all are not for both models, and take the difference\nX-learner - a more complicated modification to the T-learner also using a multi-step approach.\nMisc-learner - other meta-learners that are not as popular, but might be applicable for your problem.\nTransformed outcome: transform your uplift modeling into a regression problem in which the prediction is the difference in the potential outcomes. This simplifies the problem to a single model, and can be quite effective.\n\n\n\n\n\n\n\nMeta-learners are not to be confused with meta-analysis, which is also related to understanding causal effects. Meta-analysis attempts to combine the results of multiple studies to get a better estimate of the true effect. The studies are typically conducted by different researchers and in different settings. Meta-learning has also been used to refer to what is more commonly called ensemble learning.\n\n\n\n\n\n10.6.6 Others approaches\nNote that there are many model approaches that would fall under the umbrella of causal inference, and several that are discipline specific but really only a special application of some of the ones we’ve mentioned here. A few you might come across:\n\nMarginal structural models\nInstrumental variables and two-stage least squares\nPropensity score matching/weighting\nMeta-analysis\nBayesian networks",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#commentary",
    "href": "causal.html#commentary",
    "title": "10  Causal Modeling",
    "section": "10.7 Commentary",
    "text": "10.7 Commentary\nYou will often hear people speak very strongly about causality in the context of modeling, and those who assume that employing an experimental design solves every problem. But anyone who has actually conducted experiments knows that the implementation is never perfect, often not even close, especially when humans are involved as participants or as experimenters. Experimental design is hard, and if done well, can be very potent, but by itself does not prove anything regarding causality. You will also hear people say that you cannot infer causality from observational data, but it’s done all the time, and it’s often the only option.\nIn the end, the main thing is that when we want to make causal statements, we’ll make do with what data setting we have, and be careful that we rule out some of the other obvious explanations and issues. The better we can control the setting, or the better we can do things from a model standpoint, the more confident we can be in making causal claims. Causal modeling is an exercise in reasoning, which makes it such an interesting endeavor.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#where-to-go-from-here",
    "href": "causal.html#where-to-go-from-here",
    "title": "10  Causal Modeling",
    "section": "10.8 Where to go from here",
    "text": "10.8 Where to go from here\nWe have only scratched the surface here, and there is a lot more to learn. Here are some resources to get you started:\n\nBarrett, McGowan, and Gerke (2023)\nCunningham (2023)\nFacure Alves (2022)\n\n\n\n\n\nBarrett, Malcolm, Lucy D’Agostino McGowan, and Travis Gerke. 2023. Causal Inference in R. https://www.r-causal.org/.\n\n\nCunningham, Scott. 2023. Causal Inference The Mixtape. https://mixtape.scunning.com/.\n\n\nFacure Alves, Matheus. 2022. “Causal Inference for The Brave and True — Causal Inference for the Brave and True.” https://matheusfacure.github.io/python-causality-handbook/landing-page.html.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#footnotes",
    "href": "causal.html#footnotes",
    "title": "10  Causal Modeling",
    "section": "",
    "text": "Wright is credited with coming up with what would be called path analysis in the 1920s, which is a precursor to and part of SEM.↩︎",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "misc_models.html",
    "href": "misc_models.html",
    "title": "11  Misc Models",
    "section": "",
    "text": "11.1 Other ML\nClasical models like simple decision tree, knn-regression, svm, naive bayes, etc.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Misc Models</span>"
    ]
  },
  {
    "objectID": "misc_models.html#other-dl",
    "href": "misc_models.html#other-dl",
    "title": "11  Misc Models",
    "section": "11.2 Other DL",
    "text": "11.2 Other DL\ncould mention specific/historical models here, like resnet, bert, GANs, LSTM, etc.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Misc Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix A — References",
    "section": "",
    "text": "Barrett, Malcolm, Lucy D’Agostino McGowan, and Travis Gerke. 2023.\nCausal Inference in R. https://www.r-causal.org/.\n\n\nBrownlee, Jason. 2021. “Gradient Descent\nWith AdaGrad From\nScratch.” MachineLearningMastery.com. https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/.\n\n\nClark, Michael J. 2022. Generalized Additive\nModels. https://m-clark.github.io/generalized-additive-models/.\n\n\n———. 2023. Mixed Models with R. https://m-clark.github.io/mixed-models-with-R/.\n\n\nCohen, Jacob. 2009. Statistical Power Analysis for the Behavioral\nSciences. 2. ed., reprint. New York, NY: Psychology Press.\n\n\nCunningham, Scott. 2023. Causal Inference\nThe Mixtape. https://mixtape.scunning.com/.\n\n\nDataBricks. 2019. “What Is AdaGrad?”\nDatabricks. https://www.databricks.com/glossary/adagrad.\n\n\nFacure Alves, Matheus. 2022. “Causal Inference for\nThe Brave and True —\nCausal Inference for the Brave\nand True.” https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nFahrmeir, Ludwig, Thomas Kneib, Stefan Lang, and Brian Marx. 2013.\nRegression: Models, Methods and\nApplications. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-34333-9.\n\n\nFaraway, Julian. 2014. “Linear Models with\nR.” Routledge & CRC Press. https://www.routledge.com/Linear-Models-with-R/Faraway/p/book/9781439887332.\n\n\nGelman, Andrew. 2013. “What Are the Key Assumptions of Linear\nRegression?  Statistical\nModeling, Causal Inference, and\nSocial Science.” https://statmodeling.stat.columbia.edu/2013/08/04/19470/.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data\nAnalysis, Third Edition. CRC\nPress.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. 1st ed. Cambridge\nUniversity Press. https://doi.org/10.1017/9781139161879.\n\n\nGreene, William. 2017. Econometric Analysis - 8th\nEdition. https://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm.\n\n\nGrolemund, Hadley Wickham and Garrett. 2023. Welcome \nR for Data Science. https://r4ds.hadley.nz/.\n\n\nHarrell, Frank E. 2015. Regression Modeling\nStrategies: With Applications to\nLinear Models, Logistic and\nOrdinal Regression, and Survival\nAnalysis. Springer Series in\nStatistics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017.\nElements of Statistical Learning: Data\nMining, Inference, and Prediction. 2nd Edition. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical\nLearning. Vol. 103. Springer Texts in\nStatistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nKoenker, Roger. 2005. Quantile Regression. Vol. 38. Cambridge\nuniversity press. https://books.google.com/books?hl=en&lr=&id=WjOdAgAAQBAJ&oi=fnd&pg=PT12&dq=info:E32s5Y3j4NMJ:scholar.google.com&ots=CQFHSt5qY_&sig=E8zXmNabc0nGmYJlSYjQqhzyVT8.\n\n\nKuhn, Max, and Julia Silge. 2023. Tidy Modeling with\nR. https://www.tmwr.org/.\n\n\nMcElreath, Richard. 2020. “Statistical Rethinking:\nA Bayesian Course with\nExamples in R and STAN.”\nRoutledge & CRC Press. https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919.\n\n\nMolnar, Christoph. 2023. Interpretable Machine\nLearning. https://christophm.github.io/interpretable-ml-book/.\n\n\nMurphy, Kevin P. 2012. “Machine Learning:\nA Probabilistic\nPerspective.” MIT Press. https://mitpress.mit.edu/9780262018029/machine-learning/.\n\n\nRovine, Michael J, and Douglas R Anderson. 2004. “Peirce and\nBowditch.” The American Statistician 58\n(3): 232–36. https://doi.org/10.1198/000313004X964.\n\n\nStatQuest with Josh Starmer. 2019a. “Gradient\nDescent, Step-by-Step.” https://www.youtube.com/watch?v=sDv4f4s2SB8.\n\n\n———. 2019b. “Stochastic Gradient\nDescent, Clearly\nExplained!!!” https://www.youtube.com/watch?v=vMh0zPT0tLI.\n\n\nWikipedia. 2023. “Relationships Among Probability\nDistributions.” Wikipedia. https://en.wikipedia.org/w/index.php?title=Relationships_among_probability_distributions&oldid=1180084573.\n\n\nWood, Simon N. 2017. Generalized Additive\nModels: An Introduction with\nR, Second Edition. 2nd ed.\nBoca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781315370279.\n\n\nWooldridge, Jeffrey M. 2012. Introductory Econometrics:\nA Modern Approach. 5th\nedition. Mason, OH: Cengage Learning.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "dataset_descriptions.html",
    "href": "dataset_descriptions.html",
    "title": "Appendix B — Dataset Descriptions",
    "section": "",
    "text": "B.1 Heart Failure\nDataset from Davide Chicco, Giuseppe Jurman: â€œMachine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020)\nhttps://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide. Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\nage: Age\nanaemia: Decrease of red blood cells or hemoglobin (boolean)\ncreatinine_phosphokinase: Level of the CPK enzyme in the blood (mcg/L)\ndiabetes: If the patient has diabetes (boolean)\nejection_fraction: Percentage of blood leaving the heart at each contraction (percentage)\nhigh_blood_pressure: If the patient has hypertension (boolean)\nplatelets: Platelets in the blood (kiloplatelets/mL)\nserum_creatinine: Level of serum creatinine in the blood (mg/dL)\nserum_sodium: Level of serum sodium in the blood (mEq/L)\nsex: Woman or man (binary)\nsmoking: If the patient smokes or not (boolean)\ntime: Follow-up period (days)\nDEATH_EVENT: If the patient deceased during the follow-up period (boolean)\nFor booleans: Sex - Gender of patient Male = 1, Female =0 (renamed for our data) Age - Age of patient Diabetes - 0 = No, 1 = Yes Anaemia - 0 = No, 1 = Yes High_blood_pressure - 0 = No, 1 = Yes Smoking - 0 = No, 1 = Yes DEATH_EVENT - 0 = No, 1 = Yes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Dataset Descriptions</span>"
    ]
  },
  {
    "objectID": "dataset_descriptions.html#sec-dd-heart-disease-uci",
    "href": "dataset_descriptions.html#sec-dd-heart-disease-uci",
    "title": "Appendix B — Dataset Descriptions",
    "section": "B.2 Heart Disease UCI",
    "text": "B.2 Heart Disease UCI\nDataset from Kaggle: https://www.kaggle.com/ronitf/heart-disease-uci\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date.\nAttribute Information:\n    name     role         type demographic                                        description  units missing_values\n0 age Feature Integer Age None years no 1 sex Feature Categorical Sex None None no 2 cp Feature Categorical None None None no 3 trestbps Feature Integer None resting blood pressure (on admission to the ho… mm Hg no 4 chol Feature Integer None serum cholestoral mg/dl no 5 fbs Feature Categorical None fasting blood sugar &gt; 120 mg/dl None no 6 restecg Feature Categorical None None None no 7 thalach Feature Integer None maximum heart rate achieved None no 8 exang Feature Categorical None exercise induced angina None no 9 oldpeak Feature Integer None ST depression induced by exercise relative to … None no 10 slope Feature Categorical None None None no 11 ca Feature Integer None number of major vessels (0-3) colored by flour… None yes 12 thal Feature Categorical None None None yes 13 num Target Integer None diagnosis of heart disease None no\nFeatures and target were renamed for our data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Dataset Descriptions</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html",
    "href": "matrix_operations.html",
    "title": "Appendix C — Matrix Operations",
    "section": "",
    "text": "C.1 Addition\nMatrix addition, along with subtraction, is the easiest concept when dealing with matrices. While it is easy to grasp, you will not find it featured as prominently as matrix multiplication.\nThere is one rule for matrix addition: the matrices need to have the same dimensions.\nLet’s check out these two matrices:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n\\\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n\\]\nYou probably noticed that we gave each scalar within the matrix a label associated with its row and column position. We can use these to see how we will produce the new matrix:\nNow, we can set this up as an addition problem to produce Matrix C:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\nA_{11} + B_{11}& A_{12} + B_{12} & A_{13} + B_{13}\\\\\nA_{21} + B_{21}& A_{22} + B_{22} & A_{23} + B_{23}\n\\end{bmatrix}\n}\n\\]\nNow we can pull in the real numbers:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n1 + 7  & 2 + 8 & 3 + 9\\\\\n4 + 9 & 5 + 8 & 6 + 7\n\\end{bmatrix}\n}\n\\]\nGiving us Matrix C:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n8 & 10 & 12 \\\\\n13 & 13 & 13\n\\end{bmatrix}\n}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#subtraction",
    "href": "matrix_operations.html#subtraction",
    "title": "Appendix C — Matrix Operations",
    "section": "C.2 Subtraction",
    "text": "C.2 Subtraction\nTake everything that you just saw with addition and replace it with subtraction!\nJust like addition, every matrix needs to have the same dimensions if you are going to use subtraction.\nLet’s see those two matrices again and cast it as subtraction problem:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\nA_{11} - B_{11}& A_{12} - B_{12} & A_{13} - B_{13}\\\\\nA_{21} - B_{21}& A_{22} - B_{22} & A_{23} - B_{23}\n\\end{bmatrix}\n}\n\\]\nAnd now we can substitute in the real numbers:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n1 - 7 & 2 - 8 & 3 - 9\\\\\n4 - 9 & 5 - 8 & 6 - 7\n\\end{bmatrix}\n}\n\\]\nAnd end with this matrix:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n-6 & -6 & -6 \\\\\n-5 & -3 & -1\n\\end{bmatrix}\n}\n\\]\nAdding and subtracting matrices in R and Python is pretty simple.\nIn R, we can create a matrix a few ways: with the matrix function or by row binding numeric vectors.\n\nmatrix_A = rbind(1:3, \n                  4:6)\n\n# The following is an equivalent\n# to rbind:\n# matrix_A = matrix(c(1:3, 4:6), \n#                    nrow = 2, \n#                    ncol = 3, byrow = TRUE)\n\nmatrix_B = rbind(7:9, \n                  9:7)\n\nOnce we have those matrices created, we can use the standard + and - signs to add and subtract:\n\nmatrix_A + matrix_B\n\n     [,1] [,2] [,3]\n[1,]    8   10   12\n[2,]   13   13   13\n\nmatrix_A - matrix_B\n\n     [,1] [,2] [,3]\n[1,]   -6   -6   -6\n[2,]   -5   -3   -1\n\n\nThe task is just as easy in Python. We will import numpy and then use the matrix method to create the matrices:\n\nimport numpy as np\n\nmatrix_A = np.matrix('1 2 3; 4 5 6')\n\nmatrix_B = np.matrix('7 8 9; 9 8 7')\n\nJust like R, we can use + and - on those matrices.\n\nmatrix_A + matrix_B\n\nmatrix([[ 8, 10, 12],\n        [13, 13, 13]])\n\nmatrix_A - matrix_B\n\nmatrix([[-6, -6, -6],\n        [-5, -3, -1]])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#transpose",
    "href": "matrix_operations.html#transpose",
    "title": "Appendix C — Matrix Operations",
    "section": "C.3 Transpose",
    "text": "C.3 Transpose\nAs you progress through this book, you might see a matrix denoted as \\(A^T\\); here the superscripted T stands for transpose. If we transpose a matrix, all we are doing is flipping the rows and columns along the matrix’s main diagonal. A visual example is much easier:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-&gt;\n\\stackrel{\\mbox{Matrix A}^T}{\n\\begin{bmatrix}\n1 & 4 \\\\\n2 & 5 \\\\\n3 & 6\n\\end{bmatrix}\n}\n\\]\nLike any matrix operation, a transpose is pretty easy to do when the matrix is small; you’re best bet is to rely on software to do anything beyond a few rows or columns.\nIn R, all we need is the t function:\n\nt(matrix_A)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nIn Python, we can use numpy’s transpose method:\n\nmatrix_A.transpose()\n\nmatrix([[1, 4],\n        [2, 5],\n        [3, 6]])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#multiplication",
    "href": "matrix_operations.html#multiplication",
    "title": "Appendix C — Matrix Operations",
    "section": "C.4 Multiplication",
    "text": "C.4 Multiplication\nNow, you probably have some confidence in doing matrix operations. Just as quickly as we built that confidence, it will be crushed when learning about matrix multiplication.\nWhen dealing with matrix multiplication, we have a huge change to our rule. No longer can our dimensions be the same! Instead, the matrices need to be conformable – the first matrix needs to have the same number of columns as the number of rows within the second matrix. In other words, the inner dimensions must match.\nLook one more time at these matrices:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n\\]\nMatrix A has dimensions of \\(2x3\\), as does Matrix B. Putting those dimensions side by side – \\(2x3 * 2x3\\) – we see that our inner dimensions are 3 and 2 and do not match.\nWhat if we transpose Matrix B?\n\\[\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n\\]\nNow we have something that works!\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n. & . \\\\\n. & . \\\\\n\\end{bmatrix}\n}\n\\]\nNow we have a \\(2x3 * 3x2\\) matrix multiplication problem! The resulting matrix will have the same dimensions as our two matrices’ outer dimensions: \\(2x2\\)\nHere is how we will get at \\(2x2\\) matrix:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n=\n\\] \\[\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n(A_{11}*B_{11})+(A_{12}*B_{21})+(A_{13}*B_{31}) & (A_{11}*B_{12})+(A_{12}*B_{22})+(A_{13}*B_{32}) \\\\\n(A_{21}*B_{11})+(A_{22}*B_{21})+(A_{23}*B_{31}) & (A_{21}*B_{12})+(A_{22}*B_{22})+(A_{23}*B_{32})\n\\end{bmatrix}\n}\n\\]\nThat might look like a horrible mess and likely isn’t easy to commit to memory. Instead, we’d like to show you a way that might make it easier to remember how to multiply matrices. It also gives a nice representation of why your matrices need to be conformable.\nWe can leave Matrix A exactly where it is, flip Matrix B\\(^T\\), and stack it right on top of Matrix A:\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n7_{b} & 8_{b} & 9_{b} \\\\\n\\\\\n1_{a} & 2_{a} & 3_{a} \\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n\\]\nNow, we can let those rearranged columns from Matrix B\\(^T\\) “fall down” through the rows of Matrix A:\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n\\\\\n1_{a}*7_{b} & 2_{a}*8_{b} & 3_{a}*9_{b}\\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & .\\\\\n. & .\n\\end{bmatrix}\n}\n\\]\nAdding those products together gives us 50 for \\(C_{11}\\).\nLet’s move that row down to the next row in the Matrix A, multiply, and sum the products.\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n\\\\\n1_{a} & 2_{a} & 3_{a}\\\\\n4_{a}*7_{b} & 5_{a}*8_{b} & 6_{a}*9_{b}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & .\\\\\n122 & .\n\\end{bmatrix}\n}\n\\]\nWe have 122 for \\(C_{21}\\). That first column from Matrix B\\(^T\\) won’t be used any more, but now we need to move the second column through Matrix A.\n\\[\n\\begin{bmatrix}\n1_{a}*9_{b} & 2_{a}*8_{b} & 3_{a}*7_{b}\\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & 46\\\\\n122 & .\n\\end{bmatrix}\n}\n\\]\nThat gives us 46 for \\(C_{12}\\).\nAnd finally:\n\\[\n\\begin{bmatrix}\n1_{a} & 2_{a} & 3_{a}\\\\\n4_{a}*9_{b} & 5_{a}*8_{b} & 6_{a}*7_{b}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & 46\\\\\n122 & 118\n\\end{bmatrix}\n}\n\\]\nWe have 118 for \\(C_{22}\\).\nNow that you know how these work, you can see how easy it is to handle these tasks in R and Python.\nIn R, we need to use a fancy operator: %*%. This is just R’s matrix multiplication operator. We will also use the transpose function: t.\n\nmatrix_A %*% t(matrix_B)\n\n     [,1] [,2]\n[1,]   50   46\n[2,]  122  118\n\n\nIn Python, we can just use the regular multiplication operator and the transpose method:\n\nmatrix_A * matrix_B.transpose()\n\nmatrix([[ 50,  46],\n        [122, 118]])\n\n\nYou can see that whether we do this by hand, R, or Python, we come up with the same answer! While these small matrices can definitely be done by hand, we will always trust the computer to handle larger matrices.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#inversion",
    "href": "matrix_operations.html#inversion",
    "title": "Appendix C — Matrix Operations",
    "section": "C.5 Inversion",
    "text": "C.5 Inversion\nYou might want to think of matrix inversion as the reciprocal of the matrix, usually noted as \\(A^{-1}\\). The biggest reason that we might invert a matrix is because there is no matrix division.\nInversion can only be performed on square matrices (e.g., \\(2x2\\), \\(3x3\\), \\(4x4\\)) and the determinant of a matrix cannot be 0. Since the determinant is important for finding the inverse, we should probably have an idea about how to find the determinant.\n\nC.5.1 Matrix Determinant\nWhile we’ve been using the matrix row/column positions in our examples, we are going to shift to letters to label the positions. We can start with a \\(2x2\\) matrix:\n\\[\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\nA & B\\\\\nC & D\n\\end{bmatrix}\n}\n\\]\nTo find the determinant, we would take \\(\\mid C \\mid = (A*D) - (B*C)\\).\nReturning back to Matrix C, we have \\(\\mid C \\mid = (50_a*118_d) - (46_b*122_c) = 288\\)\n\\[\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & 46\\\\\n122 & 118\n\\end{bmatrix}\n}\n\\]\nA \\(3x3\\) matrix doesn’t pose much more of a challenge.\n\\[\n\\stackrel{\\mbox{Matrix D}}{\n\\begin{bmatrix}\nA & B & C\\\\\nD & E & F\\\\\nG & H & I\n\\end{bmatrix}\n}\n\\]\nThe canonical form might not be as intuitive, but it is worth seeing:\n\\[\n\\mid D \\mid = A\\begin{vmatrix}\nE & I\\\\\nF & H\n\\end{vmatrix}  -\nB\\begin{vmatrix}\nD & I\\\\\nF & G\n\\end{vmatrix} +\nC\\begin{vmatrix}\nD & H\\\\\nE & G\n\\end{vmatrix}\n\\]\nBreaking it down a bit further will help to see where all of the values go:\n\\[\n\\mid D \\mid = A(E*I - F*H) - B(D*I - F*G) + C(D*H - E*G)\n\\] Now we can work that out with a real matrix:\n\\[\n\\stackrel{\\mbox{Matrix D}}{\n\\begin{bmatrix}\n2 & 1 & 3\\\\\n6 & 5 & 4\\\\\n7 & 8 & 9\n\\end{bmatrix}\n}\n\\]\nTo get our determinant:\n\\[\n\\mid D \\mid = 2(5*9 - 4*8) - 1(6*9 - 4*7) + 3(6*8 - 5*7) = 39\n\\]\nAnd just to confirm that our math is correct, we can check for the determinant in R and Python.\nR has a handy function called det:\n\nmatrix_D = matrix(c(2, 1, 3,\n                     6, 5, 4,\n                     7, 8, 9), \n                   nrow = 3, \n                   ncol = 3, \n                   byrow = TRUE)\n\ndet(matrix_D)\n\n[1] 39\n\n\nWe can keep using numpy, but we will have to use det within the linalg module.\n\nmatrix_D = np.matrix('2 1 3; 6 5 4; 7 8 9')\n\nnp.linalg.det(matrix_D)\n\n38.99999999999999\n\n\nJust to show you how this pattern would continue\nYou can find a lot of examples online on how to do \\(2x2\\) and \\(3x3\\) matrix inversions, mostly because they are the easiest to do.\nHow do you know that you properly inverted your matrix? You multiply the original matrix by the inverse matrix and you will get an identity matrix.\nWe have a nice figure in Figure @ref(fig:hello), and also a table in Table @ref(tab:iris).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "appendix_placeholder.html",
    "href": "appendix_placeholder.html",
    "title": "Appendix D — Other to come",
    "section": "",
    "text": "D.1 Simulation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Other to come</span>"
    ]
  },
  {
    "objectID": "appendix_placeholder.html#bayesian-demonstration",
    "href": "appendix_placeholder.html#bayesian-demonstration",
    "title": "Appendix D — Other to come",
    "section": "D.2 Bayesian Demonstration",
    "text": "D.2 Bayesian Demonstration\nMetropolis-Hastings demo\n\n# Define the log-likelihood function for linear regression\nlog_likelihood &lt;- function(beta, X, y, sigma_sq) {\n    y_hat &lt;- X %*% beta\n    residuals &lt;- y - y_hat\n    log_likelihood &lt;- -0.5 * length(y) * log(2 * pi * sigma_sq) - 0.5 * sum(residuals^2) / sigma_sq\n    return(log_likelihood)\n}\n\n# Define the prior distribution for beta\nprior_beta &lt;- function(beta) {\n    prior_mean &lt;- rep(0, length(beta))\n    prior_sd &lt;- rep(10, length(beta))\n    log_prior &lt;- sum(dnorm(beta, mean = prior_mean, sd = prior_sd, log = TRUE))\n    return(log_prior)\n}\n\n# Define the prior distribution for sigma\nprior_sigma &lt;- function(sigma_sq) {\n    alpha &lt;- 2\n    beta &lt;- 2\n    # log_prior &lt;- dgamma(1/sigma_sq, shape = alpha, rate = beta, log = TRUE)\n    log_prior &lt;- extraDistr::dinvgamma(sigma_sq, alpha = alpha, beta = beta, log = TRUE) \n\n    return(log_prior)\n}\n\n# Define the proposal distribution for beta\nproposal_beta &lt;- function(beta, scale) {\n    beta_proposal &lt;- rnorm(length(beta), mean = beta, sd = scale)\n    return(beta_proposal)\n}\n\n# Define the proposal distribution for sigma\nproposal_sigma &lt;- function(sigma_sq, scale) {\n    # sigma_proposal &lt;- rgamma(1, shape = sigma_sq / scale, rate = scale)\n    sigma_proposal &lt;- extraDistr::rinvgamma(1, alpha = sigma_sq / scale, beta = scale)\n    return(sigma_proposal)\n}\n\n# Set up the data\n# set.seed(123)\n# n &lt;- 100\n# X &lt;- cbind(1, rnorm(n), rnorm(n), rnorm(n))\n# beta_true &lt;- c(1, 2, 3, 4)/4\n# sigma_true &lt;- 1\n# y &lt;- X %*% beta_true + rnorm(n, sd = sigma_true)\n\n# Set up the Metropolis-Hastings algorithm\n# n_iter &lt;- 10000\n\n\n# Run the Metropolis-Hastings algorithm\nmh = function(\n    X,\n    y,\n    beta = rep(0, ncol(X)), \n    sigma_sq = .5, \n    scale_beta = 0.1, \n    scale_sigma = 1,\n    chains = 2,\n    warmup = 1000,\n    n_iter = 2000,\n    seed = 123\n) {\n    set.seed(seed)\n\n    result &lt;- list()\n    beta_start &lt;- beta\n    sigma_sq_start &lt;- sigma_sq\n\n    for (c in 1:chains){\n        acceptance_beta &lt;- 0\n        acceptance_sigma &lt;- 0\n        beta_samples &lt;- matrix(0, n_iter, ncol(X))\n        sigma_sq_samples &lt;- rep(0, n_iter)\n\n        if (c &gt; 1) {\n            beta &lt;- beta_start\n            sigma_sq &lt;- sigma_sq_start\n        }       \n\n        for (i in 1:n_iter) {\n            # Update beta\n            beta_proposal &lt;- proposal_beta(beta, scale_beta)\n            log_ratio_beta &lt;- log_likelihood(beta_proposal, X, y, sigma_sq) + prior_beta(beta_proposal) -\n                log_likelihood(beta, X, y, sigma_sq) - prior_beta(beta)\n            if (log(runif(1)) &lt; log_ratio_beta) {\n                beta &lt;- beta_proposal\n                acceptance_beta &lt;- acceptance_beta + 1\n            }\n            beta_samples[i, ] &lt;- beta\n\n            # Update sigma_sq\n            sigma_sq_proposal &lt;- proposal_sigma(sigma_sq, scale_sigma)\n            log_ratio_sigma &lt;- log_likelihood(beta, X, y, sigma_sq_proposal) + prior_sigma(sigma_sq_proposal) -\n                log_likelihood(beta, X, y, sigma_sq) - prior_sigma(sigma_sq)\n            if (log(runif(1)) &lt; log_ratio_sigma) {\n                sigma_sq &lt;- sigma_sq_proposal\n                acceptance_sigma &lt;- acceptance_sigma + 1\n            }\n            sigma_sq_samples[i] &lt;- sigma_sq\n        }\n    \n        message(\"Acceptance rate for beta:\", acceptance_beta / n_iter, \"\\n\")\n        message(\"Acceptance rate for sigma:\", acceptance_sigma / n_iter, \"\\n\")\n\n        result[[c]] = list(\n            beta = beta_samples[-(1:warmup), ], \n            sigma_sq = sigma_sq_samples[-(1:warmup)],\n            # y_rep = X %*% t(beta_samples[-(1:warmup), ])\n            # +rnorm(n_iter - warmup, sd = sqrt(sigma_sq_samples[-(1:warmup)]))\n            y_rep = t(X %*% t(beta_samples[-(1:warmup), ]) + rnorm(n_iter - warmup, sd = sqrt(sigma_sq_samples[-(1:warmup)])))\n        )\n    } \n    result\n}\n\nX_train = df_happiness |&gt;\n    select(life_exp, gdp_pc, corrupt) |&gt;\n    as.matrix()\n\nour_result = mh(\n    X = cbind(1, X_train),\n    y = df_happiness$happiness, \n    beta = c(mean(df_happiness$happiness), rep(0, ncol(X_train))),\n    sigma_sq = var(df_happiness$happiness),\n    scale_sigma = .5,\n    warmup = 1000,\n    n_iter = 2000\n)\n\nstr(our_result)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Other to come</span>"
    ]
  },
  {
    "objectID": "appendix_placeholder.html#linear-programming",
    "href": "appendix_placeholder.html#linear-programming",
    "title": "Appendix D — Other to come",
    "section": "D.3 Linear Programming",
    "text": "D.3 Linear Programming",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Other to come</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models Demystified",
    "section": "",
    "text": "Preface\nTODO: Quarto bug prints this twice in ToC; tried various header tricks to no avail. Also, not unnumbered by default for krantz.cls.\nHi there, this is our book! It’s about models great and small, and we hope you’ll find some useful things here. Our main goal is to provide a resource for people who are learning about models, and we hope that it will be useful for people who are just starting out, as well as for people who are already familiar with models but want to learn from a different perspective. Whether you’re a machine learning master that would like a little bit more statistical nuance, or an academic trying to dive into the world of machine learning, we hope you’ll find it helpful.\nYou’ll definitely want tok have some familiarity with R or Python, and some basic knowledge of statistics will be helpful. We’ll try to explain things as we go, but we won’t be able to cover everything. If you’re looking for a good introduction to R, we recommend R for Data Science or the Python Data Science Handbook for Python. Beyond that, we’ll try to provide the context you need. We’re not here to make you an expert, just to help you get acquainted with the world of models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Models Demystified",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA lot of people helped when writing the book…",
    "crumbs": [
      "Preface"
    ]
  }
]