[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models Demystified",
    "section": "",
    "text": "Preface\nHello and welcome! This book is your companion to exploring the realm of modeling in data science. It is designed to provide you something useful whether you’re a beginner looking to learn some fundamentals, or an experienced practitioner seeking a fresh perspective. Our goal is to equip you with a better understanding of how models work and how to use them, including both basic and more advanced techniques, where we touch on everything from linear regression to deep learning. We’ll also show how different models relate to one another to better empower you to successfully apply them in your own data-driven projects. We aim to provide an overview on how to use both machine learning and traditional statistical modeling in a practical fashion, with a balanced emphasis on interpretability and predictive power. Join us on this exciting journey as we explore the world of models in data science!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-preface-what",
    "href": "index.html#sec-preface-what",
    "title": "Models Demystified",
    "section": "What Will You Get Out of This Book?",
    "text": "What Will You Get Out of This Book?\nWe’re hoping for a couple things for you as you read through this book. In particular, if you’re starting your journey into data science, we hope you’ll leave with:\n\nA firm understanding of modeling basics from a practical perspective\nA toolset of models and related ideas that you can instantly apply for competent modeling\n\nIf you’re already familiar with modeling, we hope you’ll leave with:\n\nAdditional context for the models you already know\nSome introduction to models you don’t know\nAdditional understanding of how to choose the right model for the job and what to focus on\n\nFor anyone reading this book, we especially hope you get a sense of the commonalities between different models and a good sense of how they work.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-preface-prereqs",
    "href": "index.html#sec-preface-prereqs",
    "title": "Models Demystified",
    "section": "Brief Prerequisites",
    "text": "Brief Prerequisites\nYou’ll definitely want to have some familiarity with R or Python (both are used for examples), and some very basic knowledge of statistics will be helpful. We’ll try to explain things as we go, but we won’t be able to cover everything. If you’re looking for a good introduction to R, we recommend R for Data Science or the Python for Data Analysis book for Python. Beyond that, we’ll try to provide the context you need so that you can be comfortable trying things out.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-preface-authors",
    "href": "index.html#sec-preface-authors",
    "title": "Models Demystified",
    "section": "About the Authors",
    "text": "About the Authors\nMichael is a senior machine learning scientist for Strong Analytics. Prior to industry he honed his chops in academia, earning a PhD in Experimental Psychology before turning to data science full-time as a consultant. His models have been used in production across a variety of industries, and can be seen in dozens of publications across several disciplines. He has a passion for helping others learn difficult stuff, and has taught a variety of data science courses and workshops for people of all skill levels in many different contexts.\nHe also maintains a blog, and has several posts and long-form documents on a variety of data science topics there. He lives in Ann Arbor Michigan with his wife and his dog, where they all enjoy long walks around the neighborhood.\n\n\nSeth is the Academic Co-Director of the Master of Science in Business Analytics (MSBA) and Associate Teaching Professor at the University of Notre Dame for the IT, Analytics, and Operations Department. He likewise has a PhD in Applied Experimental Psychology and has been teaching and consulting in data science for over a decade. He is an excellent instructor, and teaches several data science courses at the undergraduate and graduate level.\nSeth lives in South Bend, Indiana with his wife and three kids, and spends his free time lifting more weights than he should, playing guitar, and chopping wood.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is This Book?\nThis book aims to demystify the complex world of data science modeling. It serves as a practical resource, and is something you can refer to for a quick overview of a specific modeling technique, a reminder of something you’ve seen before, or perhaps a sneak peak into some new modeling details.\nThe text is focused on a few statistical and machine learning concepts that are ubiquitous, and modeling approaches that are widely employed, and especially those which form the basis for most other models in use in a variety of domains. Believe it or not, whether a lowly t-test or a complex neural network, there is a tie that binds, and you don’t have to know every detail to get a solid model that works well enough. We hope to help you understand some of the core modeling principles, and how the simpler models can be extended and applied to a wide variety of data scenarios. We also touch on some topics related to the modeling process, such as common data issues and causal inference.\nOur approach is first and foremost a practical one - models are just tools to help us reach a goal, and if a model doesn’t work in the world, it’s not very useful. But modeling is often a delicate balance of interpretation and prediction, and each data situation is unique in some way, almost always requiring a bespoke approach. What works well in one setting may be poor in another, and what may be the state of the art may only be marginally better than a simpler approach that is more easily interpreted. In addition, complexities arise even in an otherwise deceptively simple application. However, if you have the core understanding of the techniques that lie at the heart of many models, you’ll automatically have many more tools at your disposal to tackle the problems you face, and be more comfortable with choosing the best for your needs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-intro-what",
    "href": "introduction.html#sec-intro-what",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 What we hope you take away\nHere are a few things we hope you’ll take away from this book:\n\nA small set of modeling tools that will be applicable to the vast majority of tabular data problems you’ll encounter\nEnough understanding to be able to confidently apply these tools to your own data\nA sense of the common thread that runs through the modeling landscape, from simple linear models to complex neural networks\n\nWhile we recommend working through the chapters in order if you’re starting out, we hope that this book can serve as a “choose your own adventure” reference. Whether you want a surface-level understanding, a deeper dive, or just want to be able to understand what the analysts in your organization are talking about, we think you will find value in this book.\n\n\n1.1.2 What you can expect\nFor each chapter that we cover, you generally see the same type of content structure. We start with an overview and provide some key ideas to keep in mind as we go through the chapter. We then demonstrate the model with data, code, results, and visualizations. In further demystifying the modeling process, at various points we take time to show how a model comes about by estimating them by hand. We’ll also provide some concluding thoughts, connections to other techniques and topics, and suggestions on what to explore next. We’ll also provide some exercises to try on your own.\nSome topics may be a bit more in the weeds than you want, and that’s okay! We hope that you can take away the big ideas and come back to the details when you’re ready. Just having an awareness of what’s possible is often the first step to understanding how to apply it to your own data. In general though, we’ll touch a little bit on a lot of things, but hopefully not in an overwhelming way.\n\n\n1.1.3 What you can’t expect\nThis book will not teach you programming, but you really only need a basic understanding of R or Python. We also won’t be teaching you basic statistics, so won’t be delving into hypothesis testing or the intricacies of statistical theory. The text is more focused on applied modeling, prediction and performance than a normal stats book, and more focused on interpretation and uncertainty in the modeling process than a typical machine learning book. It’s not an academic treatment of the topics, so when it comes to references you’ll be more likely to find a nice blog post or youtube video that clearly demonstrates a concept, rather than a dense academic paper. That said you should have a great idea of where to go and what to search to go further for deeper content.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-intro-who",
    "href": "introduction.html#sec-intro-who",
    "title": "1  Introduction",
    "section": "1.2 Who Should Use This Book?",
    "text": "1.2 Who Should Use This Book?\n\nThis book is intended for every type of data dabbler, no matter what part of the data world you call home. If you consider yourself a data scientist, a business analyst, or a statistical hobbyist, you already know that the best part of a good dive into data is the modeling. But whatever your data persuasion, models give us the possibility to answer questions, make predictions, and understand what we’re interested in a little bit better. And no matter who you are, it isn’t always easy to understand how the models work. Even when you do get a good grasp of a modeling approach, things can still get complicated, and there are a lot of details to keep track of. In other cases, maybe you just have other things going on in your life and have forgotten a few things. In that case, we find that it’s always good to remind yourself of the basics! So if you’re just interested in data and hoping to understand it a little better, then it’s likely you’ll find something useful.\nYour humble authors have struggled mightily themselves throughout the course of their data science history, and still do! We were initially taught by those that weren’t exactly experts, and often found it difficult to get a good grasp of statistical modeling and machine learning. We’ve had to learn how to use the tools, how to interpret the results, and possibly the most difficult, how to explain what we’re doing to others! We’ve forgotten a lot, confused ourselves, and made some happy accidents in the process. That’s okay! Our goal is to help you avoid some of those pitfalls, help you understand the basics of how models work, and get a sense of how most modeling endeavors have a lot of things in common.\nWhether you enthusiastically pour over formulas and code, or prefer to skip over them, we promise that you don’t need to memorize a formula to get a good understanding of modeling and related issues. We are the first to admit that we have long dumped the ability to pull formulas out of our brain folds1; however, knowing how those individual pieces work together only helps to deepen your understanding of the model. Typically using code puts the formula into more concrete terms that you can then use in different ways to solidify and expand your knowledge. Sometimes you just need a reminder or want to see what function you’d use. And often, the visualization will reveal even more about what’s going than the formula or the code. In short, there are a lot of tools at your disposal to help learn modeling in a way that works for you. We hope that anyone that would be interested in the book will find a way to learn things in a manner that suits them best.\nThere is a bit of a caveat. We aren’t going to teach you basic statistics or how to program in R or Python. Although there is a good chance you will learn some of it here, you’ll have an easier time if you have a very basic understanding of statistics and some familiarity with coding. We will provide some resources for you to learn more about these topics, but we won’t be covering them in detail. However, we really aren’t assuming a lot of background knowledge, and are, if anything, assuming that whatever knowledge you have may be a bit loose or fuzzy. That’s okay!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-intro-lang",
    "href": "introduction.html#sec-intro-lang",
    "title": "1  Introduction",
    "section": "1.3 Which Language?",
    "text": "1.3 Which Language?\n\n \n\nYou’ve probably noticed most data science books, blogs, and courses choose R or Python. While many individuals often have a strong opinion towards teaching and using one over the other, we eschew dogmatic approaches and language flame wars. R and Python are both great languages for modeling, and both flawed in unique ways. Even if you specialize in one, it’s good to have awareness of the other as they are the most popular languages for statistical modeling and machine learning. We use both extensively in our own work for teaching, personal use, and production level code, and have found both are up to whatever task you have in mind.\nThroughout this book, we will be presenting demonstrations in both R and Python, and you can use both or take your pick, but we want to leave that choice up to you. Our goal is to use them as a tool to help understand some big model ideas. This book can be a resource for the R user who could use a little help translating their R knowledge to Python; we’d also like it to be a resource for the Python user who sees the value in R’s statistical modeling abilities and more. You’ll find that our coding style/presentation bends more toward legibility, clarity and consistency, which is not necessarily the same as a standard like PEP8 or the tidyverse style guide2. We hope that you can take the code we provide and make it your own, and that you can use it to help you understand the models we’re discussing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-intro-adventure",
    "href": "introduction.html#sec-intro-adventure",
    "title": "1  Introduction",
    "section": "1.4 Moving Towards an Excellent Adventure",
    "text": "1.4 Moving Towards an Excellent Adventure\nRemember the point we made about “choosing your own adventure”? Modeling and programming in data science is an adventure, even if you never leave your desk! Every situation calls for choices to be made and every choice you make will lead you down a different path. You will run into errors, dead ends, and you might even find that you’ve spent considerable time to conclude that nothing interesting is happening in your data. This, no doubt, is part of the fun and all of those struggles make success that much sweeter. Like every adventure, things might not be immediately clear and you might find yourself in perilous situations! If you find that something isn’t making sense upon your first read, that is okay. Both authors have spent considerable time mulling over models and foggy ideas during our assorted (mis)adventures; nobody should expect to master complex concepts on a single read through! In any arena where you strive to develop skills, distributed practice and repetition are essential. When concepts get tough, step away from the book, and come back with a fresh mind.\n\n\n\n\nIvanova, Anna A, Shashank Srikant, Yotaro Sueoka, Hope H Kean, Riva Dhamala, Una-May O’Reilly, Marina U Bers, and Evelina Fedorenko. 2020. “Comprehension of Computer Code Relies Primarily on Domain-General Executive Brain Regions.” Edited by Andrea E Martin, Timothy E Behrens, William Matchin, and Ina Bornkessel-Schlesewsky. eLife 9 (December): e58906. https://doi.org/10.7554/eLife.58906.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "We actually never had this ability.↩︎\nThe commonly used coding styles for both R and Python aren’t actually scientifically derived or tested, and only recently has research been conducted in this area (see Ivanova et al. (2020) for an example). The guidelines are generally good, but mostly reflect the preferences of the person(s) who wrote them. Our focus here is not on programming though.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "linear_models.html",
    "href": "linear_models.html",
    "title": "2  The Foundation",
    "section": "",
    "text": "2.1 Key Ideas\nTo get us started, we can pose a few concepts key to understanding linear models. We’ll cover each of these as we go along.\nAs we go along and cover these concepts, be sure that you feel you have the ‘gist’ of what we’re talking about. Almost everything that goes beyond linear models builds on what’s introduced here, so it’s important to have a firm grasp before climbing to new heights.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-key-ideas",
    "href": "linear_models.html#sec-lm-key-ideas",
    "title": "2  The Foundation",
    "section": "",
    "text": "What a model is: The model as an idea\nFeatures, targets, and input-output mappings: how do we get from input to output?\nPrediction: how do we use a model?\nInterpretation: what does a model tell us?\n\nPrediction underlies all interpretation\nWe can interpret a model at the feature level and as a whole\n\n\n\n\n2.1.1 Why this matters\nThe basic linear model and how it comes about underpins so many models, from the simplest t-test to the most complex neural network. It takes a bit to get used to the important aspects of it, but it provides a powerful foundation, and one that you’ll see in many different contexts. It’s also a model that is relatively easy to understand, and one that you can use to understand other models. So it’s a great place to start!\n\n\n2.1.2 Good to know\nWe’re just starting out here, but we’re kind of assuming you’ve had some exposure to the idea of statistical or other models, even if only from an interpretation standpoint. We assume you have an understanding of basic stats like central tendency (e.g., a mean or median), variance, and correlation, stuff like that. And if you intend to get into the code examples, you’ll need a basic familiarity with Python or R.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-what-is-a-model",
    "href": "linear_models.html#sec-lm-what-is-a-model",
    "title": "2  The Foundation",
    "section": "2.2 What is a Model?",
    "text": "2.2 What is a Model?\nAt its core, a model is just an idea. It’s a way of thinking about the world, about how things work, how things change over time, how things are different from each other, and how they are similar. The underlying thread is that a model expresses relationships about things in the world around us. One can also think of a model as a tool, one that allows us to take in information, derive some meaning from it, and act on it in some way. Just like other ideas and tools, models have consequences in the real world, and they can be used wisely or foolishly.\nOn a practical level, a model is expressed through a particular language, math, but don’t let that worry you if you’re not so inclined. As a model is still just an idea at its core, the idea is the most important thing to understand about it. The math is just a formal way of expressing the idea in a manner that can be communicated and understood by others in a standard way, and math can help make the idea precise. But in everyday terms, we’re trying to understand everyday things, like how the amount of sleep relates to cognitive functioning, how the weather affects the number of people who visit a park, how much money to spend on advertising to increase sales, how to detect fraud, and so on. Any of these could form the basis of a model, as they stem from scientifically testable ideas, and they all express relationships between things we are interested in, possibly even with an implication of causal relations.\nActually applying models to data can be simple. For example, if you wanted to create a linear model to understand the relationship between sleep and cognitive functioning, you might express it in code as:\n\nRPython\n\n\n\nlm(cognitive_functioning ~ sleep, data = df)\n\n\n\n\nfrom statsmodels.formula.api import ols\n\nmodel = ols('cognitive_functioning ~ sleep', data = df).fit()\n\n\n\n\nThe first part with the ~ is the model formula, which is how math comes into play to help us express relationships. Beyond that we just specify where, for example, the numeric values for cognitive functioning and the amount of sleep are to be located. In this case, they are found in the same data frame called df, which may have been imported from a spreadsheet somewhere. Very easy isn’t it? But that’s all it takes to express a straightforward idea. More conceptually, we’re saying that cognitive functioning is a linear function of sleep. You can probably already guess why R’s function is lm, and you’ll eventually also learn why statsmodels function is ols, but for now just know that both are doing the same thing.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-in-a-model",
    "href": "linear_models.html#sec-lm-in-a-model",
    "title": "2  The Foundation",
    "section": "2.3 What Goes into a Model? What Comes Out?",
    "text": "2.3 What Goes into a Model? What Comes Out?\n\n2.3.1 Features and targets\nIn the context of a model, how we specify the nature of the relationship between various things depends on the context. In the interest of generality, we’ll refer to the target as what we want to explain, and features as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. The table below shows some of the common terms used to refer to features and targets. Note that they can be mixed and matched, e.g. someone might refer to covariates and a response, or inputs and a label.\n\n\n\n\nTable 2.1: Common Terms for Features and Targets\n\n\n\n\n\n\n  \n    \n      Feature\n      Target\n    \n  \n  \n    independent variable\ndependent variable\n    predictor variable\nresponse\n    explanatory variable\noutcome\n    covariate\nlabel\n    x\ny\n    input\noutput\n    right-hand side\nleft-hand side\n  \n  \n  \n\n\n\n\n\n\n\nSome of these terms actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but here we’ll typically avoid those terms if we can since those connotations won’t apply. In the end though, you may find us using any of these words to describe the relationships of interest so that you are comfortable with the terminology, but typically we’ll stick with features and targets for the most part. In our opinion, these terms have the least hidden assumptions/implications, and just implies ‘features of the data’ and the ‘target’ we’re trying to explain or predict.\n\n\n2.3.2 Expressing relationships\nAs noted, a model is a way of expressing a relationship between a set of features and a target, and one way of thinking about this is in terms of inputs and outputs. But how can we go from input to output?\nWell, first off, we assume that the features and target are correlated, that there is some relationship between the feature x and target y. The output of a model will correspond to the target if they are correlated, and more closely match it with stronger correlation. If so, then we can ultimately use the features to predict the target. In the simplest setting, a correlation implies a relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down (right plot).\n\n\n\n\n\n\n\n\nFigure 2.1: Correlation\n\n\n\n\n\nIn addition, the simplest correlation suggests a linear relationship, one that is adequately captured by a straight line. There are many types of correlation metrics, but the most common one, the Pearson correlation, is explicitly a measure of the linear relationship between two variables. It’s expressed as a number between -1 and 1, where 0 means there is no linear relationship. As we move closer to a 1.0 correlation value, we would see a tighter scatterplot like the one on the left, until it became a straight line. The same happens for the negative relationship as we get closer to a value of -1, like the plot on the right. If we have only one feature and target, the Pearson correlation reflects the exact result of the linear model we’d conduct in a more complicated fashion. But even with multiple features, we still stick to this notion of correlation to help us understand how the features account for the target’s variability, or why it behaves the way it does.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-linear-model",
    "href": "linear_models.html#sec-lm-linear-model",
    "title": "2  The Foundation",
    "section": "2.4 THE Linear Model",
    "text": "2.4 THE Linear Model\nThe linear model is perhaps the simplest functional model we can use to express a relationship between features and targets. And because of that, it’s possibly still the most common model used in practice, and it is the basis for many types of other models. So why don’t we do one now?\nThe following dataset has individual movie reviews containing the movie rating (1-5 stars scale), along with features pertaining to the review (e.g., word count), those that regard the reviewer (e.g., age) and features about the movie (e.g., genre, release year).\nFor our first linear model, we’ll keep things simple. Let’s predict the rating from the length of the review in terms of word count. We’ll use the lm() function in R and the ols() function in Python2 to fit the model. Both functions take a formula as the first argument, which is a way of expressing the relationship between the features and target. The formula is expressed as y ~ x1 + x2 + ..., where y is the target name and x* are the feature names. We also need to specify what the data object is, typically a data frame.\n\nRPython\n\n\n\n# all data found on github repo\ndf_reviews = read_csv('https://tinyurl.com/moviereviewsdata')\n\nmodel_lr_rating = lm(rating ~ word_count, data = df_reviews)\n\nsummary(model_lr_rating)\n\n\nCall:\nlm(formula = rating ~ word_count, data = df_reviews)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06480 -0.35017  0.02056  0.33520  1.84983 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.491636   0.042358   82.43   &lt;2e-16 ***\nword_count  -0.042683   0.003686  -11.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5907 on 998 degrees of freedom\nMultiple R-squared:  0.1185,    Adjusted R-squared:  0.1176 \nF-statistic: 134.1 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# all data found on github repo\ndf_reviews = pd.read_csv('https://tinyurl.com/moviereviewsdata')\n\nmodel_lr_rating = smf.ols('rating ~ word_count', data = df_reviews).fit()\n\nmodel_lr_rating.summary(slim = True)\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.118\n\n\nModel:\nOLS\nAdj. R-squared:\n0.118\n\n\nNo. Observations:\n1000\nF-statistic:\n134.1\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n3.47e-29\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.4916\n0.042\n82.431\n0.000\n3.409\n3.575\n\n\nword_count\n-0.0427\n0.004\n-11.580\n0.000\n-0.050\n-0.035\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.118\n\n\nModel:\nOLS\nAdj. R-squared:\n0.118\n\n\nNo. Observations:\n1000\nF-statistic:\n134.1\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n3.47e-29\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.4916\n0.042\n82.431\n0.000\n3.409\n3.575\n\n\nword_count\n-0.0427\n0.004\n-11.580\n0.000\n-0.050\n-0.035\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nFor such a simple model, we certainly have a lot to unpack here! Don’t worry, you’ll eventually come to know what it all means. But it’s nice to know how easy it is to get the results! For now we can just say that there’s a negative relationship between the word count and the rating (the -0.043), and that the value regarding the relationship is statistically significant (p value is &lt; .05).\nGetting more into the details, we’ll start with the fact that the linear model posits a linear combination of the features. This is an important concept to understand, but really, a linear combination is just a sum of the features, each of which has been multiplied by some specific value. That value is often called a coefficient, or possibly weight, depending on the context. The linear model is expressed as (math incoming!):\n\\[\ny = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n\n\\tag{2.1}\\]\n\n\\(y\\) is the target.\n\\(x_1, x_2, ... x_n\\) are the features.\n\\(b_0\\) is the intercept, which is kind of like a baseline value or offset. If we had no features at all it would just be the mean of the target.\n\\(b_1, b_2, ... b_n\\) are the coefficients or weights for each feature.\n\nBut let’s start with something simpler, let’s say you want to take a sum of several features. In math you would write it as:\n\\[\nx_1 + x_2 + ... + x_n\n\\]\nIn the previous equation, x is the feature and n is the number identifier for the features, so \\(x_1\\) is the first feature (e.g. word count), \\(x_2\\) the second (e.g. movie release year), and so on. \\(x\\) is an arbitrary designation - you could use any letter, symbol you want, or even better, would be the actual feature name. Now look at the linear model.\n\\[\ny = x_1 + x_2 + ... + x_n\n\\]\nIn this case, the function is just a sum, something so simple we do it all the time. In the linear model sense though, we’re actually saying a bit more. Another way to understand that equation is that y is a function of x. We don’t show any coefficients here, i.e. the bs in our initial equation (Equation 2.1), but technically it’s as if each coefficient was equal to a value of 1. In other words, for this simple linear model, we’re saying that each feature contributes in an identical fashion to the target.\nIn practice, features will never contribute in the same ways, because they correlate with the target differently, or are on different scales. So if we want to relate some feature, \\(x_1\\), and some other feature, \\(x_2\\), to target \\(y\\), we probably would not assume that they both contribute in the same way from the beginning. We might give relatively more weight to \\(x_1\\) than \\(x_2\\). In the linear model, we express this by multiplying each feature by a different coefficient or weight. So the linear model is really just a sum of the features multiplied by their coefficients, i.e. a weighted sum. In fact, we’re saying that each feature contributes to the target in proportion to the coefficient. So if we have a feature \\(x_1\\) and a coefficient \\(b_1\\), then the contribution of \\(x_1\\) to the target is \\(b_1*x_1\\). If we have a feature \\(x_2\\) and a coefficient \\(b_2\\), then the contribution of \\(x_2\\) to the target is \\(b_2 * x_2\\). And so on. So the linear model is really just a sum of the features multiplied by their respective weights.\nFor our specific model, here is the mathematical representation:\n\\[\n\\textrm{rating} = b_0 + b_1 \\cdot \\textrm{word\\_count}\n\\]\nAnd with the actual results of our model:\n\\[\n\\textrm{rating} = 3.49 + -0.04 \\cdot \\textrm{word\\_count}\n\\]\nNot too complicated we hope! But let’s make sure we see what’s going on here just a little bit more.\n\nOur idea is that the length of the review in words is in some way related to the eventual rating given to the movie.\nOur target is the movie’s rating by a reviewer, and the feature is the word count\nWe map the feature to the target via the linear model, which provides an initial understanding of how the feature is related to the target. In this case, we start with a baseline of 3.49. This value makes sense only in the case of a rating with no review, and is what we would guess if the word count was 0. But we know there are reviews for every observation, so it’s not very meaningful as is. We’ll talk about ways to get a more meaningful intercept later, but for now, that is our starting point. Moving on, if we add a single word to the review, we expect the rating to decrease by -0.04 stars. So if we had a review that was 10 words long, i.e., the mean word count, we would predict a rating of 3.49 + 10*-0.04 = 3.1 stars.\n\n\n2.4.1 The linear model visualized\nWe can also express the linear model as a graph, which can be a very useful way to think about models in a visual fashion, and as we see other models, can help us literally see how different models relate to one another and are actually very similar to one another. In the following, we have three features predicting a single target, so we have three ‘nodes’ for the features, and a single node for the target. The feature nodes are combined into a linear combination to produce the output of the model. In the context of linear regression, the output is often called the linear predictor. Each ‘edge’ signifies the connection of a feature to the output, and is labeled with the coefficient or weight. The connection between the output and the target is direct, without any additional change. We’ll return to this depiction a little bit later (Section 2.9), but for our standard linear model, we’re all set.\n\n\n\n\n\n\nFigure 2.2: A linear regression as a graphical model\n\n\n\nSo at this point you have the basics of what a linear model is and how it works, and a couple ways to think about it, whether through programming, math, or just visually. But there is a lot more to it than that. Just getting the model is easy enough, but we need to be able to use it and understand the details better, so we’ll get into that now!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-what-do-we-do",
    "href": "linear_models.html#sec-lm-what-do-we-do",
    "title": "2  The Foundation",
    "section": "2.5 What Do We Do with a Model?",
    "text": "2.5 What Do We Do with a Model?\nOnce we have a working model, there are two primary ways we can use it. One way to use a model is to help us understand the relationships between the features and our outcome of interest. In this way, the focus can be said to be on explanation, or interpreting the model results. The other way to use a model is to make estimates about the target for specific observations, often ones we haven’t seen in our data. In this way the focus is on prediction. In practice, we often do both, but the focus is usually on one or the other. We’ll cover both in detail eventually, but let’s start with prediction.\n\n2.5.1 Prediction\nIt may not seem like much at first, but a model is of no use if it can’t be used to make predictions about what we can expect in the world around us. Once our model has been fit to the data, we can obtain our predictions by plugging in values for the features that we are interested in, and, using the corresponding weights and other parameters that have been estimated, come to a guess about a specific observation. Let’s go back to our results, shown in the following table.\n\n\n\n\n\n  \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n3.49\n0.04\n82.43\n0.00\n3.41\n3.57\n    word_count\n−0.04\n0.00\n−11.58\n0.00\n−0.05\n−0.04\n  \n  \n  \n\n\n\n\nThe table shows the coefficient for each feature including the intercept, which is our starting point. In this case, the coefficient for word count is -0.04, which means that for every additional word in the review, the rating goes down by -0.04 stars. So if we had a review that was 10 words long, we would predict a rating of 3.49 + 10*-0.04 = 3.1 stars.\nWhen we’re talking about the predictions (or outputs) for a linear model, we usually will see this as the following mathematically:\n\\[\n\\hat{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n\n\\tag{2.2}\\]\nWhat is \\(\\hat{y}\\)? The hat over the \\(y\\) just means that it’s a predicted value of the model, rather than the target value we actually observe in the data. Our first equations that just used \\(y\\) implicitly suggested that we would get a perfect rating value given the model, but that’s not the case. We can only get an estimate. The \\(\\hat{y}\\) is also the linear predictor in our graphical version (Figure 2.2), which makes clear it is not the actual target, but a combination of the features that is related to the target.\nTo make our first equation (Equation 2.1) accurately reflect the relationship between the target and our features, we need to add what is usually referred to as an error term, \\(\\epsilon\\), to account for the fact that our predictions will not be perfect3. So the full linear (regression) model is:\n\\[\ny = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n + \\epsilon\n\\tag{2.3}\\]\nThe error term is a random variable that represents the difference between the actual value and the predicted value, which comes from the weighted combination of features. We can’t know what the error term is, but we can estimate it, just like we can the coefficients. We’ll talk more about that in the section on estimation (Chapter 4).\n\n\n\n\n\n\nPredictions by any other name…\n\n\n\n\n\nYou’ll often see predictions referred to as fitted values, but these imply we are only talking about the data the model was trained on or ‘fit’ to. Predictions can also be referred to as expected values, estimates, outputs, or forecasts, the latter is especially common in time series analysis. Within generalized linear models and others where there may ultimately be a transformation of the output, you may see it referred to as a linear predictor.\n\n\n\n\n\n2.5.2 What kinds of predictions can we get?\nWhat predictions we get depends on the type of model we are using. For the linear model we have at present we can get predictions for the target, which is a continuous variable. Very commonly, we also can get predictions for a categorical target, such as whether the rating is ‘good’ or ‘bad’. This simple breakdown pretty much covers everything, as we typically would be predicting a continuous numeric variable or a categorical variable, or more of them, like multiple continuous variables, or a target with multiple categories, or sequences of categories (e.g. words). In our case, we can get predictions for the rating, which is a number between 1 and 5. Had our target been a binary good vs. bad rating, our predictions would still be numeric in most cases or at least amenable to such, and usually expressed as a probability between 0 and 1, say, for the ‘good’ category. Higher probabilities would mean we’d more likely predict the movie is good. We then would convert that probability to a class of good or bad depending on a chosen probability cutoff. We’ll talk about how to get predictions for categorical targets later4.\nWe previously saw a prediction for a single observation where the word count was 10 words, but we can also get predictions for multiple observations at once. In fact, we can get predictions for all observations in our dataset. Besides that, we can also get predictions for observations that we don’t even have data for! Fun! The following shows how we can get predictions for all data, and for a single observation with a word count of 5.\n\nRPython\n\n\n\nall_predictions = predict(model_lr_rating)\n\ndf_prediction = tibble(word_count = 5)\nsingle_prediction = predict(model_lr_rating, newdata = df_prediction)\n\n\n\n\nall_predictions = model_lr_rating.predict()\n\ndf_prediction = pd.DataFrame({'word_count': [5]})\nsingle_prediction = model_lr_rating.predict(df_prediction)\n\n\n\n\nHere is a plot of our predictions for the observed data versus the actual ratings6. The reference line is where the points would fall if we had perfect prediction. We can see that the predictions are definitely not perfect, but we don’t expect this. They are not completely off base either, in that generally higher predicted scores are associated with higher observed values. We’ll talk about how to assess the quality of our predictions later, but we can at least get a sense that we have a correspondence relationship between our predictions and target, which is definitely better than not having a relationship at all!\n\n\n\n\n\n\n\n\n\nFigure 2.3: Predicted vs. Observed Ratings\n\n\n\n\n\nNow let’s look at what our prediction looks like for a single observation, and we’ll add in a few more- one for 10 words, and one for a 50 word review, which is beyond the length of any review in this dataset, and one for 12.3 words, which isn’t even possible for this data, since words are only counted as whole values. To get these values we just use the same prediction approach as before, and we specify the word count value we want to predict for.\n\n\n\n\nTable 2.2: Predictions for Specific Observations\n\n\n\n\n\n\n  \n    \n      Word Count\n      Predicted Rating\n    \n  \n  \n    5.0\n3.3\n    10.0\n3.1\n    12.3\n3.0\n    50.0\n1.4\n  \n  \n  \n\n\n\n\n\n\n\nThe values reflect the negative coefficient from our model, showing decreasing ratings with increasing word counts. Furthermore, we see the power of the model’s ability to make predictions for what we don’t see in the data. Maybe we limited our data review size, but we know there are 50 word reviews out there, and we can still make a guess as to what the rating would be for such a review. Maybe in another case, we know a group of people who have on average 12.3 word reviews, and we can make a guess as to what the average rating would be for that group. Our model doesn’t know anything about the context of the data, but we can use our knowledge to make predictions about the world around us. This is a very powerful capability, and it’s one of the main reasons we use models in the first place.\n\n\n2.5.3 Prediction error\nAs we have seen, predictions are not perfect, and an essential part of the modeling endeavor is to better understand these errors and why they occur. In addition, error assessment is the fundamental way in which we assess a model’s performance, and, by extension, compare that performance to other models. In general, prediction error is the difference between the actual value and the predicted value or some function of it, and in statistical models, is also often called the residual. We can look at these individually, or we can look at them in aggregate with a single metric.\nLet’s start with looking at the residuals visually. Often the modeling package you use will have this as a default plotting method when doing a standard linear regression, so it’s wise to take advantage of it. We plot both the distribution of raw error scores and the cumulative distribution of absolute prediction error. Here we see a couple things. First, the distribution is roughly normal, which is a good thing, since statistical linear regression assumes our error is normally distributed, and the prediction error serves as an estimate of that. Second, we see that the mean of the errors is zero, which is a consequence of linear regression, and the reason we look at other metrics when assessing model performance. We can also see that most of our predictions are within ±1 star rating.\n\n\n\n\n\n\n\n\nFigure 2.4: Distribution of Prediction Errors\n\n\n\n\n\nOf more practical concern is that we don’t see extreme values or clustering, which might indicate a failure on the part of the model to pick up certain segments of the data. It can still be a good idea to look at the extremes just in case we can pick up on some aspect of the data that we could potentially incorporate into the model. So looking at our worst prediction in absolute terms, we see the observation has a typical word count, and so our simple model will just predict a fairly typical rating. But the actual rating is 1, which is 2.1 away from our prediction, a very noticeable difference. Further data inspection may be required to figure out why this came about.\n\n\n\n\nTable 2.3: Worst Prediction\n\n\n\n\n\n\n  \n    \n      rating\n      prediction\n      word_count\n    \n  \n  \n    1.0\n3.1\n10\n  \n  \n  \n\n\n\n\n\n\n\n\n\n2.5.4 Prediction Uncertainty\nWe can also look at the uncertainty of our predictions, which is a measure of how much we expect our predictions to vary. This is often expressed as an interval range of values that we expect our prediction to fall within, with a certain level of confidence. But! There are actually two types of intervals we can get, one is really about the mean prediction, or expected value we would get from the model at that observation. This is usually called a confidence interval. The other type of interval is based on the model’s ability to predict new data, and is often called a prediction interval. This interval is about the actual prediction we would get from the model for any value, whether it was data we had seen before or not. Because of this, the prediction interval is always wider than the confidence interval, and it’s the one we usually want to use when we’re making predictions about new data.\nHere is how we can obtain these from our model.\n\nRPython\n\n\n\nprediction_CI = predict(\n    model_lr_rating, \n    newdata = df_prediction, \n    se.fit = TRUE, \n    interval = \"confidence\"\n)\n\nprediction_PI = predict(\n    model_lr_rating, \n    newdata = df_prediction, \n    se.fit = TRUE, \n    interval = \"prediction\"\n)\n\npred_intervals = bind_rows(\n    as_tibble(prediction_CI$fit),\n    as_tibble(prediction_PI$fit),\n) |&gt; mutate(\n    interval = c('confidence', 'prediction'),\n    type = c('mean', 'observation')\n)\n\npred_intervals\n\n\n\n\npred_intervals = (\n    model_lr_rating\n    .get_prediction(df_prediction)\n    .summary_frame(alpha = 0.05)\n)\n\npd.DataFrame(pred_intervals)\n\n\n\n\n\n\n\n\nTable 2.4: Prediction Intervals for Specific Observations\n\n\n\n\n\n\n  \n    \n      interval\n      type\n      fit\n      lwr\n      upr\n    \n  \n  \n    confidence\nmean\n3.28\n3.23\n3.33\n    prediction\nobservation\n3.28\n2.12\n4.44\n  \n  \n  \n\n\n\n\n\n\n\nAs expected our prediction interval is wider than our confidence interval, and we can see that the prediction interval is quite wide- from a rating of 2.1 to 4.4. This is a consequence of the fact that we have a lot of uncertainty in our predictions for new observations and we can’t expect to get a very precise prediction from our model with only one feature. This is a common issue with many models, and one that having a better model can help remedy7.\nSo at this point you have the gist of prediction, prediction error, and uncertainty in a prediction, but there is still more to it! We’ll come back to global assessments of model error very shortly, and even more detail can be found in Chapter 3 where we dive deeper into our models, and Chapter 4, where we see how to estimate the parameters of our model by picking those that will reduce the prediction error the most. For now though, let’s move on to the other main use of models, which is to help us understand the relationships between the features and the target, or explanation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-interpretation",
    "href": "linear_models.html#sec-lm-interpretation",
    "title": "2  The Foundation",
    "section": "2.6 How Do We Interpret the Model?",
    "text": "2.6 How Do We Interpret the Model?\nWhen it comes to interpreting the results of our model, there are a lot of tools at our disposal, though many of the tools we can ultimately use will depend on the specifics of the model we have employed. In general though, we can group our approach to understanding results at the feature level and the model level. A feature level understanding regards the relationship between a single feature and the target. Beyond that, we also attempt comparisons of feature contributions to prediction, i.e., relative importance. Model level interpretation is focused on assessments of how well the model ‘fits’ the data, or more generally, predictive performance. We’ll start with the feature level, and then move on to the model level.\n\n2.6.1 Feature level interpretation\nAs mentioned, at the feature level, we are primarily concerned with the relationship between a single feature and the target. More specifically, we are interested in the direction and magnitude of the relationship, but in general, it all boils down to how a feature induces change in the target. For numeric features, we are curious about the change in the target given some amount of change in the feature values. It’s conceptually the same for categorical features, but often we like to express the change in terms of group mean differences or something similar, since the order of categories is not usually meaningful. An important aspect of feature level interpretation is the specific predictions we can get by holding the data at key feature values.\nLet’s start with the basics by looking again at our coefficient table from the model output.\n\n\n\n\n\n  \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n3.49\n0.04\n82.43\n0.00\n3.41\n3.57\n    word_count\n−0.04\n0.00\n−11.58\n0.00\n−0.05\n−0.04\n  \n  \n  \n\n\n\n\nHere, the main thing to look at are the actual feature coefficients and the direction of their relationship, positive or negative. The coefficient for word count is -0.04, and this means that for every additional word in the review, the rating goes down by -0.04. This interpretation gives us directional information, but how can we interpret the magnitude of the coefficient?\nLet’s try and use some context to help us. While a drop of -0.04 might not mean much to us in terms of ratings, we might not be as sure about a change in one word for a review. However, we do know the standard deviation of the rating score, i.e., how much it moves around naturally on its own, is 0.63. So the coefficient is about 6% of the standard deviation of the target. In other words, the addition of a single word to a review results in an expected decrease of 6% of what the review would normally bounce around in value. We might not consider this large, but also, a single word change isn’t much. What would be a significant change in word count? Let’s consider the standard deviation of the feature. In this case, it’s 5.1 for word count. So if we increase the word count by one standard deviation, we expect the rating to decrease by -0.04 * 5.1 = -0.2. That decrease then translates to a change of -0.2/0.63 = -0.32 standard deviation units of the target. Without additional context, many would think that’s a significant change8, or at the very least, that the coefficient is not negligible, and that the feature is indeed related to the target. But we can also see that the coefficient is not so large that it’s not believable.\n\n\n\n\n\n\nStandardized Coefficients\n\n\n\n\n\nThe calculation we just did results in what’s often called a standardized or ‘normalized’ coefficient. In the case of the simplest model with only one feature like this, it is identical to the Pearson r correlation metric, which we invite you to check and confirm on your own, which should roughly equal our calculation using rounded values. In the case of multiple features, it represents a (partial) correlation between the target and the feature, after adjusting for the other features. But before you start thinking of it as a measure of importance, it is not. It provides some measure of the feature-target linear relationship, but that does not entail practical importance, nor is it useful in the presence of nonlinear relationships, interactions, and a host of other interesting things that are typical to data and models.\n\n\n\nAfter assessing the coefficients, next up in our table is the standard error. The standard error is a measure of how much the coefficient varies from sample to sample. If we collected the data multiple times, even under practically identical circumstances, we wouldn’t get the same value each time - it would bounce around a bit, and the standard error is an estimate of how much it would bounce around. In other words, the standard error is a measure of uncertainty, and along with the coefficients, it’s used to calculate everything else in the table.\n\n\n\n\n\n\n  \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n3.49\n0.04\n82.43\n0.00\n3.41\n3.57\n    word_count\n−0.04\n0.00\n−11.58\n0.00\n−0.05\n−0.04\n  \n  \n  \n\n\n\n\nThe statistic, here a t-statistic from the student t distribution9, is the ratio of the coefficient to the standard error. This gives us a sense of the effect relative to its variability, but the statistic’s primary use is to calculate the p-value related to its distribution10, which is the probability of seeing a coefficient as large as the one we have, if we assume from the outset that the true value of the coefficient is zero. In this case, the p-value is 3.47e-29, which is very small. We can conclude that the coefficient is statistically different from zero, and that the feature is related to the target, at least statistically speaking. However, the interpretation we used regarding the coefficient previously is far more useful than the p-value, as the p-value can be affected by many things not necessarily related to the feature-target relationship, such as sample size, and is often misinterpreted.\nAside from the coefficients, the most important output is the confidence interval (CI). The CI is a range of values that encapsulates the uncertainty we have in our guess about the coefficients. While our best guess for the effect of word count on rating is -0.04, we know it’s not exactly that, and the CI gives us a range of reasonable values we might expect the effect to be based on the data at hand and the model we’ve employed.\n\n\n\n\n\n  \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n3.49\n0.04\n82.43\n0.00\n3.41\n3.57\n    word_count\n−0.04\n0.00\n−11.58\n0.00\n−0.05\n−0.04\n  \n  \n  \n\n\n\n\nIn this case, the default is a 95% confidence interval, and we can think of this particular confidence interval like throwing horseshoes. If we kept collecting data and running models, 95% of our CIs would capture the true value, and this is one of the many possible CIs we could have gotten. That’s the technical definition, which is a bit abstract11, but we can also think of it more simply as a range of values that are good guesses for the true value. In this case, the CI is -0.035 to -0.035, and we can be 95% confident that it is a good ranged estimate for the coefficient. We can also see that the CI is relatively narrow, which is good, as it implies that we have a good idea of what the coefficient is. If it was very wide, we would have a lot of uncertainty about the coefficient, and we would not likely not want to base important decisions regarding it.\n\nKeep in mind that your chosen model has a great influence on what you’ll be able to say at the feature level. As an example, as we get into machine learning models, you won’t have as easy a time with coefficients and their confidence intervals, but you still may be able to say something about how your features relate to the target, and we’ll continue to return to the topic. But first, let’s take a look at interpreting things in another way.\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\nThe confidence interval and p-value will for coefficients in typical statistical linear models will coincide with one another in that, if for a given alpha significance level, if a 1-alpha% CI includes zero, then your p-value will be greater than alpha, and vice versa. This is because the same standard error is used to calculate both. However, the framework of using a CI vs. using the p-value for claiming statistical significance actually came from individuals that were philosophically opposed. Modern day usage of both is a bit of a mess that would upset both Fisher (p-value guy) and Neyman (CI guy), but your authors find that this incorrect practical usage doesn’t make much practical difference in the end.\n\n\n\n\n\n2.6.2 Model level interpretation\nThus far, we’ve focused on interpretation at the feature level. But knowing the interpretation of a feature doesn’t do you much good if the model itself is poor! In that case, we also need to assess the model as a whole, and as with the feature level, we can go about this in a few ways. Before getting too carried away with asking whether your model is any good or not, you always need to ask yourself relative to what? Many models claim top performance under various circumstances, but which are statistically indistinguishable from many other models. So we need to be careful about how we assess our model, and what we compare it to.\nWhen we looked at the models previously Figure 2.3, we examined how well the predictions and target line up, and that gives us an initial feel for how well the model fits the data. Most model-level interpretation involves assessing and comparing model fit and variations on this theme. Here we show how easy it is to obtain such a plot.\n\nRPython\n\n\n\npredictions = predict(model_lr_rating)\ny = df_reviews$rating\n\nggplot(\n    data = data.frame(y = y, predictions = predictions), \n    aes(x = y, y = predictions)\n) +\n  geom_point() +\n  labs(x = \"Predicted\", y = \"Observed\")\n\n\n\n\nimport matplotlib.pyplot as plt\n\npredictions = model_lr_rating.predict()\ny = df_reviews.rating\n\nplt.scatter(y, predictions)\n\n\n\n\n\n2.6.2.1 Model Metrics\nWe can also get an overall assessment of the prediction error from a single metric. In the case of the linear model we’ve been looking at, we can express this in a single metric as the sum or mean of our squared errors, the latter of which is a very commonly used modeling metric- MSE or mean squared error, or also, its square root - RMSE or root mean squared error12. We’ll talk more about this and similar metrics in other chapters, but we can take a look at the RMSE for our model now.\nIf we look back at our results, we can see this expressed as the part of the output or as an attribute of the model13. The RMSE is more interpretable, as it gives us a sense that our typical errors bounce around by about 0.59. Given that the rating is on a 1-5 scale, this maybe isn’t bad, but we could definitely hope to do better than get within roughly half a point on this scale. We’ll talk about ways to improve this later.\n\nRPython\n\n\n\n# summary(model_lr_rating) # 'Residual standard error' is approx RMSE\nsummary(model_lr_rating)$sigma   # We can extract it directly\n\n[1] 0.5907288\n\n\n\n\n\nnp.sqrt(model_lr_rating.scale)   # RMSE\n\n0.590728780660127\n\n\n\n\n\nAnother metric we can use to assess model fit in this particular situation is the mean absolute error (MAE). MAE is similar to the mean squared error, but instead of squaring the errors, we just take the absolute value. Conceptually it attempts to get at the same idea, how much our predictions miss the target on average, and here the value is 0.46, which we actually showed in our residual plot (Figure 2.4). With either metric, the closer to zero the better, since as we get closer, we are reducing error.\nWe can also look at the R-squared (R2) value of the model. R2 is possibly the most popular measure of model performance with linear regression and linear models in general. Before squaring, it’s just the correlation of the values that we saw in the previous plot (Figure 2.3). When we square it, we can interpret it as a measure of how much of the variance in the target is explained by the model. In this case, our model shows the R2 is 0.12, which is not bad for a single feature model in this type of setting. We interpret the value as 12% of the target variance is explained by our model, and more specifically by the features in the model. In addition, we can also interpret R2 as 1 - the proportion of error variance in the target, which we can calculate as \\(1 - \\frac{\\textrm{MSE}}{var(y)}\\). In other words the complement of R2 is the proportion of the variance in the target that is not explained by the model. Either way, since 88% is not explained by the model, our result suggests there is plenty of work left to do!\nNote also, that with R2 we get a sense of the variance shared between all features in the model and the target, however complex the model gets. As long as we use it descriptively as a simple correspondence assessment of our predictions and target, it’s a fine metric. For various reasons, it’s not a great metric for comparing models to each other, but again, as long as you don’t get carried away, it’s okay to use.\n\n\n\n2.6.3 Prediction vs. explanation\nIn your humble authors’ views, one can’t stress enough the importance of a model’s ability to predict the target. It can be a poor model, maybe because the data is not great, or perhaps we’re exploring a new area of research, but we’ll always be interested in how well a model fits the observed data, and in most situations, we’re just as much or even more interested in how well it predicts new data.\nEven to this day, statistical significance is focused on a great deal, and much is made about models that have no little predictive power at all. As strange as it may sound, you can read results in journal articles, news features, and business reports in many fields with hardly any mention of a model’s predictive capability. The focus is almost entirely on the explanation of the model, and usually the statistical significance of the features. In those settings, statistical significance is often used as a proxy for importance, which is rarely ever justified. As we’ve noted elsewhere, statistical significance is affected by other things besides the size of the coefficient, and without an understanding of the context of the features, in this case, like how long typical reviews are, what their range is, what variability of ratings is, etc., the information it provides is extremely limited, and many would argue, not very useful. If we are very interested in the coefficient or weight value specifically, it is better to focus on the range of possible values, which is provided by the confidence interval, along with the predictions that come about based on that coefficient’s value. While a confidence interval is also a loaded description of a feature’s relationship to the target, we can use it in a very practical way as a range of possible values for that weight, and more importantly, think of possibilities rather than certainties.\nSuffice it to say at this point that how much one focuses on prediction vs. explanation depends on the context and goals of the data endeavor. There are cases where predictive capability is of utmost importance, and we care less about explanatory details, but not to the point of ignoring it. For example, even with deep learning models for image classification, where the inputs are just RGB values, we’d still like to know what the (notably complex) model is picking up on, otherwise we may be classifying images based on something like image backgrounds (e.g. outdoors vs. indoors) instead of the objects of actual interest (dogs vs. cats). In some business or other organizational settings, we are very, or even mostly, interested in the coefficients/weights, which might indicate how to allocate monetary resources in some fashion. But if those weights come from a model with no predictive power, placing much importance on them may be a fruitless endeavor.\nIn the end we’ll need to balance our efforts to suit the task at hand. Prediction and explanation are both fundamental to the modeling endeavor.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-complexity",
    "href": "linear_models.html#sec-lm-complexity",
    "title": "2  The Foundation",
    "section": "2.7 Adding Complexity",
    "text": "2.7 Adding Complexity\nWe’ve seen how to fit a model with a single feature and interpret the results, and that helps us to get oriented to the general modeling process. However, we’ll always have more than one feature for a model except under some very specific circumstances, such as exploratory data analysis. So let’s see how we can implement a model with more features and that makes more practical sense.\n\n2.7.1 Multiple features\nWe can add more features to our model very simply. Using the standard functions we’ve already demonstrated, we just add them to the formula (both R and statsmodels) as follows.\n\n'y ~ feature_1 + feature_2 + feature_3'\n\nIn other cases where we use matrix inputs, additional features will just be the additional input columns, and nothing about the model code actually changes. We might have a lot of features, and even for relatively simple linear models this could be dozens in some scenarios. A compact depiction of our model uses matrix representation, which we’ll show in the callout below, but you can find more detail in the matrix overview Appendix B. For our purposes, all you really need to know is that this:\n\\[\ny = X\\beta\\qquad  \\textrm{or}\\qquad y = \\alpha + X\\beta\n\\tag{2.4}\\]\nis the same as this:\n\\[\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 \\dots\n\\]\nwhere \\(y\\) is the target, \\(X\\) is a 2-d matrix of features14, where the rows are observations/instances and columns features, and \\(\\beta\\) is a vector of coefficients corresponding to the number of columns in \\(X\\). Matrix multiplication provides us an efficient way to get our expected value/prediction.\n\n\n\n\n\n\nMatrix Representation of a Linear Model\n\n\n\n\n\nHere we’ll show the matrix representation form of the linear model in more detail. In the following, \\(y\\) is a vector of all target observations, and \\(X\\) is a matrix of features. The \\(\\beta\\) vector is the vector of coefficients. The column of 1s serves as a means to incorporate the intercept, as it’s just multiplied by whatever the estimated intercept value is. The matrix multiplication form is just a compact way of expressing the sum of the features multiplied by their coefficients.\nHere is y as a vector of observations, n x 1.\n\\[\n\\textbf{y} = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\tag{2.5}\\]\nHere is the n x p matrix of features, including the intercept:\n\\[\n\\textbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{np}\n\\end{bmatrix}\n\\tag{2.6}\\]\nAnd finally, here is the p x 1 vector of coefficients:\n\\[\n\\bf{\\beta} = \\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\tag{2.7}\\]\nPutting it all together, along with the error term, we get the linear model in matrix form:\n\\[\n\\bf{y = X\\beta + \\epsilon}\n\\tag{2.8}\\]\nYou will also see it depicted in a transposed fashion, such that \\(y = \\beta^\\intercal X\\), or \\(f(x) = w^\\intercal X + b\\), with the latter formula is typically seen when the context is machine learning. This is just a matter of preference, except that it may assume the data is formatted in a different way, or possibly they are talking about matrix/vector operations for a single observation. You’ll want to pay close attention to what the dimensions are15.\nFor the models considered here and almost all ‘tabular data’ scenarios, the data is stored in the fashion we’ve represented in this text, but you should be aware that other data settings will force you to think of multi-dimensional arrays16 instead of 2-d matrices, for example, with image processing. So it’s good to be flexible.\n\n\n\nWith that in mind, let’s get to our model! In what follows, we keep the word count, but now we add some aspects of the reviewer, such as age and the number of children in the household, and features related to the movie, like the release year, the length of the movie in minutes, and the total reviews received. We’ll use the same approach as before, and literally just add them as we depicted in our linear model formula (Equation 2.3) .\n\nRPython\n\n\n\nmodel_lr_rating_extra = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_reviews\n)\n\nsummary(model_lr_rating_extra)\n\n\nCall:\nlm(formula = rating ~ word_count + age + review_year + release_year + \n    length_minutes + children_in_home + total_reviews, data = df_reviews)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.82310 -0.33994  0.01066  0.35665  1.51439 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -4.557e+01  7.463e+00  -6.106 1.47e-09 ***\nword_count       -3.033e-02  3.332e-03  -9.102  &lt; 2e-16 ***\nage              -1.687e-03  9.243e-04  -1.825  0.06826 .  \nreview_year       9.879e-03  3.234e-03   3.055  0.00231 ** \nrelease_year      1.328e-02  1.786e-03   7.434 2.27e-13 ***\nlength_minutes    1.669e-02  1.532e-03  10.897  &lt; 2e-16 ***\nchildren_in_home  1.028e-01  2.537e-02   4.051 5.49e-05 ***\ntotal_reviews     7.616e-05  6.161e-06  12.362  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.52 on 992 degrees of freedom\nMultiple R-squared:  0.3211,    Adjusted R-squared:  0.3163 \nF-statistic: 67.02 on 7 and 992 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nmodel_lr_rating_extra = smf.ols(\n    formula = 'rating ~ word_count \\\n        + age \\\n        + review_year \\\n        + release_year \\\n        + length_minutes \\\n        + children_in_home \\\n        + total_reviews',\n    data = df_reviews\n).fit()\n\nmodel_lr_rating_extra.summary(slim = True)\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.321\n\n\nModel:\nOLS\nAdj. R-squared:\n0.316\n\n\nNo. Observations:\n1000\nF-statistic:\n67.02\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n3.73e-79\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-45.5688\n7.463\n-6.106\n0.000\n-60.215\n-30.923\n\n\nword_count\n-0.0303\n0.003\n-9.102\n0.000\n-0.037\n-0.024\n\n\nage\n-0.0017\n0.001\n-1.825\n0.068\n-0.004\n0.000\n\n\nreview_year\n0.0099\n0.003\n3.055\n0.002\n0.004\n0.016\n\n\nrelease_year\n0.0133\n0.002\n7.434\n0.000\n0.010\n0.017\n\n\nlength_minutes\n0.0167\n0.002\n10.897\n0.000\n0.014\n0.020\n\n\nchildren_in_home\n0.1028\n0.025\n4.051\n0.000\n0.053\n0.153\n\n\ntotal_reviews\n7.616e-05\n6.16e-06\n12.362\n0.000\n6.41e-05\n8.83e-05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.82e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.321\n\n\nModel:\nOLS\nAdj. R-squared:\n0.316\n\n\nNo. Observations:\n1000\nF-statistic:\n67.02\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n3.73e-79\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-45.5688\n7.463\n-6.106\n0.000\n-60.215\n-30.923\n\n\nword_count\n-0.0303\n0.003\n-9.102\n0.000\n-0.037\n-0.024\n\n\nage\n-0.0017\n0.001\n-1.825\n0.068\n-0.004\n0.000\n\n\nreview_year\n0.0099\n0.003\n3.055\n0.002\n0.004\n0.016\n\n\nrelease_year\n0.0133\n0.002\n7.434\n0.000\n0.010\n0.017\n\n\nlength_minutes\n0.0167\n0.002\n10.897\n0.000\n0.014\n0.020\n\n\nchildren_in_home\n0.1028\n0.025\n4.051\n0.000\n0.053\n0.153\n\n\ntotal_reviews\n7.616e-05\n6.16e-06\n12.362\n0.000\n6.41e-05\n8.83e-05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.82e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\nThere is definitely more to unpack here than our simpler model, but it’s important to note that it’s just more stuff, not different stuff. The model-level components are the same in that we still see R2 etc., although they are all ‘better’ (higher R2, lower error) because we have a more predictive model. Our coefficients look the same also, and we’d interpret them in the same way. Starting with word count, we see that it’s still statistically significant, but it has been reduced just slightly from our previous model where it was the only feature (-0.04 vs. -0.03). Why? This suggests that word count has some non-zero correlation, sometimes called collinearity, with other features that are also explaining the target to some extent. Our linear model shows the effect of each feature controlling for other features, or, holding other features constant17. Conceptually this means that the effect of word count is the effect of word count after we’ve accounted for the other features in the model. In this case, an increase of a single word results in a -0.03 drop, even after adjusting for the effect of other features. Looking at another feature, the addition of a child to the home is associated with 0.1 increase in rating, accounting for the other features.\nThinking about prediction, how would we get a prediction for a movie rating with a review that is 12 words long, written in 2020, by a 30 year old with one child, for a movie that is 100 minutes long, released in 2015, with 10000 total reviews? Exactly the same as we did before (Section 2.5.2)! We just create a data frame with the values we want, and predict accordingly.\n\nRPython\n\n\n\npredict_observation = tibble(\n    word_count = 12,\n    age = 30,\n    children_in_home = 1,\n    review_year = 2020,\n    release_year = 2015,\n    length_minutes = 100,\n    total_reviews = 10000\n)\n\npredict(\n    model_lr_rating_extra,\n    newdata = predict_observation\n)\n\n       1 \n3.259518 \n\n\n\n\n\npredict_observation = pd.DataFrame(\n    {\n        'word_count': 12,\n        'age': 30,\n        'children_in_home': 1,\n        'review_year': 2020,\n        'release_year': 2015,\n        'length_minutes': 100,\n        'total_reviews': 10000\n    },\n    index = ['new_observation']\n)\n\nmodel_lr_rating_extra.predict(predict_observation)\n\nnew_observation   3.260\ndtype: float64\n\n\n\n\n\nIn our example we’re just getting a single prediction, but don’t let that hold you back! As we did before, you can predict an entire data set if you want, and use any values for the features you want. Feel free to try a different prediction of your choosing!\n\n\n2.7.2 Categorical features\n\nCategorical features can be added to a model just like any other feature. The main issue is that they have to be represented numerically, because models only work on numerically coded features and targets. The simplest and most common encoding is called a one-hot encoding scheme, which creates a new feature column for each category, and assigns a 1 if the observation has that category label, and a 0 otherwise. This is also called dummy coding when used for statistical models. Here is an example of what the coding looks like for the season feature. This is really all there is to it.\n\n\n\n\nTable 2.5: One-hot encoding of the season feature\n\n\n\n\n\n\n  \n    \n      rating\n      season\n      Fall\n      Summer\n      Winter\n      Spring\n    \n  \n  \n    2.70\nFall\n1\n0\n0\n0\n    4.20\nFall\n1\n0\n0\n0\n    3.70\nFall\n1\n0\n0\n0\n    2.70\nFall\n1\n0\n0\n0\n    2.40\nSummer\n0\n1\n0\n0\n    4.00\nSummer\n0\n1\n0\n0\n    1.80\nFall\n1\n0\n0\n0\n    2.40\nSummer\n0\n1\n0\n0\n    2.50\nWinter\n0\n0\n1\n0\n    4.30\nSummer\n0\n1\n0\n0\n  \n  \n  \n\n\n\n\n\n\n\nWhen using statistical models we don’t have to do this ourselves. Even other tools for machine learning models will typically have a way to identify and appropriately handle categorical features, even in very complex ways when it comes to deep learning models. What is important is to be aware that they require special handling, but often this is done behind the scenes. Now let’s do a quick example using a categorical feature with our data, and we’ll keep a numeric feature as well just for consistency.\n\nRPython\n\n\n\nmodel_lr_cat = lm(\n    rating ~ word_count + season,\n    data = df_reviews\n)\n\nsummary(model_lr_cat)\n\n\nCall:\nlm(formula = rating ~ word_count + season, data = df_reviews)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.91840 -0.36222  0.01327  0.35889  1.83715 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.342910   0.052971  63.109  &lt; 2e-16 ***\nword_count   -0.039446   0.003598 -10.963  &lt; 2e-16 ***\nseasonSpring -0.030051   0.062213  -0.483    0.629    \nseasonSummer  0.274325   0.044450   6.171 9.83e-10 ***\nseasonWinter -0.070034   0.059482  -1.177    0.239    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.572 on 995 degrees of freedom\nMultiple R-squared:  0.1759,    Adjusted R-squared:  0.1726 \nF-statistic: 53.09 on 4 and 995 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nmodel_lr_cat = smf.ols(\n    formula = \"rating ~ word_count + season\",\n    data = df_reviews\n).fit()\n\nmodel_lr_cat.summary(slim = True)\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.176\n\n\nModel:\nOLS\nAdj. R-squared:\n0.173\n\n\nNo. Observations:\n1000\nF-statistic:\n53.09\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n1.41e-40\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.3429\n0.053\n63.109\n0.000\n3.239\n3.447\n\n\nseason[T.Spring]\n-0.0301\n0.062\n-0.483\n0.629\n-0.152\n0.092\n\n\nseason[T.Summer]\n0.2743\n0.044\n6.171\n0.000\n0.187\n0.362\n\n\nseason[T.Winter]\n-0.0700\n0.059\n-1.177\n0.239\n-0.187\n0.047\n\n\nword_count\n-0.0394\n0.004\n-10.963\n0.000\n-0.047\n-0.032\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.176\n\n\nModel:\nOLS\nAdj. R-squared:\n0.173\n\n\nNo. Observations:\n1000\nF-statistic:\n53.09\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n1.41e-40\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.3429\n0.053\n63.109\n0.000\n3.239\n3.447\n\n\nseason[T.Spring]\n-0.0301\n0.062\n-0.483\n0.629\n-0.152\n0.092\n\n\nseason[T.Summer]\n0.2743\n0.044\n6.171\n0.000\n0.187\n0.362\n\n\nseason[T.Winter]\n-0.0700\n0.059\n-1.177\n0.239\n-0.187\n0.047\n\n\nword_count\n-0.0394\n0.004\n-10.963\n0.000\n-0.047\n-0.032\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nWe now see the usual output. There is word count again, with its slightly negative association with rating. And we have an effect for each season as well… except, wait a second, where is the fall effect? The coefficients are interpreted the same way - as we move one unit on x, we see a corresponding change in y. But moving from one category to another requires starting at some category in the first place - a reference point. So one category is chosen arbitrarily, but you would have control over this. In our model, ‘fall’ is chosen just because it is first alphabetically. So if we look at, for example, the effect of summer, we see an increase in the rating of 0.27 relative to fall. The same goes for the other seasons, they all represent a change relative to fall.\n\n2.7.2.1 Summarizing categorical features\nWhen we have a lot of categories, it’s not practical to look at the coefficients for each one, and even when there aren’t that many, we often prefer to get a sense of the total effect of the feature. For standard linear models, we can break down the target variance explained by the model into the variance explained by each feature, and this is called the ANOVA, or analysis of variance. It is not without its issues18, but it’s a good way to get a sense of whether a categorical (or other) feature as a whole is statistically significant.\n\nRPython\n\n\n\nanova(model_lr_cat)\n\n\n\n\nimport statsmodels.api as sm\n\nsm.stats.anova_lm(model_lr_cat)\n\n\n\n\n\n\n\n\nTable 2.6: ANOVA Table for Categorical Feature\n\n\n\n\n\n\n  \n    \n      Feature\n      df\n      sumsq\n      meansq\n      F stat.\n      p.value\n    \n  \n  \n    word_count\n1.0\n46.8\n46.8\n143.0\n&lt; 0.001\n    season\n3.0\n22.7\n7.6\n23.1\n&lt; 0.001\n    Residuals\n995.0\n325.6\n0.3\n\n\n  \n  \n  \n\n\n\n\n\n\n\nThe only reason to use this is for claims of statistical significance, which in this case, season is statistically significant. For practical purposes, the DF (degrees of freedom) represents the number of categories minus 1, and the F-statistic is a measure of the (mean sq) variance explained by the feature (mean square value divided by DF) relative to the (mean sq) variance not explained by the feature. The p-value is the probability of observing an F-statistic as extreme as the one observed, given that the null hypothesis is true. In this case, the null hypothesis is that the feature has no effect on the target. The p-value is less than 0.001, so we reject the null hypothesis and conclude that the feature has an effect on the target. Note that nothing here is different from what we saw in our previous regression models, and we can run an anova function on those too19.\n\n\n2.7.2.2 Group predictions\nA better approach to understanding categorical features for standard linear models is through what are called marginal effects, which can provide a kind of average prediction for each category while accounting for the other features in the model. Better still is to visualize these. It’s actually tricky to define ‘average’ when there are multiple features and interactions involved, so be careful, but we’d interpret the result similarly in those cases as best we can. In this case, we expect higher ratings for summer releases. We’ll return more to this concept in Section 3.3.3.\n\n\n\n\n\n\n\n\nFigure 2.5: Marginal Effects of Season on Rating\n\n\n\n\n\n\n\n\n2.7.3 Other model complexities\nThere are a lot more fun things we can do while still employing a linear model. We can add interactions between features, account for non-linear relationships, and enhance the linear model we’ve seen to improve predictions. We’ll talk more about these types of techniques throughout the rest of Parts I and II.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-assumptions",
    "href": "linear_models.html#sec-lm-assumptions",
    "title": "2  The Foundation",
    "section": "2.8 Assumptions and More",
    "text": "2.8 Assumptions and More\n\nEvery model you use has underlying assumptions which, if not met, could potentially result in incorrect inferences about the effects, performance, or predictive capabilities of the model. The standard linear regression model we’ve shown is no different, and it has a number of assumptions that must be met for it to be statistically valid. Briefly they are:\n\nThat your model is not grossly misspecified (e.g., you’ve included the right features and not left out important ones)\nThe data that you’re modeling reflects the population you want to make generalizations about\nThe model is linear in the parameters (i.e. no \\(e^\\beta_1\\) type stuff)\nThe features are not correlated with the error (prediction errors, unobserved causes)\nYour data observations are independent of each other\nThe prediction errors are homoscedastic (don’t have large errors with certain predictions vs low with others)\nNormality of the errors (i.e. your prediction errors). Another way to put it is that your target variable is normally distributed conditional on the features.\n\nThings a linear regression model does not assume:\n\nThat the features are normally distributed\n\nFor example, using categorical features is fine\n\nThat the relationship between the features and target is linear\n\nInteractions, polynomial terms, etc. are all fine\n\nThat the features are not correlated with each other\n\nThey usually are\n\n\nIf you do meet these assumptions, it doesn’t mean:\n\nYou have large effects\nYou have a well-performing model\nYou have causal effects\nYou (necessarily) have less uncertainty about your coefficients or predictions than other methods\n\nIf you don’t meet these assumptions, it doesn’t mean:\n\nThat your model will have poor predictions\nThat your conclusions will necessarily be incorrect or even different\n\nSo basically, whether or not you meet the assumptions of your model doesn’t actually say much about whether the model is great or terrible. For the linear regression model, if you do meet those assumptions, your coefficient estimates are unbiased20, and in general, your statistical inferences are valid ones. If you don’t meet the assumptions, there are alternative versions of the linear model you could use that would potentially address the issues. For example, data that runs over a sequence of time (time series data) violates the independence assumption, since observations closer in time are more likely to be similar than those farther apart. Violation of this assumption will result in\nBut we would use a time series or similar model instead to account for this. If normality is difficult to meet, you could assume a different data generating distribution. We’ll discuss some of these approaches explicitly in later chapters, but it’s also important to note that not meeting the assumptions for the baseline model may only mean you’ll prefer a different type of linear or other model to use in order to meet them.\n\n2.8.1 Assumptions with more complex models\nLet’s say you’re running some XGBoost or a Deep Linear Model and getting outstanding predictions. ‘Assumptions smumptions’ you say! And you might even be right! But if you want to talk confidently about feature contributions, or know something about the uncertainty in the predictions (which you’re assessing right?), well, maybe you might want to know if you’re meeting your assumptions. Some of them are:\n\nYou have enough data to make the model generalizable\nYour data isn’t biased (e.g., you don’t have 90% of your data from one region when you want to talk about a whole area)\nYou adequately sampled the hyperparameter space (e.g. you didn’t just use the defaults or a small grid search)\nYour observations are independent or at least exchangeable and don’t have data leakage, or you are explicitly modeling observation dependence\nThat all the parameter settings you set are correct or at least viable (e.g. you let the model run for a long enough set of iterations, your batch size was adequate, you had enough hidden layers, etc.)\n\nAnd if you want to talk about specific feature contributions, you are assuming:\n\nThe features are largely uncorrelated\nThe features largely do not interact (but then why are you doing a complex model that is inherently interactive), or that your understanding of feature contribution deals with the interactions\n\nThe take home message is that using models in more complex settings like machine learning doesn’t mean you don’t have to worry about theoretical and model assumptions, you still have much to consider!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-classification",
    "href": "linear_models.html#sec-lm-classification",
    "title": "2  The Foundation",
    "section": "2.9 Classification",
    "text": "2.9 Classification\nUp to this point we’ve been using a continuous, numeric target. But what about a categorical target? For example, what if we just had a binary target of whether a movie was good or bad? We will dive much more into classification models in our upcoming chapters, but it turns out that we can still formulate it as a linear model problem. The main difference is that we use a transformation of our linear combination of features, using what is sometimes called a link function, and we’ll need to use a different objective function rather than least squares, such as the binomial likelihood, to deal with the binary target. This also means we’ll move away from R2 as a measure of model fit, and look at something else, like accuracy.\nGraphically we can see it in the following way, which when compared with our linear model (Figure 2.2), doesn’t look much different. In what follows, we create our linear combination of features and put it through the sigmoid function, which is a common link function for binary targets21. The result is a probability, which we can then use to classify the observation as good or bad based on a chosen threshold. For example, we might say that any instance associated with a probability greater than or equal to 0.5 is classified as ‘good’, and less than that is classified as ‘bad’.\n\n\n\n\n\n\nFigure 2.6: A Linear Model with Transformation Can Be a Logistic Regression\n\n\n\nAs soon as we move away from the standard linear model and use transformations of our linear predictor, simple coefficient interpretation becomes difficult, sometimes exceedingly so. We will explore more of these types of models and how to interpret them in later chapters (e.g. Chapter 5).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-more",
    "href": "linear_models.html#sec-lm-more",
    "title": "2  The Foundation",
    "section": "2.10 More Linear Models",
    "text": "2.10 More Linear Models\n\nBefore we leave our humble linear model, let’s look at some others. Here is a brief overview of some of the more common ‘linear’ models you might encounter.\nGeneralized Linear Models and related\n\nTrue GLM e.g. logistic, poisson\nOther distributions: beta regression, tweedie, t (so-called robust), truncated\nPenalized regression: ridge, lasso, elastic net\nCensored outcomes: Survival models, tobit\n\nMultivariate/multiclass/multipart\n\nMultivariate regression (multiple targets)\nMultinomial/Categorical/Ordinal regression (&gt;2 classes)\nZero (or some number) -inflated/hurdle/altered\nMixture models and Cluster analysis\n\nRandom Effects\n\nMixed effects models (random intercepts/coefficients)\nGeneralized additive models (GAMs)\nSpatial models (CAR)\nTime series models (ARIMA)\nFactor analysis\n\nLatent Linear Models\n\nPCA, Factor Analysis\nMixture models\nStructural Equation Modeling, Graphical models generally\n\nAll of these are explicitly linear models or can be framed as such, and most require only a tweak or two from what you’ve already seen - e.g. a different distribution, a different link function, penalizing the coefficients, etc. In other cases, we can bounce from one to another and even get similar results. For example we can reshape our multivariate outcome to be amenable to a mixed model approach and get the exact same results. We can potentially add a random effect to any model, and that random effect can be based on time, spatial or other considerations. The important thing to know is that the linear model is a very flexible tool that expands easily, and allows you to model most of the types of outcomes we are interested in. As such, it’s a very powerful approach to modeling.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-wrap",
    "href": "linear_models.html#sec-lm-wrap",
    "title": "2  The Foundation",
    "section": "2.11 Wrapping Up",
    "text": "2.11 Wrapping Up\nLinear models such as the linear regression demonstrated in this chapter are a very popular tool for data analysis, and for good reason. They are relatively easy to implement and they are very flexible. They can be used for prediction, explanation, and inference, and they can be used for a wide variety of data types. There are also many tools at our disposal to help us use and explore them. But they are not without their limitations, and you’ll want to have more in your toolbox than just the approach we’ve seen so far.\n\n2.11.1 The common thread\nIn most of the chapters we want to highlight the connections between models you’ll encounter. Linear models are the starting point for modeling, and they can be used for a wide variety of data types and tasks. The linear regression with a single feature is identical to a simple correlation if the feature is numeric, a t-test if it is binary, and an ANOVA if it is categorical. We explored a more complex model with multiple features, and saw how to interpret the coefficients and make predictions. The creation of a combination of features to predict a target is the basis of all models, and as such the linear model we’ve just seen is the real starting point on your data science journey.\n\n\n2.11.2 Choose your own adventure\nNow that you’ve got the basics, where do you want to go?\n\nIf you want to know more about how to understand the model: Chapter 3\nIf you want a deeper dive into how we get the results from our model: Chapter 4\nIf you want to do some more modeling: Chapter 5, Chapter 6 or Chapter 7\nGot more data questions? Chapter 10\n\n\n\n2.11.3 Additional resources\nIf you are interested in a deeper dive into the theory and assumptions behind linear models, you can check out more traditional statistical/econometric treatments such as:\n\nGelman, Hill, and Vehtari (2020)\nGelman (2013)\nHarrell (2015)\nFahrmeir et al. (2021)\nFaraway (2014)\nWooldridge (2012)\nGreene (2017)\n\nFor more applied treatments, consider:\n\nNavarro (2018)\nWeed and Navarro (2021)\nKuhn and Silge (2023)\n\nBut there are many, many books on statistical analysis, linear models, and linear regression specifically. Texts tend to get more mathy and theoretical as you go back in time, to the mostly applied and code-based treatments today. You will likely need to do a bit of exploration to find one you like best. We also recommend you check out the many statistics and modeling based courses like those on Coursera, EdX, and similar, and the many tutorials and blog posts on the internet. Great demonstrations of specific topics can be found on youtube, blog posts and other places. Just start searching and you’ll find a lot of great resources!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#sec-lm-exercise",
    "href": "linear_models.html#sec-lm-exercise",
    "title": "2  The Foundation",
    "section": "2.12 Exercise",
    "text": "2.12 Exercise\nImport some data. Stick with the movie reviews data if you want and just try out other features, or maybe try the world happiness data 2018 data. You can find details about it in the appendix Section A.2, can download it here.\n\nFit a linear model, maybe keep it to three features or less\nGet all the predictions for the data, and try a prediction of at least one new observation of interest\nInterpret the coefficients\nAssess the model fit\n\n\n\n\n\nCohen, Jacob. 2009. Statistical Power Analysis for the Behavioral Sciences. 2. ed., reprint. New York, NY: Psychology Press.\n\n\nFahrmeir, Ludwig, Thomas Kneib, Stefan Lang, and Brian D. Marx. 2021. Regression: Models, Methods and Applications. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-662-63882-8.\n\n\nFaraway, Julian. 2014. “Linear Models with R.” Routledge & CRC Press. https://www.routledge.com/Linear-Models-with-R/Faraway/p/book/9781439887332.\n\n\nGelman, Andrew. 2013. “What Are the Key Assumptions of Linear Regression?  Statistical Modeling, Causal Inference, and Social Science.” https://statmodeling.stat.columbia.edu/2013/08/04/19470/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nGreene, William. 2017. Econometric Analysis - 8th Edition. https://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm.\n\n\nHarrell, Frank E. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. 2nd ed. Springer Series in Statistics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nKuhn, Max, and Julia Silge. 2023. Tidy Modeling with R. https://www.tmwr.org/.\n\n\nNavarro, Danielle. 2018. Learning Statistics with R. https://learningstatisticswithr.com.\n\n\nRovine, Michael J, and Douglas R Anderson. 2004. “Peirce and Bowditch.” The American Statistician 58 (3): 232–36. https://doi.org/10.1198/000313004X964.\n\n\nWeed, Ethan, and Danielle Navarro. 2021. Learning Statistics with Python — Learning Statistics with Python. https://ethanweed.github.io/pythonbook/landingpage.html.\n\n\nWooldridge, Jeffrey M. 2012. Introductory Econometrics: A Modern Approach. 5th edition. Mason, OH: Cengage Learning.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "linear_models.html#footnotes",
    "href": "linear_models.html#footnotes",
    "title": "2  The Foundation",
    "section": "",
    "text": "Regression in general is typically attributed to Galton, and correlation to Pearson, whose coefficient bearing his name is still the most widely used measure of association. Peirce & Bowditch were actually ahead of both (Rovine and Anderson 2004), but Bravais beat all of them.↩︎\nWe use the smf.ols approach because it is modeled on the R approach.↩︎\nIn most circumstances, if you ever have perfect prediction, or even near perfect prediction, the usual issues are that you have either asked a rather obvious/easy question of your data (e.g., predicting whether an image is of a human or a car), or have accidentally included the target in your features (or a combination of them) in some way.↩︎\nSome models such as the tree approaches outlined in Section 8.6 can directly predict categorical targets, but we still like and even prefer converting it to something like a probability.↩︎\nSome not as familiar with R should be aware that tibbles are a type of data frame. The name distinguishes them from the standard data frame, and they have some additional features that make them more user friendly.↩︎\nWord count is discrete- it can only take whole numbers like 3 or 20, and it is our only feature. Because of this, we can only make very limited predicted rating values, while the observed rating can take on many other values. Because of this, the raw plot would show a more banded result with many points overlapping, so we use a technique called jittering to move the points around a little bit so we can see them all. The points are still roughly in the same place, but they are moved around a little bit so we can see them all.↩︎\nOnce you move past OLS in statsmodels, obtaining uncertainty estimates for predictions is difficult due to the paucity of tools, especially for machine learning models, and practically non-existent for deep learning approaches. In practice, you can use bootstrapping to get a sense of the uncertainty, but this is often not a good estimate in many data scenarios and can be computationally expensive. On the other hand, conformal prediction tools are more developed in Python, and can provide a more reliable estimate of prediction uncertainty for any type of model, but are not as widely used as they should be.↩︎\nHistorically, people cite Cohen (2009) for effect size guidelines for simple models, but such guidelines are notoriously problematic. Rely on your own knowledge of the data, provide reasons for your conclusions, and let others draw their own. If you cannot tell what would constitute a notable change in your outcome of interest, you don’t know it well enough to interpret any model regarding it.↩︎\nMost statistical tables of this sort will use a t (student t distribution), Z (normal distribution), or F (F distribution) statistic. It doesn’t really matter for your purposes which is used by default, they provide the p-value of interest to claim statistical significance.↩︎\nYou can calculate this as pt(stat, df = model degrees of freedom, lower=FALSE)*2 in R, or usestats.t.cdf in Python. The model degrees of freedom are provided in the summary output (a.k.a. residual degrees of freedom), are used when obtaining the two-sided p-value, which is what we want in this case. When it comes to t and Z statistics, anything over 2 is statistically significant by the common standard of a p-value of .05 or less. Note that even though output will round it to zero, the true p-value can never be zero.↩︎\nThe interpretation regarding the CI is even more nuanced than this, but we’ll leave that for another time. For now, we’ll just say that the CI is a range of values that are good guesses for the true value. Your authors have used frequentist and Bayesian statistics for many years, and we are fine with both of them, because they both work well enough in the real world. Despite where this ranged estimate comes from, the vast majority use CIs in the same way, and they are a useful tool for understanding the uncertainty in our estimates.↩︎\nAny time we’re talking about MSE, we’re also talking about RMSE as it’s just the square root of MSE, so which you choose is mostly arbitrary.↩︎\nThe actual divisor for linear regression output depends on the complexity of the model, and in this case the sum of the squared errors is divided by N-2 (due to estimating the intercept and coefficient) instead of N. This is a technical detail that would only matter for data too small to make much of in the first place, and not important for our purposes here.↩︎\nIn the first depiction without \\(\\alpha\\), there is an additional column at the beginning of the matrix that is all ones, which is a way to incorporate the intercept into the model. However, most models that use a matrix as input will not have the intercept column, as it’s not part of the model estimation, is added behind the scenes, or is estimated separately.↩︎\nIt never ceases to amaze us how many papers show matrix/vector operations without clearly specifying the dimensions of what’s involved and/or what those dimensions specifically represent in terms of actual data, even though it’s key to understanding what’s being depicted. Even better would be simple code demonstration, but that’s rare.↩︎\nIn deep learning, models arrays are referred to as the more abstract representation of tensors, but for practical purposes the distinction doesn’t really matter for modeling, as the tensors are always represented as some n-dimensional array.↩︎\nA lot of statisticians and causal modeling folks get very hung up on the terminology here, but we’ll leave that to them as we’d like to get on with things. For our purposes, we’ll just say that we’re interested in the effect of a feature after we’ve accounted for the other features in the model.↩︎\nThere are many types of ANOVA, and different ways to calculate the variance values. One may notice the Python ANOVA result is different, even though the season coefficients and initial model is identical. R defaults with what is called Type II sums of squares, while the Python default uses Type I sums of squares. We won’t bore you with the details of their differences, and the astute modeler will not come to different conclusions because of this sort of thing, and you now have enough detail to look it up.↩︎\nFor those interested, for those features with one degree of freedom, all else being equal the F statistic here would just be the square of the t-statistic for the coefficients, and the p-value would be the same.↩︎\nThis means they are correct on average, not the true value. And if they were biased, this refers to statistical bias, and has nothing to do with the moral or ethical implications of the data, or whether the features themselves are biased in measurement. Culturally biased data is a different problem than statistical/prediction bias or measurement error, though they are not mutually exclusive. Statistical bias can more readily be tested, while other types of bias are more difficult to assess. Even statistical unbiasedness is not necessarily a goal, as we will see later Section 4.8.↩︎\nThe sigmoid function in this case is the inverse logistic function, and the resulting statistical model is called logistic regression. In other contexts the model would not be a logistic regression, but this is still a very commonly used activation function. But many others could potentially be used e.g. using a normal instead of logistic distribution, resulting in the so-called probit model.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Foundation</span>"
    ]
  },
  {
    "objectID": "knowing_models.html",
    "href": "knowing_models.html",
    "title": "3  Knowing Your Model",
    "section": "",
    "text": "3.1 Key Ideas",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#sec-knowing-key",
    "href": "knowing_models.html#sec-knowing-key",
    "title": "3  Knowing Your Model",
    "section": "",
    "text": "Metrics can help you assess how well your model is performing, and they can also help you compare different models.\nDifferent metrics can be used depending on the goals of your model.\nVisualizations can help you understand how your model is making predictions and which features are important.\nFeature importance is very difficult to ascertain even in the simplest of models, but there are tools to help you understand how much each feature contributes to a prediction.\n\n\n3.1.1 Why this matters\nIt’s never good enough to simply get model results. You need to know how well your model is performing and how it is making predictions. You also should be comparing your model to other alternatives. Doing so provides more confidence in your model and helps you to understand how it is working, and just as importantly, where it fails. This is actionable knowledge.\n\n\n3.1.2 Good to know\nThis takes some of the things we see in other chapters on linear models and machine learning, so we’d suggest having the linear model basics down pretty well.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#sec-knowing-model-metrics",
    "href": "knowing_models.html#sec-knowing-model-metrics",
    "title": "3  Knowing Your Model",
    "section": "3.2 Understanding the Model",
    "text": "3.2 Understanding the Model\nA first step in understanding our model can be done with summary statistics, typically called metrics. Regression and classification have different metrics for assessing model performance. We want to give you a sample of some of the more common ones, but we also want to acknowledge that there are many more that you can use, and any might be useful. We would always recommend looking at a few different metrics for any given model to get a better sense of how your model is performing.\nTable 3.1 illustrates some of the most commonly used performance metrics. Just because these are popular or applicable for your situation, doesn’t mean they are the only ones you can or even should use. Nothing keeps you from using more than one metric for assessment, and in fact, it is often a good idea to do so. You should have a working knowledge of these.\n\n\n\n\n\n\n\nTable 3.1: Commonly used performance metrics in machine learning.\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Metric\n      Description\n      Other Names/Notes\n    \n  \n  \n    \n      Regression\n    \n    RMSE\nRoot mean squared error\nMSE (before square root)\n    MAE\nMean absolute error\n\n    MAPE\nMean absolute percentage error\n\n    RMSLE\nRoot mean squared log error\n\n    R-squared\nAmount of variance shared by predictions and target\nCoefficient of determination\n    Deviance/AIC\nGeneralization of sum of squared error \nAlso \"deviance explained\" for similar R-sq interpretation\n    \n      Classification\n    \n    Accuracy\nProportion correct\nError rate is 1 - Accuracy\n    Precision\nProportion of positive predictions that are correct\nPositive Predictive Value\n    Recall\nProportion of positive samples that are predicted correctly\nSensitivity, True Positive Rate\n    Specificity\nProportion of negative samples that are predicted correctly\nTrue Negative Rate\n    Negative Predictive Value\nProportion of negative predictions that are correct\n\n    F1\nHarmonic mean of precision and recall\nF-Beta1\n    AUC\nArea under the ROC curve\n\n    False Positive Rate\nProportion of negative samples that are predicted incorrectly\nType I Error, alpha\n    False Negative Rate\nProportion of positive samples that are predicted incorrectly\nType II Error, beta, Power is 1 - beta\n    Phi\nCorrelation between predicted and actual\nMatthews Correlation\n    Log loss\nNegative log likelihood of the predicted probabilities\n\n  \n  \n  \n    \n      1 Beta = 1 for F1\n    \n  \n\n\n\n\n\n\n\n\n\n\n3.2.1 Regression metrics\nThe primary goal of our endeavor is to come up with a predictive model. The closer our model predictions are to the observed target values, the better our model is performing. As we saw in the above table, when we have a numeric target there are quite a few metrics that help us understand prediction-target correspondence, so let’s look at some of those.\nBut before we create a model to get us started, we are going to read in our data and then create two different splits within our data: a training set and a test set. In other words, we are going to partition our data so that we can train a model and then see how well that model performs with data it hasn’t seen1. For more on this process and the reasons why we do it see -Section 7.4 and -Section 7.6. For now, we just need to know that assessing prediction error on the test set will give us a better estimate of our metric of choice.\n\n\n\n\n\n\nSplitting Data\n\n\n\n\n\nThis basic split is the foundation of cross-validation. Cross-validation is a method for partitioning data into training and non-training sets in a way that allows you to better understand the model’s performance. You’ll find more explicit demonstration of how to do this in the machine learning chapter Chapter 8.\n\n\n\n\nRPython\n\n\n\n# all data found on github repo\ndf_reviews = read.csv('https://tinyurl.com/moviereviewsdata')\n\nset.seed(123)\n\ninitial_split = sample(\n    x = 1:nrow(df_reviews), \n    size = nrow(df_reviews) * .75, \n    replace = FALSE\n)\n\ndf_train = df_reviews[initial_split, ]\n\ndf_test = df_reviews[-initial_split, ]\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\n# all data found on github repo\ndf_reviews = pd.read_csv('https://tinyurl.com/moviereviewsdata')\n\ndf_train, df_test = train_test_split(\n    df_reviews, \n    test_size = 0.25, \n    random_state = 123\n)\n\n\n\n\nYou’ll notice that we created training data with 75% of our data and we will use the other 25% to test our model. With training data in hand, let’s produce a model to predict review rating. We’ll use the standardized (scaled _sc) versions of several features, and use the ‘year’ features starting at year 0, which is the earliest year in our data. Finally, we also include the genre of the movie as a categorical feature.\n\nRPython\n\n\n\nmodel_lr_train = lm(\n    rating ~ \n        review_year_0 \n        + release_year_0 \n        + age_sc \n        + length_minutes_sc \n        + total_reviews_sc \n        + word_count_sc \n        + genre \n        ,\n        df_train\n)\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# we'll use 'features' later also\nfeatures = [\n    \"review_year_0\", \n    \"release_year_0\",\n    \"age_sc\", \n    \"length_minutes_sc\", \n    \"total_reviews_sc\", \n    \"word_count_sc\", \n    \"genre\", \n    ]\n\nmodel =  'rating ~ ' + \" + \".join(features)\n\nmodel_lr_train = smf.ols(formula = model, data = df_train).fit()\n\n\n\n\nNow that we have a model on our training data, we can use it to make predictions on our test data:\n\nRPython\n\n\n\npredictions = predict(model_lr_train, newdata = df_test)\n\n\n\n\npredictions = model_lr_train.predict(df_test)\n\n\n\n\nThe goal now is to find out how close our predictions match reality. Let’s look at them first:\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously, our points do not make a perfect line on the left, which would indicate perfect prediction. Also, the distribution of our values suggests we’re over predicting on the lower end and under predicting on the higher end of the target’s range. But we’d like to determine how far off we are in a general sense. There are a number of metrics that can be used to measure this. We’ll go through a few of them here.\n\n3.2.1.1 R-squared\nAnyone that has done linear regression has come across the \\(R^2\\) value. It is a measure of how well the model explains the variance in the target. One way to calculate it is as follows:\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\]\nwhere \\(y_i\\) is the observed value, \\(\\hat{y}_i\\) is the predicted value, and \\(\\bar{y}\\) is the mean of the observed values. The \\(R^2\\) value is a measure of how much variance in the target (the denominator) is attributable to the model’s predictions (numerator). It is a value between 0 and 1, with 1 indicating that the model explains all of the variance in the target.\nMore simply, \\(R^2\\) is the squared correlation of our predicted values and the target. In that sense it can almost always be useful as a descriptive measure, just like we use means and standard deviations in exploratory data analysis. However, it is not so great at telling us about predictive quality. Why? Take your predictions from our rating model, and add 10 to them, or make them all negative. In both cases your predictions would be ridiculous, but your \\(R^2\\) will be the same. Another problem is that for training data, \\(R^2\\) will always increase as you add more features to your model, whether they are useful or pure noise! This is why we use other metrics to assess predictive quality.\n\nRPython\n\n\n\nresidual_ss = sum((df_test$rating - predictions)^2)\ntotal_ss = sum((df_test$rating - mean(df_test$rating))^2)\n\n1 - residual_ss / total_ss\n\n[1] 0.5193109\n\nyardstick::rsq_trad_vec(df_test$rating, predictions)\n\n[1] 0.5193109\n\n# conceptually identical, but slight difference due\n# to how internal calculations are done (not shown)\n# cor(df_test$rating, predictions)^2 \n# yardstick::rsq_vec(df_test$rating, predictions)\n\n\n\n\nfrom sklearn.metrics import r2_score\n\nresidual_ss = np.sum((df_test.rating - predictions)**2)\ntotal_ss = np.sum((df_test.rating - np.mean(df_test.rating))**2)\n\n1 - residual_ss / total_ss\n\n0.508431158347433\n\nr2_score(df_test.rating, predictions)\n\n0.508431158347433\n\n\n# conceptually identical, but slight difference due to\n# how calculations are done (not shown)\n# np.corrcoef(df_test.rating, predictions)[0, 1]**2\n\n\n\n\n\n\n\n\n\n\nR-squared variants\n\n\n\n\n\nThere are different versions of R-squared. ‘Adjusted’ R-squared is a common one, and it penalizes the model for adding features that don’t really explain the target variance. This is a nice sentiment, but its difference versus the standard R-squared would only be noticeable for very small datasets. Some have also attempted to come up with R-squared values that are more appropriate for GLMs for count, binary and other models. Unfortunately, these ‘pseudo-R-squared’ values are not as interpretable as the original R-squared, and generally suffer several issues.\n\n\n\n\n\n3.2.1.2 Mean squared error\nOne of the most common performance metrics for numeric targets is the mean squared error (MSE) and its square root, root mean squared error (RMSE). The MSE is the average of the squared differences between the predicted and actual values. It is calculated as follows:\n\\[\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nMSE penalizes large errors more- since errors are squared, the larger the error, the larger the penalty. As mentioned, the root mean squared error (RMSE) is just the square root of the MSE. Like MSE, RMSE likewise penalizes large errors, but if you want a metric that is in the same units as the original target data, RMSE is the metric for you. It is calculated as follows:\n\\[\\text{RMSE} = \\sqrt{\\text{MSE}}\\]\n\nRPython\n\n\n\nmse = mean((df_test$rating - predictions)^2)\n\nmse\n\nyardstick::rmse_vec(df_test$rating, predictions)^2\n\n[1] 0.2133124\n[1] 0.2133124\n\n\n\nsqrt(mse)\n\nyardstick::rmse_vec(df_test$rating, predictions)\n\n[1] 0.4618576\n[1] 0.4618576\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, root_mean_squared_error\n\nmse = np.mean((df_test.rating - predictions)**2)\n\nmse\n\nmean_squared_error(df_test.rating, predictions)\n\n0.20798285555421575\n0.20798285555421575\n\n\n\nnp.sqrt(mse)\n\nroot_mean_squared_error(df_test.rating, predictions)\n\n0.4560513738102493\n0.4560513738102493\n\n\n\n\n\n\n\n3.2.1.3 Mean absolute error\nThe mean absolute error (MAE) is the average of the absolute differences between the predicted and observed values. It is calculated as follows:\n\\[\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\]\nMAE is a great metric when all you really want to know is how far off your predictions are from the observed values. It is not as sensitive to large errors as the MSE.\n\nRPython\n\n\n\nmean(abs(df_test$rating - predictions))\n\nyardstick::mae_vec(df_test$rating, predictions)\n\n[1] 0.352024\n[1] 0.352024\n\n\n\n\n\nfrom sklearn.metrics import mean_absolute_error\n\nnp.mean(abs(df_test.rating - predictions))\n\nmean_absolute_error(df_test.rating, predictions)\n\n0.3704072983307527\n0.3704072983307527\n\n\n\n\n\n\n\n3.2.1.4 Mean absolute percentage error\nThe mean absolute percentage error (MAPE) is the average of the absolute differences between the predicted and observed values, expressed as a percentage of the observed values. It is calculated as follows:\n\\[MAPE = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{|y_i - \\hat{y}_i|}{y_i}\\]\n\nRPython\n\n\n\nmean(\n  abs(df_test$rating - predictions) / \n    df_test$rating\n) * 100\n\nyardstick::mape_vec(df_test$rating, predictions)\n\n[1] 12.85525\n[1] 12.85525\n\n\n\n\n\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nnp.mean(\n    abs(df_test.rating - predictions) / \n    df_test.rating\n) * 100\n\nmean_absolute_percentage_error(df_test.rating, predictions) * 100\n\n13.464399850975898\n13.464399850975898\n\n\n\n\n\n\n\n3.2.1.5 Which regression metric should I use?\nIn the end, it won’t hurt to look at a few of these metrics to get a better idea of how well your model is performing. You will always be using these metrics to compare different models, so use a few of them to get a better sense of how well your models are performing relative to one another. Does adding a feature help drive down RMSE, indicating that the feature helps to reduce large errors? In other words, does adding complexity to your model provide a big reduction in error? If adding features doesn’t help reduce error, do you really need to include it in your modelU+0203D;\n\n\n\n3.2.2 Classification metrics\nWhenever we are classifying outcomes, we don’t have the same ability to compare a predicted score to an observed score – instead, we are going to use the predicted probability of an outcome, establish a cut-point for that probability, convert everything below that cut-point to 0, and then convert everything at or above that cut-point to 1. We can then compare a table predicted versus target classes, typically called a confusion matrix2.\nLet’s start with a model to predict whether a review is “good” or “bad”. We will use the same training and testing data that we created above. Explore the summary output if desired (not shown), but we will focus on the predictions and metrics.\n\nRPython\n\n\n\nmodel_class_train = glm(\n    rating_good ~ \n        genre + review_year_0 \n        + release_year_0 \n        + age_sc \n        + length_minutes_sc \n        + total_reviews_sc \n        + word_count_sc \n        + genre     \n        , \n        df_train, \n        family = binomial\n)\n\nsummary(model_class_train)\n\n# a numeric version to use later\ny_target_testing_bin = ifelse(df_test$rating_good == \"good\", 1, 0)\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nmodel =  'rating_good ~ ' + \" + \".join(features)\n\nmodel_class_train = smf.glm(\n    formula = model,\n    data = df_train,\n    family = sm.families.Binomial()\n).fit()\n\n\n# model_class_train.summary() \n\n\n\n\nNow that we have our model trained, we can use it to get the predicted probabilities for each observation3.\n\nRPython\n\n\n\npredicted_prob = predict(\n    model_class_train,\n    newdata = df_test,\n    type = \"response\"\n)\n\n\n\n\npredicted_prob = model_class_train.predict(df_test)\n\n\n\n\nWe are going to take those probability values and make a decision to convert everything at or above .5 to the positive class (a “good” review). It is a bold assumption, but one that we will make at first!\n\nRPython\n\n\n\npredicted_class = ifelse(predicted_prob &gt;= .5 , 1, 0)\n\n\n\n\npredicted_class = np.where(predicted_prob &gt;= .5, 1, 0)\n\npredicted_class = pd.Series(predicted_class)\n\n\n\n\n\n3.2.2.1 Confusion matrix\nThe confusion matrix is a table that shows the number of correct and incorrect predictions made by the model. It’s easy enough to get one from scratch, but we recommend using a function that will give you a nice table, and possibly all of the metrics you need along with it. To get us started, we can use a package function that will take our predictions and observed target as input to create the basic table.\n\nRPython\n\n\n\nrating_cm = mlr3measures::confusion_matrix(\n    factor(df_test$rating_good),\n    factor(predicted_class),\n    positive = \"1\"\n)\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nrating_cm = confusion_matrix(df_test.rating_good, predicted_class)\n\n\n\n\n\n\n\n\nTable 3.2: Example confusion matrix\n\n\n\n\n\n\n  \n    \n       \n      True 0\n      True 1\n    \n  \n  \n    Predicted 0\nTN: 84\nFN: 22\n    Predicted 1\nFP: 27\nTP: 117\n  \n  \n  \n\n\n\n\n\n\n\n\nTN: A True Negative is an outcome where the model correctly predicts the negative class – the model correctly predicted that the review was not good.\nFN: A False Negative is an outcome where the model incorrectly predicts the negative class – the model incorrectly predicted that the review was not good.\nFP: A False Positive is an outcome where the model incorrectly predicts the positive class – the model incorrectly predicted that the review was good.\nTP: A True Positive is an outcome where the model correctly predicts the positive class – the model correctly predicted that the review was good.\n\nIn an ideal world, we would have all of our observations fitting nicely in the diagonal of that table. Unfortunately, we don’t live in that world and the more values we have in the off diagonal (i.e., in the FN and FP spots), the worse our model is at classifying outcomes.\nLet’s look at some metrics that will help to see if we’ve got a suitable model or not. We’ll describe each, then show them all after.\n\n\n\n3.2.2.2 Accuracy\nAccuracy’s allure is in its simplicity and because we use it for so many things in our everyday affairs. The accuracy is the proportion of correct predictions made by the model. Of all the metrics to assess the quality of classification, accuracy is the easiest to cheat. If you have any class imbalance (i.e., one class within the target has far more observations than the other), you can get a high accuracy by simply predicting the majority class all of the time! To get around the false sense of confidence that accuracy alone can promote, we can look at a few other metrics.\n\n\n\n\n\n\nAccuracy is not enough\n\n\n\n\n\nAccuracy is the first thing you see and the last thing that you trust! Seriously, accuracy alone should not be your sole performance metric unless you have a perfectly even split in the target! If you find yourself in a meeting where people are presenting their classification models and they only talk about accuracy, you should be very skeptical of their model; this is especially true when those accuracy values seem too good to be true. At the very least, always be ready to compare it to the baseline rate, or prevalence of the majority class.\n\n\n\n\n\n3.2.2.3 Sensitivity/Recall/True positive rate\nSensitivity, also known as recall or the true positive rate, is the proportion of observed positives that are correctly predicted by the model. If your focus is on the positive class above all else, sensitivity is the metric for you.\n\n\n3.2.2.4 Specificity/True negative rate\nSpecificity, also known as the true negative rate, is the proportion of observed negatives that are correctly predicted as such. If you want to know how well your model will work with the negative class, specificity is a great metric.\n\n\n3.2.2.5 Precision/Positive predictive value\nThe precision is the proportion of positive predictions that are correct, and is often a key metric in many business use cases. While similar to sensitivity, precision focuses on positive predictions, while sensitivity focuses on observed positive cases.\n\n\n3.2.2.6 Negative predictive value\nThe negative predictive value is the proportion of negative predictions that are correct, and is the complement to precision.\nLet’s see how we’d do this ourselves. We’ll create a basic confusion matrix then extract the values to create the metrics we need.\n\nRPython\n\n\n\nconfusion_matrix = table(\n    predicted_class,\n    df_test$rating_good\n)\n\nTN = confusion_matrix[1, 1]\nTP = confusion_matrix[2, 2]\nFN = confusion_matrix[1, 2]\nFP = confusion_matrix[2, 1]\n\nacc = (TP + TN) / sum(confusion_matrix)  # accuracy\ntpr = TP / (TP + FN)  # sensitivity, true positive rate, recall\ntnr = TN / (TN + FP)  # specificity, true negative rate\nppv = TP / (TP + FP)  # precision, positive predictive value\nnpv = TN / (TN + FN)  # negative predictive value\n\n\n\n\nconfusion_matrix = pd.crosstab(\n    predicted_class,\n    df_test.rating_good.reset_index(drop=True), \n).to_numpy()\n\nTN = confusion_matrix[0, 0]\nTP = confusion_matrix[1, 1]\nFN = confusion_matrix[0, 1]\nFP = confusion_matrix[1, 0]\n\nacc = (TP + TN) / np.sum(confusion_matrix)  # accuracy\ntpr = TP / (TP + FN)  # sensitivity, true positive rate, recall\ntnr = TN / (TN + FP)  # specificity, true negative rate\nppv = TP / (TP + FP)  # precision, positive predictive value\nnpv = TN / (TN + FN)  # negative predictive value\n\n\n\n\nNow that we have a sense of some metrics, let’s get a confusion matrix and stats using packages that will give us a lot of these metrics at once. In both cases we have an 0/1 integer where 0 is a rating of “bad” and 1 is “good”.\n\nRPython\n\n\nWe use mlr3verse in the ML chapters, so we’ll use it here too. Though our predictions are 0/1, we need to convert it to a factor for this function.\n\n# note the columns and rows are reversed from our previous confusion matrix\ncm = mlr3measures::confusion_matrix(\n    factor(df_test$rating_good),  # requires a factor\n    factor(predicted_class),\n    positive = \"1\"  # class 1 is 'good'\n)\n\ntibble(\n    metric = c('ACC', 'TPR', 'TNR', 'PPV', 'NPV'),\n    ours = c(acc, tpr, tnr, ppv, npv),\n    package = cm$measures[c('acc', 'tpr', 'tnr', 'ppv', 'npv')]\n)\n\n\n\nWe find pycm to be a great package for this purpose, practically every metric you can think of is available. You can also use sklearn.metrics and its corresponding classification_report function.\n\nfrom pycm import ConfusionMatrix\n\ncm = ConfusionMatrix(\n    df_test.rating_good.to_numpy(), \n    predicted_class.to_numpy(), \n    digit = 3\n)\n\n# print(cm) # lots of stats!\n\npackage_result = [\n    cm.class_stat[stat][1] # get results specific to class 1\n    for stat in  ['ACC', 'TPR', 'TNR', 'PPV', 'NPV']\n]\n\npd.DataFrame({\n    'metric':['ACC', 'TPR', 'TNR', 'PPV', 'NPV'],\n    'ours': [acc, tpr, tnr, ppv, npv],\n    'package': package_result\n})\n\n\n\n\nSo now we have demystified some classification metrics as well! Your results may be slightly different due to the random nature of the data splits, but they should be very similar and should match the package results regardless.\n\n\n\n\nTable 3.3: Classification Results\n\n\n\n\n\n\n  \n    \n      metric\n      ours\n      package\n    \n  \n  \n    ACC\n0.804\n0.804\n    TPR\n0.842\n0.842\n    TNR\n0.757\n0.757\n    PPV\n0.812\n0.812\n    NPV\n0.792\n0.792\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n3.2.2.7 Ideal decision points for classification\nEarlier when we obtained the predicted class, and subsequently all the metrics based on it, we used a predicted probability value of 0.5 as a cutoff for a ‘good’ vs. a ‘bad’ rating, and this is usually the default if we don’t specify it explicitly. Assuming that this is the best for a given situation is actually a bold assumption on our part, and we should probably make sure that the cut-off value we choose is going to offer us the best result given the modeling context.\nBut what is the ‘best’ result? That’s going to depend on the situation. If we are predicting whether a patient has a disease, we might want to minimize false negatives, since if we miss the diagnosis, the patient could be in serious trouble. Meanwhile if we are predicting whether a transaction is fraudulent, we might want to minimize false positives, since if we flag a transaction as fraudulent when it isn’t, we could be causing a lot of trouble for the customer. In other words, we might want to maximize sensitivity or specificity, respectively.\nWhatever we decide, we ultimately are just shifting the metrics around relative to one another. As an easy example, if we were to classify all of our observations as ‘good’, we would have a sensitivity of 1 because all good ratings would be classified correctly. However, our positive predictive value would not be 1, and we’d have a specificity of 0. No matter which cutpoint we choose, we are going to have to make a tradeoff between these metrics.\nWhere this comes into play is with model selection, where we choose a model based on a particular metric, and something we will talk about very soon. If we are comparing models based on accuracy, we might choose a different model than if we are comparing based on sensitivity. And given a particular threshold, we might choose a different model based on the same metric, than we would have with a different threshold.\nTo help us with the task of choosing a threshold, we will start by creating what’s called a Receiver Operating Characteristic (ROC) curve. This curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The area under the curve (AUC) is a measure of how well the model is able to distinguish between the two classes. The closer the AUC is to 1, the better the model is at distinguishing between the two classes. The AUC is a very popular metric because it is not sensitive to our threshold, and actually concerns two metrics we are often interested in4.\n\nRPython\n\n\n\nroc = performance::performance_roc(model_class_train, new_data = df_test)\nroc\n\n# requires the 'see' package\nplot(roc) \n\n\n\n\nfrom sklearn.metrics import roc_curve, auc, RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(\n    df_test.rating_good, \n    predicted_prob\n)\n\nRocCurveDisplay(fpr=fpr, tpr=tpr).plot()\nauc(fpr, tpr)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: ROC curve and AUC value\n\n\n\n\n\nWith ROC curves and AUC values in hand, now we can find the ideal cut-point for balancing the TPR and FPR. There are different ways to do this, but one common way is to use the Youden’s J statistic, which both of the following do.\n\nRPython\n\n\n\n\n# produces the same value as before\nroc_ = pROC::roc(df_test$rating_good, predicted_prob)\nthreshold = pROC::coords(roc_, \"best\", ret = \"threshold\")\n\npredictions = ifelse(\n    predict(model_class_train, df_test, type='response') &gt;= threshold$threshold, \n    1, \n    0\n)\n\ncm_new = mlr3measures::confusion_matrix(\n    factor(df_test$rating_good), \n    factor(predictions), \n    positive = \"1\"\n)\n\ntibble(\n    threshold = threshold,\n    TNR = cm_new$measures['tnr'],\n    TPR = cm_new$measures['tpr']\n)\n\n# A tibble: 1 × 3\n  threshold$threshold   TNR   TPR\n                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1               0.505 0.766 0.835\n\n\n\n\n\ncut = thresholds[np.argmax(tpr - fpr)]\n\npd.DataFrame({\n    'threshold': [cut],\n    'TNR': [1 - fpr[np.argmax(tpr - fpr)]],\n    'TPR': [tpr[np.argmax(tpr - fpr)]]\n})\n\n   threshold   TNR   TPR\n0      0.483 0.726 0.887\n\n\n\n\n\nThe result is a “best” decision cut-point for converting our predicted probabilities to classes, though again, there are different and equally valid ways of going about this. The take home point is that instead of being naive about setting our probability to .5, this will provide a cut-point that will lead to a more balanced result that recognizes other metrics that are important beyond accuracy. We will leave it to you to take that ideal cut-point value and update your metrics to see how much of a difference it will make.\nNote that this only changes the values relative to one another, not the overall performance of the model - the actual predicted probabilities don’t change after all. For example, accuracy may go down while recall increases. You’ll need to match these metrics to your use case to see if the change is worth it. Whether it is a meager, modest, or meaningful improvement is going to vary from situation to situation, as will how you determine if your model is “good” or “bad”. Is this a good model? Are you more interested in correctly identifying the positive class, or the negative class? Are you more interested in avoiding false positives/negatives? These are all questions that you will need to answer depending on the modeling context.\n\n\n\n3.2.3 Model selection & comparison\nAnother important way to understand our model is by looking at how it compares to other models in terms of performance, however we choose to define that. One common way we can do this is by comparing models based on the metric(s) of our choice, for example, with RMSE or AUC. Let’s see this in action for our regression model. Here we will compare three models: one with three features, our original model, and the three feature model with interactions with genre. Our goal will be to see how these perform on the test set based on RMSE.\n\nRPython\n\n\n\n# create the models\nmodel_lr_3feat = lm(\n    rating ~ review_year_0 + release_year_0 + age_sc,\n    df_train\n)\n\nmodel_lr_interact = lm(\n    rating ~ review_year_0 * genre + release_year_0 * genre + age_sc * genre, \n    df_train\n)\n\nmodel_lr_train = lm(\n    rating ~ \n        review_year_0 \n        + release_year_0 \n        + age_sc \n        + length_minutes_sc \n        + total_reviews_sc \n        + word_count_sc \n        + genre \n        , \n        df_train\n)\n\n# get the predictions, calculate RMSE\nresult = map(\n    list(model_lr_3feat, model_lr_train, model_lr_interact), \n    ~ predict(.x, newdata = df_test)\n    ) |&gt; \n    map_dbl(\n        ~ yardstick::rmse_vec(df_test$rating, .)\n    )\n\n\n\n\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import root_mean_squared_error\n\n# create the models\nmodel_lr_3feat = smf.ols(\n    formula='rating ~ review_year_0 + release_year_0 + age_sc',\n    data=df_train\n).fit()\n\nmodel_lr_interact = smf.ols(\n    formula='rating ~ review_year_0 * genre + release_year_0 * genre + age_sc * genre',\n    data=df_train\n).fit()\n\nmodel_lr_train = smf.ols(\n    formula=\n    '''\n    rating ~ \n    review_year_0\n    + release_year_0\n    + age_sc\n    + length_minutes_sc\n    + total_reviews_sc\n    + word_count_sc\n    + genre\n    ''' \n    ,\n    data=df_train\n).fit()\n\nmodels = [model_lr_3feat, model_lr_train, model_lr_interact]\n\n# get the predictions, calculate RMSE\nresult = pd.DataFrame({\n    'model': [\"3 features\", \"original\", \"3 feat+interact\"],\n    'rmse': [\n        root_mean_squared_error(\n            df_test.rating, \n            model.predict(df_test[features])\n        )\n        for model in models\n    ]\n})\n\n\n\n\n\n\n\n\nTable 3.4: RMSE for different models\n\n\n\n\n\n\n  \n    \n      model\n      rmse\n    \n  \n  \n    original\n0.46\n    3 feat+interact\n0.55\n    3 features\n0.63\n  \n  \n  \n\n\n\n\n\n\n\nIn this case, the three feature model does worst, but adding interactions of those features with genre improves the model. However, we see that our original model with 7 features has the lowest RMSE, indicating that it is the best model under these circumstances. This is a simple example, but it is a typical way to compare models that you would use frequently. The same approach would work for classification models, just using an appropriate metric like AUC or F1.\nAnother thing to consider is that even with a single model, the model fitting procedure is always comparing a model with the current parameter estimates, or more generally the current objective function value, to a previous one with other parameter estimates or objective function value. In this case, our goal is model selection, or how we choose the best result from a single model. While this is an automatic process here, the details of how this actually happens is the focus of Chapter 4. In other cases, we are selecting models through the process of cross-validation (Section 7.6), but the idea is largely the same in that we are comparing our current parameter estimates to other possibilities. We are always doing model selection and comparison, and as such we’ll be demonstrating these often.\n\n\n3.2.4 Model visualization\nA key method for understanding how our model is performing is through visualization. You’ll recall that we started out way back by look at the predicted values against the observed values to see if there was any correspondence (Figure 2.3), but another key way to understand our model is to look at the residuals, or errors in prediction, which again is the difference in our prediction versus the observed value. Here are a couple plots that can help us understand our model:\n\nResiduals vs. Fitted: This type of plot shows predicted values vs. the residuals (or some variant of the residuals, like their square root). If you see a pattern, that potentially means your model is not capturing something in the data. For example, if you see a funnel shape, that would suggest that you are systematically having worse predictions for some part of the data. For some plots, patterns may suggest an underlying nonlinear relationship in the data is yet to be uncovered. For our main regression model, we don’t see any patterns which would indicate that the model has a notable prediction issue of some kind.\n\n\n\n\n\n\n\n\n\nFigure 3.3: Residuals vs. Fitted plot for a regression model\n\n\n\n\n\n\nTraining/Test Performance: For iterative approaches like deep learning, we may want to see how our model is performing across iterations, typically called epochs. We can look at the training and testing performance to see if our model is overfitting or underfitting. We can actually do this with standard models as well if the estimation approach is iterative, but it’s not as common. We can also visualize performance across samples of the data, such as in cross-validation. The following shows performance for a model similar to the multilayer perceptron (MLP) model demonstrated later (Section 8.7), where we can see we get to a relatively low objective function value after just a few epochs.\n\n\n\n\n\n\n\n\n\nFigure 3.4: MSE Loss over 25 epochs in an MLP model\n\n\n\n\n\n\nPosterior Predictive Check: This is a basic comparison of predicted vs. observed target values. We simulate the target based on the model parameter estimates and their uncertainty, and compare that distribution to the observed target distribution. If the two distributions are similar, then the model is doing a good job of capturing the target distribution. This plot is ubiquitous in Bayesian modeling, but can be used for any model that has uncertainty estimates or is otherwise able to generate random draws of the target distribution. For our regression model, our predictions match the target distribution well.\n\n\n\n\n\n\n\n\n\nFigure 3.5: Posterior Predictive Check for a regression model\n\n\n\n\n\n\nOther Plots: Other plots may look at the distribution of residuals, check for extreme values, see if there is an overabundance of zero values, and other issues, some of which may be specific to the type of model you are using.\n\nThe following shows how to get a residuals vs. fitted plot and a posterior predictive check.\n\nRPython\n\n\n\nperformance::check_model(model_lr_train, check = c('linearity', 'pp_check'))\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Residual Plot\nsns.residplot(\n    x = model_lr_train.fittedvalues, \n    y = df_train.rating, \n    lowess = True, \n    line_kws={'color': 'red', 'lw': 1}\n)\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs. Fitted')\nplt.show()\n\n\n## Posterior Predictive Check\n# get the model parameters\npp = model_lr_train.model.get_distribution(\n    params = model_lr_train.params, \n    scale  = model_lr_train.scale, \n    exog   = model_lr_train.model.exog\n)\n\n# Generate 10 simulated predictive distributions\npp_samples = [pp.rvs() for _ in range(10)]\n\n# Plot the distribution of pp_samples\nfor sample in pp_samples:\n    sns.kdeplot(sample, label='pp.rvs()', alpha=0.25)\n\n# Overlay the density plot of df_train.rating\nsns.kdeplot(\n    df_train.rating.to_numpy(), \n    label='df_train.rating', \n    linewidth=2\n)\n\nplt.xlabel('Rating')\nplt.ylabel('Density')\nplt.title('Distribution of predictions vs. observed rating')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTests of Assumptions\n\n\n\n\n\nFor standard GLM models there are an abundance of statistical tests available for some of these checks, for example heterogeneity of variance, or whether your residuals are normally distributed. On a practical level, these are not usually helpful, and often misguided. For example, if you have a large sample size, you will almost always reject the null hypothesis that your residuals are normally distributed. It also starts the turtles all the way down problem of whether you need to check the assumptions of your test of assumptions! We prefer the ‘seeing is believing’ approach. It is often pretty clear when there are model and data issues, and if it isn’t, it’s probably not a big deal.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#sec-knowing-feature-metrics",
    "href": "knowing_models.html#sec-knowing-feature-metrics",
    "title": "3  Knowing Your Model",
    "section": "3.3 Understanding the Features",
    "text": "3.3 Understanding the Features\nAssuming our model is adequate, let’s now turn our attention to the features. There’s a lot to unpack here, so let’s get started!\n\n3.3.1 Basic model parameters\nWe saw in the linear model chapter (2.6.1) that we can get a lot out of the basic output from standard linear models. Our starting point should be the coefficients or weights, which can give us a sense of the direction and magnitude of the relationship between the feature and the target given their respective scales. We can also look at the standard errors and confidence intervals to get a sense of the uncertainty in those estimates. Here is a basic summary of the coefficients for our regression model on the training data.\n\n\n\n\nTable 3.5: Coefficients for our regression model\n\n\n\n\n\n\n  \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n2.44\n0.08\n32.29\n0.00\n2.29\n2.59\n    review_year_0\n0.01\n0.00\n2.65\n0.01\n0.00\n0.01\n    release_year_0\n0.01\n0.00\n5.29\n0.00\n0.01\n0.01\n    age_sc\n−0.05\n0.02\n−3.21\n0.00\n−0.09\n−0.02\n    length_minutes_sc\n0.19\n0.02\n10.40\n0.00\n0.15\n0.22\n    total_reviews_sc\n0.26\n0.02\n14.43\n0.00\n0.22\n0.30\n    word_count_sc\n−0.12\n0.02\n−6.94\n0.00\n−0.15\n−0.08\n    genreComedy\n0.53\n0.06\n8.26\n0.00\n0.40\n0.65\n    genreDrama\n0.58\n0.04\n13.50\n0.00\n0.50\n0.67\n    genreHorror\n0.00\n0.08\n0.03\n0.98\n−0.16\n0.16\n    genreKids\n0.07\n0.07\n1.05\n0.30\n−0.06\n0.20\n    genreOther\n0.03\n0.07\n0.36\n0.72\n−0.12\n0.17\n    genreRomance\n0.07\n0.07\n1.05\n0.30\n−0.06\n0.21\n    genreSci-Fi\n−0.01\n0.08\n−0.17\n0.87\n−0.18\n0.15\n  \n  \n  \n\n\n\n\n\n\n\nWe also noted how we can get a bit more relative comparison by using standardized coefficients, or some other scaling of the coefficients that allows for a bit of a more apples-to-apples comparison. But as we’ll see, in the real world even if we have just apples, there are fuji, gala, granny smith, honeycrisp, and many other types of apples, and some may be good for snacks, others for baking pies, some are good for cider, etc. In other words, there is no one size fits all approach to understanding how a feature contributes to understanding the target, and the sooner you grasp that, the better.\n\n\n3.3.2 Feature contributions\nWe can also look at the contribution of a feature to the model’s explanatory power, namely through its predictions. To start our discussion, we don’t want to lean too heavily on the phrase feature importance yet, because as we’ll see later, trying to rank features by an importance metric is difficult at best, and a misguided endeavor at worst. We can however look at the feature contribution to the model’s predictions, and we can come to a conclusion about whether we think a feature is practically important, but just we need to be careful about how we do it.\nTruly understanding feature contribution is a bit more complicated than just looking at the coefficient if using any model that isn’t a linear regression, and there are many ways to go about it. We know we can’t compare raw coefficients across features, because they are on different scales. But even when we put them on the same scale, it may be very easy for some features to move, e.g., one standard deviation, and very hard for others. Binary features can only be on or off, while numeric features can move around more, but numeric features may also be highly skewed. We also can’t use statistical significance based on p-values, because they reflect sample size as much or more than effect size.\nSo what are we to do? What you need to know to get started looking at a feature’s contribution includes the following:\n\nfeature range and variability\nfeature distributions (e.g. skewness)\nrepresentative values of the feature\ntarget range and variability\nfeature interactions and correlations\n\nWe can’t necessarily do a whole lot about these aspects, but we can at least be aware of them, and just as importantly, we can be aware of the limitations of our understanding of these effects. In any case, let’s try to get a sense of how we can understand the contribution of a feature to our model.\n\n\n3.3.3 Marginal effects\nOne way to understand the contribution of a feature to the model is to look at the marginal effect of the feature, which conceptually attempts to boil a feature effect to something simple. Unfortunately, not everyone means the same thing when they use this term and it can be a bit confusing. Marginal effects typically refer to a partial derivative of the target with respect to the feature. Oh no! Math! However, as an example, this becomes very simple for standard linear models with no interactions and all linear effects as in linear regression. The derivative of our coefficient with respect to the feature is just the coefficient itself! But for more complicated models, even just a classification model like our logistic regression, we need to do a bit more work to get the marginal effect, or other so-called average effects. Let’s think about a couple common versions:\n\nAverage slope, Average Marginal Effect\nMarginal effect at the mean\nMarginal Means (for categorical features)\nCounterfactuals and other predictions at key feature values\n\n\n3.3.3.1 Marginal effects at the mean\nFirst let’s think about an average slope. This is the average of the slopes across the feature’s values or values of another feature it interacts with. But let’s just look at the effect of word count first. A good question is, how do we visualize that? Here are two plots, and both are useful, neither is inherently wrong, and yet they both tell us something different. The first plot shows the predicted probability of a good review as word count changes, with all other features at their mean (or mode for categorical). The second plot shows what is called a partial dependence plot, which shows the average predicted probability of a good review as word count changes. In both cases we make predictions with imputed values- the left plot imputes the other features to be their mean or mode, while the right plot leaves the other features at their actual values, and then, using a range of values for word count, gets a prediction as if every observation had that value for word count. We then plot the average of the predictions for each value in the range.\n\n\n\n\n\n\n\n\nFigure 3.6: Marginal Effect at the Mean vs. Partial Dependence Plot\n\n\n\n\n\nWhen word count is zero, i.e. its mean and everything else is at its mean/mode, we’d predict a probability of a good review of about 84%. As such, we interpret this as ‘when everything is typical’, we have a pretty good chance of getting a good review. The average prediction we’d get if we predicted every observation as if it were the mean word count is more like 55%, which is notably less. Which is correct? Both, or neither! They are telling us different things, either of which may be useful, or not. If it’s doubtful that the feature values used in the calculation are realistic (e.g. everything at its mean at the same time, or an average word count when length of a movie is at its minimum) then they may both be misleading. You have to know your features and your target to know best use the information.\n\n\n3.3.3.2 Average marginal effects\nLet’s say we want to boil our understanding of the effect to a single number. In this case, the coefficient is fine if we’re dealing with an entirely linear model. In this classification case, the raw coefficient tells us what we need to know, but on the log odds scale, which is not very intuitive for most folks. We can understand the probability scale, but this means things get nonlinear. As an example, a .1 to .2 change in the probability is doubling it, while a .8 to .9 change is a 12.5% increase in the probability. But is there any way we can stick with probabilities and get a single value to understand the change in the probability of a good review as word count changes by 1 unit?\nYes! We can look at what’s called the average marginal effect of word count. This is the average of the slope of the predicted probability of a good review as word count changes. This is a bit more complicated than just looking at the coefficient, but it’s a bit more intuitive. How do we get it? By a neat little trick where we predict the target with the feature at two values, one with the observed value is changed by adding or subtracting a very small amount. Then we take the difference in the predictions. This results in the same thing as taking the derivative of the target with respect to the feature.\n\nRPython\n\n\n\nfudge_factor = 1e-3\n\nfudge_plus = predict(\n    model_class_train, \n    newdata = df_train |&gt; mutate(word_count_sc = word_count_sc + fudge_factor/2),\n    type = \"response\"\n)\n\nfudge_minus = predict(\n    model_class_train, \n    newdata = df_train |&gt; mutate(word_count_sc = word_count_sc - fudge_factor/2),\n    type = \"response\"\n)\n\n# compare\n# mean(fudge_plus - fudge_minus) / fudge_factor\n\nmarginaleffects::avg_slopes(\n    model_class_train, \n    variables = \"word_count_sc\", \n    type = 'response'\n)\n\n\n          Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 %\n word_count_sc   -0.105     0.0155 -6.74   &lt;0.001 35.8 -0.135 -0.0742\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n\n\nfudge_factor = 1e-3\n\nfudge_plus = model_class_train.predict(\n    df_train.assign(\n        word_count_sc = df_train.word_count_sc + fudge_factor/2\n    )\n)\n\nfudge_minus = model_class_train.predict(\n    df_train.assign(\n        word_count_sc = df_train.word_count_sc - fudge_factor/2\n    )\n)\n\nnp.mean(fudge_plus - fudge_minus) / fudge_factor\n\n-0.09447284318194568\n\n\n# note that the marginaleffects is available in Python, but still very fresh!\n# we'll add a comparison in the future, but it doesn't handle models\n# with categoricals right now.\n\n# import marginaleffects as me\n# me.avg_slopes(model_class_train, variables = \"word_count_sc\")\n\n\n\n\nOur result above suggests we’re getting about a .09 drop in the expected probability of a good review for a 1 standard deviation increase in word count. This is a bit more intuitive than the coefficient or odds ratio based on it, and we probably don’t want to ignore that sort of change. It also doesn’t take much to get with the right package or even on our own.\n\n\n3.3.3.3 Marginal means\nMarginal means are just about getting an average prediction for the levels of categorical features. As an example, we can get the average predicted probability of a good review for each level of the genre feature, and then compare them. To do this we just have to make predictions as if every observation had a certain value for genre, and then average the predictions. This is also the exact same approach that produced the PDP for word count we saw earlier.\n\nRPython\n\n\n\nmm = map_df(\n    unique(df_train$genre), \n    ~ tibble(\n        genre = .x,\n        avg_pred = predict(\n            model_lr_train, \n            newdata = df_train |&gt; \n                mutate(genre = .x), \n            type = \"response\"\n        ) |&gt; \n            mean()\n    )\n)\n\n# mm \nmarginaleffects::avg_predictions(model_lr_train, variables = \"genre\")\n\n\n\n\nmarginal_means = pd.DataFrame({\n    \"genre\": df_train.genre.unique(),\n    \"avg_pred\": [\n        model_lr_train.predict(df_train.assign(genre = g)).mean()\n        for g in df_train.genre.unique()\n    ]\n})\n\nmarginal_means\n\n\n\n\n\n\n\n\nTable 3.6: Average Predicted Probability of a Good Review by Genre\n\n\n\n\n\n\n  \n    \n      genre\n      estimate\n      conf.low\n      conf.high\n    \n  \n  \n    Comedy\n3.34\n3.23\n3.45\n    Drama\n3.39\n3.33\n3.45\n    Action/Adventure\n2.81\n2.75\n2.87\n    Horror\n2.81\n2.66\n2.96\n    Sci-Fi\n2.80\n2.65\n2.95\n    Romance\n2.88\n2.76\n3.00\n    Other\n2.84\n2.70\n2.97\n    Kids\n2.88\n2.76\n2.99\n  \n  \n  \n    \n       Select output from the R package marginaleffects.\n    \n  \n\n\n\n\n\n\n\n\n\n\n3.3.4 Counterfactual predictions\nThe nice thing about having a model is that we can make predictions for any set of feature values we want to explore. This is a great way to understand the contribution of a feature to the model. We can make predictions for a range of feature values, and then compare the predictions to see how much the feature contributes to the model. Counterfactual predictions allow us to ask “what if?” questions of our model, and see how it responds. As an example, we can get a prediction as if every review was made for a drama, and then see what we’d expect if every review pertained to a comedy. This is a very powerful approach, and often utilized in causal inference, but it’s also a great way to understand the contribution of a feature to a model in general.\nConsider an experimental setting where we have lots of control over how the data is produced for different scenarios. Ideally we’d be able to look at the same instances under when everything about them was identical, but in one case, the instance was part of the control group, and in another, part of the treatment group. Unfortunately, not only is it impossible to have everything be identical, but it’s also impossible to have the same instance be in two experimental group settings at the same time. Counterfactual predictions are the next best thing though, because once we have a model, we can predict an observation as if it was in the treatment, and then when it is a control. If we do this for all observations, we can get a sense of the average treatment effect, one of the main points of interest in causal inference.\n\n\nBut you don’t need an experiment for this. Let’s try a new data set to really drive the point home. We’ll use some data at the global stage- the world happiness data set. For our model we’ll predict the happiness score, considering freedom to make life choices, GDP and other things. We’ll then switch the freedom to make life choices and GDP values for the US and Russia, and see how the predictions change!\n\nRPython\n\n\n\n# data available on repo (full link in appendix)\ndf_happiness_2018 = read_csv('https://tinyurl.com/527pbc4v')\n\nmodel_happiness = lm(\n    happiness_score ~ \n    log_gdp_per_capita \n    + healthy_life_expectancy_at_birth\n    + generosity \n    + freedom_to_make_life_choices\n    + confidence_in_national_government, \n    data = df_happiness_2018\n)\n\ndf_us_russia = df_happiness_2018 |&gt; \n    filter(country %in% c('United States', 'Russia'))\n\nhappiness_gdp_freedom_values = df_us_russia  |&gt; \n    arrange(country) |&gt; \n    select(log_gdp_per_capita, freedom_to_make_life_choices)\n\nbase_predictions = predict(\n    model_happiness, \n    newdata = df_us_russia\n)\n\n# switch up their GDP and freedom!\ndf_switch = df_us_russia |&gt; \n    mutate(\n        log_gdp_per_capita = rev(log_gdp_per_capita),\n        freedom_to_make_life_choices = rev(freedom_to_make_life_choices)\n    )\n\nswitch_predictions = predict(\n    model_happiness, \n    newdata = df_switch\n)\n\ntibble(\n    country = c('Russia', 'USA'),\n    base_predictions,\n    switch_predictions\n) |&gt; \n    mutate(\n        diff_in_happiness = switch_predictions - base_predictions\n    )\n\n\n\n\n# data available on repo (full link in appendix)\ndf_happiness_2018 = pd.read_csv('https://tinyurl.com/527pbc4v')\n\nmodel_happiness = smf.ols(\n    formula = 'happiness_score ~ \\\n        log_gdp_per_capita \\\n        + healthy_life_expectancy_at_birth \\\n        + generosity \\\n        + freedom_to_make_life_choices \\\n        + confidence_in_national_government',\n    data = df_happiness_2018\n).fit()\n\ndf_us_russia = df_happiness_2018[\n        df_happiness_2018.country.isin(['United States', 'Russia'])\n    ]\n\nhappiness_gdp_freedom_values = df_happiness_2018.loc[\n    df_happiness_2018.country.isin(['United States', 'Russia']),\n    ['log_gdp_per_capita', 'freedom_to_make_life_choices']\n]\n\nbase_predictions = model_happiness.predict(df_us_russia)\n\n# switch up their GDP and freedom!\ndf_switch = df_us_russia.copy()\n\ndf_switch[['log_gdp_per_capita', 'freedom_to_make_life_choices']] = (\n    df_switch[['log_gdp_per_capita', 'freedom_to_make_life_choices']].values[::-1]\n)    \n\nswitch_predictions = model_happiness.predict(df_switch)\n\npd.DataFrame({\n    'country': ['Russia', 'USA'],\n    'base_predictions': base_predictions,\n    'switch_predictions': switch_predictions,\n    'diff_in_happiness': switch_predictions - base_predictions\n}).round(3)\n\n\n\n\n\n\n\n\nTable 3.7: Predictions for happiness score for Russia and the US with switched freedom and GDP\n\n\n\n\n\n\n  \n    \n      country\n      base_predictions\n      switch_predictions\n      diff_in_happiness\n    \n  \n  \n    Russia\n5.7\n6.4\n0.7\n    United States\n6.8\n6.1\n−0.7\n  \n  \n  \n\n\n\n\n\n\n\nIn this case, we see that the happiness score is expected to be very lopsided in favor of the US, which our base prediction would suggest the US to be almost a full standard deviation higher in happiness than Russia given their current values. But if the US was just a bit more like Russia, we’d see a significant drop even if it maintained its life expectancy, generosity, and faith in government. Likewise, if Russia was a bit more like the US, we’d expect to see a significant increase in their happiness score.\nIt’s very easy even with base package functions to see some very interesting things about our data and model. Counterfactual predictions get us thinking more explicitly about what the situation would be if things were much different, but in the end, we’re just playing around with predicted values and thinking about possibilities!\n\n\n3.3.5 SHAP values\nAs we’ve suggested, most models are more complicated than can be explained by a simple coefficient, e.g. nonlinear effects in generalized additive models, or there may not even be feature-specific coefficients available, like gradient boosting models, or we may even have many parameters associated with a feature, as in deep learning. Such models typically won’t come with statistical output like standard errors and confidence intervals either. But we’ll still have some tricks up our sleeve to help us figure things out!\nA very common interpretation tool is called a SHAP value. SHAP stands for SHapley Additive exPlanations, and it provides a means to understand how much each feature contributes to a specific prediction. It’s based on a concept from game theory called the Shapley value, which is a way to understand how much each player contributes to the outcome of a game. For our modeling context, SHAP values break down a prediction to show the impact of each feature. The reason we bring it up here is that it has a nice intuition in the linear model case, and seeing it in that context is a good way to get a sense of how it works. Furthermore, it builds on what we’ve been talking about with our various prediction approaches.\nWhile the actual computations behind the scenes can be tedious, the basic idea is relatively straightforward- for a given prediction at a specific observation with set feature values, we can calculate the difference between the prediction at that observation versus the average prediction for the model as a whole. We can break this down for each feature, and see how much each contributes to the difference. This provides us the local effect of the feature. The SHAP approach also has the benefit of being able to be applied to any model, whether a simple linear or deep learning model. Very cool! To demonstrate we’ll use the simple model from our model comparison demo, but keep the features on the raw scale.\n\nRPython\n\n\n\nmodel_lr_3feat = lm(\n    rating ~\n    age\n    + release_year\n    + length_minutes,\n    data = df_reviews\n)\n\n# inspect if desired\n# summary(model_lr_3feat)\n\n\n\n\nmodel_lr_3feat = smf.ols(\n    formula = 'rating ~ \\\n    age \\\n    + release_year \\\n    + length_minutes',\n    data = df_reviews\n).fit()\n\n# inspect if desired\n# model_lr_3feat.summary(slim = True)\n\n\n\n\nWith our model in place let’s look at the SHAP values for the features. We’ll start by choosing the instance we want to explain. Here we’ll consider an observation where the release year is 2020, age of reviewer is 30, and a movie length of 110 minutes. To aid our understanding, we calculate the SHAP values at that observation by hand, and using a package. The by hand approach consists of the following steps.\n\nGet the average prediction for the model\nGet the prediction for the feature at the value of interest for all observations, and average the predictions\nCalculate the SHAP value as the difference between the average prediction and the average prediction for the feature value of interest\n\nNote that this approach only works for our simple linear regression case, and we’d need to use a package incorporating an appropriate method for more complicated settings. But it helps get our bearings on what SHAP values tell us. Also be aware that our focus is a feature’s marginal contribution at a single observation. Our coefficient already tells us the average contribution of a feature across all observations for this linear regression setting, i.e the AME discussed previously.\n\nRPython\n\n\n\n# first we need to get the average prediction\navg_pred = mean(predict(model_lr_3feat))\n\n# observation of interest we want shap values for\nobs_of_interest = tibble(\n    age = 30,\n    length_minutes = 110,\n    release_year = 2020\n)\n\n# then we need to get the prediction for the feature value of interest\n# for all observations, and average them\npred_age_30 = predict(\n    model_lr_3feat,\n    newdata = df_reviews |&gt; mutate(age = obs_of_interest$age) \n)\n\npred_year_2022 = predict(\n    model_lr_3feat,\n    newdata = df_reviews |&gt; mutate(release_year = obs_of_interest$release_year) \n)\n\npred_length_110 = predict(\n    model_lr_3feat,\n    newdata = df_reviews |&gt; mutate(length_minutes = obs_of_interest$length_minutes) \n)\n\n# then we can calculate the shap values\nshap_value_ours = tibble(\n    age    = mean(pred_age_30) - avg_pred,\n    release_year   = mean(pred_year_2022) - avg_pred,\n    length_minutes = mean(pred_length_110) - avg_pred\n)\n\n\n\n\n# first we need to get the average prediction\navg_pred = model_lr_3feat.predict(df_reviews).mean()\n\n# observation of interest we want shap values for\nobs_of_interest = pd.DataFrame({\n    'age': 30,\n    'release_year': 2020,\n    'length_minutes': 110\n}, index = ['new_observation'])\n\n# then we need to get the prediction for the feature value of interest\n# for all observations, and average them\npred_age_30 = model_lr_3feat.predict(\n    df_reviews.assign(\n        age = obs_of_interest['age']\n    )\n)\n\npred_year_2022 = model_lr_3feat.predict(\n    df_reviews.assign(\n        release_year = obs_of_interest['release_year']\n    )\n)\n\npred_length_110 = model_lr_3feat.predict(\n    df_reviews.assign(\n        length_minutes = obs_of_interest['length_minutes']\n    )\n)\n\n# then we can calculate the shap values\nshap_value_ours = pd.DataFrame({\n    'age': pred_age_30.mean() - avg_pred,\n    'release_year': pred_year_2022.mean() - avg_pred,\n    'length_minutes': pred_length_110.mean() - avg_pred\n}, index = ['new_observation'])\n\n\n\n\nNow that we have our own part set up. We can use a package to do the work more formally, and compare the results.\n\nRPython\n\n\n\n# we'll use the DALEX package for this\nexplainer = DALEX::explain(model_lr_3feat, verbose = FALSE)\n\nshap_value_package = DALEX::predict_parts(\n    explainer,\n    obs_of_interest,\n    type = 'shap'\n)\n\nrbind(\n    shap_value_ours,\n    shap_value_package[c('age', 'release_year', 'length_minutes'), 'contribution']\n)\n\n\n\n\n# now use the shap package for this; it does not work with statsmodels though,\n# but we still get there in the end!\nimport shap\nfrom sklearn.linear_model import LinearRegression\n\n# set data up for shap and sklearn\nfnames = [\n    'age', \n    'release_year', \n    'length_minutes'\n]\n\nX = df_reviews[fnames]\ny = df_reviews['rating']\n\n# use a linear model that works with shap\nmodel_reviews = LinearRegression().fit(X, y)\n\n# 1000 instances for use as the 'background distribution'\nX_sample = shap.maskers.Independent(data = X, max_samples = 1000)  \n\n# # compute the SHAP values for the linear model\nexplainer = shap.Explainer(\n    model_reviews.predict, \n    X_sample   \n)\n\nshap_values = explainer(obs_of_interest)\n\nshap_value_package = pd.DataFrame(\n    shap_values.values[0, :], \n    index = fnames, \n    columns = ['new_observation']\n).T\n\npd.concat([shap_value_ours, shap_value_package])\n\n\n\n\n\n\n\n\n\nTable 3.8: SHAP Value Comparison\n\n\n\n\n\n\n  \n    \n      source\n      age\n      release_year\n      length_minutes\n    \n  \n  \n    By Hand\n0.063\n0.206\n−0.141\n    Package\n0.063\n0.206\n−0.141\n  \n  \n  \n\n\n\n\n\n\n\nAnd there you have it- we’ve demystified the SHAP value! Things get more complicated in nonlinear settings, dealing with correlated features, and other cases, but hopefully this provides you some context. SHAP values are useful because they tell us how much each feature contributes to the prediction for the observation under consideration. We can visualize these as well, via a force plot or waterfall plot, the latter of which is shown below. The dotted line at E[f(x)] represents the average prediction from our model (~3.05), and the prediction we have for the observation at f(x), which is about 3.18.\nWith the average prediction as our starting point, we add the SHAP values for each feature to get the prediction for the observation. First we add the SHAP value for age, which bumps the value by 0.063, then the SHAP value for movie length, which decreases the prediction -0.141, and finally the SHAP value for release year, which brings us to the final predicted value by increasing the prediction 0.206.\n\n\n\n\n\n\n\n\nFigure 3.7: SHAP Visualizations\n\n\n\n\n\nPretty neat huh? So for any observation we want to inspect, and more importantly, for any model we might use, we can get a sense of how features contribute to that prediction. We also can get a sense of how much each feature contributes to the model as a whole by aggregating these values across all observations in our data, and this potentially provides a measure of feature importance, but we’ll come back to that in a bit.\n\n\n3.3.6 Related visualizations\nWe’ve seen how we can get some plots for predictions in different ways previously with what’s called a partial dependence plot (Figure 3.6). A PDP shows the average prediction of a feature on the target across the feature values, which is in fact what we were just doing to calculate our SHAP value, and for the linear case, the PDP has a direct correspondence to the SHAP. As we saw, the SHAP value is the difference between the average prediction and the point on the PDP for a feature at a specific feature value. With regard to the PDP, this is the difference the point on the PDP and the average prediction for the model at that feature value, shown in the red line below.\n\n\n\n\n\n\nFigure 3.8: PDP, ICE, and ALE Plots\n\n\n\nWe can also look at the individual conditional expectation (ICE) plot, which is a PDP plot for a single observation, but across all values of a select feature. By looking at several observations, as in the second plot above, we can get a sense of the variability in the feature’s effect. As we can see, there is not much to tell beyond a PDP when we have a simple linear model, but it becomes more interesting when we have interactions or other nonlinearities in our model.\nIn addition, there are other plots that are similar to the PDP and ICE, such as the accumulated local effect (ALE) plot, shown last, which is more robust to correlated features than the PDP plot, while also showing the general feature-target relationship. Where the PDP and ICE plots show the average effect of a feature on the target, the ALE plot focuses on average differences in predictions for the feature at a specific value, versus predictions at feature values nearby, and then centers the result so that the average difference is zero. In general, all our plots reflect the positive linear relationship between movie length and rating.\n\n\n\n\n\n\n\nVisualization Tools\n\n\n\n\n\nThe waterfall plot was created using DALEX, but the shap python package will also provide this. For PDP, ICE and ALE plots in R, you can look to the iml package, and in Python, the shap or scikit-learn package. Many others are available though, so feel free to explore!\n\n\n\n\n\n3.3.7 Global assessment of feature importance\nHow important is a feature? It’s a common question, and one that is often asked of models, but the answer ranges from ‘it depends’ and ‘it doesn’t matter’. Let’s start with some hard facts:\n\nThere is no single definition of importance for any given model.\nThere is no single metric for any model that will conclusively tell you how important a feature is relative to others in all data/model contexts.\nThere are multiple metrics of importance for a given model that are equally valid, but which may come to different conclusions.\nAny non-zero feature contribution is potentially ‘important’, however small.\nMany metrics of importance fail to adequately capture interactions and/or deal with correlated features.\nAll measures of importance are measured with uncertainty, and the uncertainty can be large.\nA question for feature importance is relative to… what? A poor model will still have relatively ‘important’ features, but they still may not be useful since the model itself isn’t.\nIt rarely makes sense to drop features based on importance alone, and doing so will typically drop performance as well.\nIn the end, what will you do with the information?\n\nAs we noted previously, if I want to know how a feature relates to a target, I have to know how a feature moves, and I need to know what types of feature values are more likely than others, and what a typical movement in its range of values would be. If a feature is skewed, then the mean may not be the best value to use for prediction, and basing ‘typical’ movement on its standard deviation may be misguided. If a unit movement in a feature results in a movement in the target of 2 units, what does that mean? Is it a large movement? If I don’t know the target very well I can’t answer that. As an example, if the target is in dollars, a $2 movement is nothing for salary, but might be large for a stock price. We have to know the target as well as we do the feature predicting it.\nOn top of all this, we need to know how the feature interacts with other features. If a feature is highly correlated with another feature, then it may not be adding much to the model even if some metrics would indicate a notable contribution. In addition, some approaches will either spread the contribution of correlated features across them, or just pick one of them to include in the model. It may be mostly arbitrary which one is included, or you might miss both if the weights are split.\nIf a feature interacts with another feature, then there really is no way to say how much it contributes to the model without knowing the value of the other feature. Full stop. Synergistic effects cannot be understood by pretending they don’t exist. A number of metrics will still be provided for a single feature, either by trying to include its overall contribution or averaging over the values of the other feature, but this is a problematic approach because it still ignores the other feature values. As an example, if a drug doesn’t work for your age group or for someone with your health conditions, do you really care if it works ‘in general’ or ‘on average’?\nTo help us further understand this issue, consider the following two plots. On the left we show an interaction between two binary features. If we were to look at the contribution of each feature without the interaction, their respective coefficients would be estimated as essentially zero5. On the right we show a feature that has a strong relationship with the target, but only for a certain range of values. If we were to look at a single ‘effect’ of the feature, we would likely underestimate how strong it is with smaller values and overestimate the relationship at the upper range.\n\n\n\n\n\n\n\n\nFigure 3.9: Two plots showing the importance of understanding feature interactions and non-linear relationships\n\n\n\n\n\nAll this is to say, as we get into measures of feature importance, we need to be very careful about how we interpret and use them!\n\n3.3.7.1 Example: Feature Importance for Linear Regression\nTo show just how difficult measuring feature importance is, we only have to stick with our simple linear regression. Think again about R2: it tells us the proportion of the target explained by our features. An ideal measure of importance would be able to tell us how much each feature contributes to that proportion, or in other words, one that decomposes R2 into the relative contributions of each feature. One of the most common measures of importance in linear models is the standardized coefficient we have demonstrated previously. You know what it doesn’t do? It doesn’t decompose R2 into relative feature contributions. Even the more complicated SHAP approach will not do this.\nThe easiest situation we could hope for with regard to feature importance is the basic linear regression model we’ve been using. Everything is linear, with no interactions or other things going on, as in our demonstration model. And yet there are many logical ways to determine feature importance, and some even break down R2 into relative contributions, but they won’t necessarily agree with each other in ranking or relative differences. If you can get a measure of statistical difference between whatever metric you choose, it’s often the case that ‘top’ features will not be statistically different from other features. So what do we do? We’ll show a few methods here, but the main point is that there is no single answer, and it’s important to understand what you’re trying to do with the information.\nLet’s start things off by using one of our previous linear regression models with several features, but which has no interactions or other complexity (Section 2.7.1). It’s just a model with simple linear relationships and nothing else. We even remove categorical features to avoid having to aggregate group effects. In short, it doesn’t get any easier than this!\n\nRPython\n\n\n\nmodel_importance = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_reviews\n)\n\n\n\n\nmodel_importance = smf.ols(\n    formula = 'rating ~ \\\n        word_count \\\n        + age \\\n        + review_year \\\n        + release_year \\\n        + length_minutes \\\n        + children_in_home \\\n        + total_reviews',\n    data = df_reviews\n).fit()\n\n\n\n\nOur first metric available for us to use is just the raw coefficient value, but they aren’t comparable because the features are on very different scales- moving a unit in length is not the same as moving a unit in age. We can standardize them which helps in this regard, and you might start there. Another approach we can use comes from the SHAP value, which, as we saw, provides a measure of contribution of a feature to the prediction. These can be positive or negative and are specific to the observation. But If we take the average absolute SHAP value for each feature, we get a sense of the typical contribution size for the features. We can then rank order them accordingly. Here we see that the most important features here are the number of reviews and the length of the movie. Note that we can’t speak to direction here, only magnitude. We can also see that word count is relatively less important.\n\n\n\n\n\n\n\n\nFigure 3.10: SHAP Importance\n\n\n\n\n\nNow here are some additional methods6. Some of these decompose R2 into the relative contributions to it from each feature (car, lmg, and pratt). The others do not (SHAP, permutation-based, standardized coefficient squared). On the left, values represent the proportion of the R2 value that is attributable to the feature- their sum is equal to the overall R2 = 0.32. These are in agreement for the most part and seem to think more highly of word count as a feature. The others on the right suggest word count and age should rank lower, and length and review year higher. Which is best? Which is correct? Any of them. But by looking at a few of these, we can at least get a sense that total reviews, word count, release year, and length in minutes are probably useful features to our model, while age, review year, and children in the home are less so, at least in the context of the model we have.\n\n\n\n\n\n\n\n\nFigure 3.11: Feature importance by various methods\n\n\n\n\n\nAll of the metrics shown have uncertainty in their estimate, and some packages make it easy to plot or extract. As an example one could bootstrap a metric, or use the permutations as a means to get at the uncertainty. However, the behavior and distribution of these metrics is not always well understood, and in some cases, the computation would often be notable (e.g. with SHAP). You could also look at the range of the ranks created by bootstrapping or permuting, and take the upper bound as the worst case for a given feature. Although this might possibly be conservative, the usual problem is that people are too optimistic about their feature importance result, so this might be a good thing.\nThe take home message is that in the best of circumstances, there is no automatic way of saying one feature is more important than another. It’s nice that we can use approaches like SHAP and permutation methods for more complicated models like boosting and deep learning models, but they’re not perfect, and they still suffer from most of the same issues as the linear model. In the end, understanding a feature’s role within a model is ultimately a matter of context, and highly dependent on what you’re trying to do with the information.\n\n\n\n\n\n\nSHAP as a Global Metric\n\n\n\n\n\nSHAP values can be useful for observational level interpretation under the right circumstances, but they really shouldn’t be used for importance. The mean of its absolute value is not a good measure of importance except in the unlikely case you have purely balanced/symmetric features of the exact same scale, and which do not correlate with each other (or have any interactions).\n\n\n\n\n\n\n3.3.8 Feature metrics for classification\nAll of what’s been demonstrated for feature metrics applies to classification models. Counterfactual predictions, average marginal effects, SHAP, and permutation-based methods for feature importance would be done in the exact same way. The only real difference is of course the outcome- we’d be talking in terms of probabilities and using a different loss metric to determine importance, that sort of thing. Everything we’ve talked about holds for that case as well.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#sec-knowing-wrap",
    "href": "knowing_models.html#sec-knowing-wrap",
    "title": "3  Knowing Your Model",
    "section": "3.4 Wrapping Up",
    "text": "3.4 Wrapping Up\nIt is easy to get caught up in the excitement of creating a model and then using it to make predictions. It is also easy to get caught up in the excitement of seeing a model perform well on a test set. It is much harder to take a step back and ask yourself, “Is this model really doing what I want it to do?” You should always be looking at which features are pulling the most weight in your model and how predictions are being made. It takes a lot of work to trust what a model is telling you.\n\n3.4.1 The common thread\nMuch of what you’ve seen in this chapter can be applied to any model. From linear regression to deep learning, we often use similar metrics to help select and compare models. In most model scenarios, but not all, we are interested in feature-level interpretation. This can take a statistical focus, or use tools that are more model-agnostic like SHAP. In almost every modeling scenario, we are usually interested in how the model is making its predictions, and how we can use that information to make better decisions.\n\n\n3.4.2 Choose your own adventure\nIf you haven’t already, feel free to take your linear models further in Chapter 6 and Chapter 5, where you’ll see how to handle different distributions for your target, add interactions, nonlinear effects, and more. Otherwise, you’ve got enough at this point to try your hand at the Chapter 7 section, where you can dive into machine learning!\n\n\n3.4.3 Additional resources\nIf this chapter has piqued your curiosity, we would encourage you to check out the following resources.\nEven though we did not use the mlr3 package in this chapter, the Evaluation and Benchmarking chapter of the associated book, Applied Machine Learning Using mlr3 in R, offers a great conceptual take on model metrics and evaluation.\nFor a more Pythonic look at model evaluation, we would highly recommend going through the sci-kit learn documentation on Model Evaluation. It has you absolutely covered with code examples and concepts.\nTo get the most out of DaLEX visualizations, check out the authors’ book Explanatory Model Analysis.\nWe also recommend checking out Christoph Molnar’s book, Interpretable Machine Learning. It is a great resource for learning more about model explainers and how to use them, and provides a nice package that has a lot of the functionality we’ve shown here.\nThe marginal effects zoo, written by the marginaleffects package author, is your goto for getting started with marginal effects, but we also recommend the excellent blog post by Andrew Heiss as a very nifty overview and demonstration.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#sec-knowing-exercise",
    "href": "knowing_models.html#sec-knowing-exercise",
    "title": "3  Knowing Your Model",
    "section": "3.5 Exercise",
    "text": "3.5 Exercise\nUse the world happiness data set to create a model of your choosing that predicts happiness score. Create a simpler or more complex model for comparison. Interpret the model as a whole, then compare it to the simpler/more complex model. Choose the ‘best’ model, and justify your reason for doing so. Given that model, explore the predictions and interpret the features. Can you comfortably claim which is the most important? Why or why not?",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "knowing_models.html#footnotes",
    "href": "knowing_models.html#footnotes",
    "title": "3  Knowing Your Model",
    "section": "",
    "text": "For anyone comparing Python to R results, the data splits are not the same so outputs likewise will not be identical, though they should be very similar.↩︎\nThe origin of the term “confusion matrix” is a bit muddled, and it’s not clear why it’s not just called a classification table/matrix (as it actually is from time to time). If you call it a classification table, probably everyone will know exactly what you mean, but if you call it a confusion matrix, few outside of data science (or domains that use it) will likely know what you’re talking about.↩︎\nMost machine learning libraries in Python will have a predict_proba method that will give you the probability of each class, while the predict method will give you the predicted class.↩︎\nThe precision-recall curve is a very similar approach which visualizes the tradeoff between precision and recall. The area under the precision-recall curve is its corresponding metric.↩︎\nTo understand why, for the effect of Feature 1, just take the mean of the two points on the left vs. the mean of the two points on the right. It would basically be a straight line of no effect as you move from group 0 to group 1. For the effect of Feature 2, the two group means for A and B would be at the intersection of the two lines.↩︎\nThe car, lmg, pratt, and beta-squared values were provided by the relaimpo package in R. See the documentation there for details. Permutation based importance was provided by the iml package, though we supplied a custom function that returns the drop in R-squared value. SHAP values were calculated using the fastshap package. In python you can use the shap output, and sklearn has a permutation-based importance function as well.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Knowing Your Model</span>"
    ]
  },
  {
    "objectID": "estimation.html",
    "href": "estimation.html",
    "title": "4  How Did We Get Here?",
    "section": "",
    "text": "4.1 Key Ideas\nA few concepts we’ll keep using here are fundamental to understanding estimation and optimization. We’ll should note that we’re qualifying our present discussion of these topics to typical linear models and similar settings, but they are much more broad and general than presented here. We’ve seen some of this before, but we’ll be getting a bit cozier with the concepts now.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-key-ideas",
    "href": "estimation.html#sec-estim-key-ideas",
    "title": "4  How Did We Get Here?",
    "section": "",
    "text": "Parameters are the values associated with a model that we have to estimate.\nEstimation is the process of finding the parameters associated with a model.\nThe objective (loss) function takes input and produces a value that we want to maximize or minimize.\nPrediction error is the difference between the observed value of the target and the predicted value, and is often used to calculate the objective function.\nOptimization is the specific process of finding the parameters that maximize or minimize some objective function.\nModel Selection is the process of choosing the best model from a set of models.\n\n\n\n\n\n\n\nEstimation vs. Optimization\n\n\n\n\n\nWe can use estimation as general term for finding parameters, while optimization can be seen as a term for finding parameters that maximize or minimize some objective function, or even a combination of objectives. In some cases, we can estimate parameters without optimization, because there is a known way of solving the problem, but in most modeling situations we are going to use some optimization approach to find a ‘best’ set of parameters.\n\n\n\n\n4.1.1 Why this matters\nWhen it comes to modeling, even knowing just a little bit about what goes on behind the scenes is a great demystifier. And if models are less of a mystery, you’ll feel more confident using them. Parts of what you see here are a part of almost every common model used for statistics and machine learning, providing you even more of a foundation for understanding what’s going on.\n\n\n4.1.2 Good to know\n\nThis chapter is more involved and technical than most of the others, so might be more suited for those who like to get their hands dirty. It’s all about ‘rolling your own’, and so we’ll be doing a lot of the work ourselves. If you’re not one of those types of people that gets much out of that, that’s ok, you can skip this chapter and still get a lot out of the rest of the book. But if you’re curious about how things work, or you want to be able to do more than just run a function, then we think you’ll find the following useful. You’d want to at least have your linear model basics down (Chapter 2).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-data-setup",
    "href": "estimation.html#sec-estim-data-setup",
    "title": "4  How Did We Get Here?",
    "section": "4.2 Data Setup",
    "text": "4.2 Data Setup\nFor the examples here, we’ll use the world happiness dataset for the year 2018. We’ll use the happiness score as our target. Let’s take an initial look at the data here, but for more information see the appendix Section A.2.\n\n\n\n\n\n\nFigure 4.1: World happiness data summary\n\n\n\nOur happiness score has values from around 3-7, life expectancy and gdp appear to have some notable variability, and corruption perception is skewed toward lower values. We can also see that the features and target are correlated with each other, which is not surprising.\n\n\n\n\nTable 4.1: Correlation matrix for world happiness data\n\n\n\n\n\n\n  \n    \n      term\n      happiness\n      life_exp\n      log_gdp_pc\n      corrupt\n    \n  \n  \n    happiness\n1.00\n0.78\n0.82\n−0.47\n    life_exp\n0.78\n1.00\n0.86\n−0.34\n    log_gdp_pc\n0.82\n0.86\n1.00\n−0.34\n    corrupt\n−0.47\n−0.34\n−0.34\n1.00\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nFor our purposes here, and we’ll drop any rows with missing values, and we’ll use scaled features so that they have the same variance, which as noted in the data chapter, can help make estimation easier.\n\n\nRPython\n\n\n\ndf_happiness = read_csv('https://tinyurl.com/worldhappiness2018') |&gt;\n    drop_na() |&gt; \n    renname(happiness = happiness_score)\n\n\n\n\nimport pandas as pd\n\ndf_happiness = (\n    pd.read_csv('https://tinyurl.com/worldhappiness2018')\n    .dropna()\n    .rename(columns = {'happiness_score': 'happiness'})\n)\n\n\n\n\n\n4.2.1 Other Setup\nFor the R examples, after the above nothing beyond base R is needed. For Python examples, the following should be enough to get you through the examples:\n\nimport pandas as pd\nimport numpy as np\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-guessing",
    "href": "estimation.html#sec-estim-guessing",
    "title": "4  How Did We Get Here?",
    "section": "4.3 Starting Out by Guessing",
    "text": "4.3 Starting Out by Guessing\nSo, we’ll start with a model in which we predict a country’s level of happiness by their life expectancy, where if you can expect to live longer, maybe you’re probably in a country with better health care, higher incomes, and other important stuff. We’ll stick with our simple linear regression model as well.\nAs a starting point we can just guess what the parameter should be, but how would we know what to guess? How would we know which guesses are better than others? Let’s try a couple and see what happens. Let’s say that we think all countries start at the bottom on the happiness scale (around 3), but life expectancy makes a big impact- for every standard deviation of life expectancy we go up a whole point on happiness1. We can plug this into the model and see what we get:\n\\[\n\\text{prediction} = 3.3 + 1\\cdot\\text{life\\_exp}\n\\]\nFor a different model, we’ll say countries start with a mean of happiness score, and moving up a standard deviation of life expectancy would move us up a half point of happiness. \\[\n\\text{prediction} = \\overline{\\text{happiness}} + .5\\cdot\\text{life\\_exp}\n\\]\nHow do we know which is better? Let’s find out!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-prediction-error",
    "href": "estimation.html#sec-estim-prediction-error",
    "title": "4  How Did We Get Here?",
    "section": "4.4 Prediction Error",
    "text": "4.4 Prediction Error\nWe’ve seen that a key component to model assessment involves comparing the predictions from the model to the actual values of the target. This difference is known as the prediction error, or residuals in more statistical contexts. We can express this as:\n\\[\n\\epsilon = y - \\hat{y}\n\\] \\[\n\\text{error} = \\text{target} - \\text{model-based guess}\n\\]\nThis prediction error tells us how far off our model prediction is from the observed target values, and it gives us a way to compare models. With our measure of prediction error, we can calculate a total error for all observations/predictions (Section 3.2), or similarly, the average error. If one model or parameter set has less total or average error, we can say it’s a better model than one that has more (Section 3.2.3). Ideally we’d like to choose a model with the least error, but we’ll see that this is not always possible2.\nHowever, if we just take the average of our errors from a linear regression model, you’ll see that it is roughly zero! This is by design for many common models, where we even will explicitly write the formula for the error as coming from a normal distribution with mean of zero. So, to get a meaningful error metric, we need to use the squared error value or the absolute value. These also allow errors of similar value above and below the observed value to cost the same3. We’ll use squared error here, and we’ll calculate the mean of the squared errors for all our predictions.\n\nRPython\n\n\n\ny = df_happiness$happiness\n\n# Calculate the error for the guess of 4\nprediction = min(df_happiness$happiness) + 1*df_happiness$life_exp_sc\nmse_model_A = mean((y - prediction)^2)\n\n# Calculate the error for our other guess\nprediction = mean(y) + .5 * df_happiness$life_exp_sc\nmse_model_B = mean((y - prediction)^2)\n\ntibble(\n    Model = c('A', 'B'),\n    MSE = c(mse_model_A, mse_model_B)\n)\n\n\n\n\ny = df_happiness['happiness']\n\n# Calculate the error for the guess of four\nprediction = np.min(df_happiness['happiness']) + 1 * df_happiness['life_exp_sc']\nmse_model_A   = np.mean((y - prediction)**2)\n\n# Calculate the error for our other guess\nprediction = y.mean() + .5 * df_happiness['life_exp_sc']\nmse_model_B  = np.mean((y - prediction)**2)\n\npd.DataFrame({\n    'Model': ['A', 'B'],\n    'MSE': [mse_model_A, mse_model_B]\n})\n\n\n\n\nNow let’s look at our Mean Squared Error (MSE), and we’ll also inspect the square root of it, or the Root Mean Squared Error, as that puts things back on the original target scale, and tells us the standard deviation of our prediction errors. We also add the Mean Absolute Error (MAE) as another metric with straightforward interpretation.\n\n\n\n\nTable 4.2: Comparison of error metrics for two models\n\n\n\n\n\n\n  \n    \n      Model\n      MSE\n      RMSE\n      MAE\n      RMSE % drop\n      MAE % drop\n    \n  \n  \n    A\n5.09\n2.26\n1.52\n\n\n    B\n0.64\n0.80\n0.58\n65%\n62%\n  \n  \n  \n\n\n\n\n\n\n\nInspecting the metrics, we can see that we are off on average by over a point for model A (MAE), and a little over half a point on average for model B. So we can see that model B is not only better, but results in a 65% drop in RMSE, and similar for MAE. We’d definitely prefer it over model A, and we can see how to compare models in a general fashion.\nNow all of this is useful, and at least we can say one model is better than another. But you’re probably hoping there is an easier way to get a good guess for our model parameters, especially when we have possibly dozens of features and/or parameters to keep track of, and there is!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-ols",
    "href": "estimation.html#sec-estim-ols",
    "title": "4  How Did We Get Here?",
    "section": "4.5 Ordinary Least Squares",
    "text": "4.5 Ordinary Least Squares\nIn a simple linear model, we often use the Ordinary Least Squares (OLS) method to estimate parameters. This method finds the coefficients that minimize the sum of the squared differences between the predicted and actual values4. In other words, it finds the coefficients that minimize the sum of the squared differences between the predicted values and the actual values, which is what we just did in our previous example. The sum of the squared errors is also called the residual sum of squares (RSS), as opposed to the ‘total’ sums of squares (i.e. the variance of the target), and the part explained by the model (‘model’ or ‘explained’ sums of squares). We can express this as follows, where \\(y_i\\) is the observed value of the target for observation \\(i\\), and \\(\\hat{y_i}\\) is the predicted value from the model.\n\\[\n\\text{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n\\tag{4.1}\\]\nIt’s called ordinary least squares because there are other least squares methods - generalized least squares, weighted least squares, and others, but we don’t need to worry about that for now. The sum or mean of the squared errors is our objective value. The process of taking the predictions and observed target values as inputs, and returning this value as an output is our objective function. We can use this value to find the best parameters for a specific model, as well as compare models with different parameters.\nNow let’s calculate the OLS estimate for our model. We need our own function to do this, but it doesn’t take much. We need to map our inputs to our output, which are the model predictions. We then calculate the error, square it, and then average the squared errors to provide the mean squared error.\n\nRPython\n\n\n\n# for later comparison\nmodel_lr_happy = lm(happiness ~ life_exp_sc, data = df_happiness)\n\nols = function(X, y, par) {\n    # add a column of 1s for the intercept\n    X = cbind(1, X)\n\n    # Calculate the predicted values\n    y_hat = X %*% par  # %*% is matrix multiplication\n\n    # Calculate the error\n    error = y - y_hat\n\n    # Calculate the value as mean squared error\n    value = sum(error^2) / nrow(X)\n\n    # Return the objective value\n    return(value)\n}\n\n\n\n\n# for later comparison\nmodel_lr_happy = smf.ols('happiness ~ life_exp_sc', data = df_happiness).fit()\n\ndef ols(par, X, y):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    y_hat = X @ par  # @ is matrix multiplication\n    \n    # Calculate the mean of the squared errors\n    value = np.mean((y - y_hat)**2)\n    \n    # Return the objective value\n    return value\n\n\n\n\nWe’ll want to make a bunch of guesses for the parameters, so let’s create data for those guesses. Then choose the guess that gives us the lowest objective value. But before getting carried away, try it out for just one guess to see how it works!\n\nRPython\n\n\n\n# create a grid of guesses\nguesses = crossing(\n    b0 = seq(1, 7, 0.1),\n    b1 = seq(-1, 1, 0.1)\n)\n\n# Example for one guess\nols(\n    X = df_happiness$life_exp_sc,\n    y = df_happiness$happiness,\n    par = unlist(guesses[1, ])\n)\n\n[1] 23.777\n\n\n\n\n\n# create a grid of guesses\nfrom itertools import product\n\nguesses = pd.DataFrame(\n    product(\n        np.arange(1, 7, 0.1),\n        np.arange(-1, 1, 0.1)\n    ),\n    columns = ['b0', 'b1']\n)\n\n# Example for one guess\nols(\n    par = guesses.iloc[0,:],\n    X = df_happiness['life_exp_sc'],\n    y = df_happiness['happiness']\n)\n\n23.793842044979073\n\n\n\n\n\nNow we’ll calculate the loss for each guess and find which one gives us the smallest function value.\n\nRPython\n\n\n\n# Calculate the function value for each guess, mapping over\n# each combination of b0 and b1\nguesses = guesses |&gt;\n    mutate(\n        objective = map2_dbl(\n            guesses$b0, \n            guesses$b1,\n            \\(b0, b1) ols(\n                par = c(b0, b1), \n                X = df_happiness$life_exp_sc, \n                y = df_happiness$happiness\n            )\n        )\n    )\n\nmin_loss = guesses |&gt; filter(objective == min(objective))\n\nmin_loss\n\n# A tibble: 1 × 3\n     b0    b1 objective\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1   5.4   0.9     0.491\n\n\n\n\n\n# Calculate the function value for each guess, mapping over\n# each combination of b0 and b1\nguesses['objective'] = guesses.apply(\n    lambda x: ols(\n        par = x, \n        X = df_happiness['life_exp_sc'], \n        y = df_happiness['happiness']\n    ),\n    axis = 1\n)\n\nmin_loss = guesses[guesses['objective'] == guesses['objective'].min()]\n\nmin_loss\n\n       b0    b1  objective\n899 5.400 0.900      0.491\n\n\n\n\n\nThe following plot shows the objective value for each guess in a smooth fashion as if we had a more continuous space. Darker values indicate we’re getting closer to our smallest objective value. The star notes our best guess.\n\n\n\n\n\n\nFigure 4.2: Results of parameter search\n\n\n\nIf we inspect our results from the standard functions, we had estimates of 5.44 and 0.89 for our coefficients versus the best guess from our approach of 5.4 and 0.9. These are very similar but not exactly the same, but this is mostly due to the granularity of our guesses. Even so, in the end we can see that we get pretty dang close to what our basic lm or statsmodels functions would get us. Pretty neat!\n\n\n\n\n\n\nEstimation as ‘Learning’\n\n\n\n\n\nEstimation and/or optimization can be seen as the process of a model learning which parameters will best allow the predictions to match the observed data, and hopefully, predict as-yet-unseen future data. This is a very common way to think about estimation in machine learning, and it is a useful way to think about our simple linear model also.\nOne thing to keep in mind is that it is not a magical process. It takes good data, a good idea (model), and an appropriate estimation method to get good results.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-opt",
    "href": "estimation.html#sec-estim-opt",
    "title": "4  How Did We Get Here?",
    "section": "4.6 Optimization",
    "text": "4.6 Optimization\nBefore we get into other objective functions, let’s think about a better way to find good parameters for our model. Rather than just guessing, we can use a more systematic approach, and thankfully, there are tools out there to help us. We just use a function like our OLS function, give it a starting point, and let the algorithms do the rest! These tools eventually arrive at a pretty good set of parameters, and are optimized for speed.\nPreviously we created a set of guesses, and tried each one in a manner called a grid search, and it is a bit of a brute force approach to finding the best fitting model. You can maybe imagine a couple of unfortunate scenarios for this approach, such as having a very large number of parameters to search. Or it may be that our range of guesses doesn’t allow us to find the right set of parameters, or we specify a very large range, but the best fitting model is within a very narrow part of that, so it takes a long time to find. In any of these cases we waste a lot of time or may not find an optimal solution.\nIn general, we can think of optimization as employing a smarter, more efficient way to find what you’re looking for. Here’s how it works:\n\nStart with an initial guess for the parameters, then…\nCalculate the objective function given the parameters\nUpdate the parameters to a new guess (that hopefully improves the objective function)\nRepeat, until the improvement is small enough, or a maximum number of iterations is reached.\n\nThe key idea now is how we update the old parameters with a new guess at each iteration. Different optimization algorithms use different approaches to find those new guesses. The process stops when the improvement is within a set tolerance or the maximum number of iterations is reached. If the objective is met, we say that our model has converged. Sometimes, the number of iterations is not enough for us to reach convergence in terms of tolerance, and we have to try again with a different set of parameters, a different algorithm, maybe use some data transformations, or something else.\nSo, let’s try it out! We start out with several inputs:\n\nthe objective function\nthe initial guess for the parameters to get things going\nother related inputs to the objective function\noptions for the optimization process, e.g. algorithm, maximum number of iterations, etc.\n\nWith these inputs, we’ll let the optimization functions do the rest of the work. We’ll also compare our results to the standard functions to make sure we’re on the right track.\n\nRPython\n\n\nWe’ll use the optim function in R.\n\nour_result = optim(\n    par = c(1, 0),\n    fn  = ols,\n    X   = df_happiness$life_exp_sc,\n    y   = df_happiness$happiness,\n    method  = 'BFGS', # optimization algorithm\n    control = list(   \n        tol   = 1e-6, # tolerance\n        maxit = 500   # max iterations\n    )  \n)\n\nour_result\n\n\n\nWe’ll use the minimize function in Python.\n\nfrom scipy.optimize import minimize\n\nour_result = minimize(\n    fun  = ols,\n    x0   = np.array([1., 0.]),\n    args = (\n        np.array(df_happiness['life_exp_sc']), \n        np.array(df_happiness['happiness'])\n    ),\n    method  = 'BFGS',   # optimization algorithm\n    tol     = 1e-6,     # tolerance\n    options = {\n        'maxiter': 500  # max iterations\n    }\n)\n\nour_result\n\n\n\n\nOptimization functions typically return multiple values, including the best parameters found, the value of the objective function at that point, and sometimes other information like the number of iterations it took to reach the returned value and whether or not the process converged. This can be quite a bit of stuff so we don’t show the output above, but we definitely encourage you to inspect it closely. The following table shows the estimated parameters and the objective value for our model, and we can compare it to the standard functions to see how we did.\n\n\n\n\nTable 4.3: Comparison of our results to standard function\n\n\n\n\n\n\n  \n    \n      Parameter\n      Standard\n      Our Result\n    \n  \n  \n    Intercept\n5.4450\n5.4450\n    Life Exp. Coef.\n0.8880\n0.8880\n    Objective/MSE\n0.4890\n0.4890\n  \n  \n  \n\n\n\n\n\n\n\nSo, our little function and the right tool allows us to come up with the same thing as base R and statsmodels! I hope you’re feeling pretty good at this point because you should! You just proved you could do what seemed before to be like magic, but really all it took is just a little knowledge about some key concepts to demystify the process. So, let’s keep going!\n\n\n\n\n\n\nA Note on Terminology\n\n\n\n\n\nThe objective function is often called the loss function, and sometimes the cost function. However, these both imply that we are trying to minimize the function, which is not always the case5, and it’s arbitrary whether you want to minimize or maximize the function.\nThe term metric, such as the MSE or AUC we’ve seen elsewhere, is a value that you might also want to use to evaluate the model. Some metrics are also used as an objective function. For instance, we might minimize MSE as our objective, but also calculate other metrics like Adjusted R-squared or Mean Absolute Error to evaluate the model.\nThis can be very confusing when starting out! We’ll try to stick to the term metric for additional values that we might want to examine, apart from the objective function value, which is specifically used for estimating model parameters.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-maxlike",
    "href": "estimation.html#sec-estim-maxlike",
    "title": "4  How Did We Get Here?",
    "section": "4.7 Maximum Likelihood",
    "text": "4.7 Maximum Likelihood\nIn our example, we’ve been minimizing the mean of the squared errors to find the best parameters for our model. But let’s think about this differently. Now we’d like you to think about the data generating process. Ignoring the model, imagine that each happiness value is generated by some random process, like drawing from a normal distribution. So, something like this would describe it mathematically:\n\\[\n\\text{happiness} \\sim N(\\text{mean}, \\text{sd})\n\\]\nwhere the mean is just the mean of happiness, and sd is its standard deviation. In other words, we can think of happiness as a random variable that is drawn from a normal distribution with mean and standard deviation as the parameters of that distribution.\nLet’s apply this idea to our linear model setting. In this case, the mean is a function of life expectancy, and we’re not sure what the standard deviation is, but we can go ahead and write our model as follows.\n\\[\n\\text{mean} = \\beta_0 + \\beta_1 * \\text{life\\_exp}\n\\] \\[\n\\text{happiness} \\sim N(\\text{mean}, \\text{sd})\n\\]\nNow, our model is estimating the parameters of the normal distribution. We have an extra parameter to estimate - the standard deviation, which is similar to our RMSE.\nIn our analysis, instead of merely comparing the predicted happiness score to the actual score by looking at their difference, we do something a little different to get a sense of their difference. We consider how likely it is to observe the actual happiness score based on our prediction. This value, known as the likelihood, depends on our model’s parameters, which include the coefficients. The likelihood is determined by the statistical distribution we’ve selected for our analysis. We can write this as:\n\\[\n\\text{Pr}(\\text{happiness} \\mid \\text{life\\_exp}, \\beta_0, \\beta_1, \\text{sd})\n\\]\n\\[\n\\text{Pr}(\\text{happiness} \\mid \\text{mean}, \\text{sd})\n\\]\nThinking more generally, the likelihood gives us the probability of the observed data given the parameter estimates.\n\\[\n\\text{Pr}(\\text{Data} \\mid \\text{Parameters})\n\\]\nThe following shows how to calculate a likelihood for our model. The values you see are called probability density values. They’re not exactly probabilities, but they show the relative likelihood of each observation6. You can think of them like you do for probabilities, but remember that likelihoods are slightly different.\n\nRPython\n\n\n\n# two example life expectancy scores, at the mean (0) and 1 sd above\nlife_expectancy = c(0, 1)\n\n# observed happiness scores\nhappiness = c(4, 5.2)\n\n# predicted happiness with rounded coefs\nmu = 5 + 1 * life_expectancy\n\n# just a guess for sigma\nsigma = .5\n\n# likelihood for each observation\nL = dnorm(happiness, mean = mu, sd = sigma)\nL\n\n[1] 0.1079819 0.2218417\n\n\n\n\n\nfrom scipy.stats import norm\n\n# two example life expectancy scores, at the mean (0) and 1 sd above\nlife_expectancy = np.array([0, 1])\n\n# observed happiness scores\nhappiness = np.array([4, 5.2])\n\n# predicted happiness with rounded coefs\nmu = 5 + 1 * life_expectancy\n\n# just a guess for sigma\nsigma = .5\n\n# likelihood for each observation\nL = norm.pdf(happiness, loc = mu, scale = sigma)\nL\n\narray([0.1080, 0.2218])\n\n\n\n\n\nWith a guess for the parameters and an assumption about the data’s distribution, we can calculate the likelihood of each data point. We get a total likelihood for all observations, similar to how we added squared errors previously. But unlike errors, we want more likelihood, not less. In theory we’d multiply each likelihood, but in practice we sum the log of the likelihood, otherwise values would get too small for our computers to handle. We can also turn our problem into a minimization problem by supply the negative log-likelihood, and then minimizing that value, which many optimization algorithms are designed to do7.\nThe following is a function we can use to calculate the likelihood of the data given our parameters. The actual likelihood value isn’t easily interpretable, but it reflects the relative likelihood of the data given the parameters, so higher is generally better. Since many default optimization algorithms are designed to minimize, we’ll multiply the likelihood by -1 to turn it into a minimization problem, so lower is better in that case. The value can also be used to compare models with different parameter guesses8. We’ll hold off with our result\n\nRPython\n\n\n\nmax_likelihood = function(par, X, y) {\n\n    # setup\n    X = cbind(1, X)     # add a column of 1s for the intercept\n    beta = par[-1]      # coefficients\n    sigma = exp(par[1]) # error sd, exp keeps positive\n    N = nrow(X)\n\n    LP = X %*% beta     # linear predictor\n    mu = LP             # identity link in the glm sense\n\n    # calculate (log) likelihood\n    ll = dnorm(y, mean = mu, sd = sigma, log = TRUE)\n\n    value = -sum(ll)    # negative for minimization\n\n    return(value)\n}\n\nour_result = optim(\n    par = c(1, 0, 0),   # first param is sigma\n    fn  = max_likelihood,\n    X   = df_happiness$life_exp_sc,\n    y   = df_happiness$happiness\n)\n\nour_result\n\n\n\n\ndef max_likelihood(par, X, y):\n    \n    # setup\n    X = np.c_[np.ones(X.shape[0]), X]  # add a column of 1s for the intercept\n    beta = par[1:]         # coefficients\n    sigma = np.exp(par[0])  # error sd, exp keeps positive\n    N = X.shape[0]\n\n    LP = X @ beta            # linear predictor\n    mu = LP                  # identity link in the glm sense\n\n    # calculate (log) likelihood\n    ll = norm.logpdf(y, loc = mu, scale = sigma)\n\n    value = -np.sum(ll)      # negative for minimization\n\n    return value\n\nour_result = minimize(\n    fun  = max_likelihood,\n    x0   = np.array([1, 0, 0]), # first param is sigma\n    args = (\n        np.array(df_happiness['life_exp_sc']), \n        np.array(df_happiness['happiness'])\n    )\n)\n\nour_result\n\n\n\n\nWe can compare our result to a built-in function that has capabilities beyond OLS, and the table shows we’re duplicating the basic result. We show more decimal places on the log likelihood estimate to prove we aren’t getting exactly the same result.\n\n\n\n\nTable 4.4: Comparison of our results to the built-in function\n\n\n\n\n\n\n  \n    \n      Parameter\n      Standard\n      Our Result\n    \n  \n  \n    Intercept\n5.445\n5.445\n    Life Exp. Coef.\n0.888\n0.888\n    Sigma1\n0.705\n0.699\n    LogLik (neg)\n118.804\n118.804\n  \n  \n  \n    \n      1 Parameter estimate is exponentiated for 'our result'\n    \n  \n\n\n\n\n\n\n\nTo use a maximum likelihood approach for linear models, you can use functions like glm in R or GLM in Python, which is the reference used in the table above. We can also use different likelihoods corresponding to the binomial, poisson and other distributions. Still other packages would allow even more distributions for consideration. In general, we choose a distribution that we feel best reflects the data generating process. For binary targets for example, we typically would feel a bernoulli or binomial distribution is appropriate. For count data, we might choose a poisson or negative binomial distribution. For targets that fall between 0 and 1, we might go for a beta distribution. You can see some of these demonstrated in Chapter 5.\nThere are many distributions to choose from, and the best one depends on your data. Sometimes, even if one distribution seems like a better fit, we might choose another one because it’s easier to use. Some distributions are special cases of others, or they might become more like a normal distribution under certain conditions. For example, the exponential distribution is a special case of the gamma distribution, and a t distribution with many degrees of freedom looks like a normal distribution. Here is a visualization of the relationships among some of the more common distributions (Wikipedia (2023)).\n\n\n\n\n\n\n\n\nFigure 4.3: Relationships among some of probability distributions\n\n\n\n\n\nWhen you realize that many distributions are closely related, it’s easier to understand why we might choose one over another, but also why we might use a simpler option even if it’s not the best fit - you likely won’t come to a different conclusion about your model. Ultimately, you’ll get a better feel for this as you work with different types of data and models.\nHere are examples of standard GLM functions, which just require an extra argument for the family of the distribution.\n\nRPython\n\n\n\nglm(happiness ~ life_exp_sc, data = df_happiness, family = gaussian)\nglm(binary_target ~ x1 + x2, data = some_data, family = binomial)\nglm(count ~ x1 + x2, data = some_data, family = poisson)\n\n\n\n\nimport statsmodels.formula.api as smf\n\nsmf.glm(\n    'happiness ~ life_exp_sc', \n    data = df_happiness, \n    family = sm.families.Gaussian()\n)\n\nsmf.glm(\n    'binary_target ~ x1 + x2', \n    data = some_data, \n    family = sm.families.Binomial()\n)\n\nsmf.glm(\n    'count ~ x1 + x2', \n    data = some_data, \n    family = sm.families.Poisson()\n)\n\n\n\n\n\n4.7.1 Diving deeper\nLet’s think more about what’s going on here. It turns out that our objective function defines a ‘space’ or ‘surface’. You can imagine the process as searching for the lowest point on a landscape, with each guess a point on this landscape. Let’s start to get a sense of this with the following visualization, based on a single parameter. The following visualization shows this for a single parameter. The data comes from a variable with a true average of 5. As our guesses get closer to 5, the likelihood increases. However, with more and more data, the final guess converges on the true value. Model estimation finds that maximum on the curve, and optimization algorithms are the means to find it.\n\n\n\n\n\n\n\n\nFigure 4.4: Likelihood function one parameter\n\n\n\n\n\nNow let’s add a parameter. If we have more than one parameter, we now have a surface to deal with. Given a starting point, an optimization procedure then travels along the surface looking for a minimum/maximum point. For simpler settings such as this, we can visualize the likelihood surface and its minimum point. However, even our simple model has three parameters plus the likelihood, so it would be difficult to visualize without additional complexity. Instead, we show the results for an alternate model where happiness is standardized also, which means the intercept is zero9, and so not shown.\n\n\n\n\n\n\n\n\nFigure 4.5: Likelihood surface for happiness and life expectancy (interactive)\n\n\n\n\nWe can also see the path our estimates take. Starting at a fairly bad estimate, the optimization algorithm quickly updates to estimates that result in a better likelihood value. We also see little exploratory jumps creating a star like pattern, before things ultimately settle to the best values. In general, these updates and paths are dependent on the optimization algorithm one uses.\n\n\n\n\n\n\n\n\nFigure 4.6: Optimization path for two parameters\n\n\n\n\n\n\nWhat we’ve shown here with maximum likelihood applies in general to searching for the best solution along an objective function surface. The optimization algorithms we’ll use are general purpose, and can be used for many different types of problems. The key is to define an appropriate objective function, and then let the algorithm do the work.\n\n\n\n\n\n\nMLE & OLS\n\n\n\n\n\nFor linear regression assuming a normal distribution, the maximum likelihood estimate of the standard deviation is the OLS estimate of the standard deviation of the residuals. Furthermore, the maximum likelihood coefficient estimates and OLS estimates converge to the same estimates as the sample size increases. For most data sizes in practice these estimates are indistinguishable, and the OLS estimate is the maximum likelihood estimate for linear regression. So OLS and variants (such as those used for GLM) are maximum likelihood estimation methods.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-penalty",
    "href": "estimation.html#sec-estim-penalty",
    "title": "4  How Did We Get Here?",
    "section": "4.8 Penalized Objectives",
    "text": "4.8 Penalized Objectives\n\nOne thing we may want to take into account with our models is their complexity, especially in the context of overfitting. We talk about this with machine learning also (Chapter 7), but the basic idea is that we can get too familiar with the data we have, and when we try to predict on new data the model hasn’t seen before, our performance suffers, or even gets worse than a simpler model. In other words, we are not generalizing well (Section 7.4).\nOne way to deal with this is to penalize the objective function value for complexity, or at least favor simpler models that might do as well. In some contexts this is called regularization, and in other contexts shrinkage, since the parameter estimates are typically shrunk toward some specific value (e.g., zero).\nAs a starting point, in our basic linear model we can add a penalty that is applied to the size of coefficients. This is called ridge regression, or, more mathily, as L2 regularization. We can write this formally as:\n\\[\n\\text{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n\\tag{4.2}\\]\nThe first part is the same as basic OLS (Equation 4.1), but the second part is the penalty for \\(p\\) features. The penalty is the sum of the squared coefficients multiplied by some value, which we call \\(\\lambda\\). This is an additional model parameter that we typically want to estimate, e.g. through cross-validation. This kind of parameter is often called a hyperparameter, mostly just to distinguish it from those that may be of actual interest. For example, we could probably care less what the actual value for \\(\\lambda\\) is, but we would still be interested in the coefficients.\nIn the end this is just a small change to ordinary least squares (OLS) regression (Equation 4.1), but it can make a big difference. It introduces some bias in the coefficients - recall that OLS is unbiased if assumptions are met - but it can help to reduce variance, which can help the model perform better on new data (Section 7.4.2). In other words, we are willing to accept some bias in order to get a model that generalizes better.\nBut let’s get to a code example to demystify this a bit! Here is an example of a function that calculates the ridge objective. To make things interesting, let’s add the other features we talked about regarding GDP per capita and perceptions of corruption.\n\nRPython\n\n\n\nridge = function(par, X, y, lambda = 0) {\n    # add a column of 1s for the intercept\n    X = cbind(1, X)\n    \n    mu = X %*% par # linear predictor\n\n    # Calculate the value as sum squared error\n    error = sum((y - mu)^2)\n\n    # Add the penalty\n    value = error + lambda * sum(par^2)\n\n    return(value)\n}\n\nX = df_happiness |&gt;\n    select(-happiness, -country) |&gt; \n    as.matrix()\n\nour_result = optim(\n    par = c(0, 0, 0, 0),\n    fn = ridge,\n    X = X,\n    y = df_happiness$happiness,\n    lambda = 0.1,\n    method = 'BFGS'\n)\n\nour_result$par\n\n\n\n\n# we use lambda_ because lambda is a reserved word in python\ndef ridge(par, X, y, lambda_ = 0):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    mu = X @ par\n    \n    # Calculate the error\n    value = np.sum((y - mu)**2)\n    \n    # Add the penalty\n    value = value + lambda_ * np.sum(par**2)\n    \n    return value\n\nour_result = minimize(\n    fun  = ridge,\n    x0   = np.array([0, 0, 0, 0]),\n    args = (\n        np.array(df_happiness.drop(columns=['happiness', 'country'])),\n        np.array(df_happiness['happiness']), \n        0.1\n    )\n)\n\nour_result['x']\n\n\n\n\nWe can compare this to built-in functions as we have before, and can see that the results are very similar, but not exactly the same. We would not worry about such differences in practice, but the main point is again, we can use simple functions that do just about as well as any what we’d get from package output.\n\n\n\n\nTable 4.5: Comparison of ridge regression results\n\n\n\n\n\n\n  \n    \n      Parameter\n      Standard1\n      Our Result\n    \n  \n  \n    Intercept\n5.44\n5.44\n    life_exp_sc\n0.49\n0.52\n    corrupt_sc\n−0.12\n−0.11\n    gdp_pc_sc\n0.42\n0.44\n  \n  \n  \n    \n      1 Showing results from R glmnet package with alpha = 0, lambda = .1\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytical Solution for Ridge Regression\n\n\n\n\n\nIt turns out that given a fixed \\(\\lambda\\) penalty ridge regression estimates can be estimated analytically. Michael has an example if interested at his Model Estimation by Example demos.(Clark (2021)).\n\n\n\nAnother very common penalized approach is to use the sum of the absolute value of the coefficients, which is called lasso regression or L1 regularization. An interesting property of the lasso is that in typical implementations, it will potentially zero out coefficients, which is the same as dropping the feature from the model altogether. This is a form of feature selection or variable selection. The true values are never zero, but if we want to use a ‘best subset’ of features, this is one way we could do so. We can write the lasso objective as follows. The chapter exercise asks you to implement this yourself.\n\\[\n\\text{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\tag{4.3}\\]",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-classification",
    "href": "estimation.html#sec-estim-classification",
    "title": "4  How Did We Get Here?",
    "section": "4.9 Classification",
    "text": "4.9 Classification\nSo far, we’ve been assuming a continuous target, but what if we have a categorical target? Now we have to learn a bunch of new stuff for that situation, right? Actually, no! When we want to model categorical targets, conceptually, nothing changes! We still have an objective function that maximizes or minimizes some goal, and we can use the same algorithms to estimate parameters. However, we need to think about how we can do this in a way that makes sense for the binary target.\n\n4.9.1 Misclassification rate\nA straightforward correspondence to MSE is a function that minimizes classification error, or by the same token, maximizes accuracy. In other words, we can think of the objective function as the proportion of incorrect classifications. This is called the misclassification rate.\n\\[\n\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i \\neq \\hat{y_i})\n\\tag{4.4}\\]\nIn the equation, \\(y_i\\) is the actual value of the target for observation \\(i\\), arbitrarily coded as 1 or 0, and \\(\\hat{y_i}\\) is the predicted class from the model. The \\(\\mathbb{1}\\) is an indicator function that returns 1 if the condition is true, and 0 otherwise. In other words, we are counting the number of times the predicted value is not equal to the actual value, and dividing by the number of observations. Very straightforward, so let’s do this ourselves!\n\nRPython\n\n\n\nmisclassification = function(par, X, y, class_threshold = .5) {\n    X = cbind(1, X)\n\n    # Calculate the 'linear predictor'\n    mu = X %*% par\n\n    # Convert to a probability ('sigmoid' function)\n    p = 1 / (1 + exp(-mu))\n\n    # Convert to a class\n    predicted_class = as.integer(\n        ifelse(p &gt; class_threshold, 'good', 'bad')\n    )\n\n    # Calculate the mean error\n    value = mean(y - predicted_class)\n\n    return(value)\n}\n\n\n\n\ndef misclassification_rate(par, X, y, class_threshold = .5):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the 'linear predictor'\n    mu = X @ par \n    \n    # Convert to a probability ('sigmoid' function)\n    p = 1 / (1 + np.exp(-mu))\n    \n    # Convert to a class\n    predicted_class = np.where(p &gt; class_threshold, 1, 0)\n    \n    # Calculate the mean error\n    value = np.mean(y - predicted_class)\n\n    return value\n\n\n\n\nNote that our function first adds a step to convert the linear predictor (called mu) to a probability. Once we have a probability, we use some threshold to convert it to a ‘class’. In this case, we use 0.5 as the threshold, but this could be different depending on the context, something we talk more about elsewhere (Section 3.2.2.7). We’ll leave it as an exercise for you to play around with this, as the next objective function is more commonly used. But at least you can see how easy it can be to switch to the classification case.\n\n\n4.9.2 Log loss\nAnother approach is to use the log loss, sometimes called logistic loss or cross-entropy. If we have just the binary case it is:\n\\[\n\\text{Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})\n\\tag{4.5}\\]\nWhere \\(y_i\\) is the actual value of the target for observation \\(i\\), and \\(\\hat{y_i}\\) is the predicted value from the model (essentially a probability). It turns out that this is the same as the log-likelihood used in a maximum likelihood approach for logistic regression, made negative so we can minimize it.\nWe typically prefer this objective function to classification error because it results in a smooth optimization surface, like in the visualization we showed before for maximum likelihood, which means it is differentiable in a mathematical sense. This is important because it allows us to use optimization algorithms that rely on derivatives in updating the parameter estimates. You don’t really need to get into that too much, but just know that a smoother objective function is something we prefer. Here’s some code to try out.\n\nRPython\n\n\n\nobjective = function(par, X, y) {\n    X = cbind(1, X)\n\n    # Calculate the predicted values on the raw scale\n    y_hat = X %*% par\n\n    # Convert to a probability ('sigmoid' function)\n    y_hat = 1 / (1 + exp(-y_hat))\n\n    # likelihood\n    ll = y * log(y_hat) + (1 - y) * log(1 - y_hat)\n\n    # alternative\n    # dbinom(y, size = 1, prob = y_hat, log = TRUE)\n\n    value = -sum(ll)\n\n    return(value)\n}\n\n\n\n\ndef objective(par, X, y):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    y_hat = X @ par\n    \n    # Convert to a probability ('sigmoid' function)\n    y_hat = 1 / (1 + np.exp(-y_hat))\n    \n    # likelihood\n    ll = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n\n    value = -np.sum(ll)\n    \n    return value\n\n\n\n\nLet’s go ahead and demonstrate this. To create a classification problem, we’ll say that a country is ‘happy’ if the happiness score is greater than 5.5, and ‘unhappy’ otherwise. We’ll use the same features as before.\n\nRPython\n\n\n\ndf_happiness_bin = df_happiness |&gt;\n    mutate(happiness = ifelse(happiness &gt; 5.5, 1, 0))\n\nmodel_logloss = optim(\n    par = c(0, 0, 0, 0),\n    fn = objective,\n    X = df_happiness_bin |&gt;\n        select(life_exp_sc:gdp_pc_sc) |&gt;\n        as.matrix(),\n    y = df_happiness_bin$happiness\n)\n\nmodel_glm = glm(\n    happiness ~ life_exp_sc + corrupt_sc + gdp_pc_sc,\n    data   = df_happiness_bin,\n    family = binomial\n)\n\nmodel_logloss$par\n\n\n\n\ndf_happiness_bin = df_happiness.copy()\ndf_happiness_bin['happiness'] = np.where(df_happiness['happiness'] &gt; 5.5, 1, 0)\n\nmodel_logloss = minimize(\n    objective,\n    x0 = np.array([0, 0, 0, 0]),\n    args = (\n        df_happiness_bin[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']],\n        df_happiness_bin['happiness']\n    )\n)\n\nmodel_glm = smf.glm(\n    'happiness ~ life_exp_sc + corrupt_sc + gdp_pc_sc',\n    data   = df_happiness_bin,\n    family = sm.families.Binomial()\n).fit()\n\nmodel_logloss['x']\n\n\n\n\nOnce again, we can see that the results are very similar between our classification result and the built-in function.\n\n\n\n\nTable 4.6: Comparison of log loss results\n\n\n\n\n\n\n  \n    \n      name\n      Ours\n      GLM\n    \n  \n  \n    LogLike\n40.6635\n40.6635\n    intercept\n−0.1642\n−0.1637\n    life_exp_sc\n1.8168\n1.8172\n    corrupt_sc\n−0.4638\n−0.4648\n    gdp_pc_sc\n1.1307\n1.1311\n  \n  \n  \n\n\n\n\n\n\n\nSo, when it comes to classification, you should feel confident in what’s going on under the hood, just like you did with a numeric target. Too much is made of the distinction between ‘regression’ and ‘classification’ and it can be confusing to those starting out. In reality, classification just requires a slightly different way of thinking about the target. Conceptually it really is the same approach.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-opt-algos",
    "href": "estimation.html#sec-estim-opt-algos",
    "title": "4  How Did We Get Here?",
    "section": "4.10 Optimization Algorithms",
    "text": "4.10 Optimization Algorithms\nWhen it comes to optimization, there are a number of algorithms that have been developed over time. The main thing to keep in mind is that these are all just ways to find the best fitting parameters for a model. Some may be better suited for certain data tasks, or provide computational advantages, but often the choice of algorithm is not as important as many other modeling choices.\nHere are some of the options available in R’s optim or scipy’s minimize function:\n\nNelder-Mead\nBFGS\nL-BFGS-B (provides constraints)\nConjugate gradient\nSimulated annealing\nNewton’s method\nGenetic algorithms\nOther Popular SGD extensions and variants\n\nRMSProp\nAdam/momentum\nAdagrad\n\n\nThe main reason to choose one method over another usually is based on factors like speed, memory use, or how well they work for certain models. For statistical contexts, many functions for generalized linear models use Newton’s method by default, but more complicated models may implement a different approach for better convergence. In machine learning, stochastic gradient descent is popular because it can be efficient in large data settings and relatively easy to implement.\nIn general, we can always try different methods to see which works best, but usually the results will be similar if the results reach convergence. We’ll now demonstrate one of the most popular optimization methods used in machine learning - gradient descent, but know that there are many variants of this one might use.\n\n4.10.1 Gradient descent\nOne of the most popular approaches in optimization is called gradient descent. It uses the gradient of the function we’re trying to optimize to find the best parameters. We still use objective functions as before, and gradient descent is just a way to find that path along the objective surface. More formally, the gradient is the vector of partial derivatives of the objective function with respect to each parameter. That may not mean much to you, but the basic idea is that the gradient provides a direction that points in the direction of steepest increase in the function. So if we want to maximize the objective function, we can take a step in the direction of the gradient, and if we want to minimize it, we can take a step in the opposite direction of the gradient (use the negative gradient). The size of the step is called the learning rate, and, like our penalty parameter we saw with penalized regression, it is a hyperparameter that we can tune through cross-validation (Section 7.7). If the learning rate is too small, it will take a longer time to converge. If it’s too large, we might overshoot the objective and miss the best parameters. There are a number of variations on gradient descent that have been developed over time. Let’s see this in action with the world happiness model.\n\nRPython\n\n\n\ngradient_descent = function(\n    par,\n    X,\n    y,\n    tolerance = 1e-3,\n    maxit = 1000,\n    learning_rate = 1e-3\n) {\n    \n    X = cbind(1, X) # add a column of 1s for the intercept\n    N = nrow(X)\n\n    # initialize\n    beta = par\n    names(beta) = colnames(X)\n    mse = crossprod(X %*% beta - y) / N  # crossprod provides sum(x^2)\n    tol = 1\n    iter = 1\n\n    while (tol &gt; tolerance && iter &lt; maxit) {\n        LP = X %*% beta\n        grad = t(X) %*% (LP - y)\n        betaCurrent = beta - learning_rate * grad\n        tol = max(abs(betaCurrent - beta))\n        beta = betaCurrent\n        mse = append(mse, crossprod(LP - y) / N)\n        iter = iter + 1\n    }\n\n    output = list(\n        par    = beta,\n        loss   = mse,\n        MSE    = crossprod(LP - y) / nrow(X),\n        iter   = iter,\n        predictions = LP\n    )\n\n    return(output)\n}\n\nX = df_happiness |&gt;\n    select(life_exp_sc:gdp_pc_sc) |&gt;\n    as.matrix()\n\nour_result = gradient_descent(\n    par = c(0, 0, 0, 0),\n    X = X,\n    y = df_happiness$happiness,\n    learning_rate = 1e-3\n)\n\n\n\n\ndef gradient_descent(\n    par, \n    X, \n    y, \n    tolerance = 1e-3, \n    maxit = 1000, \n    learning_rate = 1e-3\n):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n    \n    # initialize\n    beta = par\n    loss = np.sum((X @ beta - y)**2)\n    tol = 1\n    iter = 1\n\n    while (tol &gt; tolerance and iter &lt; maxit):\n        LP = X @ beta\n        grad = X.T @ (LP - y)\n        betaCurrent = beta - learning_rate * grad\n        tol = np.max(np.abs(betaCurrent - beta))\n        beta = betaCurrent\n        loss = np.append(loss, np.sum((LP - y)**2))\n        iter = iter + 1\n\n    output = {\n        'par': beta,\n        'loss': loss,\n        'MSE': np.mean((LP - y)**2),\n        'iter': iter,\n        'predictions': LP\n    }\n\n    return output\n\nour_result = gradient_descent(\n    par = np.array([0, 0, 0, 0]),\n    X = df_happiness[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']].to_numpy(),\n    y = df_happiness['happiness'].to_numpy(),\n    learning_rate = 1e-3\n)\n\n\n\n\nComparing our results, we have the following table. As usual, we see that the results are very similar. Once again, we have demystified a step in the modeling process!\n\n\n\n\nTable 4.7: Comparison of gradient descent results\n\n\n\n\n\n\n  \n    \n      Value\n      Standard\n      Our Result\n    \n  \n  \n    Intercept\n5.445\n5.437\n    life_exp_sc\n0.525\n0.521\n    corrupt_sc\n−0.105\n−0.107\n    gdp_pc_sc\n0.438\n0.439\n    MSE\n0.367\n0.367\n  \n  \n  \n\n\n\n\n\n\n\nIn addition, when we visualize the loss function across iterations, we see a smooth decline in the MSE value as we go along each iteration. This is a good sign that we are converging to an optimal solution.\n\n\n\n\n\n\n\n\nFigure 4.7: Loss with gradient descent\n\n\n\n\n\n\n\n4.10.2 Stochastic gradient descent\nStochastic gradient descent (SGD) is a version of gradient descent that uses a random sample of data to guess the gradient, instead of using all the data. This makes it less accurate in some ways, but it’s faster and can be parallelized. This speed is useful in machine learning when there’s a lot of data, which often makes the discrepancy between standard GD and SGD small. As such you will see variants of it incorporated in many models in deep learning, but it can be with much simpler models as well.\nLet’s see this in action with the happiness model. The following is a conceptual version of the AdaGrad approach10, which is a variation of SGD that adjusts the learning rate for each parameter. We will also add a variation that averages the parameter estimates across iterations, which is a common approach to improve the performance of SGD, but by default it is not used, just something you can play with. We are going to use a ‘batch size’ of one, which is similar to a ‘streaming’ or ‘online’ version where we update the model with each observation. Since our data are alphabetically ordered, we’ll shuffle the data first. We’ll also use a stepsize_tau parameter, which is a way to adjust the learning rate at early iterations. We’ll set it to zero for now, but you can play with it to see how it affects the results. The values for the learning rate and stepsize_tau are arbitrary, selected after some initial playing around, but you can play with them to see how they affect the results.\n\nRPython\n\n\n\nstochastic_gradient_descent = function(\n    par, # parameter estimates\n    X,   # model matrix\n    y,   # target variable\n    learning_rate = 1, # the learning rate\n    stepsize_tau = 0   # if &gt; 0, a check on the LR at early iterations\n    ) {\n    # initialize\n    set.seed(123)\n\n    # shuffle the data\n    idx = sample(1:nrow(X), nrow(X))\n    X = X[idx, ]\n    y = y[idx]\n\n    X = cbind(1, X)\n    beta = par\n\n    # Collect all estimates\n    betamat = matrix(0, nrow(X), ncol = length(beta))\n\n    # Collect fitted values at each point))\n    fits = NA\n\n    # Collect loss at each point\n    loss = NA\n\n    # adagrad per parameter learning rate adjustment\n    s = 0\n\n    # a smoothing term to avoid division by zero\n    eps = 1e-8\n\n    for (i in 1:nrow(X)) {\n        Xi = X[i, , drop = FALSE]\n        yi = y[i]\n\n        # matrix operations not necessary here,\n        # but makes consistent with previous gd func\n        LP = Xi %*% beta\n        grad = t(Xi) %*% (LP - yi)\n        s = s + grad^2  # adagrad approach\n\n        # update\n        beta = beta - learning_rate / \n            (stepsize_tau + sqrt(s + eps)) * grad\n        betamat[i, ] = beta\n\n        fits[i] = LP\n        \n        loss[i] = crossprod(LP - yi)\n        \n    }\n\n    LP = X %*% beta\n    lastloss = crossprod(LP - y)\n\n    output = list(\n        par = beta,           # final estimates\n        par_chain = betamat,  # estimates at each iteration\n        MSE = sum(lastloss) / nrow(X),\n        predictions = LP\n    )\n    \n    return(output)\n}\n\nX_train = df_happiness |&gt;\n    select(life_exp_sc, corrupt_sc, gdp_pc_sc) |&gt;\n    as.matrix()\n\ny_train = df_happiness$happiness\n\nour_result = stochastic_gradient_descent(\n    par = c(mean(df_happiness$happiness), 0, 0, 0),\n    X = X_train,\n    y = y_train,\n    learning_rate = .15,\n    stepsize_tau = .1\n)\n\nour_result$par\n\n\n\n\ndef stochastic_gradient_descent(\n    par, # parameter estimates\n    X,   # model matrix\n    y,   # target variable\n    learning_rate = 1, # the learning rate\n    stepsize_tau = 0,  # if &gt; 0, a check on the LR at early iterations\n    average = False    # a variation of the approach\n):\n    # initialize\n    np.random.seed(1234)\n\n    # shuffle the data\n    idx = np.random.choice(\n        df_happiness.shape[0], \n        df_happiness.shape[0], \n        replace = False\n    )\n    X = X[idx, :]\n    y = y[idx]\n    \n    X = np.c_[np.ones(X.shape[0]), X]\n    beta = par\n\n    # Collect all estimates\n    betamat = np.zeros((X.shape[0], beta.shape[0]))\n\n    # Collect fitted values at each point))\n    fits = np.zeros(X.shape[0])\n\n    # Collect loss at each point\n    loss = np.zeros(X.shape[0])\n\n    # adagrad per parameter learning rate adjustment\n    s = 0\n\n    # a smoothing term to avoid division by zero\n    eps = 1e-8\n\n    for i in range(X.shape[0]):\n        Xi = X[None, i, :]\n        yi = y[i]\n\n        # matrix operations not necessary here,\n        # but makes consistent with previous gd func\n        LP = Xi @ beta\n        grad = Xi.T @ (LP - yi)\n        s = s + grad**2 # adagrad approach\n\n        # update\n        beta = beta - learning_rate / \\\n            (stepsize_tau + np.sqrt(s + eps)) * grad\n\n        betamat[i, :] = beta\n\n        fits[i] = LP\n        loss[i] = np.sum((LP - yi)**2)\n\n    LP = X @ beta\n    lastloss = np.sum((LP - y)**2)\n\n    output = {\n        'par': beta,          # final estimates\n        'par_chain': betamat, # estimates at each iteration\n        'MSE': lastloss / X.shape[0],\n        'predictions': LP\n    }\n\n    return output\n\nX_train = df_happiness[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']]\ny_train = df_happiness['happiness']\n\nour_result = stochastic_gradient_descent(\n    par = np.array([np.mean(df_happiness['happiness']), 0, 0, 0]),\n    X = X_train.to_numpy(),\n    y = y_train.to_numpy(),\n    learning_rate = .15,\n    stepsize_tau = .1\n)\n\nour_result['par']\n\n\n\n\nNext we’ll compare it to OLS estimates and our previous ‘batch’ gradient descent results. Even though SGD normally would not be used for such a small dataset, we at least get close11!\n\n\n\n\nTable 4.8: Comparison of stochastic gradient descent results\n\n\n\n\n\n\n  \n    \n      Value\n      Standard\n      Our Result\n      Batch SGD\n    \n  \n  \n    Intercept\n5.445\n5.469\n5.437\n    life_exp_sc\n0.525\n0.514\n0.521\n    corrupt_sc\n−0.105\n−0.111\n−0.107\n    gdp_pc_sc\n0.438\n0.390\n0.439\n    MSE\n0.367\n0.370\n0.367\n  \n  \n  \n\n\n\n\n\n\n\nHere’s a plot of the estimates as they moved along the data. For this plot we don’t include the intercept as it’s on a notably different scale. We can see that the estimates are moving around a bit, but they appear to be converging to a solution.\n\n\n\n\n\n\n\n\nFigure 4.8: Stochastic gradient descent path\n\n\n\n\n\n\n\nTo wrap things up, here are the results for the happiness model using different optimization algorithms, with a comparison to the standard linear regression model function. We can see that the results are very similar, and for simpler modeling endeavors they should converge on the same result.\n\n\n\n\nTable 4.9: Comparison of optimization results\n\n\n\n\n\n\n  \n    \n      parameter\n      NM1\n      BFGS2\n      CG3\n      GD4\n      Standard5\n    \n  \n  \n    Intercept\n5.445\n5.445\n5.445\n5.437\n5.445\n    life_exp_sc\n0.525\n0.525\n0.525\n0.521\n0.525\n    gdp_pc_sc\n−0.105\n−0.105\n−0.105\n−0.107\n−0.105\n    corrupt_sc\n0.437\n0.438\n0.438\n0.439\n0.438\n    MSE\n0.367\n0.367\n0.367\n0.367\n0.367\n  \n  \n  \n    \n      1 NM = Nelder-Mead\n    \n    \n      2 BFGS = Broyden–Fletcher–Goldfarb–Shanno\n    \n    \n      3 CG = Conjugate gradient\n    \n    \n      4 GD = Gradient descent\n    \n    \n      5 Standard = Standard package function",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-other",
    "href": "estimation.html#sec-estim-other",
    "title": "4  How Did We Get Here?",
    "section": "4.11 Other Estimation Approaches",
    "text": "4.11 Other Estimation Approaches\nBefore leaving our estimation discussion, we should mention there are other approaches one could use, including variations on least squares, the method of moments, generalized estimating equations, robust estimation, and more. We’ve focused on the most common ones, but it’s good to be aware of others that might be more common in some domains. But there are two we want to discuss in a little bit detail given their widespread usage, and that is the bootstrap and Bayesian estimation.\n\n4.11.1 Bootstrap\nThe bootstrap is a method where we create new data sets by randomly sampling the data from our original set, allowing the same data to be picked more than once. We then use these new data sets to estimate our model. We do this many times, collecting parameter estimates, predictions, or anything we want to calculate along the way. Ultimately, we end up with a distribution of all the things we calculated.\nThese results give us a range of possible outcomes, which is useful for inference12, as we can use the distribution to calculate confidence intervals, prediction intervals or intervals for any value we calculate. The average estimate is often the same as whatever the underlying model used would produce, but the bootstrap provides a way to get at a measure of uncertainty with fewer assumptions about how that distribution should take shape. The approach is very flexible, and it can be used with any model. Let’s see this in action with the happiness model.\n\nRPython\n\n\n\nbootstrap = function(X, y, nboot = 100, seed = 123) {\n    \n    N = nrow(X)\n    p = ncol(X) + 1 # add one for intercept\n\n    # initialize\n    beta = matrix(NA, p*nboot, nrow = nboot, ncol = p)\n    colnames(beta) = c('Intercept', colnames(X))\n    mse = rep(NA, nboot)\n\n    # set seed\n    set.seed(seed)\n\n    for (i in 1:nboot) {\n        # sample with replacement\n        idx = sample(1:N, N, replace = TRUE)\n        Xi = X[idx,]\n        yi = y[idx]\n\n        # estimate model\n        mod = lm(yi ~., data = Xi)\n\n        # save results\n        beta[i, ] = coef(mod)\n        mse[i] = sum((mod$fitted - yi)^2) / N\n    }\n\n    # given mean estimates, calculate MSE\n    y_hat = cbind(1, as.matrix(X)) %*% colMeans(beta)\n    final_mse = sum((y - y_hat)^2) / N\n\n    output = list(\n        par = as_tibble(beta),\n        MSE = mse,\n        final_mse = final_mse\n    )\n\n    return(output)\n}\n\nX = df_happiness |&gt;\n    select(life_exp_sc:gdp_pc_sc)\n\nour_result = bootstrap(\n    X = X,\n    y = df_happiness$happiness,\n    nboot = 250\n)\n\ncolMeans(our_result$par)\n\n  Intercept life_exp_sc  corrupt_sc   gdp_pc_sc \n  5.4424683   0.5083511  -0.1023755   0.4639788 \n\n\n\n\n\n# from sklearn.linear_model import LinearRegression \n\ndef bootstrap(X, y, nboot=100, seed=123):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n    N = X.shape[0]\n\n    # initialize\n    beta = np.empty((nboot, X.shape[1]))\n    \n    # beta = pd.DataFrame(beta, columns=['Intercept'] + list(cn))\n    mse = np.empty(nboot)    \n\n    # set seed\n    np.random.seed(seed)\n\n    for i in range(nboot):\n        # sample with replacement\n        idx = np.random.randint(0, N, N)\n        Xi = X[idx, :]\n        yi = y[idx]\n\n        # estimate model\n        model = LinearRegression(fit_intercept=False)\n        mod = model.fit(Xi, yi)\n\n        # save results\n        beta[i, :] = mod.coef_\n        mse[i] = np.sum((mod.predict(Xi) - yi)**2) / N\n\n    # given mean estimates, calculate MSE\n    y_hat = X @ beta.mean(axis=0)\n    final_mse = np.sum((y - y_hat)**2) / N\n\n    output = {\n        'par': beta,\n        'mse': mse,\n        'final_mse': final_mse\n    }\n\n    return output\n\nour_result = bootstrap(\n    X = df_happiness[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']],\n    y = df_happiness['happiness'],\n    nboot = 250\n)\n\nnp.mean(our_result['par'], axis=0)\n\n\n\n\nHere are the results of the interval estimates for the coefficients. Each parameter has the mean estimate, the lower and upper bounds of the 95% confidence interval, and the width of the interval. The bootstrap intervals are a bit wider than the OLS intervals, possibly better capturing the uncertainty in this model based on not too many observations.\n\n\n\n\nTable 4.10: Bootstrap parameter estimates\n\n\n\n\n\n\n  \n    \n      Parameter\n      mean\n      Lower BS\n      Upper BS\n      Lower OLS\n      Upper OLS\n      Diff Width1\n    \n  \n  \n    Intercept\n5.44\n5.33\n5.55\n5.33\n5.56\n−0.02\n    life_exp_sc\n0.51\n0.30\n0.70\n0.35\n0.70\n0.04\n    corrupt_sc\n−0.10\n−0.29\n0.09\n−0.25\n0.04\n0.09\n    gdp_pc_sc\n0.46\n0.18\n0.76\n0.24\n0.64\n0.18\n  \n  \n  \n    \n      1 Width of bootstrap estimate minus width of OLS estimate\n    \n  \n\n\n\n\n\n\n\nLet’s look more closely at the distributions for each coefficient. Standard statistical estimates assume a specific shape like the normal distribution. But with the bootstrap method, we have more flexibility, even though it often leans towards the assumed distribution. These distributions aren’t perfectly symmetrical like a normal distribution, but they suit our needs in that we can extract the lower and upper quantiles to create an interval estimate.\n\n\n\n\n\n\n\n\nFigure 4.9: Bootstrap distributions of parameter estimates\n\n\n\n\n\nThe bootstrap is often used for predictions and other metrics. However, it is computationally inefficient, and might not be suitable with large data sizes. It also may not estimate the appropriate uncertainty for some types of statistics (e.g. extreme values) or in some data contexts (e.g. correlated observations). Despite these limitations, the bootstrap method is a useful tool and can be used together with other methods to understand uncertainty in a model.\n\n\n4.11.2 Bayesian estimation\n\nThe Bayesian approach to modeling is many things - a philosophical viewpoint, an entirely different way to think about probability, a different way to measure uncertainty, and on a practical level, just another way to get model parameter estimates. It can be as frustrating as it is fun to use, and one of the really nice things about using Bayesian estimation is that it can handle model complexities that other approaches can’t do well.\nThe basis of Bayesian estimation is the likelihood, the same as with maximum likelihood, and everything we did there applies here. So you need a good grasp of maximum likelihood to understand the Bayesian approach. However, the Bayesian approach is different because it also lets us use our knowledge about the parameters through prior distributions. For example, we may think that the coefficients for a linear model come from a normal distribution centered on zero with some variance. That would serve as our prior. The combination of a prior distribution with the likelihood results in the posterior distribution, which is a distribution of possible parameter values.\n\n\n\n\n\n\nFigure 4.10: Prior, likelihood, posterior and distributions\n\n\n\nJust like with the bootstrap which also provided distributions for the parameters, we can use the Bayesian approach to understand how certain we are about our estimates. We can look at any range of values in the posterior distribution to get what is often referred to as a credible interval, which is the Bayesian equivalent of a confidence interval13. Here is an example of the posterior distribution for the parameters of our happiness model, along with 95% intervals14.\n\n\n\n\n\n\n\n\nFigure 4.11: Posterior distribution of parameters\n\n\n\n\n\nWith Bayesian estimation we also provide starting values for the algorithm to get things going. We also typically specify a number of iterations, or times the model will run, as the stopping rule. Each iteration gives us a new guess for each parameter, which amounts to a random draw from the posterior distribution. With more iterations the model takes longer to run, but the length often reflects the complexity of the model.\nWe also specify multiple chains, which do the same estimation procedure, but due to the random nature of the Bayesian approach, take different estimation paths. We can then compare the chains to see if they are converging to the same result, which is a check on the model. If they are not converging, we may need to run the model longer, or it may indicate a problem with how we set up the model. Here’s an example of the four chains for our happiness model for the life expectancy coefficient. They’re giving similar results, so we know the model is working well. Nowadays, we have model statistics in the output that also provide this information, which makes it easier to quickly check many parameters.\n\n\n\n\n\n\n\n\nFigure 4.12: Bayesian chains for life expectancy coefficient\n\n\n\n\n\nWhen we are interested in making predictions, we can use the results to generate a distribution of possible predictions for each observation, which can be very useful when we want to quantify uncertainty for complex models. This is referred to as posterior predictive distribution, which is explored in non-bayesian context in Section 3.2.4. Here is a plot of several draws of predicted values against the true happiness scores.\n\n\n\n\n\n\n\n\nFigure 4.13: Posterior predictive distribution of happiness values\n\n\n\n\n\nWith the Bayesian approach, every metric we calculate has a range of possible values, not just one. For example, if you have a classification model and want to know the accuracy or true positive rate of the model, instead of a single number, you now have access to a whole distribution of values for that metric. How? For each possible set of model parameters from the posterior distribution, we apply those values and model to data to make a prediction. We can then assign it to a class, and compare it to the actual class. This gives us a range of possible predictions and classes. We can then calculate metrics like accuracy or true positive rate for each possible prediction set. As an example, we did this for our happiness model with a numeric target to obtain the interval estimate for R-squared. Pretty neat!\n\n\n\n\nTable 4.11: Bayesian R2\n\n\n\n\n\n\n  \n    \n      Bayes R2\n      Lower\n      Upper\n    \n  \n  \n    0.71\n0.65\n0.75\n  \n  \n  \n    \n       95% Credible interval for R-squared\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nFrequentist PP check\n\n\n\n\n\nAs we saw in Section 3.2.4, nothing is keeping you from doing ‘posterior predictive checks’ with other estimation approaches, and it’s a very good idea to do so. For example, in a GLM you have the beta estimates and the covariance matrix for them, and can simulate from a normal distribution with those estimates. It’s just more straightforward with the Bayesian approach, where packages will do it for you with little effort.\n\n\n\n\n4.11.2.1 Additional Thoughts\nIt turns out that any standard (frequentist) statistical model can be seen as a Bayesian one from a particular point of view. Here are a couple:\n\nGLM and related models estimated via maximum likelihood: Bayesian estimation with a flat/uniform prior on the parameters.\nRidge Regression: Bayesian estimation with a normal prior on the coefficients, penalty parameter is related to the variance of the prior.\nLasso Regression: Bayesian estimation with a Laplace prior on the coefficients, penalty parameter is related to the variance of the prior.\n\nSo, in many modeling contexts, you’re actually doing a restrictive form of Bayesian estimation already.\nWe can see that the Bayesian approach is very flexible, and can be used for many different types of models, and can be used to get at uncertainty in a model in ways that other approaches can’t. It’s not a panacea, and it’s not always the best approach, but it’s a good one to have in your toolbox15. Hopefully we’ve helped to demystify the Bayesian approach a bit, and you feel more comfortable switching to it.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-wrap",
    "href": "estimation.html#sec-estim-wrap",
    "title": "4  How Did We Get Here?",
    "section": "4.12 Wrapping Up",
    "text": "4.12 Wrapping Up\nWow, we covered a lot here! But this is the sort of stuff that can take you from just having some fun with data, to doing that and also understanding how things are actually happening. Just having the gist of how modeling actually is done ‘under the hood’ makes so many other things make sense, and can give you a lot of confidence, even in less familiar modeling domains.\n\n4.12.1 The common thread\nSimply put, the content in this chapter ties together any and every model you will ever undertake, from linear regression to reinforcement learning, computer vision, and large language models. Estimation and optimization are the core of any modeling process, and understanding them is key to understanding how models work.\n\n\n4.12.2 Choose your own adventure\nSeriously, after this chapter, you should feel fine with any of the others in this book, so dive in!\n\n\n4.12.3 Additional resources\nOLS and Maximum Likelihood Estimation:\nFor OLS and maximum likelihood estimation, there are so many resources out there, we recommend just taking a look and seeing which one suits you best. Practically any more technical statistical book will cover these topics in detail.\n\nA list of classical references\n\nGradient Descent:\n\nGradient Descent, Step-by-Step StatQuest with Josh Starmer (2019a)\nStochastic Gradient Descent, Clearly Explained StatQuest with Josh Starmer (2019b)\nA Visual Explanation of Gradient Descent Methods Jiang (2020)\n\nMore demonstration of the simple AdaGrad algorithm used above:\n\nBrownlee (2021)\nDataBricks (2019)\n\nBootstrap:\nClassical treatments:\n\nEfron and Tibshirani (1994)\nDavison and Hinkley (1997)\nBootstrapping Main Ideas StatQuest with Josh Starmer (2021)\n\nBayesian:\n\nBDA Gelman et al. (2013)\nStatistical Rethinking McElreath (2020)\nChoosing priors",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-estim-exercise",
    "href": "estimation.html#sec-estim-exercise",
    "title": "4  How Did We Get Here?",
    "section": "4.13 Exercise",
    "text": "4.13 Exercise\nTry creating an objective function for a continuous target that uses the mean absolute error, and compare your estimated parameters to the previous results for ordinary least squares. Alternatively, use the ridge regression demonstration and change it to use the lasso approach (this would require altering just one line).\n\n\n\n\nBrownlee, Jason. 2021. “Gradient Descent With AdaGrad From Scratch.” MachineLearningMastery.com. https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/.\n\n\nClark, Michael. 2021. Model Estimation by Example. https://m-clark.github.io/models-by-example/.\n\n\nDataBricks. 2019. “What Is AdaGrad?” Databricks. https://www.databricks.com/glossary/adagrad.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511802843.\n\n\nEfron, Bradley, and R. J. Tibshirani. 1994. An Introduction to the Bootstrap. New York: Chapman; Hall/CRC. https://doi.org/10.1201/9780429246593.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. CRC Press.\n\n\nJiang, Lili. 2020. “A Visual Explanation of Gradient Descent Methods (Momentum, AdaGrad, RMSProp, Adam).” Medium. https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c.\n\n\nMcElreath, Richard. 2020. “Statistical Rethinking: A Bayesian Course with Examples in R and STAN.” Routledge & CRC Press. https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919.\n\n\nMurphy, Kevin P. 2012. “Machine Learning: A Probabilistic Perspective.” MIT Press. https://mitpress.mit.edu/9780262018029/machine-learning/.\n\n\nStatQuest with Josh Starmer. 2019a. “Gradient Descent, Step-by-Step.” https://www.youtube.com/watch?v=sDv4f4s2SB8.\n\n\n———. 2019b. “Stochastic Gradient Descent, Clearly Explained!!!” https://www.youtube.com/watch?v=vMh0zPT0tLI.\n\n\n———. 2021. “Bootstrapping Main Ideas!!!” https://www.youtube.com/watch?v=Xz0x-8-cgaQ.\n\n\nWikipedia. 2023. “Relationships Among Probability Distributions.” Wikipedia. https://en.wikipedia.org/wiki/Relationships_among_probability_distributions.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "estimation.html#footnotes",
    "href": "estimation.html#footnotes",
    "title": "4  How Did We Get Here?",
    "section": "",
    "text": "Since life expectancy is scaled, a standard deviation is 1, in this case about 7 years.↩︎\nIt turns out that our error metric is itself an estimate of the true error. We’ll get more into this later, but for now this means that we can’t ever know the true error, and so we can’t ever really know the best or true model. However, we can still choose a good or better model relative to others based on our estimate.↩︎\nWe don’t have to do it this way, but it’s the default in most scenarios. As an example, maybe for your situation overshooting is worse than undershooting, and so you might want to use an approach that would weight those errors more heavily.↩︎\nSome disciplines seem to confuse models with estimation methods and link functions. It doesn’t really make sense, nor is it informative, to call something an OLS model or a logit model. Many models are estimated using a least squares objective function, even deep learning, and different types of models use a logit link, from logistic regression, to beta regression, to activation functions used in deep learning.↩︎\nYou may find that some packages will only minimize (or maximize) a function, even to the point of reporting nonsensical things like negative squared values, so you’ll need to take care when implementing your own metrics.↩︎\nThe actual probability of a specific value in this setting is 0, but the probability of a range of values is greater than 0. You can find out more about likelihoods and probabilities at the discussion here, but in general many traditional statistical texts will cover this also.↩︎\nThe negative log-likelihood is often what is reported in the model output.↩︎\nThose who have experience here will notice we aren’t putting a lower bound on sigma. You typically want to do this otherwise you may get nonsensical results by not keeping sigma positive. You can do this by setting a specific argument for an algorithm that uses boundaries, or more simply by exponentiating the parameter so that it can only be positive. In the latter case, you’ll have to exponentiate the final parameter estimate to get back to the correct scale. We leave this detail out of the code for now to keep things simple.↩︎\nLinear regression will settle on a line that cuts through the means, and when standardizing all variables, the mean of the features and target are both zero, so the line goes through the origin.↩︎\nMC does not recall exactly where this origin of his function came from except that Murphy’s PML book was a key reference when he came up with it (Murphy (2012)).↩︎\nYou’d get better results by also standardizing the target. The initial shuffling that we did can help as well in case the data are ordered. When we’re dealing with larger data and repeated runs/epochs, shuffling allows the samples/batches to be more representative of the entire data set. Also, we had to ‘hand-tune’ our learning rate and stepsize, which is not ideal, and normally we would use cross-validation to find the best values.↩︎\nWe’re using inference here in the standard statistical/philosophical sense, not as a synonym for prediction or generalization, which is how it is often used in machine learning. We’re not exactly sure how that terminological muddling arose in ML, but be on the lookout for it.↩︎\nMany people’s default interpretation of a standard confidence interval is incorrectly the actual interpretation of a Bayesian confidence interval. This is partly because the Bayesian interpretation of confidence intervals and p-values is how we tend to naturally think about those statistics. But that’s okay, everyone is in the same boat. We also think it’s fine if you want to call the Bayesian version a confidence interval.↩︎\nWe used the R package for brms for these results.↩︎\nR has excellent tools here for modeling and post-processing, like brms and tidybayes, and Python has pymc3, numpyro, and arviz, which are also useful. Honestly R has way more going on here, with many packages devoted to Bayesian estimation of specific models even, but if you want to stick with Python it’s gotten a lot better recently.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How Did We Get Here?</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html",
    "href": "generalized_linear_models.html",
    "title": "5  Generalized Linear Models",
    "section": "",
    "text": "5.1 Key Ideas",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-key",
    "href": "generalized_linear_models.html#sec-glm-key",
    "title": "5  Generalized Linear Models",
    "section": "",
    "text": "A simple tweak to our previous approach allows us to generalize our linear model to account for other types of target data.\nCommon distributions such as binomial, Poisson, and others can often improve model fit and interpretability.\nGetting familiar with just a couple distributions will allow you to really expand your modeling repertoire.\n\n\n5.1.1 Why this matters\nThe linear model is powerful on its own, but even more so when you realize you can extend many other data settings, some of which are implicitly nonlinear! When we want to classify observations, count them, or deal with proportions and other things, very simple tweaks of our standard linear model allow us to handle such situations.\n\n\n5.1.2 Good to know\nGeneralized linear models are a broad class of models that extend the linear model to different distributions of the target variable. In general, you’d need to have a pretty good grasp of linear regression before getting too carried away here.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-distributions",
    "href": "generalized_linear_models.html#sec-glm-distributions",
    "title": "5  Generalized Linear Models",
    "section": "5.2 Distributions & Link Functions",
    "text": "5.2 Distributions & Link Functions\nRemember how linear models really enjoy the whole Gaussian, i.e. ‘normal’, distribution scene? The essential form of the linear model can be expressed as follows1. We create the linear combination of our features, and then we add a normal distribution that uses that as the mean, which will naturally vary for each sample of data.\n\\[\ny \\sim \\textrm{Normal}(\\mu,\\sigma)\n\\] \\[\n\\mu =  \\alpha + X\\beta\n\\]\nSometimes though, our data doesn’t follow a normal (Gaussian) distribution, and in these cases we rely on some other distribution that potentially fits the data better. But often these other distributions don’t have a direct link to our features, and that’s where a link function comes in.\nThink of the link function as a bridge between our features and the distribution we want to use. It lets us use a linear combination of features to predict the mean or other parameters of the distribution. As an example, we can use a log to link the mean to the linear predictor, or conversely, exponentiate the linear predictor to get the mean. In this example, the log is the link function and we use its inverse to map the linear predictor back to the mean.\nIf you know a distribution’s ‘canonical’ link function, which is like the default for a given distribution, that is all the deeper you will probably ever need to go. At the end of the day, these link functions will link your model output to the parameters required for the distribution. The take-away here is that the link function describes how the mean or other parameters of interest are generated from the (linear) combination of features.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-logistic",
    "href": "generalized_linear_models.html#sec-glm-logistic",
    "title": "5  Generalized Linear Models",
    "section": "5.3 Logistic Regression",
    "text": "5.3 Logistic Regression\nAs we’ve seen, you will often have a binary variable that you might want to use as a target – it could be dead/alive, lose/win, quit/retain, etc. You might be tempted to use a linear regression, but you will quickly find that it’s not the best option in that setting. So let’s try something else.\n\n\n5.3.1 The binomial distribution\nLogistic regression is differs from linear regression mostly because it is used with a binary target instead of a continuous one as with linear regression. We typically assume that the target follows a binomial distribution. Unlike the normal distribution,, which is characterized by its mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\)), the binomial distribution is defined by the parameters: p (also commonly \\(pi\\)) and a known value n. Here, p represents the probability of a specific event occurring (like flipping heads, winning a game, or defaulting on a loan), and n is the number of trials or attempts under consideration.\nHowever, it’s important to note that the binomial distribution doesn’t just describe the probability of a single event. It actually represents the distribution of the number of successful outcomes in n trials. In other words, it’s a count distribution that tells us how many times we can expect the event to occur in a given number of trials.\nLet’s see how the binomial distribution looks with 100 trials and probabilities of ‘success’ at p =  .25, .5, and .75:\n\n\n\n\n\n\n\n\nFigure 5.1: Binomial distributions for different probabilities\n\n\n\n\n\nIf we examine the distribution for a probability of .5, we will see that it is roughly centered over a total success count of 50. This tells us that we have the highest probability of encountering 50 successes if we ran 100 trials. Shifting our attention to a .75 probability of success, we see that our distribution is centered over 75. In practice we probably end up with something around that value, but on average and over repeated runs of 100 trials, the value would be \\(p\\cdot n\\).\nSince we are dealing with a number of trials, it is worth noting that the binomial distribution is a discrete distribution. If we have any interest in knowing the probability for a number of successes we can use the following formula, where \\(n\\) is the number of trials, \\(x\\) is the number of successes, and \\(p\\) is the probability of success:\n\\[\nP(x) = \\frac{n!}{(n-x)!x!}p^x(1-p)^{n-x}\n\\tag{5.1}\\]\nNow let’s see how the binomial distribution relates to the linear model space:\n\\[y \\sim \\textrm{Binomial}(n, p) \\]\n\\[\n\\textrm{logit}(p) = \\alpha + X\\beta\n\\]\nThe logit function is defined as:\n\\[\\textrm{log}\\frac{p}{1-p}\\]\nWe are literally just taking the log of the odds (the log odds become important later).\nNow we can map this back to our model:\n\\[\\textrm{log}\\frac{p}{1-p} = \\alpha + X\\beta\\]\nAnd finally, we can take that logistic function and invert it (the inverse-logit function) to produce the probabilities.\n\\[p = \\frac{\\textrm{exp}(\\alpha + X\\beta)}{1 + \\textrm{exp}(\\alpha + X\\beta)}\\]\nor equivalently:\n\\[p = \\frac{1}{1 + \\textrm{exp}(-(\\alpha + X\\beta))}\\]\nWhenever we get coefficients for the logistic regression model, the default result is almost always on the log odds scale. We usually exponentiate them to get the odds ratio. For example, if we have a coefficient of .5, we would say that for every one unit increase in the feature, the odds of the target being a ‘success’ increase by a factor of exp(.5) = 1.6.\n\n\n5.3.2 Probability, odds, and log odds\nProbability lies at the heart of all of this, so let’s look more closely at the relationship between the probability and log odds. In our model, the log odds are produced by the linear combination of our features. Let’s say we have a model that gives us those values for each observation. We can then convert them from the linear space to the (nonlinear) probability space with our inverse-logit function, which might look something like this.\n\n\n\n\n\n\n\n\nFigure 5.2: Log odds and probability values\n\n\n\n\n\nWe can see that the probability of success approaches 0 when the log odds are negative and approaches 1 when the log odds are positive. The shape is something like an S, which also tells us that we are not in linear space when we switch to probabilities.\nLog odds have a nice symmetry around 0, where the probability of success is 0.5. Any value above 0 indicates a probability of success greater than 0.5, and any value below 0 indicates a probability of success less than 0.5. However, don’t get too hung up on a .5 probability as being fundamentally important for any given problem.\nAs mentioned, logistic regression models usually report coefficients on the log-odds scale by default. The coefficients reflect the odds associated with predicted probabilities given the feature at different values one unit apart. Log-odds are not the most intuitive thing to interpret. When we are more concerned with model interpretability, we often convert the coefficients to odds ratios by exponentiating them. In logistic regression models, the odds ratio is the ratio of the odds of the outcome occurring for a one unit increase in the feature.\n\nThe following function will calculate the odds ratio for two probabilities, which we can think of as prediction outcomes for two values of a feature one unit apart.\n\nRPython\n\n\n\ncalculate_odds_ratio = function(p_1, p_2) {\n    odds_1 = p_1 / (1 - p_1)\n    odds_2 = p_2 / (1 - p_2)\n    odds_ratio = odds_2 / odds_1\n    \n    tibble(\n        value    = c('1', '2'),\n        p        = c(p_1, p_2),\n        odds     = c(odds_1, odds_2),\n        log_odds = log(odds),\n        odds_ratio = c(NA, odds_ratio)\n    )\n}\n\nresult_A = calculate_odds_ratio(.5, .6)\nresult_B = calculate_odds_ratio(.1, .2)\nresult_C = calculate_odds_ratio(.9, .8)  # inverse of the .1, .2 example \n\nresult_A\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndef calculate_odds_ratio(p_1, p_2):\n    odds_1 = p_1 / (1 - p_1)\n    odds_2 = p_2 / (1 - p_2)\n    odds_ratio = odds_2 / odds_1\n    \n    return pd.DataFrame({\n        'value': ['1', '2'],\n        'p': [p_1, p_2],\n        'odds': [odds_1, odds_2],\n        'log_odds': [np.log(odds_1), np.log(odds_2)],\n        'odds_ratio': [np.nan, odds_ratio]\n    })\n\nresult_A = calculate_odds_ratio(.5, .6)\nresult_B = calculate_odds_ratio(.1, .2)  \nresult_C = calculate_odds_ratio(.9, .8)  # inverse of the .1, .2 example\n\nresult_A\n\n\n\n\n\n\n\n\nTable 5.1: Odds ratios for different probabilities\n\n\n\n\n\n\n  \n    \n      \n      value\n      p\n      odds1\n      log_odds\n      odds_ratio2\n    \n  \n  \n    A\n1\n0.50\n1.00\n0.00\nNA\n    2\n0.60\n1.50\n0.41\n1.50\n    B\n1\n0.10\n0.11\n−2.20\nNA\n    2\n0.20\n0.25\n−1.39\n2.25\n    C\n1\n0.90\n9.00\n2.20\nNA\n    2\n0.80\n4.00\n1.39\n0.44\n  \n  \n  \n    \n      1 The odds are p / (1 - p)\n    \n    \n      2 The odds ratio refers to value 2 versus value 1\n    \n  \n\n\n\n\n\n\n\nIn the table we see that even though each probability difference is the same, the odds ratio is different. Comparing A to B, the difference between a probability of .5 to .6 is not as much of a change on the odds (linear) scale as the difference between .1 to .2. The first setting is a 50% increase in the odds, whereas the second more than doubles the odds. However, the difference between .9 to .8 is the same as the difference between .1 to .2, as they reflect points that are the same distance from the boundary. The odds ratio for setting C is just the inverse of setting B. The following shows our previous plot with the corresponding settings shaded.\n\n\n\n\n\n\n\n\nFigure 5.3: Comparison of Probability and Odds Differences\n\n\n\n\n\nOdds ratios might be more interpretable to some, but since they are ratios of ratios, people have historically had a hard time with those as well. Furthermore, doubling the odds is not the same as doubling the probability, so we’re left doing some mental calisthenics to interpret them. Odds ratios are often used in academic settings, but in practice, they are not as common as you might think. The take-home message is that we can interpret our result in a linear or nonlinear space, but it can be a bit difficult2. Our own preference is to stick with predicted probabilities, but it’s good to know how to interpret odds ratios.\n\n\n5.3.3 A logistic regression model\nFor our model let’s return to our movie review data, but now we are going to use rating_good as our target. Before we get to modeling, see if you can find out the frequency of ‘good’ and ‘bad’ reviews, and the probability of getting a ‘good’ review. We will use word_count and gender as our features.\n\n\nRPython\n\n\n\ndf_reviews = read_csv('https://tinyurl.com/moviereviewsdata')\n\n# for the by-hand option later\nX = df_reviews |&gt; \n    select(word_count, male = gender) |&gt; \n    mutate(male = ifelse(male == 'male', 1, 0)) |&gt; \n    as.matrix()\n\ny = df_reviews$rating_good\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf_reviews = pd.read_csv('https://tinyurl.com/moviereviewsdata')\n\n# for the by-hand option later\nX = (\n    df_reviews[['word_count', 'gender']]\n    .rename(columns = {'gender': 'male'})\n    .assign(male = np.where(df_reviews[['gender']] == 'male', 1, 0))\n)\n\ny = df_reviews['rating_good']\n\n\n\n\nFor an initial logistic regression model, we can use standard and common functions in our chosen language. Running a logistic regression model requires the specification of the family, but that’s pretty much the only difference compared to our previous linear regression. The default link function for the binomial distribution is the ‘logit’ link, so we don’t have to specify it explicitly.\n\nRPython\n\n\n\nmodel_logistic = glm(\n    rating_good ~ word_count + gender, \n    data = df_reviews,\n    family = binomial\n)\n\nsummary(model_logistic)\n\n\nCall:\nglm(formula = rating_good ~ word_count + gender, family = binomial, \n    data = df_reviews)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.71240    0.18136   9.442   &lt;2e-16 ***\nword_count  -0.14639    0.01551  -9.436   &lt;2e-16 ***\ngendermale   0.11891    0.13751   0.865    0.387    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1370.4  on 999  degrees of freedom\nResidual deviance: 1257.4  on 997  degrees of freedom\nAIC: 1263.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nmodel_logistic = smf.glm(\n    'rating_good ~ word_count + gender', \n    data = df_reviews,\n    family = sm.families.Binomial()\n).fit()\n\nmodel_logistic.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nrating_good\nNo. Observations:\n1000\n\n\nModel:\nGLM\nDf Residuals:\n997\n\n\nModel Family:\nBinomial\nDf Model:\n2\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-628.70\n\n\nDate:\nTue, 13 Aug 2024\nDeviance:\n1257.4\n\n\nTime:\n18:20:44\nPearson chi2:\n1.02e+03\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.1068\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.7124\n0.181\n9.442\n0.000\n1.357\n2.068\n\n\ngender[T.male]\n0.1189\n0.138\n0.865\n0.387\n-0.151\n0.388\n\n\nword_count\n-0.1464\n0.016\n-9.436\n0.000\n-0.177\n-0.116\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Regression\n\n\n\n\n\nThe binomial distribution is a count distribution. The logistic regression model is used to model binary outcomes, but we can use the binomial distribution because the binary setting is a special case of the binomial distribution where the number of trials is 1, and the number of successes can only be 0 or 1. In this case, we can also use the Bernoulli distribution, which does not require the number of trials, since, when the number of trials is 1 the factorial part of Equation 5.1 drops out.\nMany coming from a non-statistical background are not aware that their logistic model can actually handle count and/or proportional outcomes.\n\n\n\n\n\n5.3.4 Interpretation and visualization\nWe need to know what those results mean. The coefficients that we get from our model are in log odds, but as we demonstrated we can exponentiate them to get the odds ratio. Interpreting log odds is difficult, but we can at least get a feeling for them directionally. A log odds of 0 (odds ratio of 1) would indicate no relationship between the feature and target. A positive log odds would indicate that an increase in the feature will increase the log odds of moving from ‘bad’ to ‘good’, whereas a negative log odds would indicate that a decrease in the feature will decrease the log odds of moving from ‘bad’ to ‘good’. On the log odds scale, the coefficients are symmetric as well, such that, e.g., a +1 coefficient denotes a similar increase in the log odds as a -1 coefficient denotes a decrease.\n\n\n\n\nTable 5.2: Raw coefficients and odds ratios for logistic regression model\n\n\n\n\n\n\n  \n    \n      Parameter\n      Coefficient\n      Exp. Coef (OR)\n    \n  \n  \n    (Intercept)\n1.71\n5.54\n    word_count\n−0.15\n0.86\n    gendermale\n0.12\n1.13\n  \n  \n  \n\n\n\n\n\n\n\nFortunately, the intercept is easy – it is the odds of a ‘good’ review when word count is 0 and gender is ‘female’. We see that we’ve got an odds ratio of 0.86 for the word count variable and 1.13 for the male variable. This means that for every one unit increase in word count, the odds of a ‘good’ review decreases by about 14%. Males are associated with an odds of a ‘good’ review that is 13 times higher than females.\nWe feel it is much more intuitive to interpret things on the probability scale, so we’ll get predicted probabilities for different values of the features. The way we do this is through the (inverse) link function, which will convert our log odds of the linear predictor to probabilities. We can then plot these probabilities to see how they change with the features. For the word count feature, we hold gender at the reference group (‘female’), and for the gender feature, we hold word count at its mean. In addition we convert the probability to the percentage chance of a ‘good’ review.\n\n\n\n\n\n\n\n\nFigure 5.4: Model predictions for word count feature\n\n\n\n\n\nIn Figure 5.4, we can see a clear negative relationship between the number of words in a review and the probability of being considered a ‘good’ movie. As we get over 20 words, the predicted probability of being a ‘good’ movie is less than .2. We also see the increase in the chance for a good rating with males vs. females, but our model results suggest this is not a statistically significant difference.\n\n\n\n\n\n\n\n\nFigure 5.5: Model predictions for word count feature\n\n\n\n\n\nIn the end, whether you think these differences are practically significant is up to you. And you’ll still need to do the standard model exploration to further understand the model (Chapter 3 has lots of detail on this). But this is a good start.\n\n\n\n\n\n\nA Note on \\(R^2\\) for Logistic Regression\n\n\n\n\n\nLogistic regression does not have an \\(R^2\\) value in the way that a linear regression model does. Instead, there are pseudo-\\(R^2\\) values, but they are not the same as the \\(R^2\\) value that you are used to seeing Computing (2023).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-poisson",
    "href": "generalized_linear_models.html#sec-glm-poisson",
    "title": "5  Generalized Linear Models",
    "section": "5.4 Poisson Regression",
    "text": "5.4 Poisson Regression\nPoisson regression also belongs to the class of generalized linear models, and is used specifically when you have a count variable as your target. After logistic regression for binary outcomes, Poisson regression is probably the next most common type of generalized linear model you will encounter. Unlike continuous targets, a count starts at 0 and can only be a whole number. Often it is naturally skewed as well, so we’d like a model well-suited to this situation. Unlike the binomial, there is no concept of number of trials, just the count of events.\n\n5.4.1 The Poisson distribution\nThe Poisson distribution is very similar to the binomial distribution, because the binomial is also a count distribution, and in fact generalizes the poisson3. The Poisson has a single parameter noted as \\(\\lambda\\), which makes it the simplest model setting we’ve seen so far4. Conceptually, this rate parameter is going to estimate the expected number of events during a time interval. This can be accidents in a year, pieces produced in a day, or hits during the course of a baseball season.\nLet’s see what the particular distribution might look like for different rates. We can see that for low count values, the distribution is skewed to the right, but note how the distribution becomes more symmetric and bell-shaped as the rate increases5. You might also be able to tell that the variance increases along with the mean, and in fact, the variance is equal to the mean for the Poisson distribution.\n\n\n\n\n\n\n\n\nFigure 5.6: Poisson distributions for different rates\n\n\n\n\n\n\n\n\n\n\n\nMore Poisson\n\n\n\n\n\nA cool thing about these distributions is that they can deal with different exposure rates. They can also be used to model inter-arrival times and time-until-events.\n\n\n\nLet’s make a new variable that will count the number of times a person uses a personal pronoun word in their review. We’ll use it as our target variable and see how it relates to the number of words and gender in a review as we did before.\n\nRPython\n\n\n\ndf_reviews$poss_pronoun = stringr::str_count(\n    df_reviews$review_text, \n    '\\\\bI\\\\b|\\\\bme\\\\b|\\\\b[Mm]y\\\\b|\\\\bmine\\\\b|\\\\bmyself\\\\b'\n    )\n\nhist(df_reviews$poss_pronoun)\n\n\n\n\ndf_reviews['poss_pronoun'] = (\n    df_reviews['review_text']\n    .str.count('\\\\bI\\\\b|\\\\bme\\\\b|\\\\b[Mm]y\\\\b|\\\\bmine\\\\b|\\\\bmyself\\\\b')\n    )\n\ndf_reviews['poss_pronoun'].hist()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: Distribution of the Personal Pronouns Seen Across Reviews\n\n\n\n\n\n\n\n5.4.2 A Poisson regression model\n\nRecall that every GLM distribution has a default link function. The Poisson distribution uses a log link function:\n\\[y \\sim \\textrm{Poisson}(\\lambda)\\]\n\\[\\text{log}(\\lambda) = \\alpha + X\\beta\\]\nUsing the log link keeps the outcome non-negative when we use the inverse of it. For model fitting with standard functions, all we have to do is switch the family from ‘binomial’ to ‘poisson’. As the default link is the ‘log’, so we don’t have to specify it explicitly6. We can run the model and get the results as we did before, but we keep our presentation here to just the coefficients.\n\nRPython\n\n\n\nmodel_poisson = glm(\n    poss_pronoun ~ word_count + gender,\n    data = df_reviews,\n    family = poisson\n)\n\nsummary(model_poisson)\n\nexp(model_poisson$coefficients)\n\n\n\n\nmodel_poisson = smf.glm(\n    formula = 'poss_pronoun ~ word_count + gender',\n    data = df_reviews,\n    family = sm.families.Poisson()\n).fit()\n\nmodel_poisson.summary()\n\nnp.exp(model_poisson.params)\n\n\n\n\n\n\n5.4.3 Interpretation and visualization\nLike with logistic, we can exponentiate the coefficients to get what’s now referred to as the rate ratio. This is the ratio of the rate of the outcome occurring for a one unit increase in the feature.\n\n\n\n\nTable 5.3: Rate ratios for poisson regression model\n\n\n\n\n\n\n  \n    \n      Parameter\n      Coefficient\n      Exp. Coef\n    \n  \n  \n    (Intercept)\n−1.888\n0.151\n    word_count\n0.104\n1.109\n    gendermale\n0.080\n1.083\n  \n  \n  \n\n\n\n\n\n\n\nFor this model, an increase in one review word leads to an expected log count increase of ~0.1. We can exponentiate this to get 1.11, and this tells us that every added word in a review gets us a 11% increase in the number of possessive pronouns. This is probably not a surprising result - wordier stuff has more types of words!\nBut as usual, the visualization tells the better story. Notice that the predictions are not discrete, but continuous. This is because predictions here are the same as with our other models, and reflect the expected or average counts.\n\n\n\n\n\n\n\n\nFigure 5.8: Poisson regression predictions for word count feature\n\n\n\n\n\nWith everything coupled together, we have an interpretable coefficient for word_count, a clear plot, and adequate model fit. Therefore, we might conclude that there is a positive relationship between the number of words in a review and the number of times a person uses a personal possessive.\n\n\n\n\n\n\nNonlinear linear models?\n\n\n\n\n\nDid you notice that both our effects for word count in the logistic (Figure 5.4) and Poisson (Figure 5.8) models were not exactly the straightest of lines? Once we’re on the probability and count scales, we’re not going to see the same linear relationships that we might expect from a basic linear model due to the transformation. If we plot the effect on the log-odds or log-count scale, we’re back to straight lines. This is a first taste in how the linear model can be used to get at nonlinear relationships, which are of the focus of Chapter 6.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-objective",
    "href": "generalized_linear_models.html#sec-glm-objective",
    "title": "5  Generalized Linear Models",
    "section": "5.5 How Did We Get Here?",
    "text": "5.5 How Did We Get Here?\nIf we really want to demystify the modeling process, let’s create our own function to estimate the coefficients. We can use maximum likelihood estimation to estimate the parameters of our model, which is the approach used by standard package functions. Feel free to skip this part if you only wanted the basics, but for even more information on maximum likelihood estimation, see Section 4.7 where we take a deeper dive into the topic and with a similar function. The following code is a simple version of what goes on behind the scenes with ‘glm’ type functions.\n\nRPython\n\n\n\nglm_simple = function(par, X, y, family = 'binomial') {\n      # add an column for the intercept\n      X = cbind(1, X)\n\n      # Calculate the linear predictor\n      mu = X %*% par # %*% is matrix multiplication\n\n      # get the likelihood for the binomial or poisson distribution\n      if (family == 'binomial') {\n          # Convert to a probability ('logit' link/inverse)\n          p = 1 / (1 + exp(-mu))\n          L = dbinom(y, size = 1, prob = p, log = TRUE)\n      }\n      else if (family == 'poisson') {\n          # Convert to a count ('log' link/inverse)\n          p = exp(mu)\n          L = dpois(y, lambda = p, log = TRUE)\n      }\n\n      # return the negative sum of the log-likelihood (for minimization)\n      value = -sum(L) \n\n      return(value)\n}\n\n\n\n\nfrom scipy.stats import poisson, binom\n\ndef glm_simple(par, X, y, family = 'binomial'):\n    # add an column for the intercept\n    X = np.column_stack((np.ones(X.shape[0]), X))\n\n    # Calculate the linear predictor\n    mu = X @ par  # @ is matrix multiplication\n    \n    # get the likelihood for the binomial or poisson distribution\n    if family == 'binomial':\n        p = 1 / (1 + np.exp(-mu))\n        L = binom.logpmf(y, 1, p)\n    elif family == 'poisson':\n        lambda_ = np.exp(mu)\n        L = poisson.logpmf(y, lambda_)\n    \n    # return the negative sum of the log-likelihood (for minimization)\n    value = -np.sum(L)\n    \n    return value\n\n\n\n\nNow that we have our objective function, we can fit our models, starting with the logistic model. We will use the optim function in R and the minimize function in Python.\n\nRPython\n\n\n\ninit = rep(0, ncol(X) + 1)\n\nnames(init) = c('intercept', 'b1', 'b2')\n\nfit_logistic = optim(\n    par = init,\n    fn = glm_simple,\n    X = X,\n    y = y,\n    control = list(reltol = 1e-8)\n)\n\nfit_logistic$par\n\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ninit = np.zeros(X.shape[1] + 1)\n\nfit_logistic = minimize(\n    fun = glm_simple,\n    x0 = init,\n    args = (X, y),\n    method = 'BFGS'\n)\n\nfit_logistic['x']\n\n\n\n\nAnd here is our comparison table of the raw coefficients. Looks like our little function worked well!\n\n\n\n\nTable 5.4: Comparison of coefficients\n\n\n\n\n\n\n  \n    \n      Parameter\n      Ours\n      Standard\n    \n  \n  \n    Intercept\n1.7122\n1.7124\n    Word Count\n−0.1464\n−0.1464\n    Male\n0.1189\n0.1189\n  \n  \n  \n\n\n\n\n\n\n\nSimilarly, we can also use our function to estimate the coefficients for the poisson model. Just like the GLM function we might normally use, we can change the family option to specify the distribution we want to use.\n\nRPython\n\n\n\nfit_poisson = optim(\n    par = c(0, 0, 0),\n    fn = glm_simple,\n    X = X,\n    y = df_reviews$poss_pronoun,\n    family = 'poisson'\n)\n\nfit_poisson$par\n\n\n\n\nfit_poisson = minimize(\n    fun = glm_simple,\n    x0 = init,\n    args = (\n        X, \n        df_reviews['poss_pronoun'], \n        'poisson'\n    )\n)\n\nfit_poisson['x']\n\n\n\n\nAnd once again we’re able to get the same results (raw coefficients shown).\n\n\n\n\nTable 5.5: Comparison of coefficients\n\n\n\n\n\n\n  \n    \n      Parameter\n      Ours\n      Standard\n    \n  \n  \n    Intercept\n−1.8876\n−1.8877\n    Word Count\n0.1036\n0.1036\n    Male\n0.0800\n0.0798\n  \n  \n  \n\n\n\n\n\n\n\nThis goes to show that just a little knowledge of the underlying mechanics can go a long way in understanding how many models work.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-wrap",
    "href": "generalized_linear_models.html#sec-glm-wrap",
    "title": "5  Generalized Linear Models",
    "section": "5.6 Wrapping Up",
    "text": "5.6 Wrapping Up\nSo at this point you have standard linear regression with the normal distribution for continuous targets, logistic regression for binary/proportional ones via the binomial distribution, and Poisson regression for counts. These models combine to provide much of what you need for starting out in the modeling world, and all serve well as baseline models for comparison when using more complex methods (Section 8.4). However, what we’ve seen is just a tiny slice of the potential universe of distributions that you could use. Here is a brief list of some that are still in the GLM family proper and others that can be useful7:\nOther Core GLM (available in standard functions):\n\nGamma: For continuous, positive targets that are skewed.\nInverse Gaussian: For continuous, positive targets that are skewed and have a long tail.\n\nOthers (some fairly common):\n\nBeta: For continuous targets that are bounded between 0 and 1.\nLog-Normal: For continuous targets that are skewed. Essentially what you get with linear regression and logging the target8.\nTweedie: Generalizes several core GLM family distributions.\n\nIn the ballpark:\n\nNegative Binomial: For count targets that are overdispersed.\nMultinomial: Typically used for categorical targets with more than two categories, but like the binomial, it is actually a more general (multivariate) count distribution.\nStudent t: For continuous targets that are distributed similar to normal but with heavier tails.\nCategorical/Ordinal: For categorical targets with more than two categories, or ordered categories. In the former case, it’s a different distribution than the multinomial but applied to the same setting.\nQuasi *: For example quasipoisson. These ‘quasi-likelihoods’ served a need at one point that is best served by other methods.\n\nYou’ll typically need separate packages to fit some of these, but most often the tools keep to a similar functional approach. The main thing is to know that certain distributions might fit your data a bit better than others, and that you can use both the same basic framework and mindset to fit them, and maybe get a little closer to the answer you seek about your data!\n\n5.6.1 The common thread\nGLMs extend your standard linear model as a powerful tool for modeling a wide range of data types. They are a great way to get started with more complex models, and even allow us to linear models in a not so linear way. It’s best to think of GLMs more broadly than the strict statistical definition, and consider many models like ordinal regression, ranking models, survival analysis, and more as part of the same extension.\n\n\n5.6.2 Choose your own adventure\nAt this point you have a pretty good sense of what linear models have to offer, but there’s even more! You can start to look at more complex models that build on these ideas, like mixed models, generalized additive models and more in Chapter 6. You can also feel confident heading into the world of machine learning (Chapter 7), where you’ll find additional ways to think about your modeling approach.\n\n\n5.6.3 Additional resources\nIf you are itching for a textbook, there isn’t any shortage of them out there and you can essentially take your pick, though most purely statistical treatments are going to be a bit dated at this point, but still accurate and maybe worth your time.\n\nGeneralized Linear Models (McCullagh (2019)) is a classic text on the subject, but it is a bit dense and not for the faint of heart, or even Nelder and Wedderburn (1972), which is a very early treatment.\n\nFor more accessible fare that doesn’t lack on core details either:\n\nAn Introduction to Generalized Linear Models is generally well regarded (Dobson and Barnett (2018)).\nGeneralized Linear Models is more accessible (Hardin and Hilbe (2018)).\nRoback and Legler’s Beyond Multiple Linear Regression, available for free.\nApplied Regression Analysis and Generalized Linear Models (Fox (2015))\nGeneralized Linear Models with Examples in R (Dunn and Smyth (2018))",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#sec-glm-exercise",
    "href": "generalized_linear_models.html#sec-glm-exercise",
    "title": "5  Generalized Linear Models",
    "section": "5.7 Exercise",
    "text": "5.7 Exercise\nUse the fish data (Section A.4) to conduct a Poisson regression and see how well you can predict the number of fish caught based on the other variables like how many people were on the trip, how many children, whether live bait was used etc.\nIf you would prefer to try a logistic regression, change the count to just 0 and 1 for whether any fish were caught, and see how well you can predict that.\n\n\n\n\nComputing, UCLA Advanced Research. 2023. “FAQ: What Are Pseudo R-Squareds?” https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/.\n\n\nDobson, Annette J., and Adrian G. Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. New York: Chapman; Hall/CRC. https://doi.org/10.1201/9781315182780.\n\n\nDunn, Peter K., and Gordon K. Smyth. 2018. Generalized Linear Models With Examples in R. Springer.\n\n\nFox, John. 2015. Applied Regression Analysis and Generalized Linear Models. SAGE Publications.\n\n\nHardin, James W., and Joseph M. Hilbe. 2018. Generalized Linear Models and Extensions. Stata Press.\n\n\nMcCullagh, P. 2019. Generalized Linear Models. 2nd ed. New York: Routledge. https://doi.org/10.1201/9780203753736.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Royal Statistical Society. Journal. Series A: General 135 (3): 370–84. https://doi.org/10.2307/2344614.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "generalized_linear_models.html#footnotes",
    "href": "generalized_linear_models.html#footnotes",
    "title": "5  Generalized Linear Models",
    "section": "",
    "text": "The y in the formula is more properly expressed as \\(y | X, \\theta\\), where X is the matrix of features and \\(\\theta\\) the parameters estimated by the model. We’ll keep it simple here.↩︎\nFor more on interpreting odds ratios, see this article.↩︎\nIf your binomial setting has a very large number of trials relative to the number of successes, which amounts to very small proportions \\(p\\), you would find that the binomial distribution would converge to the Poisson distribution.↩︎\nNeither the binomial nor the Poisson have a variance parameter to estimate, as the variance is determined by the mean. This is in contrast to the normal distribution, where the variance is a separate parameter. For the Poisson, the variance is equal to the mean, and for the binomial, the variance is equal to \\(n*p*(1-p)\\). The Poisson assumption of equal variance rarely holds up in practice, so people often use the negative binomial distribution instead.↩︎\nFrom a modeling perspective, for large mean counts you can just go back to using the normal distribution if you prefer without much losing much predictively and a gaining in interpretability.↩︎\nIt is not uncommon in many disciplines to use different link functions for logistic models, but the log link is always used for Poisson models.↩︎\nThere is not strict agreement about what qualifies for being in the GLM family.↩︎\nBut there is a variance issue to consider.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html",
    "href": "linear_model_extensions.html",
    "title": "6  Extending the Linear Model",
    "section": "",
    "text": "6.1 Key Ideas",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-key-ideas",
    "href": "linear_model_extensions.html#sec-lm-extend-key-ideas",
    "title": "6  Extending the Linear Model",
    "section": "",
    "text": "The standard and generalized linear models are great and powerful starting points for modeling, but there’s even more we can do!\nLinear models can be used to model nonlinear feature-target relationships\nWhile these seem like different approaches, we can still use our linear model concepts and approach at the core, take similar estimation steps, and even have similar, albeit more, interpretation.\n\n\n6.1.1 Why this matters\nThe linear model is a great starting point for modeling. It is a simple approach that can be used to model a wide variety of relationships between features and targets, and it’s also a great way to get a feel for how to think about modeling. But linear and generalized models are just the starting point, and the models depicted here are common extensions used in a variety of disciplines and industries. More generally, the following techniques allow for nonlinear relationships while still employing a linear model approach. This is a very powerful tool to have in your toolkit, and it’s a great way to start thinking about how to model more complex relationships in your data.\n\n\n6.1.2 Good to know\nWhile these models are extensions of the linear model, they not significantly more complicated in terms of how they are implemented or how they are interpreted. However, like anything new, it can take a bit more effort to understand. You likely want to be comfortable with standard linear models at least before you start to explore these extensions.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-interactions",
    "href": "linear_model_extensions.html#sec-lm-extend-interactions",
    "title": "6  Extending the Linear Model",
    "section": "6.2 Interactions",
    "text": "6.2 Interactions\nThings can be quite complex in a typical model with multiple features, but just adding features may not be enough to capture the complexity of the relationships between features and target. Sometimes, we need to consider how features interact with each other to better understand how they correlate with the target. A common way to add complexity in linear models is through interactions. This is where we allow the effect of a feature to vary depending on the values of another feature, or even itself!\nAs a conceptual example, we might expect a movie’s rating is different for movies from different genres (much higher for kids movies, maybe lower for horror movies), or that genre and season work together in some way to affect rating (e.g. action movies get higher ratings in summer), or having kids in the home number might also interact with genre ratings. We might also consider that the length of a movie might plateau or even have a negative effect on rating after a certain point, i.e., it would have a curvilinear effect. All of these are types of interactions we can explore. Interactions allow us to incorporate nonlinear relationships into the model, and so greatly extend the linear model’s capabilities - we basically get to use a linear model in a nonlinear way!\nWith that in mind, let’s explore how we can add interactions to our models. Going with our first example, let’s see how having kids impacts the relationship between genre and rating. We’ll start with a standard linear model, and then add an interaction term. Using a formula approach makes it very straightforward to add an interaction term. We just need to add a : between the two features we want to interact, or a * to denote both main effects and the interaction. As elsewhere, we present simplified results in the next table.\n\nRPython\n\n\n\ndf_reviews = read_csv('https://tinyurl.com/moviereviewsdata')\n\nmodel_baseline = lm(rating ~ children_in_home + genre, data = df_reviews)\nmodel_interaction = lm(rating ~ children_in_home * genre, data = df_reviews)\n\nsummary(model_interaction)\n\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ndf_reviews = pd.read_csv('https://tinyurl.com/moviereviewsdata')\n\nmodel_baseline = smf.ols(\n    formula = 'rating ~ children_in_home + genre', \n    data = df_reviews\n).fit()\n\nmodel_interaction = smf.ols(\n    formula = 'rating ~ children_in_home * genre', \n    data = df_reviews\n).fit()\n\nmodel_interaction.summary()\n\n\n\n\nHere is a quick look at the model output for the interaction vs. no interaction interaction model. Starting with the baseline model, the coefficients look like what we’ve seen before, but we have several coefficients for genre. The reason is that genre is composed of several categories, and converted to a set of dummy variables (refer to Section 2.7.2 and Section 10.2.2). In the baseline model, the intercept tells us what the mean is for the reference group, in this case Action/Adventure, and the genre coefficients tell us the difference between the mean for that genre and the reference. For example, the mean rating for Action/Adventure is 2.76, and the difference between that genre rating for the drama genre is 0.55. Adding the two gives us the mean for drama movies 2.76 + 0.55 = 3.32. We also have the coefficient for the number of children in the home, and this does not vary by genre in the baseline model.\n\n\n\n\nTable 6.1: Model coefficients with interaction\n\n\n\n\n\n\n  Model coefficients with and without an interaction\n  \n    \n      feature\n      coef_base\n      coef_inter\n    \n  \n  \n    (Intercept)\n2.764\n2.764\n    children_in_home\n0.142\n0.142\n    genreComedy\n0.635\n0.637\n    genreDrama\n0.554\n0.535\n    genreHorror\n0.129\n0.194\n    genreKids\n−0.199\n−0.276\n    genreOther\n0.029\n0.084\n    genreRomance\n0.227\n0.298\n    genreSci-Fi\n−0.123\n−0.109\n    children_in_home:genreComedy\n\n−0.006\n    children_in_home:genreDrama\n\n0.053\n    children_in_home:genreHorror\n\n−0.127\n    children_in_home:genreKids\n\n0.231\n    children_in_home:genreOther\n\n−0.106\n    children_in_home:genreRomance\n\n−0.124\n    children_in_home:genreSci-Fi\n\n−0.029\n  \n  \n  \n\n\n\n\n\n\n\nBut in our other model we have an interaction between two features: ‘children in the home’ and ‘genre’. So let’s start with the coefficient for children. It is 0.14, which means that for every additional child, the rating increases by that amount. But because of the interaction, we now interpret that as the effect of children when genre is the reference group Action/Adventure.\nNow let’s look at the interaction effect for children and the kids genre. It is 0.23, which means that for the kids genre, the effect of having children in the home increases by that amount. So our actual effect for an additional child for the kids genre is 0.14 + 0.23 = 0.37 increase in the expected review rating. It is also correct to say that the difference in rating between the kids genre and the reference group Action/Adventure is 0.23, but, with an increase in children, the difference in rating between the kids genre and Action/Adventure increases by 0.23. In other words, it is a difference in differences1.\nWhen we talk about differences in coefficients across values of features, it can get a little bit hard to follow. In every case that you employ an interaction, you should look at the interaction visually for interpretation. Here is a plot of the predictions from the interaction model. We can see that the effect of children in the home is strongest for kids movies than for other genres, which makes a whole lot of sense! In other genres, the effect of having children seems to have little effect, and in others it still has a positive effect, but not as strong as for kids movies.\n\n\n\n\n\n\n\n\n\nFigure 6.1: Interaction plot\n\n\n\n\n\nSo we can see that interactions can allow a linear effect to vary depending on the values of another feature. But the real take home message from this is that the general effect is actually not just a straight line! The linear effect changes depending on the setting. Furthermore, the effect for children only applies when ‘genre’ is at its default group, or when other features are at their default or zero.\nSo, when we have interactions, we can’t talk about a feature’s effect without considering the other features. Some might see this as a downside, but it’s actually how most relationships between features and targets work. Interactions let us model these complex relationships, and they’re used a lot in real-world situations.\n\n6.2.1 Average effects\nSo what is the effect of children in the home? Or a particular genre, for that matter? This is a problematic question, because the effect of one feature depends on the setting of the other feature. We can say what the effect of a feature is on average across the settings of the other features. This is called the average marginal effect2, something we’ve talked about in Section 3.3.3.2. We can compute this by averaging the effect of a feature across the values of the other features.\n\n\n\n\nTable 6.2: Average Marginal Effects of Children in the Home\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    children_in_home\n0.152\n0.03\n5.68\n0.00\n0.10\n0.20\n  \n  \n  \n\n\n\n\n\n\n\nSo-called marginal effects, and related approaches such as SHAP values (see Section 3.3.5), attempt to boil down the effect of a feature to a single number, but this is difficult even in the simpler GLM settings, and can be downright misleading in more complex settings like our interaction model. Here we see the average coefficient for children in the home is 0.15, but we saw in Table 6.1 that this is slightly larger than what we would estimate in the non-interaction model, and we saw in Figure 6.1 it’s actually near zero (flat) for some genres. So what is the average effect really telling us? Consider a more serious case of drug effects across demographic groups, where the effect of the drug is much stronger for some groups than others. Would you want your doctor to prescribe you a drug based on the average effect across all groups, or the specific group to which you belong?\nWhen dealing with interactions in a model, it’s best to consider how a feature’s effect changes based on the values of other features it interacts with. Visualizing these effects can help us understand how the relationships change. It’s also helpful to consider what the predicted outcome is at important feature values, and how this changes with different feature values. This is the approach we’ve used with interactions, and it’s a good strategy overall.\n\n\n6.2.2 ANOVA\nA common method for summarizing categorical effects in linear models is through analysis of variance or ANOVA, something we briefly mentioned in our chapter introducing the linear model Section 2.7.2.1. ANOVA breaks down the variance in a target attributable to different features or their related effects such as interactions. It’s a bit beyond the scope here to get into all the details, but we demonstrate it here.\n\nRPython\n\n\n\nanova(model_interaction)\n\n\n\n\nsmf.stats.anova_lm(model_interaction)\n\n\n\n\n\n\n\n\nTable 6.3: ANOVA table for interaction model\n\n\n\n\n\n\n  \n    \n      feature\n      df\n      sum_sq\n      mean_sq\n      f\n      p\n    \n  \n  \n    children_in_home\n1.00\n6.45\n6.45\n21.25\n0.00\n    genre\n7.00\n86.17\n12.31\n40.55\n0.00\n    children_in_home:genre\n7.00\n3.75\n0.54\n1.76\n0.09\n    Residuals\n984.00\n298.69\n0.30\n\n\n  \n  \n  \n\n\n\n\n\n\n\nIn this case, it doesn’t appear that the interaction effect is statistically significant if we use the typical .05 cut-off. We know the effect of children in the home varies across genres, but this result suggests maybe it’s not as much as we might think. However, we also saw that the estimate for the children effect more than doubled for the kids genre, so maybe we don’t want to ignore it. That’s for you to decide.\nThe ANOVA approach can be generalized to provide a statistical test to compare models. For example, we can compare the baseline model to the interaction model to see if the interaction model is a better fit. However, it’s entirely consistent with just looking at the interaction’s statistical result in the ANOVA for the interaction model alone, so it doesn’t provide any additional information in this case. Note that the only models that can be compared with ANOVA must be nested, i.e., one model is an explicit subset of the other.\nIt’s worth noting that ANOVA is often confused with being a model itself. When people use it this way, it is just a linear regression with only categorical features, something that is usually only seen within strict experimental designs that do not have interactions with continuous features. It’s pretty difficult to think of a linear regression setting where no continuous features would be of interest, but back when people were doing this stuff by hand, they just categorized everything to enable doing an ANOVA, which was tedious arithmetic but manageable. It’s a bit of a historical artifact, but might be useful for exploratory purposes. Other approaches are a little more general or not confined to nested models – ones that can be seen as subsets of another. An example is using AIC or the other methods employed in machine learning.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-mixed-models",
    "href": "linear_model_extensions.html#sec-lm-extend-mixed-models",
    "title": "6  Extending the Linear Model",
    "section": "6.3 Mixed Models",
    "text": "6.3 Mixed Models\n\n6.3.1 Knowing your data\nAs much fun as modeling is, knowing your data is far more important. You can throw any model you want at your data, from simple to fancy, but you can count on disappointment if you don’t fundamentally know the structure that lies within your data. Let’s take a look at the following visualizations. In Figure 6.2, we see a positive relationship between the length of the movie and ratings.\n\n\n\n\n\n\n\n\nFigure 6.2: Linear relationship between length of movie and rating.\n\n\n\n\n\nWe could probably just stop there, but given what we just saw, we might think to ourselves to be ignoring something substantial within our data: genre. We might want to ask a question, “Does this relationship work the same way across the different genres?”\n\n\n\n\n\n\n\n\nFigure 6.3: Genre Effects on Length and Rating\n\n\n\n\n\nA very quick examination of Figure 6.3 might suggest that the rating varies by genre, and that the relationship between length and rating varies significantly over the different genres. The group means in the right panel show variability across genre. In addition, on the left panel, some genres show a strong positive relationship, some show less of a positive relationship, a couple even show a negative relationship, and one even looks flat. We can also see that they would have different intercepts. This is a very important thing to know about your data! If we had just run a model with length as a feature and nothing else, we would have missed this important information.\nNow consider something a bit more complicated. Here is a plot of the relationship between the length of a movie and the rating, but across release year. Again we might think there is notable variability in the effect across years, as some slopes are positive, some very strongly positive, and some are even negative. How can we account for this?\n\n\n\n\n\n\n\n\nFigure 6.4: Release Year Effects on Length and Rating\n\n\n\n\n\n\n\n6.3.2 Overview of mixed models\nWhat we’ve just seen might initially bring to mind an interaction effect, and that’s the right way to think about it! A mixed model can be used to get at that type of relationship into our model, which we can think of as a group interaction, without much hassle and additional explainability. But it’s actually a quite flexible class that can also allow for more complicated but related types.\nBefore going too much further, the term mixed model is as vanilla as we can possibly make it, but you might have heard of different names such as hierarchical linear models, or multilevel models, or maybe mixed-effects models tossed around before. Maybe you’ve even been exposed to ideas like random effects or random slopes. These are in fact all instances of what we’re calling a mixed model.\nWhat makes a model a mixed model? The mixed model is characterized by the idea that a model can have fixed effects and random effects. Fortunately, you’ve already encountered fixed effects – those are the features that we have been using in all of our models so far! We are assuming a single true parameter (e.g. coefficient/weight) for each of those features to estimate, and that parameter is fixed.\nIn mixed models, a random effect typically comes from a specific distribution, almost always a normal distribution, that adds a unique source of variance in the target variable. This distribution of effects can be based on a grouping variable (such as genre), where we let those parameters, i.e. coefficients (or weights), vary across the groups, creating a distribution of values.\nLet’s take our initial example with movie length and genre. Formally, we might specify something like this:\n\\[\n\\text{rating} = b_{\\text{int[genre]}} + b_\\text{length}*\\text{length} + \\epsilon\n\\]\nIn this formula, we are saying that genre has its own unique effect for this model in the form of specific intercepts for each genre3. This means that whenever an observation belongs to a specific genre, it will have an intercept that reflects that genre, and that means that two observations with the same movie length but from different genres would have different predictions.\nWe also posit that those come from a random distribution. We can specify that as:\n\\[b_{\\text{int[genre]}} \\sim \\text{N}(b_\\text{intercept}, \\sigma_\\text{int\\_genre})\\]\nThis means that the random intercepts will be normally distributed and the overall intercept is just the mean of those random intercepts, and with its own variance. It also means our model will have to estimate that variance along with our residual variance. Another very common depiction is:\n\\[\\text{re}_{[\\text{int\\_genre}]} \\sim \\text{N}(0, \\sigma_\\text{int\\_genre})\\]\n\\[b_{\\text{int[genre]}} = b_\\text{intercept} +\\text{re}_{[\\text{int\\_genre}]}\\]\nThe same approach would apply with a random slope, where we would have a random slope for each group, and that random slope would be normally distributed with its own variance.\n\\[b_{\\text{length[genre]}} \\sim \\text{N}(b_\\text{length}, \\sigma_\\text{length\\_genre})\\]\nA simple random intercept and slope is just the start. As an example, we can let the intercepts and slopes correlate, and we could have multiple grouping factors contribute, as well as allowing other features and even interactions themselves to vary by group! This is where mixed models can get quite complex, but the basic idea is still the same: we are allowing parameters to vary across groups, and we are estimating the variance of those parameters.\n\n\n6.3.3 Using a mixed model\nOne of the key advantages of a mixed is that we can use it when the observations within a group are not independent. This is a very common situation in many fields, and it’s a good idea to consider this when you have grouped data. As an example we’ll use the happiness data for all available years, and we’ll consider the country as a grouping variable. In this case, observations within a country are likely to be more similar to each other than to observations from other countries. This is a classic example of when to use a mixed model. This is also a case where we wouldn’t just throw in country as a feature like any other factor, since there are 164 countries in the data.\nIn general, to use mixed models we have to specify a group effect for the categorical feature of focus, but that’s the primary difference from our previous approaches used for linear or generalized linear models. For our example, we’ll look at a model with a random intercept for country, and one that adds a random slope for the yearly trend across countries. This means that we are allowing the intercepts and slopes to vary across countries, and the intercepts and slopes can correlate with one another. Furthermore, by recoding year to start at zero, the intercept will represent the happiness score at the start of the data. In addition, to see a more reasonable effect, we also divide the yearly trend by 10, so the coefficient provides the change in happiness score per decade.\n\nRPython\n\n\nWe’ll use the lme4 package in R which is the most widely used package for mixed models.\n\nlibrary(lme4)\n\ndf_happiness_all = read_csv(\"https://tinyurl.com/worldhappinessallyears\")\n\ndf_happiness_all = df_happiness_all |&gt; \n    mutate(year_0 = (year - min(year))/10)\n\n# random intercepts are specified by a 1\nmodel_ran_int = lmer(\n    happiness_score ~ year_0 + (1| country), \n    df_happiness_all\n)\n\nmodel_ran_slope = lmer(\n    happiness_score ~ year_0 + (1 + year_0 | country), \n    df_happiness_all\n)\n\n# not shown\nsummary(model_ran_int)\nsummary(model_ran_slope)\n\n\n\nAs with our recommendation with GAMs later, R is the better tool for mixed models, as the functionality is overwhelmingly better there for modeling and post-processing. However, you can use statsmodels in Python to fit them as well4.\n\nimport statsmodels.api as sm\n\ndf_happiness_all = pd.read_csv(\"https://tinyurl.com/worldhappinessallyears\")\n\ndf_happiness_all = (\n    df_happiness_all\n    .assign(year_0 = lambda x: (x['year']- x['year'].min())/10)\n)\n\nmodel_ran_int = sm.MixedLM.from_formula(\n    \"happiness_score ~ year_0\", \n    df_happiness_all, \n    re_formula='1',\n    groups=df_happiness_all[\"country\"]  \n).fit()\n\nmodel_ran_slope = sm.MixedLM.from_formula(\n    \"happiness_score ~ year_0\", \n    df_happiness_all, \n    re_formula='1 + year_0',\n    groups=df_happiness_all[\"country\"]  \n).fit()\n\n# not shown\n# model_ran_int.summary()\n# model_ran_slope.summary()\n\n\n\n\nTable 6.4 shows some typical output from a mixed model, focusing on the random slope model. The fixed effect part (Fixed) is your basic GLM result and interpreted the same way. Nothing new there, and we can see a slight positive decade trend in happiness, though maybe not a strong one. But the random effects (Random) are where the action is! We can see the standard deviation of the random effects, i.e., the intercepts and slopes. We can also see the residual (observation level) standard deviation, which is conceptually the same as what you saw with standard linear regression. We can also see the correlation between the random intercepts and random slopes. Depending on your tool, the default may be in terms of variances and covariances rather than standard deviations and correlations, but otherwise the same.\n\n\n\n\nTable 6.4: Mixed model results\n\n\n\n\n\n\n  \n    \n      Parameter\n      Coefficient\n      SE\n      CI_low\n      CI_high\n      Group\n    \n  \n  \n    \n      Fixed\n    \n    (Intercept)\n5.34\n0.09\n5.16\n5.52\n\n    year_0\n0.06\n0.05\n-0.04\n0.16\n\n    \n      Random\n    \n    SD (Intercept)\n1.15\n\n\n\ncountry\n    SD (year_0)\n0.57\n\n\n\ncountry\n    Cor (Intercept~year_0)\n-0.38\n\n\n\ncountry\n    SD (Observations)\n0.34\n\n\n\nResidual\n  \n  \n  \n\n\n\n\n\n\n\nIn this case, we can see notable variability attributable to the random effects. How do we know this? Well, if our happiness score is on a 1 - 8 scale, and we have a standard deviation of 1.13 for it before accounting for anything else, we might surmise that having an effect of that size for just the country effect is a relatively notable amount (roughly 1.15). We can also see that the correlation between the random intercepts and random slopes is negative, which means that the groups with higher starting points have more negative slopes. Now let’s look at the estimates for the random effects for the model with both intercepts and slopes5.\n\nRPython\n\n\n\nestimated_RE = ranef(model_ran_slope)\n# mixedup::extract_random_effects(model_ran_slope) # prettier version\n\n\n\n\nestimated_RE = pd.DataFrame(model_ran_slope.random_effects)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.5: Estimated random effects\n\n\n\n\n\nHow do we interpret these deviations? For starters, they are deviations from the fixed effect for the intercept and year trend coefficient. So here that means anything negative is an intercept or slope below the corresponding fixed effect value, and anything positive is above that value. If we want the specific effect for a country, we just add the random effect value to the fixed effect value, and we can refer to those as random coefficients. So, for example, if we wanted to know the effects for the US, we would add its random effects to the fixed effects. This is exactly what we did in the previous section with the interaction model. However, you can typically get these from the package functionality directly. The results shows that the US starts at a very high happiness score, and actually has a slightly negative trend over time6.\n\nRPython\n\n\n\n# coef(model_ran_slope) stored here\nranef_usa = estimated_RE$country\nranef_usa = ranef_usa |&gt; \n    rownames_to_column('country') |&gt; \n    filter(country == 'United States') \n\nranef_usa[1, c('(Intercept)',  'year_0')] + fixef(model_ran_slope) \n\n  (Intercept)     year_0\n1    7.296452 -0.2753249\n\n\n\n\n\nranef_usa = estimated_RE['United States'].rename({'Group': 'Intercept'})\nranef_usa + model_ran_slope.fe_params\n\nIntercept    7.296\nyear_0      -0.275\ndtype: float64\n\n\n\n\n\nLet’s plot those random coefficients together to see how they relate to each other.\n\n\n\n\n\n\n\n\nFigure 6.6: Random effects for mixed model\n\n\n\n\n\nFrom this plot, we can sense why the estimated random effect correlation was negative. For individual country results, we can see that recently war-torn regions like Syria and Afghanistan have declined over time even while they started poorly as well. Some like Guinea and Togo started poorly but have improved remarkably over time. Many western countries started high and mostly stayed that way, though generally with a slight decline. Perhaps there’s only one direction to go when you’re already starting off well!\n\n\n\n\n\n\nAlways scale features for mixed models\n\n\n\n\n\nYour authors have run a lot of these models. Save yourself some trouble and standardize or otherwise scale your features before fitting the model. Just trust us, or at least, don’t be surprised when your model doesn’t converge.\n\n\n\n\n\n6.3.4 Mixed model summary\nFor a model with just one feature, we certainly had a lot to talk about! And this is just a glimpse of what mixed models have to offer, and the approach can be even richer than what we’ve just seen. But you might be asking- Why don’t I just put genre into the model like other categorical features? In the case of genre for movie reviews where there are few groups, that’s okay. But doing that with features with a lot of levels would typically result in estimation issues due to having so many parameters to estimate. In general mixed models provide several advantages for the data scientist:\n\nAny coefficient can be allowed to vary by groups, including other random effects. It actually is just an interaction in the end as far as the linear predictor and conceptual model is concerned.\nThe group-specific effects are penalized, which shrinks them toward the overall mean, and makes this a different approach from just adding a ‘mere interaction’. This helps avoid overfitting, and that penalty is related to the variance estimate of the random effect. In other words, you can think of it as running a penalized linear model where the penalty is applied to the group-specific effects (see Section 4.8).\nAlso unlike standard interaction approaches, we can estimate the covariance of the random effects, and specify different covariance structures for observations within groups.\nBecause of the way they are estimated, mixed models can account for lack of independence of observations7, which is a common issue in many datasets. This is especially important when you have repeated measures, or when you have a hierarchical structure in your data, such as students within schools, or patients within hospitals.\nStandard modeling approaches can estimate these difficult models very efficiently, even with thousands of groups.\nThe group effects are like a very simplified embedding (Section 10.2.2), where we have taken a categorical feature and turned it into a numeric one, like those shown in Figure 6.5. This may help you understand other embedding techniques that are used in other places like deep learning if you think of this as the simplest embedding approach.\nWhen you start to think about random effects and/or distributions for effects, you’re already thinking like a Bayesian (Section 4.11.2), who is always thinking about the distributions for various effects. Mixed models are the perfect segue from standard linear model estimation to Bayesian estimation, where everything is random.\nThe random effect is akin to a latent variable of ‘unspecified group causes’. This is a very powerful idea that can be used in many different ways, but importantly, you might want to start thinking about how you can figure out what those ‘unspecified’ causes may be!\nGroup effects will almost always improve your model’s performance relative to not having them, especially if you weren’t including those groups in your model because of how many groups there were.\n\nIn short, mixed models are a fun way to incorporate additional interpretive color to your model, while also getting several additional benefits to help you understand your data!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-gam",
    "href": "linear_model_extensions.html#sec-gam",
    "title": "6  Extending the Linear Model",
    "section": "6.4 Generalized Additive Models",
    "text": "6.4 Generalized Additive Models\n\n\n6.4.1 When straight lines aren’t enough\nFitting a line through your data isn’t always going to be the best approach. Not every relationship is linear and not every relationship is monotonic. Sometimes, you need to be able to model a relationship that has a fair amount of nonlinearity – they can appear as slight curves, waves, and any other type of wiggle that you can imagine.\nIn other words, we can go from the straight line here:\n\n\n\n\n\n\n\n\nFigure 6.7: A standard linear effect\n\n\n\n\n\nTo the curve seen here:\n\n\n\n\n\n\n\n\nFigure 6.8: A curvilinear effect\n\n\n\n\n\nThat curved line in Figure 6.8 is called a spline. It is created by a feature and expanding it to multiple columns, each of which is a function of the original feature. We then fit a model to that data as usual. What this ultimately means is that we can use a linear model to fit a curve through the data. While this might not give us the same tidy explanation that a typical line would offer, we will certainly get better predictions if it’s appropriate, and a better understanding of the reality and complexity of the true relationship. But often it’s useful for exploratory purposes, and visualization tools like ggplot, plotly8 and others make it easy. Here is some demonstration code (result not shown).\n\nRPython\n\n\n\nx = rnorm(1000)\ny = sin(x)\n\ntibble(x, y) |&gt; \n    ggplot(aes(x = x, y = y)) +\n    geom_smooth(method = 'gam', se = FALSE) \n\n\n\n\nimport plotly.graph_objects as go\nimport numpy as np\n\nx = np.random.normal(size = 1000)\ny = np.sin(x)\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x = x, \n        y = y,\n        line_shape = 'spline'\n    )\n)\n\n\n\n\nModels incorporating this type of effect belong to a broad group of generalized additive models (GAMs). When we explored interactions and mixed models, we explored how the feature-target relationship varies with another feature. There we focused on our feature and its relationship to the target at different values of other features. When we use a GAM, our initial focus is just on a specific feature and how its relationship with the target changes at different feature values. How are we going to do this, you might ask? Conceptually, we will have a model that looks like this:\n\\[\ny = f(x) + \\epsilon\n\\]\nThis isn’t actually any different than what you’ve seen - it really isn’t! It’s just shorthand for the input \\(x\\) being fed into a function \\(f()\\) of some kind. That function is very flexible, and can be anything you want, and up until now we just used to to create a simple sum of the inputs.\nThe difference now is that we are going to expand the feature \\(x\\) in some way, and so it will actually become multiple columns of input. Some approaches can be quite complex, tackling spatial, temporal, or other aspects of the data. But practically, it’s just extra columns in the model matrix that go into the model fitting function like any other feature. This helps us capture non-linear patterns in our data.\nAt this point, you might be asking yourself, “Why couldn’t I just use some type of polynomial regression or even a nonlinear regression?”. Of course you could, but both have limitations relative to a GAM. If you are familiar with polynomial regression, where we add columns that are squares, cubes, etc. of the original feature, you can think of GAMs as a more general approach, and very similar in spirit. But that polynomial approach assumes a specific form of nonlinearity, and has no regularization. This means that it tends to overfit the data you currently have, and you are forcing curves to fit through the data.\nAnother approach besides polynomials is to use a nonlinear regression model. In this setting, you need to know what the underlying functional form is. An example is a logistic growth curve model for population growth. Without taking extra steps, such models can also overfit. Furthermore, outside of well-known physical, chemical, or biological processes, it’s rarely clear what the underlying functional form should be. At the very least, we wouldn’t know a formula for life expectancy and happiness!\nGAMs are better in such settings because they fit the data well without needing to know the underlying form. They also prevent overfitting in smaller data and/or more complex settings by using a penalized estimation approach. We can use them for multiple features at once, and even include interactions between features. We also can use different types of splines to capture different types of nonlinearities. Here is another formal definition of a GAM that makes more clear we can deal with multiple features.\n\\[\n\\hat{y} = \\sum \\mathbf{X_j}\\beta_j\n\\]\nIn this case, each \\(X_j\\) is a matrix of the feature and its basis expansion, and the \\(\\beta_j\\) are the coefficients for each of those basis expansion columns. But a specific X could also just be a single feature and it’s coefficient to model a linear relationship.\nThe nice thing is that you don’t have to worry about the details of the basis expansion - the package you choose will take care of that for you. You’ll have different options, and often the default is fine, but sometimes you’ll want to adjust the technique and how ‘wiggly’ you want the curve to be.\n\n\n6.4.2 A standard GAM\nNow that you have some background, let’s give this a shot! In most respects, we can use the same sort of approach as we did with our other linear model examples. For our example here, we’ll use the model that was depicted in Figure 6.8, which looks at the relationship between life expectancy and happiness score from the world happiness data (2018). Results are simplified in the subsequent table.\n\nRPython\n\n\nWe’ll use the very powerful mgcv package in R. The s function will allow us to use a spline approach to capture the nonlinearity.\n\nlibrary(mgcv)\n\ndf_happiness_2018 = read_csv('https://tinyurl.com/worldhappiness2018')\n\nmodel_gam = gam(\n    happiness_score ~ s(healthy_life_expectancy_at_birth, bs = 'bs'), \n    data = df_happiness_2018\n)\n\nsummary(model_gam)\n\n\n\nWe can use the statsmodels package in Python to fit a GAM, or alternatively, pygam, and for consistency with previous models, we’ll choose the former. Honestly though, you should use R’s mgcv, as these require notably more work without having even much of basic functionality. In addition, there is an ecosystem of R packages to further extend mgcv’s capabilities.\n\nfrom statsmodels.gam.api import GLMGam, BSplines\n\ndf_happiness_2018 = pd.read_csv('https://tinyurl.com/worldhappiness2018')\n\nbs = BSplines(\n    df_happiness_2018['healthy_life_expectancy_at_birth'],\n    df = 9,\n    degree = 3\n)\n\ngam_happiness = GLMGam.from_formula(\n    'happiness_score ~ healthy_life_expectancy_at_birth', \n    smoother = bs,\n    data = df_happiness_2018\n)\n  \ngam_happiness_result = gam_happiness.fit()\n\ngam_happiness_result.summary()\n\n\n\n\n\n\n\n\n\nTable 6.5: GAM model output\n\n\n\n\n\n\n  \n    \n      Component\n      Term\n      Estimate\n      Std.Error\n      t.value\n      p.value\n    \n  \n  \n    parametric coefficients\nIntercept\n5.44\n0.06\n92.73\n0\n     \n \nEDF\nRef.df\nF.value\np.value\n    smooth terms\ns(Life Exp.)\n5.55\n6.49\n40.11\n0\n  \n  \n  \n\n\n\n\n\n\n\nWhen you look at the model output, what you get will depend a lot on the tool you use, and the details are mostly beyond the scope we want to present here (check out Michael’s (2022) document on GAMs for more ). But in general, the following information will be provided as part of the summary or as an attribute of the model object:\n\ncoefficients: The coefficients for each of the features in the model. For a GAM, these are the coefficients for the basis expansion columns, as well as standard linear feature effects.\nglobal test of a feature: Some tools will provide a statistical test of the significance of the entire feature’s basis expansion, as opposed to just the individual coefficients. Above we have the intercept and the summarized smooth term.\nedf/EDoF: Effective degrees of freedom. This is a measure of wiggle in the relationship between the feature and the target. The higher the value, the more wiggle you have. If you have a value close to 1, then you have a linear relationship. With our current result, we can be pretty confident that a nonlinear relationship gives a better idea about the relationship between a country’s life expectancy and happiness than a linear one.\nR-squared: Adjusted/Pseudo \\(R^2\\) or deviance explained. This is a measure of how much of the variance in the target is explained by the model. The higher the value, the better the model. Deviance explained is an analog to the unadjusted \\(R^2\\) value for a Gaussian model that is used in the GLM setting. It’s fine as a general assessment of prediction-target correspondence, and in this case, we might be feeling pretty good about the model.\n\nFar more important than any of these is the visual interpretation, and we can get plots from GAMs easily enough.\n\nRPython\n\n\n\n# not shown\nplot(model_gam)\n\n\n\n\n# not shown\nres_bs.plot_partial(0, cpr=True)\n\n\n\n\nUnfortunately the default package plots are not pretty, and sadly aren’t provided in the same way we’d expect for interpretation. But they’re fine for a quick look at your wiggly result. We provide a better looking one here9. The main interpretation is that there is not much relationship between healthy_life_expectancy_at_birth and happiness_score until you get to about 60 years of life expectancy, and then it increases at a faster rate. Various tools are available to easily plot the derivatives for more understanding.\n\n\n\n\n\n\n\n\nFigure 6.9: Visualizing a GAM\n\n\n\n\n\nTo summarize, we can use a GAM to model nonlinear relationships with a linear model approach. We can use splines to capture those nonlinearities, and we can use a penalized approach to control the amount of wiggle in our model. What’s more, we can interact the wiggle with other categorical and numeric features to capture even more complexity in our data. This allows us to model spatial, temporal, and other types of data that have complex relationships.\nGAMs are a very powerful modeling tool that take us a step toward more complex models, but without the need to go all the way to a neural network or similar. Plus they still provide standard statistical inference information. In short, they’re a great tool for modeling!\n\n\n\n\n\n\nGAMs are random effects models\n\n\n\n\n\nIt turns out that GAMs have a very close relationship to mixed models, where splines can be thought of as random effects (see Gavin Simpson’s post also), and GAMs can even incorporate the usual random effects for categorical variables. So you can think of GAMMs, or generalized additive mixed models, as a way to combine the best of both worlds into one extremely powerful modeling that covers a lot of ground. It’s also a great baseline and sanity check model for when you’re trying boosting or deep learning models, where a well-specified GAMM can be hard to beat for tabular data and still be highly interpretable, while coming with built-in uncertainty estimates.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-quantile",
    "href": "linear_model_extensions.html#sec-lm-extend-quantile",
    "title": "6  Extending the Linear Model",
    "section": "6.5 Quantile Regression",
    "text": "6.5 Quantile Regression\n\nPeople generally understand the concept of the arithmetic mean, or ‘average’. You see it some time during elementary school, it gets tossed around in daily language, and it is statistically important. After all, so many distributions depend on it! Why, though, do we feel so tied to it from a regression modeling perspective? Yes, it has handy features, but it can limit the relationships we can otherwise model effectively. Here we’ll show you what to do when the mean betrays you – and trust us, the mean will betray you at some point!\n\n6.5.1 When the mean breaks down\nIn a perfect data world, we like to assume the mean is equal to the middle observation of the data: the median. But that is only when things are symmetric though, and usually our data comes loaded with challenges. Skewness and even just a few extreme scores in your data may cause a rift between the median and the mean.\nLet’s say we take the integers between 1 and 10, and find the mean.\n\\[\\frac{1+2+3+4+5+6+7+8+9+10}{10} =  5.5\\]\nThe middle value in that vector of numbers would also be 5.5.\nWhat happens if we replace the 1 with a more extreme value, like -10?\n\\[\\frac{-10+2+3+4+5+6+7+8+9+10}{10} =  4.5\\]\nWith just one dramatic change, our mean went down by a whole point. The median observation, though, is still 5.5. In short, the median is invariant to wild swings out in the tails of your numbers.\nYou might be saying to yourself, “Why should I care about this central tendency chicanery?” Let us tell you why you should care – the least squares approach to the standard linear model dictates that the regression line needs to be fit through the means of the variables. If you have extreme scores that influence the mean, then your regression line will also be influenced by those extreme scores.\nConsider the following regression line:\n\n\n\n\n\n\n\n\nFigure 6.10: Linear line without extreme scores\n\n\n\n\n\nNow, what would happen if we replaced a few of our observations with extreme scores?\n\n\n\n\n\n\n\n\nFigure 6.11: Linear line with extreme scores\n\n\n\n\n\nWith just a casual glance, it doesn’t look like our two regression lines are that different. They both look like they have a similar positive slope, so all should be good. To offer a bit more clarity, though, let’s put those lines in the same space:\n\n\n\n\n\n\n\n\nFigure 6.12: Line lines with and without extreme scores\n\n\n\n\n\nWith 1000 observations, we see that having just 10 relatively extreme scores is enough to change the regression line, even if just a little. But that little bit can mean a huge difference for predictions or just the conclusions we come to.\nThere are a few approaches we could take here, with common approaches being dropping those observations or winsorizing them. Throwing away data because you don’t like the way it behaves is almost statistical abuse, and winsorization is just replacing those extreme values with numbers that you like a little bit better. Let’s not do that!\nA better answer to this challenge might be to try a median-based approach instead. This is where a model like quantile regression becomes handy. Formally, the objective function for the model can be expressed as:\n\\[\n\\text{Objective} =  \\Sigma \\left((\\tau - 1)\\sum_{y_{i} &lt; q}(y_{i} - q)+\\tau\\sum_{y_{i}\\geq q}(y_{i} - q) \\right)\n\\tag{6.1}\\]\nWith quantile regression, we are given an extra parameter for the model: \\(\\tau\\) or tau. It’s a number between 0 and 1 representing the desired quantile (e.g., 0.5 for the median). The objective function also treats positive residuals differently than negative residuals. If the residual is positive, then we multiply it by the tau value. If the residual is negative, then we multiply it by -1 plus the tau value.\nTo demonstrate this type of model, let’s use our movie reviews data. Let’s say that we are curious about the relationship between the word_count variable and the rating variable to keep things simple. To make it even more straightforward, we will use the standardized (scaled) version of the variable. In our default approach, we will start with a median regression, in other words, a quantile associated with a tau of .5.\n\nRPython\n\n\n\nlibrary(quantreg)\n\nmodel_median = rq(rating ~ word_count_sc, tau = .5, data = df_reviews)\n\nsummary(model_median)\n\n\n\n\nmodel_median = smf.quantreg('rating ~ word_count_sc',  data = df_reviews)\nmodel_median = model_median.fit(q = .5)\n\nmodel_median.summary()     \n\n\n\n\n\n\n\n\nTable 6.6: Quantile regression model output\n\n\n\n\n\n\n  \n    \n      feature\n      coef\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n3.09\n3.05\n3.26\n    word_count_sc\n−0.29\n−0.40\n−0.20\n  \n  \n  \n\n\n\n\n\n\n\nFortunately, our interpretation of this result isn’t all that different from a standard linear model – the rating should decrease by -0.29 for every bump in a standard deviation for the number of words, which in this case is about 5 words. However, this concerns the median rating, not the mean, as would be the case with standard linear regression.\nQuantile regression is not a one-trick-pony though. Remember, it is called quantile regression – not median regression. Being able to compute a median regression is just the default. What we can do also is to model different quantiles of the same data. It gives us the ability to answer brand new questions – does the relationship between user age and their ratings change at different quantiles of rating? Very cool!\nWe can now examine the trends within 5 different quantiles of the data - .1, .3 .5, .7, and .910. We aren’t limited to just those quantiles though, and you can examine any of them that you might find interesting. Here is a plot of the results of these models.\n\n\n\n\n\n\n\n\nFigure 6.13: Quantile regression lines\n\n\n\n\n\nTo interpret our visualization, we could say that all of the quantiles show a negative relationship. The 10th and 90th quantiles seem weakest, while those in the middle show a notably stronger relationship. We can also see that the 90th percentile model is better able to capture those values that would otherwise be deemed as outliers using other standard techniques. The following table shows the estimated coefficients for each of the quantiles, and suggests that all word count relationships are statistically significant, since the confidence intervals do not include zero.\n\n\n\n\nTable 6.7: Quantile regression model output\n\n\n\n\n\n\n  \n    \n      feature\n      coef\n      SE\n      CI_low\n      CI_high\n      quantile\n    \n  \n  \n    (Intercept)\n2.27\n0.03\n2.21\n2.33\ntau (0.1)\n    word_count_sc\n−0.13\n0.03\n−0.19\n−0.07\ntau (0.1)\n    (Intercept)\n2.79\n0.03\n2.73\n2.84\ntau (0.3)\n    word_count_sc\n−0.23\n0.02\n−0.27\n−0.19\ntau (0.3)\n    (Intercept)\n3.09\n0.02\n3.06\n3.12\ntau (0.5)\n    word_count_sc\n−0.29\n0.01\n−0.31\n−0.26\ntau (0.5)\n    (Intercept)\n3.32\n0.02\n3.28\n3.36\ntau (0.7)\n    word_count_sc\n−0.30\n0.02\n−0.34\n−0.27\ntau (0.7)\n    (Intercept)\n3.85\n0.05\n3.76\n3.95\ntau (0.9)\n    word_count_sc\n−0.14\n0.06\n−0.25\n−0.03\ntau (0.9)\n  \n  \n  \n    \n       95% confidence intervals are shown.\n    \n  \n\n\n\n\n\n\n\n\n\n6.5.2 The quantile loss function\nGiven how relatively simple the objective function is, let’s demystify this model further by creating our own quantile regression model. We’ll start by creating a loss function that we can use to fit our model.\n\nRPython\n\n\n\nquantile_loss = function(par, X, y, tau) {\n    q = X %*% par\n    \n    residual = y - q\n    \n    loss = ifelse(\n        residual &lt; 0, \n        (tau - 1)*residual, \n        tau*residual\n    )\n    \n    sum(loss)\n}\n\n\n\n\ndef quantile_loss(par, X, y, tau):\n    linear_predictor = X.dot(par)\n    \n    residual = y - linear_predictor\n    \n    loss = np.where(\n        residual &lt; 0, \n        (tau-1)*residual, \n        tau*residual\n    )\n    \n    return sum(loss)\n\n\n\n\nThis code is just the embodiment of Equation 6.1. Compared to some of our other approaches, we add an argument for tau, but otherwise proceed very similarly. We calculate the residuals, and then we calculate the loss based on the residuals.\n\n\n6.5.3 Fitting a quantile regression model\nNow that we have our data and our loss function, we can fit the model almost exactly like our standard linear model. Again, note the difference here with our tau value, which we’ve set to .5 to represent the median.\n\nRPython\n\n\n\nX = cbind(1, df_reviews$word_count_sc)\ny = df_reviews$rating\n\noptim(\n    par = c(intercept = 0, word_count_sc = 0),\n    fn  = quantile_loss,\n    X   = X,\n    y   = y,\n    tau = .5\n)$par\n\n    intercept word_count_sc \n    3.0886074    -0.2852232 \n\n\n\n\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\nX = pd.DataFrame(\n    {'intercept': 1, \n    'word_count_sc': df_reviews['word_count_sc']}\n)\ny = df_reviews['rating']\n\nminimize(quantile_loss, x0 = np.array([0, 0]), args = (X, y, .5)).x\n\narray([3.0901, -0.2842])\n\n\n\n\n\nLet’s compare this to our previous result, and the OLS results as well. As usual, our simple code does what we need it to do! We also see that the linear regression model would produce a relatively smaller coefficient.\n\n\n\n\nTable 6.8: Comparison of quantile regression models\n\n\n\n\n\n\n  \n    \n      model\n      intercept\n      word_count_sc\n    \n  \n  \n    OLS\n3.051\n−0.216\n    Median\n3.089\n−0.285\n    Ours\n3.089\n−0.285\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nYet Another Interaction\n\n\n\n\n\nOne way to interpret this result is that we have a nonlinear relationship between the word count and the rating, in the same way we had an interaction previously. In this case, our effect of number of reviews interacts with the target! In other words, we have a different word count effect for different ratings. This is a bit of a mind bender to process, but it’s another good example of how a linear approach can be used to model quirky relationships!",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-wrap",
    "href": "linear_model_extensions.html#sec-lm-extend-wrap",
    "title": "6  Extending the Linear Model",
    "section": "6.6 Wrapping Up",
    "text": "6.6 Wrapping Up\nThe standard linear model is useful across many different data situations. It does, unfortunately, have some issues when data becomes a little bit more “real”. When you have extreme scores or relationships that a standard model might miss, you don’t need to abandon your linear model in favor of something more exotic. Instead, you might just need to think about how you are actually fitting the line through your data.\n\n6.6.1 The common thread\nThe models discussed in this chapter are all linear models, but they add flexibility in how they model the relationship between the features and the target, and provide a non-linear aspect to the otherwise linear model. Furthermore, with tools like mixed models, GAMs, and quantile regression, we generalize our GLMs to handle even more complex data settings.\n\n\n6.6.2 Choose your own adventure\nNo matter how much we cover in this book, there is always more to learn. Hopefully you’ve got a good grip on linear models and related topics, so feel free to try out some machine learning in Chapter 7!\n\n\n6.6.3 Additional resources\nThere is no shortage of great references for mixed effects models. If you are looking for a great introduction to mixed models, we would recommend starting with yet another tutorial by one of your fearless authors! Michael Clark’s Mixed Models with R (2023) is a great introduction to mixed models and is freely available. For a more comprehensive treatment, you can’t go wrong with Gelman & Hill’s Data Analysis Using Regression and Multilevel/Hierarchical Models (2006), and their more recent efforts in Regression and Other Stories (2020), which will soon have an added component for mixed models - Advanced Regression and Multilevel Models (2024).\nIf you want to dive more into the GAM world, we would recommend that you start with the Moving Beyond Linearity chapter in An Introduction to Statistical Learning (James et al. 2021). Not only do they have versions for both R and Python, but both have been made available online. If you are wanting more after that, you can’t beat Simon Wood’s book, Generalized Additive Models: An Introduction with R (2017), or a more digestible covering of the same content by one of your own humble authors (Clark 2022).\nFor absolute depth on quantile regression, we will happily point you to the OG of quantile regression, Roger Koenker. His book, Quantile Regression (2005) is a must read for anyone wanting to dive deeper into quantile regression , or just play around with his R package quantreg. Galton, Edgeworth, Frisch, and Prospects for Quantile Regression in Econometrics is another of his. And finally, you might also consider Fahrmeir et al. (2021), which takes an even more generalized view of GLMs, GAMs, mixed models, quantile regression, and more (very underrated).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#sec-lm-extend-exercise",
    "href": "linear_model_extensions.html#sec-lm-extend-exercise",
    "title": "6  Extending the Linear Model",
    "section": "6.7 Exercise",
    "text": "6.7 Exercise\nThese models are so much fun, you should feel comfortable just swapping any feature(s) in and out of the models. For example, for the mixed model, try using gdp per capita or life expectancy instead of (just a) trend effect. For the GAM, try using several features with nonlinear effects and see what shakes out. For quantile regression, try a different feature like movie length using different quantiles.\n\n\n\n\nClark, Michael. 2022. Generalized Additive Models. https://m-clark.github.io/generalized-additive-models/.\n\n\n———. 2023. Mixed Models with R. https://m-clark.github.io/mixed-models-with-R/.\n\n\nFahrmeir, Ludwig, Thomas Kneib, Stefan Lang, and Brian D. Marx. 2021. Regression: Models, Methods and Applications. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-662-63882-8.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge university press.\n\n\nGelman, Andrew, Jennifer Hill, Ben Goodrich, Jonah Gabry, Daniel Simpson, and Aki Vehtari. 2024. “Advanced Regression and Multilevel Models.” http://www.stat.columbia.edu/~gelman/armm/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nKoenker, Roger. 2005. Quantile Regression. Vol. 38. Cambridge university press. https://books.google.com/books?hl=en&lr=&id=WjOdAgAAQBAJ&oi=fnd&pg=PT12&dq=koenker+quantile+regression&ots=CQFHSt5o-W&sig=G1TpKPHo-BRdJ8qWcBrIBI2FQAs.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with R, Second Edition. 2nd ed. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781315370279.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "linear_model_extensions.html#footnotes",
    "href": "linear_model_extensions.html#footnotes",
    "title": "6  Extending the Linear Model",
    "section": "",
    "text": "Some models that employ an interaction that investigates categorical group differences like this actually call their model a difference-in-difference model.↩︎\nThese results are provided by the marginaleffects R package, which is a great tool.↩︎\nThe error term \\(\\epsilon\\) is still assumed normal with mean zero and variance \\(\\sigma^2\\).↩︎\nOne of your authors worked for several years with the key developer of the mixed models functionality in statsmodels. As such, we can say there is zero doubt about the expertise going into its development, as there are few in the world with such knowledge. Even so, you probably won’t find the functionality is not as mature or as expansive as what you get in R.↩︎\nOne of your authors provides a package for mixed models in R called mixedup. It provides a nice way to extract random effects and summarize such models (link).↩︎\nLife expectancy in the US has actually declined recently.↩︎\nIndependence of observations is a key assumption in linear regression models, and when it’s violated, the standard errors of the coefficients are biased, which can lead to incorrect inferences. Rather than hacking a model (so-called ‘fixed effects’ models) or ‘correcting’ the standard error (e.g. with some ‘sandwich’ or estimator), mixed models can account for this lack of independence through the model itself.↩︎\nPlotly is directly available in R and Python, and plotnine is the ggplot equivalent in Python.↩︎\nWe used the see in R for a quick plot. We also recommend its functionality via the gratia package to visualize the derivatives, which will show where the feature effect is changing most.↩︎\nThe R function can take a vector of quantiles, while the Python function can only take a single quantile, so you would need to loop through the quantiles.↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extending the Linear Model</span>"
    ]
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "7  Core Concepts in Machine Learning",
    "section": "",
    "text": "7.1 Key Ideas\nHere are the key ideas we’ll cover in this chapter:",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#key-ideas",
    "href": "machine_learning.html#key-ideas",
    "title": "7  Core Concepts in Machine Learning",
    "section": "",
    "text": "Machine learning is an approach that prioritizes making accurate predictions using a variety of tools and methods.\nModels used in machine learning are typically more complex and difficult to interpret than those used in standard statistical models. However, any model can be used with ML.\nThere are many performance metrics used in machine learning, and care should be taken to choose the appropriate one for your situation. You can also use multiple performance metrics to evaluate a model.\nObjective functions likewise should be chosen for the situation, and are often different from the performance metric.\nRegularization is a general approach to penalize complexity in a model, and is typically used to improve generalization.\nCross-validation is a technique that helps us choose parameters for our models and compare different models.\n\n\n7.1.1 Why this matters\nMachine learning applications help define the modern world and how we interact with it. There are few aspects of modern society that have not been touched by it in some way. With a basic understanding of the core ideas behind machine learning, you will better understand the models and techniques that are used in ML applications, and be able to apply them to your own work. You’ll also be able to understand the limitations of these models, and not think of machine learning as ‘magic’.\n\n\n7.1.2 Good to know\nTo dive into applying machine learning models, you really only need a decent grasp of linear models as applied to regression and classification problems (Chapter 2, Chapter 5). It would also be good to have an idea behind how they are estimated, as the same basic logic serves as a starting point here (Chapter 4).",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-objective",
    "href": "machine_learning.html#sec-ml-objective",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.2 Objective Functions",
    "text": "7.2 Objective Functions\nWe’ve implemented a variety of objective functions in other chapters such as mean squared error for numeric targets and log loss for binary targets (Chapter 4). The objective function is what we used to estimate model parameters, but not necessarily the same as the performance metric we ultimately use to select a model. For example, we may use log loss as the objective function, but then use accuracy as the performance metric. In that setting, the log loss provides a ‘smooth’ objective function to search the parameter space over, while accuracy is a straightforward and more interpretable metric for stakeholders. In this case, the objective function is used to optimize the model, while the performance metric is used to evaluate the model. In some cases, the objective function and performance metric are the same (e.g. (R)MSE), and even if not, they might have selected the same ‘best’ model, but this is not always the case.\n\n\n\n\n\n\nTable 7.1: Commonly used objective functions in machine learning for regression and classification tasks.\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Objective Function\n      Description\n    \n  \n  \n    \n      Regression\n    \n    Mean Squared Error (MSE)\nAverage of the squared differences between the predicted and actual values.\n    Mean Absolute Error (MAE)\nAverage of the absolute differences between the predicted and actual values.\n    Huber Loss\nA robust approach that is less sensitive to outliers than MSE.\n    Log Likelihood\nMaximizes the likelihood of the data given the model parameters.\n    \n      Classification\n    \n    Binary Cross-Entropy / Log Likelihood (Loss)\nUsed for binary classification problems. Same as log-likelihood .\n    Categorical Cross-Entropy\nBinary approach extended to multi-class classification problems.\n  \n  \n  \n\n\n\n\n\n\n\n\nFor specific types of tasks and models you might use something else, but the above will suffice to get you started with many common settings. Even when dealing with different types of targets, such as counts, proportions, etc., one can use an appropriate likelihood objective, which allows you to cover a bit more ground.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-metrics",
    "href": "machine_learning.html#sec-ml-metrics",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.3 Performance Metrics",
    "text": "7.3 Performance Metrics\nWhen discussing how to understand our model (Section 3.2), we noted there are many performance metrics used in machine learning. Care should be taken to choose the appropriate one for your situation. Usually we have a standard set we might use for the type of predictive problem. For example, for numeric targets, we typically are interested in (R)MSE and MAE. For classification problems, many metrics are based on the confusion matrix, which is a table of the predicted classes versus the observed classes. From that we can calculate things like accuracy, precision, recall, AUROC, etc. (refer to Table 3.1).\nAs an example, and as a reason to get our first taste of machine learning, let’s get some metrics for a movie review model. Depending on the tool used, getting one type of metric should be as straightforward as most others if we’re using common metrics. As we start our journey into machine learning, we’ll show Python code first, as it’s the dominant tool. Here we’ll model the target in both numeric and binary form with corresponding metrics.\n\nPythonR\n\n\nIn Python, we can use the sklearn.metrics module to get a variety of metrics for both regression and classification problems.\n\n\n\nfrom sklearn.metrics import (\n    mean_squared_error, root_mean_squared_error,\n    mean_absolute_error, r2_score,\n    accuracy_score, precision_score, recall_score, \n    roc_auc_score, roc_curve, auc, confusion_matrix\n)\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\n\nimport pandas as pd\n\ndf_reviews = pd.read_csv('https://tinyurl.com/moviereviewsdata')\n\nX = df_reviews[\n    [\n        'word_count',\n        'age',\n        'review_year',\n        'release_year',\n        'length_minutes',\n        'children_in_home',\n        'total_reviews',\n    ]\n]\n\ny = df_reviews['rating']\ny_class = df_reviews['rating_good']\n\nmodel_lin_reg = LinearRegression().fit(X, y)\n\n# note that sklearn uses regularization by default for logistic regression\nmodel_log_reg = LogisticRegression().fit(X, y_class)\n\ny_pred_linreg = model_lin_reg.predict(X)\ny_pred_logreg = model_log_reg.predict(X)\n\n# regression metrics\nrmse = root_mean_squared_error(y, y_pred_linreg)\nmae = mean_absolute_error(y, y_pred_linreg)\nr2 = r2_score(y, y_pred_linreg)\n\n# classification metrics\naccuracy = accuracy_score(y_class, y_pred_logreg)\nprecision = precision_score(y_class, y_pred_logreg)\nrecall = recall_score(y_class, y_pred_logreg)\n\n\n\nIn R, we can use mlr3measures, which has a variety of metrics.\n\nlibrary(mlr3measures)\n\n# convert rating_good to factor for some metric inputs\ndf_reviews = read_csv('https://tinyurl.com/moviereviewsdata') |&gt; \n    mutate(rating_good = factor(rating_good, labels = c('bad', 'good'))) \n\nmodel_lin_reg = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_reviews\n)\n\nmodel_log_reg = glm(\n    rating_good ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_reviews,\n    family = binomial(link = 'logit')\n)\n\ny_pred_linreg = predict(model_lin_reg)\ny_pred_logreg = predict(model_log_reg, type = 'response')\ny_pred_logreg = factor(ifelse(y_pred_logreg &gt; .5, 'good', 'bad'))\n\n\n# regression metrics  \nrmse_val = rmse(df_reviews$rating, y_pred_linreg)\nmae_val  = mae(df_reviews$rating, y_pred_linreg)\nr2_val   = rsq(df_reviews$rating, y_pred_linreg)\n\n# classification metrics\naccuracy  = acc(df_reviews$rating_good, y_pred_logreg)\nprecision = precision(df_reviews$rating_good, y_pred_logreg, positive = 'good')\nrecall    = recall(df_reviews$rating_good, y_pred_logreg, positive = 'good')\n\n\n\n\nWe put them all together in the following table. Now we know how to get them, and it was easy! But as we’ll see later, there is a lot more to think about before we use these for model assessment.\n\n\n\n\nTable 7.2: Example Metrics for Linear and Logistic Regression Models\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Metric\n      Value\n    \n  \n  \n    \n      Linear Regression\n    \n    RMSE\n0.52\n    MAE\n0.41\n    R-squared\n0.32\n    \n      Logistic Regression\n    \n    Accuracy\n0.71\n    Precision\n0.72\n    Recall\n0.79",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-generalization",
    "href": "machine_learning.html#sec-ml-generalization",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.4 Generalization",
    "text": "7.4 Generalization\nGetting metrics is easy enough, but how will we use them? One of the key differences separating ML from traditional statistical modeling approaches is the assessment of performance on unseen or future data, a concept commonly referred to as generalization. The basic idea is that we want to build a model that will perform well on new data, and not just the data we used to train the model. This is because ultimately data is ever evolving, and we don’t want to be beholden to a particular set of data we just happened to have at a particular time and context.\nBut how do we do this? As a starting point, we can simply split (often called partitioning) our data into two sets, a training set and a test set, often called a holdout set. The test set is typically a smaller subset, say 25% of the original data, but this amount is arbitrary, and will reflect the data situation. We fit or train the model on the training set, and then use the model to make predictions on, or score, the test set. This general approach is also known as the holdout method. Consider a simple linear regression. We can fit the linear regression model on the training set, which provides us coefficients, etc. We can then use that model result to predict on the test set, and then compare the predictions to the observed target values in the test set. Here we demonstrate this with our simple linear model.\n\nPythonR\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_reviews[[\n        'word_count',\n        'age',\n        'review_year',\n        'release_year',\n        'length_minutes',\n        'children_in_home',\n        'total_reviews',\n    ]]\n\ny = df_reviews['rating']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size=0.25, \n    random_state=123\n)\n\nmodel_linreg_train = LinearRegression().fit(X_train, y_train)\n\n# get predictions\ny_pred_train = model_linreg_train.predict(X_train)\ny_pred_test = model_linreg_train.predict(X_test)\n\n# get RMSE\nrmse_train = root_mean_squared_error(y_train, y_pred_train)\nrmse_test = root_mean_squared_error(y_test, y_pred_test)\n\npd.DataFrame(\n    dict(\n        prediction = ['Train', 'Test'],\n        rmse = [rmse_train, rmse_test]\n    )\n).round(3)\n\n\n\n\n# create a train and test set\nlibrary(rsample)\n\nset.seed(123)\n\nsplit = initial_split(df_reviews, prop = .75)\n\nX_train = training(split)\nX_test  = testing(split)\n\nmodel_linreg_train = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = X_train\n)\n\n# get predictions\ny_train_pred = predict(model_linreg_train, newdata = X_train)\ny_test_pred  = predict(model_linreg_train, newdata = X_test)\n\n# get RMSE\nrmse_train = rmse(X_train$rating, y_train_pred)\nrmse_test  = rmse(X_test$rating, y_test_pred)\n\ntibble(\n    prediction = c('Train', 'Test'),\n    rmse = c(rmse_train, rmse_test)\n)\n\n\n\n\n\n\n\n\nTable 7.3: RMSE for Linear Regression Model on Train and Test Sets\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      prediction\n      rmse\n    \n  \n  \n    Train\n0.510\n    Test\n0.545\n  \n  \n  \n\n\n\n\n\n\n\nSo there you have it, you just did some machine learning! And now we have a model that we can use to predict with any new data that comes along with ease. But as we’ll soon see, there are limitations to doing things this simply. But conceptually this is an important idea, and one we will continue to return to.\n\n7.4.1 Using metrics for model evaluation and selection\nAs we’ve seen elsewhere, there are many performance metrics to choose from to assess model performance, and the choice of metric depends on the type of problem (Section 3.2). It also turns out that assessing the metric on the data we used to train the model does not give us the best assessment of that metric. This is because the model will do better on the data it was trained on than on new data it wasn’t trained on, and we can generally always improve that metric in training by making the model more complex. However, in many modeling situations, this complexity comes at the expense of generalization. So what we really want to ultimately say about our model will regard performance on the test set with our chosen metric, and not the data we used to train the model. At that point, we can also compare multiple models to one another given their performance on the test set, and select the one that performs best.\nIn the previous section you can compare our results on the tests vs. training set. Metrics are notably better on the training set on average, and that’s what we see here. But since we should be more interested in how well the model will do on new data, we use the test set to get a sense of that.\n\n\n7.4.2 Understanding test error and generalization\n\nThis part gets into the weeds a bit. If you are not so inclined, skip to the summary of this section.\n\nIn the following discussion, you can think of a standard linear model scenario, e.g. with squared-error loss function, and a data set where we split some of the observations in a random fashion into a training set, for initial model fitting, and a test set, which will be kept separate and independent, and used to measure generalization performance. We note training error as the average loss over all the training sets we could create in this process of random splitting. The test error is the average prediction error obtained when a model fitted on the training data is used to make predictions on the test data.\n\n\n7.4.2.1 Generalization in the classical regime\nSo what result should we expect in this scenario? Let’s look at the following visualization inspired by Hastie, Tibshirani, and Friedman (2017).\n\n\n\n\n\n\n\nFigure 7.1: Bias Variance Tradeoff\n\n\n\nPrediction error on the test set, shown in red, is a function of several components, and two of these are bias and variance.\nA key idea is that as the model complexity increases, we potentially capture more of the data variability. This reduces bias, which is the difference in our average prediction and the true model prediction. But this only works for training error, shown in blue, where eventually our model can potentially fit the training data perfectly!\nFor test error though, as the model complexity increases, the bias decreases, but the variance eventually increases. This variance reflects how much our prediction changes with different data. If our model gets too cozy with the training data, it will do poorly when we try to generalize beyond it, and this will be reflected in increased variance. This is traditionally known as the bias-variance tradeoff - we can reduce one source of error in the test set at the expense of the other, but not both at the same time indefinitely. In other words, we can reduce bias by increasing model complexity, but this will eventually increase variance in our test predictions. We can reduce variance by reducing model complexity, but this will increase bias. One additional thing to note is that even if we had the ‘true’ model given the features specified correctly, for the vast majority of cases there would still be prediction error due to the random data generating process (noise). This can potentially be reduced using additional valid features, getting better measurements, etc., but it will still be there to some extent in practice, and so will limit test set performance.\nThe ultimate goal is to find the sweet spot. We want a model that’s complex enough to capture the data, but not so complex that it overfits to the training data.\n\n\n7.4.2.2 Generalization in deep learning\n\nIt turns out that with lots of data and very complex models, or maybe even in most settings, our ‘classical’ understanding just described doesn’t hold up. In fact, it is possible to get a model that fits the training data perfectly, and yet ultimately still generalizes well to new data!\nThis phenomenon is encapsulated in the notion of double descent. The idea is that, with overly complex models such as those employed with deep learning, we can get to the point of interpolating the data exactly. But as we continue to increase the complexity of the model, we actually start to generalize better again, and visually this displays as a ‘double descent’ in terms of test error. We see an initial decrease in test error as the model gets better in general. After a while, it begins to rise as we would expect in the classical regime (Figure 7.1). Eventually it peaks at the point where we have as many parameters as data points. Beyond that however, as we get even more complex with our model, we can possibly see a decrease in test error again. Crazy!\nWe can demonstrate this on the classic mtcars dataset3, which has only 32 observations! We repeatedly trained a model to predict miles per gallon on only 10 of those observations, and assess test error on the rest. The model we used is a form of ridge regression, but we implemented splines for the car’s weight, horsepower, and displacement4, i.e. we GAMed it up (Section 6.4). We trained increasingly complex models, and in what follows we visualize the error as a function of model complexity.\nOn the left part of the visualization, we see that the test error dips as we get a better model. Our best test error is noted by the large gray dot. Eventually though, the test error rises as expected, even as training error gets better. Test error eventually hits a peak when the number of parameters equals the number of training observations. But then we keep going, and the test error starts to decrease again! By the end we have essentially perfect training prediction, and our test error is as good as it was with the simpler models. This is the double descent phenomenon with one of the simplest datasets around. Cool!\n\n\n\n\n\n\n\n\n\nFigure 7.2: Double Descent on the classic mtcars dataset\n\n\n\n\n\n\n\n7.4.2.3 Generalization summary\nThe take home point is this: our primary concern is generalization error. We can reduce this error by increasing model complexity, but this may eventually cause test error to increase. However, with enough data and model complexity, we can get to the point where we can fit the training data perfectly, and yet still generalize well to new data. In many standard or at least smaller data and model settings, you can maybe assume the classical regime holds. But when employing deep learning with massive data and billions of parameters, you can worry less about the model’s complexity. But no matter what, we should use tools to help make our model work better, and we prefer smaller and simpler models that can do as well as more complex ones, even if those ‘smaller’ models are still billions of parameters!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-regularization",
    "href": "machine_learning.html#sec-ml-regularization",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.5 Regularization",
    "text": "7.5 Regularization\nWe now are very aware that a key aspect of the machine learning approach is having our model to work well with new data. One way to improve generalization is through the use of regularization, which is a general approach to penalize complexity in a model, and is typically used to prevent overfitting. Overfitting occurs when a model fits the training data very well, but does not generalize well to new data. This usually happens when the model is too complex and starts fitting to random noise in the training data. We can also have the opposite problem, where the model is too simple to capture the patterns in the data, and this is known as underfitting5.\nIn the following demonstration, the first plot shows results from a model that is probably too complex for the data setting. The curve is very wiggly as it tries as much of the data as possible, and is an example of overfitting. The second plot shows a straight line fit as we’d get from linear regression. It’s too simple for the underlying feature-target relationship, and is an example of underfitting. The third plot shows a model that is a better fit to the data, and is an example of a model that is complex enough to capture the nonlinear aspect of the data, but not so complex that it capitalizes on a lot of noise.\n\n\n\n\n\n\n\n\nFigure 7.3: Overfitting and Underfitting\n\n\n\n\n\nWhen we examine generalization performance6, we see that the overfit model does best on training data, but relatively very poorly on test- nearly a 20% increase in the RMSE value. The underfit model doesn’t change as much in test performance because it was poor to begin with, and is the worst performer for both. Our ‘better’ model wasn’t best on training, but was best on the test set.\n\n\n\n\nTable 7.4: RMSE for each model on new data\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Model\n      RMSE\n      % change\n    \n  \n  \n    \n      Train\n    \n    Better\n2.18\n\n    Over\n1.97\n\n    Under\n3.05\n\n    \n      Test\n    \n    Better\n2.19\n0.6\n    Over\n2.34\n19.1\n    Under\n3.24\n6.1\n  \n  \n  \n\n\n\n\n\n\n\nA fairly simple example of regularization can be seen with a ridge regression model (Section 4.8), where we add a penalty term to the objective function. The penalty is a function of the size of the coefficients, and helps keep the model from getting too complex. It is also known as L2 regularization due to squaring the coefficients. Another type is the L1 penalty, used in the ‘lasso’ model, which is based on the absolute values of the coefficients. Yet another common approach combines the two, called elastic net. There we adjust the balance between the L1 and L2 penalties, and use cross-validation to find the best balance. L1 and/or L2 penalties are applied in many other models such as gradient boosting, neural networks, and others, and are a key aspect of machine learning.\nRegularization is used in many modeling scenarios. Here is a quick rundown of some examples.\n\nGAMs use penalized regression for estimation of the coefficients for the basis functions (typically with L2). This keeps the ‘wiggly’ part of the GAM from getting too wiggly, as in the overfit model above (Figure 7.3). This shrinks the feature-target relationship toward a linear one.\nSimilarly, the variance estimate of a random effect in mixed models, e.g. for the intercept or slope, is inversely related to an L2 penalty on the effect estimates for that group effect. The more penalization applied, the less random effect variance, and the more the random effect is shrunk toward the overall mean7.\n\n\nStill another form of regularization occurs in the form of priors in Bayesian models. There we use priors to control the influence of the data on the final model. A small variance on the prior shrinks the model towards the prior mean. If large, there is little influence of the prior on the posterior. In regression models, there is correspondence between ridge regression and using a normal distribution prior for the coefficients in Bayesian regression, where the L2 penalty is related to the variance of that prior. Even in deep learning, there is usually a ‘Bayesian interpretation’ of the regularization approaches employed.\nAs a final example of regularization, dropout is a technique used in deep learning to prevent overfitting. It works by randomly dropping out some of the nodes in intervening/hidden layers in the network during training. This tends to force the network to learn more robust features, allowing for better generalization.\n\n\n\n\n\n\n\nFigure 7.4: A neural net with dropout\n\n\n\nIn the end, regularization comes in many forms across the modeling landscape, and is a key aspect of machine learning and traditional statistical modeling alike. The primary goal is to decrease model complexity in the hopes of increasing our ability to generalize the selected model to new data scenarios.\n\n\n\n\n\n\nRegularization with Large Data\n\n\n\n\n\nFor the linear model and related models for typical tabular data, a very large dataset can often lessen the need for regularization. This is because the model can learn the patterns in the data without overfitting, and the penalty ultimately is overwhelmed by the other parts of the objective function. However, regularization is still useful in many cases, and can help with model stability and speed of convergence.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-cv",
    "href": "machine_learning.html#sec-ml-cv",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.6 Cross-validation",
    "text": "7.6 Cross-validation\nSo we’ve talked a lot about generalization, so now let’s think about some ways to go about a general process of selecting parameters for a model and assessing performance.\nWe previously used a simple approach where we split the data into training and test sets, fitted the model on the training set, and then assessed performance on the test set. This is fine, but the test set error, or any other metric, has uncertainty. It would be slightly different with any training-test split we came up with.\nWe’d also like to get better model assessment when searching the parameter space, because there are parameters for which we have no way of guessing the value beforehand, and we’ll need to try out different ones. An example would be the penalty parameter in lasso regression. In this case, we need to figure out the best parameters before assessing a final model’s performance.\nOne way to do this is to split the training data into different partitions, which we now call validation sets. We fit the model on the training set, and then assess performance on the validation set(s). We then repeat this process for many different splits of the data into training and validation sets, and average the results. This is known as K-fold cross-validation. It’s important to note that we still want a test set to be held out that is in no way used during the training process. The validation sets are used to help us choose the best model based on some metric, and the test set is used to assess the final model’s performance.\nHere is a visualization of 3-fold cross validation. We split the data such that 2/3 of it will be used for training, and 1/3 for validation. We then do this for a total of 3 times, so that the validation set is on a different part of the data each time, and all observations are used for both training and validation at some point. We then average the results of any metric across the validation sets. Note that in each case here, there is no overlap of data between the training and validation sets.\n\n\n\n\n\n\nFigure 7.5: 3-fold Cross Validation\n\n\n\nThe idea is that we are trying to get a better estimate of the error by averaging over many different validation sets. The number of folds, or splits, is denoted by \\(K\\). The value of \\(K\\) can be any number, but typically is 10 or less. The larger the value of \\(K\\), the more accurate the estimate of the metric, but the more computationally expensive it is, and in application, you generally don’t need much to get a good estimate. However, with smaller datasets, one can even employ a leave-one-out approach, where \\(K\\) is equal to the number of observations in the data.\nSo cross-validation provides a better measure of the metric we use to choose our model. When comparing a model with different parameter settings, we can look at the (average) metric each has from the validation process, and select the model parameter set that has the best metric value. This process is typically known as model selection. This works for choosing a model across different sets of hyperparameter settings, for example, with different penalty parameters for regularized regression. But can also aid in choosing a model from a set of different model types, for example, standard linear model approach vs. boosting. In that case we apply the cross-validation approach for each model, and the ‘winner’ is the one with the best average metric value on the test set.\nNow how might we go about this for modeling purposes? Very easily with modern packages. In the following we demonstrate cross-validation with a logistic regression model.\n\nPythonR\n\n\n\nfrom sklearn.linear_model import LogisticRegressionCV\n\nX = df_reviews.filter(regex='_sc$') # grab the standardized features\ny = df_reviews['rating_good']\n\n# Cs is the (inverse) penalty parameter;\nmodel_logistic_l2 = LogisticRegressionCV(\n    penalty='l2',      # penalty type\n    Cs=[1],            # penalty parameter value \n    cv=5, \n    max_iter=1000, \n    verbose=False\n).fit(X, y)\n\n# model_logistic_l2.scores_  # show the accuracy score for each fold\n\n# print the average accuracy score\nmodel_logistic_l2.scores_[1].mean()\n\n0.671\n\n\n\n\nFor R, we prefer mlr3 for our machine learning demonstrations, as we feel it is more like sklearn in spirit, as well as offering computational advantages for when you want to actually do ML with R8. The tidymodels ecosystem is also a good option.\n\nlibrary(mlr3)\nlibrary(mlr3learners)\n\nX = df_reviews |&gt;  \n    select(matches('_sc|good'))  # grab the standardized features/target\n\n# Define task\ntask_lr_l2 = TaskClassif$new('movie_reviews', X, target = 'rating_good')\n\n# Define learner (alpha = 0 is ridge/l2 regression)\nlearner_lr_l2 = lrn('classif.cv_glmnet', alpha = 0, predict_type = 'response')\n\n# set the penalty parameter to some value\nlearner_lr_l2$param_set$values$lambda = c(.1, .2) \n\n# Define resampling strategy\nmodel_logistic_l2 = resample(\n    task = task_lr_l2,\n    learner = learner_lr_l2,\n    resampling = rsmp('cv', folds = 5),\n    store_models = TRUE\n)\n\n# show the accuracy score for each fold\n# model_logistic_l2$score(msr('classif.acc')) \n\nmodel_logistic_l2$aggregate(msr('classif.acc'))\n\n\n\nclassif.acc \n      0.656 \n\n\n\n\n\nIn each case above, we end up with five separate accuracy values, one for each fold. Our final assessment of the model’s accuracy is the average of these five values, which is shown. This is a better estimate of the model’s accuracy than if we had just used a single test of the model, and in the end it is still based on the entire training data.\n\n7.6.1 Methods of cross-validation\nThere are different approaches we can take for cross-validation that we may need for different data scenarios. Here are some of the more common ones.\n\nShuffled: Shuffling prior to splitting can help avoid data ordering having undue effects.\nGrouped/stratified: In cases where we want to account for the grouping of the data, e.g. for data with a hierarchical structure. We may want groups to appear in training or test, but not both, as with grouped k-fold. Or we may want to ensure group proportions across training and test sets, as with stratified k-fold.\nTime-based: for time series data, where we only want to assess error on future values\nCombinations: For example, grouped and time-based\n\nHere are images from the scikit-learn library documentation depicting some different cross-validation approaches. In general, the type we use will be based on our data needs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.6: A comparison of cross-validation strategies.\n\n\n\n\n\n\n\n\n\nStratified Cross-validation\n\n\n\n\n\nIt’s generally always useful to use a stratified approach to cross-validation, especially with classification problems, as it helps ensure a similar balance of the target classes across training and test sets. You can also use this with numeric targets, enabling you to have a similar distribution of the target across training and test sets.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-tuning",
    "href": "machine_learning.html#sec-ml-tuning",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.7 Tuning",
    "text": "7.7 Tuning\nOne problem with the previous ridge logistic model we just used is that we set the penalty parameter to a fixed value. We can do better by searching over a range of values instead, and picking a ‘best’ value based on which model performs to our liking. This is generally known as hyperparameter tuning, or simply tuning. We can do this with k-fold cross-validation to assess the error for each value of the penalty parameter values. We then select the value of the penalty parameter that gives the lowest average error. This is a form of model selection.\nAnother potential point of concern is that we are using the same data to both select the model and assess its performance. This is a form of a more general phenomenon of data leakage, and may result in an overly optimistic assessment of performance. One solution is to do as we’ve discussed before, which is to split the data into three parts: training, validation, and test. We use the training set(s) to fit the model, assess their performance on the validation set(s), and select the best model. Then finally we use the test set to assess the best model’s performance. So the validation approach is used to select the model, and the test set is used to assess that model’s performance. The following visualizations from the scikit-learn documentation illustrates the process.\n\n\n\n\n\n\n\n\nTrain-Validation-Test Workflow\n\n\n\n\n\n\n\n5-fold Cross-Validation\n\n\n\n\n\n\nFigure 7.7: A tuning workflow.\n\n\n\n\n\n\n\n\n\nNested Cross-Validation\n\n\n\n\n\nAs the performance on test is not without uncertainty, we can actually nest the entire process within a validation approach, where we have an inner loop of k-fold cross-validation and an outer loop to assess the model’s performance on multiple hold out sets. This is known as nested cross-validation. This is a more computationally expensive approach, and generally would require more data, but it would result in a more robust assessment of performance\n\n\n\n\n7.7.1 A tuning example\nWhile this may start to sound complicated, it doesn’t have to be, as tools are available to make our generalization journey a lot easier. In the following we demonstrate this with the same ridge logistic regression model. The approach we use is called a grid search, where we explicitly step through potential values of the penalty parameter, fitting a model with the selected value through cross-validation. While we only look at one parameter here, for a given modeling approach we could construct a ‘grid’ of sets of parameter values to search over as well9. For each hyperparameter value, we are interested in the average accuracy score across the folds to assess the best performance. The final model can then be assessed on the test set10\n\nPythonR\n\n\nAgain we use the LogisticRegression function in sklearn to perform k-fold cross-validation to select the best penalty parameter. We then apply the best model to the test set and calculate accuracy. We do the same thing in R with the mlr3tuning package.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size=0.25, \n    random_state=42\n)\n\n# define the parameter values for GridSearchCV\nparam_grid = {\n    'C': [0.1, 1, 2, 5, 10, 20],\n}\n\n# perform k-fold cross-validation to select the best penalty parameter\n# Note that LogisticRegression by default is ridge regression for scikit-learn\nmodel_logistic_grid = GridSearchCV(\n    LogisticRegression(), \n    param_grid=param_grid, \n    cv=5, \n    scoring='accuracy'\n).fit(X_train, y_train)\n\n# if you want to inspect\nbest_model = model_logistic_grid.best_estimator_  \nbest_param = model_logistic_grid.best_params_['C']\n\n# apply the best model to the test set and calculate accuracy\nacc_train = model_logistic_grid.score(X_train, y_train)\nacc_test  = model_logistic_grid.score(X_test, y_test)\n\n\n\n\nTable 7.5: Results of hyperparameter tuning\n\n\n\nBest C: 2\nAccuracy on train set: 0.661\nAccuracy on test set: 0.692\n\n\n\n\n\n\nWe use the AutoTuner function to perform k-fold cross-validation to select the best penalty parameter.\n\n# Load necessary libraries\nlibrary(mlr3verse)\nlibrary(paradox) # for tuning\nlibrary(rsample) # for cross validation\n\nX = df_reviews |&gt; \n    mutate(rating_good = as.factor(rating_good)) |&gt; \n    select(matches('sc|rating_good'))\n\n# Define task\ntask = TaskClassif$new('movie_reviews', X, target = 'rating_good')\n\n# split the dataset into training and test sets\nsplits = partition(task, ratio = 0.75)\n\n# Define learner\nlearner = lrn('classif.glmnet', alpha = 0, predict_type = 'response')\n\n# Define resampling strategy\ncv_k5 = rsmp('cv', folds = 5)\n\n# Define measure\nmeasure = msr('classif.acc')\n\n# Define parameter space\nparam_set = ParamSet$new(list(\n    ParamDbl$new('lambda', lower = 1e-3, upper = 1)\n))\n\n# Define tuner\nmodel_logistic_grid = AutoTuner$new(\n    learner = learner,\n    resampling = cv_k5,\n    measure = measure,\n    search_space = param_set,\n    tuner = tnr('grid_search', resolution = 10),\n    terminator = trm('evals', n_evals = 10)\n)\n\n# Tune hyperparameters\nmodel_logistic_grid$train(task, row_ids = splits$train)\n\n# Get best hyperparameters\nbest_param = model_logistic_grid$model$learner$param_set$values\n\n# Use the best model to predict and get metrics\nacc_train = model_logistic_grid$predict(task, row_ids=splits$train)$score(measure)\nacc_test  = model_logistic_grid$predict(task, row_ids=splits$test)$score(measure)\n\n\n\n\nTable 7.6: Results of hyperparameter tuning\n\n\n\nBest lambda: 0.112\nAccuracy on train set: 0.688\nAccuracy on test set: 0.668\n\n\n\n\n\n\n\nSo there you have it. We searched the parameter space, chose the best set of parameters via k-fold cross validation, and got an assessment of generalization error. Neat!\n\n\n7.7.2 Parameter spaces\nIn the previous example, we used a grid search to search over a range of values for the penalty parameter. It is a quick and easy way to get started, but generally we want something that can search a better space of parameter values rather than a limited grid. It can also be computationally expensive with many hyperparameters, as we might have with boosting methods. We can do better by using more efficient approaches. For example, we can use a random search, where we randomly sample from the parameter space. This is generally faster than a grid search, and can be just as effective. Other methods are available that better explore the space and do so more efficiently.\n\n\n\n\n\n\nTuning and Overfitting\n\n\n\n\n\nA word of caution. Cross-validation is not a perfect solution, and you can still overfit the model selection process. This is especially true when you have a large number of parameters and other model aspects to search over. It may help to use more sophisticated approaches to search the parameter space, such as bayesian optimization, hyperband, or genetic algorithms, along with the nested cross-validation mentioned before (e.g., Cawley and Talbot (2010)).",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-pipelines",
    "href": "machine_learning.html#sec-ml-pipelines",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.8 Pipelines",
    "text": "7.8 Pipelines\nFor production-level work, or just for reproducibility, it is often useful to create a pipeline for your modeling work. A pipeline is a series of steps that are performed in sequence. For example, we might want to perform the following steps:\n\nImpute missing values\nTransform features\nCreate new features\nSplit the data into training and test sets\nFit the model on the training set\nAssess the model’s performance on the test set\nCompare the model with others\nSave the ‘best’ model\nUse the model for prediction on future data, sometimes called scoring\nRedo the whole thing on a regular basis\n\nWe can create a pipeline that performs all of these steps in sequence. This is useful for a number of reasons:\n\nUsing a pipeline makes it far easier to reproduce the results as needed. Running the pipeline means you are running each of the same exact steps in the same exact order.\nIt is relatively easy to change the steps in the pipeline. For example, we might want to try a different imputation method, or add a new model. The pipeline is already built to handle these steps, so any modification is straightforward and more easily applied.\nIt is straightforward to use the pipeline to new data. We can just start with the new data, and it will perform all of the steps in sequence.\nHaving a pipeline facilitates model comparison, as we can ensure that the models are receiving the same data process.\n\nWe can save the pipeline for later use. We just save the pipeline as a file, and then load it later when we want to use it again.\n\nWhile pipelines are useful for any modeling work, they are especially useful for machine learning, where we often have many steps to perform, and where we are often trying to compare many different models. You don’t have to have a formal pipeline, but it is a good practice to have a script that performs all of the steps in sequence, and that can be run at any time to reproduce the results. Formal pipeline tools make it easier to manage the process, and the following demonstrates how that might look.\n\nPythonR\n\n\nHere is an example of a pipeline in Python. We use the make_pipeline function from scikit-learn. This function takes a series of steps as arguments, and then performs them in sequence. We can then use the pipeline to fit the model, assess its performance, and save it for later use.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# create pipeline\nlogistic_cv_pipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    LogisticRegressionCV(penalty='l2', Cs=[1], cv=5, max_iter=1000),\n)\n\n# Fit the pipeline\nlogistic_cv_pipeline.fit(X_train, y_train)\n\n# Assess the pipeline on test\ny_pred = logistic_cv_pipeline.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n# Save the pipeline\n# from joblib import dump, load\n# dump(logistic_cv_pipeline, 'logistic_cv_pipeline.joblib')\n\n\n\n0.692\n\n\n\n\nWith R, mlr3 works in a similar fashion to scikit-learn. We create a pipeline with the po, or pipe operator function, which takes a series of steps as arguments, and then performs them in sequence.\n\n# Using task/splits/resampling from tuning section\n\n# Define pipeline\nlogistic_cv_pipeline =  po('imputemean') %&gt;&gt;%\n    po('scale') %&gt;&gt;%\n    po(\n        'learner', \n        lrn('classif.cv_glmnet', predict_type = 'response'), \n        alpha = to_tune(1e-04, 1e-1, logscale = TRUE)\n    )\n\nmodel_logistic_cv_pipeline = AutoTuner$new(\n    learner = logistic_cv_pipeline,\n    resampling = cv_k5, # defined earlier 5-fold cv\n    measure = measure,\n    tuner = tnr('grid_search', resolution = 10),\n    terminator = trm('evals', n_evals = 10)\n)\n\n# Fit pipeline\nmodel_logistic_cv_pipeline$train(task, row_ids = splits$train)\n\n# Assess pipeline on test\npreds = model_logistic_cv_pipeline$predict(task, row_ids = splits$test)\npreds$score(msr('classif.acc'))\n\n# Save pipeline\n# saveRDS(logistic_cv_pipeline, 'pipeline.rds')\n\n\n\nclassif.acc \n       0.66 \n\n\n\n\n\nDevelopment and deployment of pipelines will depend on your specific use case, and can get notably complicated. Think of a case where your model is the culmination of features drawn from dozens of wildly different databases, and the model itself being a complex ensemble of models, each with their own hyperparameters. You can imagine the complexity of the pipeline that would be required to handle all of that, but it is possible. Even then the basic approach is the same, and pipelines are a great way to organize your modeling work.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-where",
    "href": "machine_learning.html#sec-ml-where",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.9 Wrapping Up",
    "text": "7.9 Wrapping Up\nWhen machine learning began to take off, it seemed many in the field of statistics sat on their laurels, and often scoffed at these techniques that didn’t bother to test their assumptions11! ML was, after all, mostly just a rehash of statistics right? But the machine learning community, which actually comprised both computer scientists and statisticians, was able to make great strides in predictive performance, and the application of machine learning in myriad domains continues to enable us to push the boundaries of what is possible. Statistical analysis wasn’t going to provide ChatGPT or self-driving cars, but it remains vitally important whenever we need to understand the uncertainty of our predictions, or when we need to make inferences about the data world. Eventually, a more general field of data science became the way people use traditional statistical analysis and machine learning to solve their data challenges. The best data scientists will be able to draw from both, use the best tool for the job, and as importantly, have fun with modeling!\n\n7.9.1 The common thread\nIf using a model like the lasso or ridge regression, machine learning is simply a different focus to modeling compared to what we see in traditional linear modeling contexts. You could still do standard interpretation and statistical inference regarding the coefficient output even. However, in traditional statistical application of linear models, we rarely see cross-validation or hyperparameter tuning. It does occur in some contexts though and definitely should be more common.\nAs we will see though, the generality of machine learning’s approach allows us to use a wider variety of models than in standard linear model settings, and incorporates those that are not easily summarized from a statistical standpoint, such as boosting and deep learning models. The key is that any model, from linear regression to deep learning, can be used with the tools of machine learning.\n\n\n7.9.2 Choose your own adventure\nAt this point you’re ready to dive in and run some common models used in machine learning for tabular data, so head to Chapter 8!\n\n\n7.9.3 Additional resources\nIf looking for a deeper dive into some of these topics, here are some resources to consider:\n\nA core ML text is Elements Statistical Learning (Hastie, Tibshirani, and Friedman (2017)) which paved the way for modern ML.\nA more recent treatment is Probabilistic Machine Learning (Murphy (2023))\n\nOn the more applied side you might consider the courses like those found on Coursera and similar, as some are both good and taught by some very well known folks in machine learning. MC got his first formal taste of ML from Andrew Ng’s course on Coursera back in the day, and it was a great introduction. You can also get overviews on Google’s Developer pages (Google (2023)). And if we’re being honest, one of the mostly widely used resources for ML is the scikit-learn documentation.\nPython resources include:\n\nMachine Learning with PyTorch and Scikit-Learn (Raschka (2022))\nAn Introduction to Statistical Learning (Python) (James et al. (2021))\n\nR resources include:\n\nAn Introduction to Statistical Learning (R) (James et al. (2021))\nApplied Machine Learning for Tabular Data (Kuhn and Johnson (2023))\nApplied Machine Learning Using mlr3 in R (Bischl et al. (2024))\n\nMiscellaneous resources related to topics covered:\n\nRidge - Bayesian connection\nBias-Variance tradeoff\nReconciling modern machine-learning practice and the classical bias-variance trade-off\nOverview of dropout in deep learning\nAnnotated History of Modern AI and Deep Learning (Schmidhuber (2022))\nMachine Learning Flashcards (Albon (2024))",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#sec-ml-exercise",
    "href": "machine_learning.html#sec-ml-exercise",
    "title": "7  Core Concepts in Machine Learning",
    "section": "7.10 Exercise",
    "text": "7.10 Exercise\nWe did not run the pipeline above and think that doing so would be a good way for you to put your new skills to the test.\n\nStart by using the non-standardized features from the movie_reviews dataset.\nSplit the data into training and test sets.\nCreate a pipeline as we did previously that has at least two steps, e.g., scales the data and fits a model. Try a different model than the logistic regression we fit earlier (your choice).\nExamine the validation set results.\nAssess the pipeline’s performance on the test set, but use a different metric than accuracy.\nBonus: tune a hyperparameter for the model using a grid search or random search.\n\nYou can just modify the previous pipeline. Here is some helper code to get you going.\n\nPythonR\n\n\n\n# import the metrics and model you want\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score, recall_score \nfrom sklearn.tree import DecisionTreeClassifier\n\npipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    RandomizedSearchCV(\n        DecisionTreeClassifier(), \n        param_distributions={'max_depth': [2, 5, 7]}, \n        cv=5, \n        scoring='???',  # change to some other metric\n    ),\n)\n\n\n# extract the best model from the pipeline\nbest_model = pipeline.named_steps['randomizedsearchcv'].best_estimator_\n\n# extract the best parameter from the pipeline\nbest_model.max_depth\n\n# ???(y_test, y_pred) # use your chosen metric on the test set\n\n\n\n\ntask = TaskClassif$new('movie_reviews', df_reviews, target = 'rating_good')\nsplit = partition(task, ratio = 0.75) # set train/test split\n\n# Define learner\nlearner = lrn(\n    'classif.rpart', \n    predict_type = 'prob', # get predicted probabilities\n    cp = to_tune(1e-04, 1e-1, logscale = TRUE)\n)\n\n# pipeline = ??? same as above\n\nat = auto_tuner(\n    tuner = tnr('random_search'),\n    learner = pipeline,\n    resampling = rsmp ('cv', folds = 5),\n    measure = msr('classif.???'),  # change ??? e.g. try auc, recall, logloss\n    term_evals = 10\n)\n\n#\nat$train(task, row_ids = split$train)\n\nat$model$learner$param_set$values # get the best parameter\n\nat$predict(task, row_ids = split$test)$score(msr('classif.???')) # change ???\n\n\n\n\n\n\n\n\nAlbon, Chris. 2024. “Machine Learning Notes.” https://chrisalbon.com/Home.\n\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang, eds. 2024. Applied Machine Learning Using Mlr3 in R. https://mlr3book.mlr-org.com/.\n\n\nCawley, Gavin C., and Nicola L. C. Talbot. 2010. “On Over-Fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation.” The Journal of Machine Learning Research 11 (August): 2079–2107.\n\n\nGoogle. 2023. “Machine Learning  Google for Developers.” https://developers.google.com/machine-learning.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd Edition. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nKuhn, Max, and Kjell Johnson. 2023. Applied Machine Learning for Tabular Data. https://aml4td.org/.\n\n\nMurphy, Kevin P. 2023. “Probabilistic Machine Learning.” MIT Press. https://mitpress.mit.edu/9780262046824/probabilistic-machine-learning/.\n\n\nRaschka, Sebastian. 2022. Machine Learning with PyTorch and Scikit-Learn. https://sebastianraschka.com/books/machine-learning-with-pytorch-and-scikit-learn/.\n\n\nSchmidhuber, Juergen. 2022. “Annotated History of Modern AI and Deep Learning.” arXiv. https://doi.org/10.48550/arXiv.2212.11279.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#footnotes",
    "href": "machine_learning.html#footnotes",
    "title": "7  Core Concepts in Machine Learning",
    "section": "",
    "text": "The description of ML as machines learning ‘without being programmed’ can be misleading to the newcomer. In fact, many of the most common models used in machine learning are not capable of learning ‘on their own’ at any level, and require human intervention to provide processed data, specify the model, its parameters, set up the search through that parameter space, analyze the results, update the model, etc. We only very recently, post-2020, have developed models that appear to be able to generalize well to new tasks as if they have learned them without human involvement, but we still don’t want to ignore all the hands-on work that went into the development of those models, which never could have such capabilities otherwise. When you see this ‘learning without being programmed’ it is an odd way to say that we don’t have to guess the parameters ourselves (aside from the first guess). That said, it does feel like The Matrix, Star Trek and the rest is just around the corner though, doesn’t it?↩︎\nGeneralization in statistical analysis is more about generalizing from our sample of data to the population from which it’s drawn. In order to do that well or precisely, one needs to meet certain assumptions about the model. In machine learning, generalization is more about how well the model will perform on new data, and is often referred to as ‘out-of-sample’ performance.↩︎\nIf not familiar, the mtcars object is a data frame that comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models).↩︎\nIt’s actually called ridgeless regression.↩︎\nUnderfitting is a notable problem in many academic disciplines, where the models are often too simple to capture the complexity of the underlying process. Typically the models assume linear relationships without any interactions, and the underlying process may be anything but. These disciplines were slow to adopt machine learning techniques as they are often more difficult to interpret, and so seen as not as useful for understanding the underlying theoretical process. However, one could make the rather obvious argument that ‘understanding’ an unrealistic result is not very useful either, and that the goal should be to understand the underlying process however we can, and not just the model we’ve chosen to use.↩︎\nThe data is based on a simulation (using mgcv::gamSim), with training sample of 200 and scale of 1, so the test data is just more simulated data points.↩︎\nOne more reason to prefer a random effects approach over so-called fixed effects models, as the latter are not penalized at all, and thus are more prone to overfitting.↩︎\nIn this case we’re using glmnet for the logistic regression. To say that it is a confusing implementation of a model function compared to most of R is an understatement. While it’s hard to argue with the author of the lasso itself (who is an author of the package), it’s not the most user-friendly package in the world, and has confused most who’ve used it. Our example does actually set the penalty parameter, but it’s not the most straightforward thing to do.↩︎\nWe can use expand.grid or crossing in R, or pandas’ expand_grid to easily construct these values to iterate over. scikit-learn’s GridSearchCV function does this for us when we provide the dictionary of values for each parameter.↩︎\nIf you’re comparing the Python vs. R approaches, while the name explicitly denotes no penalty, the scikit-learn model by default uses ridge regression. In R we set the value alpha to enforce the ridge penalty, since glmnet by default uses the elastic net, a mixture of lasso and ridge. Also, scikit-learn uses the inverse of the penalty parameter, while mlr3 uses the penalty parameter directly. And obviously, no one will agree on what we should name the value (we have no idea where ‘C’ comes from, maybe ‘complexity’(?), though we have seen λ used in various statistical publications).↩︎\nBrian Ripley, a core R developer in the early days, said ‘To paraphrase provocatively, ’machine learning is statistics minus any checking of models and assumptions’’. Want to know what’s even crazier than that statement? It was said by the guy that literally wrote the book on neural networks before anyone was even using them in any practical way! He’s also the author of the nnet package in R, which existed before there was a scikit-learn in Python. Also interesting to note is that techniques like the lasso, random forests, and others associated with machine learning actually came from established statisticians. In short, there never was a statistics vs. machine learning divide. Tools are tools, and the best data scientists will have many at their disposal for any project.↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Core Concepts in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html",
    "href": "ml_common_models.html",
    "title": "8  Common Models in Machine Learning",
    "section": "",
    "text": "8.1 Key Ideas\nThe take home messages from this section include the following:",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-key-ideas",
    "href": "ml_common_models.html#sec-ml-common-key-ideas",
    "title": "8  Common Models in Machine Learning",
    "section": "",
    "text": "Any model can be used with machine learning\nA good and simple baseline is essential for interpreting your performance results\nOne only needs a small set of tools (models) to go very far with machine learning\n\n\n8.1.1 Why this matters\nHaving the right tools in data science saves time and improves results, and using well-known tools means you’ll have plenty of resources for help. It also allows you to focus more on the data and the problem, rather than the details of the model. A simple model might be all you need, but if you need something more complex, these models can still provide a performance benchmark.\n\n\n8.1.2 Good to Know\nBefore diving in, it’d be helpful to be familiar with the following:\n\nLinear models, esp. linear and logistic regression (Chapter 2, Chapter 5)\nBasic machine learning concepts as outlined in the ML Concepts chapter (Chapter 7)\nModel estimation as outlined in the Estimation chapter (Chapter 4)",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-general-approach",
    "href": "ml_common_models.html#sec-ml-common-general-approach",
    "title": "8  Common Models in Machine Learning",
    "section": "8.2 General Approach",
    "text": "8.2 General Approach\nLet’s start with a general approach to machine learning to help us get some bearings. Here is an example outline of the process we could take. This incorporates some of the ideas we also cover in other chapters, and we’ll demonstrate most of this in the following sections.\n\nDefine the problem, including the target variable(s)\nSelect the model(s) to be used, including one baseline model\nDefine the performance objective and metric(s) used for model assessment\nDefine the search space (parameters, hyperparameters) for those models\nDefine the search method (optimization)\nImplement some sort of validation technique and collect the corresponding performance metrics\nEvaluate the results on unseen data with the chosen model\nInterpret the results\n\nHere is a more concrete example:\n\nDefine the problem: predict the probability of heart disease given a set of features\nSelect the model(s) to be used: ridge regression, standard regression with no penalty as baseline\nDefine the objective and performance metric(s): RMSE, R-squared\nDefine the search space (parameters, hyperparameters) for those models: penalty parameter\nDefine the search method (optimization): grid search\nImplement some sort of cross-validation technique: 5-fold cross-validation\nEvaluate the results on unseen data: RMSE on test data\nInterpret the results: the ridge regression model performed better than the baseline model, and the coefficients tell us something about the nature of the relationship between the features and the target\n\nAs we go along in this chapter, we’ll see most of this in action. So let’s get to it!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-data-setup",
    "href": "ml_common_models.html#sec-ml-common-data-setup",
    "title": "8  Common Models in Machine Learning",
    "section": "8.3 Data Setup",
    "text": "8.3 Data Setup\nFor our demonstration here, we’ll use the heart disease dataset. This is a popular ML binary classification problem, where we want to predict whether a patient has heart disease, given information such as age, sex, resting heart rate etc (Section A.3).\nThere are two forms of the data - one which is mostly in raw form, and one that is purely numeric, where the categorical features are dummy coded and where numeric variables have been standardized (Section 10.2). The purely numeric version will allow us to forgo any additional data processing for some model/package implementations. We have also dropped the handful of rows with missing values. This form of the data will allow us to use any model and make direct comparisons later.\nIn this data, roughly 46% suffered from heart disease, so that is an initial baseline if we’re interested in accuracy- we could get 54% correct by just guessing the majority class of no disease. Hopefully we can do better than that!\n\nPythonR\n\n\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.neural_network import MLPClassifier\n\nfrom lightgbm import LGBMClassifier\n\n\ndf_heart = pd.read_csv('https://tinyurl.com/heartdiseaseprocessed')\ndf_heart_num = pd.read_csv('https://tinyurl.com/heartdiseaseprocessednumeric')\n\n# convert appropriate features to categorical\nfor col in df_heart.select_dtypes(include='object').columns:\n    df_heart[col] = df_heart[col].astype('category')\n\nX = df_heart_num.drop(columns=['heart_disease']).to_numpy()\ny = df_heart_num['heart_disease'].to_numpy()\n\nprevalence = np.mean(y)\nmajority = np.max([prevalence, 1 - prevalence])\n\n\n\n\nlibrary(tidyverse)\n\ndf_heart = read_csv(\"https://tinyurl.com/heartdiseaseprocessed\") |&gt; \n    mutate(across(where(is.character), as.factor))\n\ndf_heart_num = read_csv(\"https://tinyurl.com/heartdiseaseprocessednumeric\")\n\n# for use with for mlr3\nX = df_heart_num |&gt; \n    as_tibble() |&gt; \n    mutate(heart_disease = factor(heart_disease)) |&gt; \n    janitor::clean_names() # remove some symbols\n\n\n\n\nOne last thing, as we go along, performance metrics will vary depending on your setup (e.g. Python vs. R), package versions used, and other things. As such your results may not look exactly like these, and that’s okay! The important thing is to understand the concepts and how to apply them to your own data.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-baseline",
    "href": "ml_common_models.html#sec-ml-common-baseline",
    "title": "8  Common Models in Machine Learning",
    "section": "8.4 Beat the Baseline",
    "text": "8.4 Beat the Baseline\n\nBefore getting carried away with models, we should have a good reference point for performance - a baseline model. The baseline model should serve as a way to gauge how much better your model performs over one that is simpler, probably more computationally efficient, more interpretable, and is still viable. It could also be a model that is sufficiently complex to capture something about the data you are exploring, but not as complex as the models you’re also interested in. Take a classification model for example, where we often use a logistic regression as a baseline. It is a viable model to begin answering some questions, but is often too simple to be adequately performant for many situations. We should be able to get better performance with more complex models, or there is little justification for using them.\n\n8.4.1 Why do we do this?\nHaving a baseline model can help you avoid wasting time and resources implementing more complex tools, and to avoid mistakenly thinking performance is better than expected. It is probably rare, but sometimes relationships for the chosen features and target are mostly or nearly linear and have little interaction. In this case, no amount of fancy modeling will make complex feature targets exist if they don’t already. Furthermore, if our baseline is a more complex model that actually incorporates nonlinear relationships and interactions (e.g. a GAMM), you’ll often find that the more complex models often don’t significantly improve on it. As a last example, in time series settings, a moving average can often be a difficult baseline to beat, and so can be a good starting point.\nSo in general, you may find that the initial baseline model is good enough for present purposes, and you can then move on to other problems to solve, like acquiring data that is more predictive. This is especially true if you are working in a business setting where you have limited time and resources, but should be of mind in many other settings as well\n\n\n8.4.2 How much better?\nIn many settings, it often isn’t enough to merely beat the baseline model. Your model should perform statistically better. For instance, if your advanced model accuracy is 75% and your baseline model’s accuracy is 73%, that’s great. But, it’s good to check if this 2% difference is statistically significant. Remember, accuracy and other metrics are estimates and come with uncertainty1. This means you can get a ranged estimate for them, as well as test whether they are different from one another (see Table 8.1). If the difference is not statistically significant, then it’s possible that there is no difference, and you should probably stick with the baseline model, or maybe try a different approach. Such a result means that the next time you run the model, the baseline may actually perform better, or at least you can’t be sure that it won’t.\n\n\n\n\nTable 8.1: Interval Estimates for Accuracy\n\n\n\n\n\n\n  \n    \n      Sample Size\n      Lower Bound\n      Upper Bound\n      p-value\n    \n  \n  \n    1000\n−0.06\n0.02\n0.31\n    10000\n−0.03\n−0.01\n0.00\n  \n  \n  \n    \n       Confidence intervals are for the difference in proportions at values of .73 and .75, and p-values are for the difference in proportions.\n    \n  \n\n\n\n\n\n\n\nThat said, in some situations any performance increase is worth it, and even if we can’t be certain a result is statistically better, any sign of improvement is worth pursuing. For example, if you are trying to predict the next word in a sentence, and your baseline is 10% accurate, and your complex model is 12% accurate, that’s a 20% increase over the baseline, and which may be significant in terms of user experience. You should still try and show that this is a consistent increase and not a fluke.\nIn other settings, you’ll need to make sure the cost is worth it. Is 2% worth millions of dollars? Six months of research? These are among many of the practical considerations you may have to make as well.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-penalized",
    "href": "ml_common_models.html#sec-ml-common-penalized",
    "title": "8  Common Models in Machine Learning",
    "section": "8.5 Penalized Linear Models",
    "text": "8.5 Penalized Linear Models\nSo let’s get on with some models already! Let’s use the classic linear model as our starting point for ML. We show explicitly how to estimate models like lasso and ridge regression in Section 4.8. Those work well as a baseline, and so should be in your ML modeling toolbox.\n\n8.5.1 Elastic Net\nAnother common linear model approach is elastic net, which we also saw in Chapter 7. It combines two techniques: lasso and ridge regression. We will not show how to estimate elastic net by hand here, but all you have to know is that it combines the two penalties- one for lasso and one for ridge, along with a standard objective function for a numeric or categorical target. The relative proportion of the two penalties is controlled by a mixing parameter, and the optimal value for it is determined by cross-validation. So for example, you might end up with a 75% lasso penalty and 25% ridge penalty. In the end though, we’re just going to do a slightly fancier logistic regression!\nLet’s apply this to the heart disease data. We are only doing simple cross-validation here to get a better performance assessment, but you are more than welcome to tune both the penalty parameter and the mixing ratio as we have demonstrated before (Section 7.7). We’ll revisit hyperparameter tuning towards the end of this chapter.\n\nPythonR\n\n\n\nmodel_elastic = LogisticRegression(\n    penalty = 'elasticnet',\n    solver = 'saga',\n    l1_ratio = 0.5,\n    random_state = 42,\n    max_iter = 10000,\n    verbose = False,\n)\n\n# use cross-validation to estimate performance\nmodel_elastic_cv = cross_validate(\n    model_elastic,\n    X,\n    y,\n    cv = 5,\n    scoring = 'accuracy',\n)\n\n# pd.DataFrame(model_elastic_cv) # default output\n\n\n\nTraining accuracy:  0.828 \nGuessing:  0.539\n\n\n\n\n\nlibrary(mlr3verse)\n\ntsk_elastic = as_task_classif(\n    X,\n    target = \"heart_disease\"\n)\n\nmodel_elastic = lrn(\n    \"classif.cv_glmnet\", \n    nfolds = 5, \n    type.measure = \"class\", \n    alpha = 0.5\n)\n\nmodel_elastic_cv = resample(\n    task = tsk_elastic,\n    learner = model_elastic,\n    resampling = rsmp(\"cv\", folds = 5)\n)\n\n# model_elastic_cv$aggregate(msr('classif.acc')) # default output\n\n\n\nTraining Accuracy: 0.825\nGuessing: 0.539\n\n\n\n\n\nSo we’re starting off with what seems to be a good model. Our average accuracy across the validation sets is definitely doing better than guessing, an increase of almost 79%! Now let’s see if we can do better with other models!\n\n\n8.5.2 Strengths & weaknesses\nStrengths\n\nIntuitive approach. In the end, it’s still just a standard regression model you’re already familiar with.\nWidely used for many problems. Lasso/Ridge/ElasticNet would be fine to use in any setting you would use linear or logistic regression.\nA good baseline for tabular data problems.\n\nWeaknesses\n\nDoes not automatically seek out interactions and non-linearity, and as such will generally not be as predictive as other techniques.\nVariables have to be scaled or results will largely reflect data types.\nMay have interpretability issues with correlated features.\n\n\n\n8.5.3 Additional thoughts\nUsing penalized regression is a very good default method in the tabular data setting, and is something to strongly consider for more interpretative model settings like determining causal effects. These approaches predict better on new data than their standard, non-regularized complements, so they provide a nice balance between interpretability and predictive power. However, in general they are not going to be as strong of a method as others typically used in the machine learning world, and may not even be competitive without a lot of feature engineering. If prediction is all you care about, you’ll likely want to try something else.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-trees",
    "href": "ml_common_models.html#sec-ml-common-trees",
    "title": "8  Common Models in Machine Learning",
    "section": "8.6 Tree-based Models",
    "text": "8.6 Tree-based Models\nLet’s move beyond standard linear models and get into a notably different type of approach. Tree-based methods are a class of models that are very popular in machine learning, and for good reason, they work very well. To get a sense of how they work, consider the following classification example where we want to predict a binary target as ‘Yes’ or ‘No’.\n\n\n\nA simple classification tree\n\n\nWe have two numeric features, \\(X_1\\) and \\(X_2\\). At the start, we take \\(X_1\\) and make a split at the value of 5. Any observation less than 5 on \\(X_1\\) goes to the right with a prediction of No. Any observation greater than or equal to 5 goes to the left, where we then split based on values of \\(X_2\\), and specifically at the value of 3. Any observation less than 3 (and greater than or equal to 5 on \\(X_1\\)) goes to the right with a prediction of Yes. Any observation greater than or equal to 3 (and greater than or equal to 5 on \\(X_1\\)) goes to the left with a prediction of No. So in the end, we see that an observation that is relatively lower on \\(X_1\\), or relatively higher on both, results in a prediction of No. On the other hand, an observation that is high on \\(X_1\\) and low on \\(X_2\\) results in a prediction of Yes.\nThis is a simple example, but it illustrates the core idea of a tree-based model, where the tree reflects the total process, and branches are represented by the splits going down, ultimately ending at leaves where predictions are made. We can also think of the tree as a series of if-then statements, where we start at the top and work our way down until we reach a leaf node, which is a prediction for all observations that qualify for that leaf.\nIf we just use a single tree, this would be the most interpretable model we could probably come up with. It also incorporates nonlinearities (multiple branches on a single feature), interactions (branches across features), and feature selection all in one (some features may not result in useful splits for the objective). However, a single tree is not a very stable model unfortunately, and so does not generalize well. For example, just a slight change in data, or even just starting with a different feature, might produce a very different tree2.\nThe solution to that problem is straightforward though - by using the power of a bunch of trees, we can get predictions for each observation from each tree, and then average the predictions, resulting in a most stable estimate. This is the concept behind both random forests and gradient boosting, which can be seen as different algorithms to produce a bunch of trees. They are also considered types of ensemble models, which are models that combine the predictions of multiple models, to ultimately produce a single prediction for each observation. In this case each tree serves as a model.\nRandom forests (RF) and boosting methods (GB) are very easy to implement, to a point. However, there are typically several hyperparameters to consider for tuning. Here are just a few to think about:\n\nNumber of trees\nLearning rate (GB)\nMaximum depth of each tree\nMinimum number of observations in each leaf\nNumber of features to consider at each tree/split\nRegularization parameters (GB)\nOut-of-bag sample size (RF)\n\nFor these models, the number of trees and learning rate play off of each other. Having more trees allows for a smaller rate3, which might improve the model but will take longer to train. However, it can lead to overfitting if other steps are not taken.\nThe depth of each tree refers to how many levels we allow the model to branch out, and is a crucial parameter. It controls the complexity of each tree, and thus the complexity of the overall model- less depth helps to avoid overfitting, but if the depth is too shallow, you won’t be able to capture the nuances of the data. The minimum number of observations in each leaf is also important for similar reasons.\nIt’s also generally a good idea to take a random sample of features for each tree (or possibly even each branch), to also help reduce overfitting, but it’s not obvious what proportion to take. The regularization parameters are typically less important in practice, but help reduce overfitting as in other modeling circumstances. As with hyperparameters in other model settings, you’ll use something like cross-validation to settle on final values.\n\n8.6.1 Example with LightGBM\nHere is an example of gradient boosting with the heart disease data. Although boosting methods are available in scikit-learn for Python, in general we recommend using the lightgbm or xgboost packages directly for boosting implementation, which have a sklearn API anyway (as demonstrated). Also, they both provide R and Python implementations of the package, making it easy to not lose your place when switching between languages. We’ll use lightgbm here, but xgboost is also a very good option 4.\n\nPythonR\n\n\n\nmodel_boost = LGBMClassifier(\n    n_estimators = 1000,\n    learning_rate = 1e-3,\n    max_depth = 5,\n    verbose = -1,\n    random_state=42,\n)\n\nmodel_boost_cv = cross_validate(\n    model_boost,\n    df_heart.drop(columns='heart_disease'),\n    df_heart['heart_disease'],\n    cv = 5,\n    scoring='accuracy',\n)\n\n# pd.DataFrame(model_boost_cv)\n\n\n\nTraining accuracy:  0.835 \nGuessing:  0.539\n\n\n\n\nNote that as of writing, the mlr3 implementation of lightgbm doesn’t seem to handle factors even though the lightgbm R package does. So we’ll use the numeric version of the data here.\n\n\nlibrary(mlr3verse)\n\n# for lightgbm, you need mlr3extralearners and lightgbm package installed\n# we suggest the latest available from github\n# remotes::install_github(\"mlr-org/mlr3extralearners@*release\")\nlibrary(mlr3extralearners) \n\nset.seed(1234)\n\n# Define task\n# For consistency we use X, but lgbm can handle factors and missing data \n# and so we can use the original df_heart if desired\ntsk_boost = as_task_classif(\n    df_heart,                   # can use the 'raw' data\n    target = \"heart_disease\"\n)\n\n# Define learner\nmodel_boost = lrn(\n    \"classif.lightgbm\",\n    num_iterations = 1000,\n    max_depth = 5,\n    learning_rate = 1e-3\n)\n\n# Cross-validation\nmodel_boost_cv = resample(\n    task = tsk_boost,\n    learner = model_boost,\n    resampling = rsmp(\"cv\", folds = 5)\n)\n\n\n\nTraining Accuracy: 0.804\nGuessing: 0.539\n\n\n\n\n\nSo here we have a model that is also performing well, though not significantly better or worse than our elastic net model. For most situations, we’d expect boosting to do better, but this shows why we want a good baseline or simpler model for comparison. We’ll revisit hyperparameter tuning using this model later.\n\n\n\n\n8.6.2 Strengths & weaknesses\nRandom forests and boosting methods, though not new, are still ‘state of the art’ in terms of performance on tabular data like the type we’ve been using for our demos here. As of this writing, you’ll find that it will usually take considerable effort to beat them, though many have tried with many deep learning models.\nStrengths\n\nA single tree is highly interpretable.\nEasily incorporates features of different types (the scale of numeric features, or using categorical features*, doesn’t matter).\nTolerance to irrelevant features.\nSome tolerance to correlated inputs.\nHandling of missing values. Missing values are just another value to potentially split on5.\n\nWeaknesses\n\nHonestly few, but like all techniques, it might be relatively less predictive in certain situations. There is no free lunch.\nIt does take more effort to tune relative to linear model methods.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-dl-nn",
    "href": "ml_common_models.html#sec-ml-common-dl-nn",
    "title": "8  Common Models in Machine Learning",
    "section": "8.7 Deep Learning and Neural Networks",
    "text": "8.7 Deep Learning and Neural Networks\n\n\n\n\n\n\nFigure 8.1: A neural network\n\n\n\nDeep learning has fundamentally transformed the world of data science, and the world itself. It has been used to solve problems in image detection, speech recognition, natural language processing, and more, from assisting with cancer diagnosis to summarizing entire novels. As of now, it is not a panacea for every problem, and is not always the best tool for the job, but it is an approach that should be in your toolbox. Here we’ll provide a brief overview of the key concepts behind neural networks, the underlying approach to deep learning, and then demonstrate how to implement a simple neural network to get things started.\n\n8.7.1 What is a neural network?\nNeural networks form the basis of deep learning models. They have actually been around a while - computationally and conceptually going back decades67. Like other models, they are computational tools that help us understand how to get outputs from inputs. However, they weren’t quickly adopted due to computing limitations, similar to the slow adoption of Bayesian methods. But now neural networks, or deep learning more generally, have recently become the go-to method for many problems.\n\n\n8.7.2 How do they work?\nAt its core, a neural network can be seen as a series of matrix multiplications and other operations to produce combinations of features, and ultimately a desired output. We’ve been talking about inputs and outputs since the beginning (Section 2.3.2), but neural networks like to put a lot more in between the inputs and outputs than we’ve seen with other models. However, the core operations are often no different than what we’ve done with a basic linear model, and sometimes even simpler! But the combinations of features they produce can represent many aspects of the data that are not easily captured by simpler models.\nOne notable difference from models we’ve been seeing is that neural networks implement multiple combinations of features, where each combination is referred to as hidden nodes or units8. In a neural network, each feature has a weight, just like in a linear model. These features are multiplied by their weights and then added together. But we actually create multiple such combinations, as depicted in the ‘H’ or ‘hidden’ nodes in the following visualization.\n\n\n\n\n\n\nFigure 8.2: The first hidden layer\n\n\n\nThe next phase is where things can get more interesting. We take those hidden units and add in nonlinear transformations before moving deeper into the network. The transformations applied are typically referred to as activation functions9. So, the output of the current (typically linear) part is transformed in a way that allows the model to incorporate nonlinearities. While this might sound new, this is just like how we use link functions in generalized linear models (Section 5.2). Furthermore, these multiple combinations also allow us to incorporate interactions between features.\nBut we can go even further! We can add more layers, and more nodes in each layer, to create a deep neural network. We can also add components specific to certain types of processing, have some parts only connected to certain other parts and more. The complexity really is only limited by our imagination, and computational power! This is what helps make neural networks so powerful - given enough nodes and layers they can potentially approximate any function. Ultimately though, the feature inputs become an output or multiple outputs that can then be assessed in the similar ways as other models.\n\n\n\n\n\n\nFigure 8.3: A more complex neural network\n\n\n\n\nBefore getting carried away, let’s simplify things a bit by returning to some familiar ground. Consider a logistic regression model. There we take the linear combination of features and weights, and then apply the sigmoid function (inverse logit) to it, and that is the output of the model that we compare to our observed target.\nWe can revisit a plot we saw earlier (Figure 2.6) to make things more concrete. The input features are \\(X_1\\), \\(X_2\\), and \\(X_3\\), and the output is the probability of a positive outcome of a binary target. The weights are \\(w_1\\), \\(w_2\\), and \\(w_3\\), and the bias10 is \\(w_0\\). The hidden node is just our linear predictor which we can create via matrix multiplication of the feature matrix and weights. The sigmoid function is the activation function, and the output is the probability of the chosen label.\n\n\n\n\n\n\nFigure 8.4: A logistic regression as a neural network with one hidden layer, one hidden node, and sigmoid activation\n\n\n\nThis shows that we can actually think of logistic regression as a very simple neural network, with a linear combination of the inputs as a single hidden node and a sigmoid activation function adding the nonlinear transformation. Indeed, the earliest multilayer perceptron models were just composed of multiple layers of logistic regressions!\n\n\n\n\n\n\nGAMs and Neural Networks\n\n\n\n\n\nAs noted, you can think of neural networks as nonlinear extensions of linear models. Regression approaches like GAMs and gaussian process regression can be seen as approximations to neural networks (see also Rasmussen and Williams (2005)), bridging the gap between the simpler, and more interpretable linear model and black box of a deep neural network. This brings us back to having a good baseline. If you know some simpler tools that can approximate more complex ones, you can often get ‘good enough’ results with the simpler models.\n\n\n\n\n\n8.7.3 Trying it out\nFor simplicity we’ll use similar tools as before. Our model is a multi-layer perceptron (MLP), which is a model like the one we’ve been depicting. It consists of multiple hidden layers of varying sizes, and we can incorporate activation functions as we see fit.\nDo know this would be considered a bare minimum approach for a neural network, and generally you’d need to do more. To begin with, you’d want to tune the architecture, or structure of hidden layers. For example, you might want to try more layers, as well as ‘wider’ layers, or more nodes per layer. Also, as noted in the data discussion, we’d usually want to use embeddings for categorical features as opposed to the one-hot approach used here (Section 10.2.2)11.\nFor our example, we’ll use the data with one-hot encoded features. For our architecture, we’ll use three hidden layers with 200 nodes each. As noted, these and other settings are hyperparameters that you’d normally prefer to tune.\n\nPythonR\n\n\nFor our demonstration we’ll use sklearn’s builtin MLPClassifier. We set the learning rate to 0.001. We’ll also use a validation set of 20% of the data to help with early stopping. We set an adaptive learning rate, which is a way to automatically adjust the learning rate as the model trains. The ReLU activation function is default. We’ll also use the nesterov momentum approach, which is a way to help the model avoid local minima. We use a warm start, which allows us to train the model in stages, which is useful for early stopping. We’ll also set the validation fraction, which is the proportion of data to use for the validation set. And finally, we’ll use shuffle to randomly select observations for each batch.\n\nmodel_mlp = MLPClassifier(\n    hidden_layer_sizes = (200, 200, 200),  \n    learning_rate = 'adaptive',\n    learning_rate_init = 0.001,\n    shuffle = True,\n    random_state = 123,\n    warm_start = True,\n    nesterovs_momentum = True,\n    validation_fraction =  .2,\n    verbose = False,\n)\n\n# with the above settings, this will take a few seconds\nmodel_mlp_cv = cross_validate(\n    model_mlp, \n    X, \n    y, \n    cv = 5\n) \n\n# pd.DataFrame(model_mlp_cv) # default output\n\n\n\nTraining accuracy:  0.818 \nGuessing:  0.539\n\n\n\n\nFor R, we’ll use mlr3torch, which calls pytorch directly under the hood. We’ll use the same architecture as was done with the Python example. It uses the ReLU activation function as a default. We’ll also use adam as the optimizer, which is a popular choice and the default for the sklearn approach also. We’ll also use cross entropy as the loss function, which is the same as the log loss objective function used in logistic regression and other ML classification models. We use a batch size of 16, which is the number of observations to use for each batch of training. We’ll also use epochs of 200, which is the number of times to train on the entire dataset. We’ll also use predict type of prob, which is the type of prediction to make. Finally, we’ll use both logloss and accuracy as the metrics to track. As specified, this took over a minute.\n\nlibrary(mlr3torch)\n\nlearner_mlp = lrn(\n    \"classif.mlp\",\n    # defining network parameters\n    layers = 3,\n    d_hidden = 200,\n    # training parameters\n    batch_size = 16,\n    epochs = 50,\n    # Defining the optimizer, loss, and callbacks\n    optimizer = t_opt(\"adam\", lr = 1e-3),\n    loss = t_loss(\"cross_entropy\"),\n    # # Measures to track\n    measures_train = msrs(c(\"classif.logloss\")),\n    measures_valid = msrs(c(\"classif.logloss\", \"classif.ce\")),\n    # predict type (required by logloss)\n    predict_type = \"prob\",\n    seed = 123\n)\n\ntsk_mlp = as_task_classif(\n    x = X,\n    target = 'heart_disease'\n)\n\n# this will take a few seconds depending on your chosen settings and hardware\nmodel_mlp_cv = resample(\n    task = tsk_mlp,\n    learner = learner_mlp,\n    resampling = rsmp(\"cv\", folds = 5),\n)\n\nmodel_mlp_cv$aggregate(msr(\"classif.acc\")) # default output\n\n\n\nTraining Accuracy: 0.842\nGuessing: 0.539\n\n\n\n\n\nThis neural network model actually did pretty well, and we’re on par with our accuracy as we were with the other two models. This is somewhat surprising given the nature of the data- small number of observations with different data types- a type of situation in which neural networks don’t usually do as well as others. Just goes to show, you never know until you try!\n\n\n\n\n\n\nDeep and Wide\n\n\n\n\n\nA now relatively old question in deep learning is what is the better approach: deep networks, with more layers, or extremely wide (lots of neurons) and fewer layers? The answer is that it can depend on the problem, but in general, deep networks are more efficient and easier to train, and will generalize better. Deeper networks have the ability to build upon what the previous layers have learned, basically compartmentalizing different parts of the task to learn. More important is creating an architecture that is able to learn the appropriate aspects of the data and generalize well.\n\n\n\n\n\n8.7.4 Strengths & weaknesses\nStrengths\n\nGood prediction generally.\nIncorporates the predictive power of different combinations of inputs.\nSome tolerance to correlated inputs.\nCan be added as a component to other deep learning models.\n\nWeaknesses\n\nSusceptible to irrelevant features.\nDoesn’t outperform other methods that are (currently) easier to implement on tabular data.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-tuned-ex",
    "href": "ml_common_models.html#sec-ml-common-tuned-ex",
    "title": "8  Common Models in Machine Learning",
    "section": "8.8 A Tuned Example",
    "text": "8.8 A Tuned Example\nWe noted in the chapter on machine learning concepts that there are often multiple hyperparameters we are concerned with for a given model (Section 7.7). We had hyperparameters for each of the models in this chapter also. For the elastic net model, we might want to tune the penalty parameters and the mixing ratio. For the boosting method, we might want to tune the number of trees, the learning rate, the maximum depth of each tree, the minimum number of observations in each leaf, and the number of features to consider at each tree/split. And for the neural network, we might want to tune the number of hidden layers, the number of nodes in each layer, the learning rate, the batch size, the number of epochs, and the activation function. There is plenty to explore!\nHere is an example using the boosting model. We’ll tune the number of trees, the learning rate, the minimum number of observations in each leaf, and the maximum depth of each tree. We’ll use a randomized search across the parameter space to sample from the set of hyperparameters, rather than searching every possible combination as in a grid search. This is a good approach when you have a lot of hyperparameters to tune, and/or when you have a lot of data.\n\nPythonR\n\n\n\n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    df_heart.drop(columns='heart_disease'), \n    df_heart_num['heart_disease'],\n    test_size = 0.2,\n    random_state = 42\n)\n\nmodel_boost = LGBMClassifier(\n    verbose = -1\n)\n\nparam_grid = {\n    'n_estimators': [500, 1000],\n    'learning_rate': [1e-3, 1e-2, 1e-1],\n    'max_depth': [3, 5, 7, 9],\n    'min_child_samples': [1, 5, 10],\n}\n\n# this will take a few seconds\nmodel_boost_cv_tune = RandomizedSearchCV(\n    model_boost, \n    param_grid, \n    n_iter = 10,\n    cv = 5, \n    scoring = 'accuracy', \n    n_jobs = -1,\n    random_state = 42\n)\n\nmodel_boost_cv_tune.fit(X_train, y_train)\n\ntest_predictions = model_boost_cv_tune.predict(X_test)\naccuracy_score(y_test, test_predictions)\n\n\n\n\nTest Accuracy 0.817 \nGuessing:  0.539\n\n\n\n\n\nset.seed(1234)\n\nlibrary(mlr3verse)\nlibrary(rsample)\n\ntsk_model_boost_cv_tune = as_task_classif(\n    df_heart,\n    target = \"heart_disease\"\n)\n\nsplit = partition(tsk_model_boost_cv_tune, ratio = .8)\n\nlrn_lgbm = lrn(\n    \"classif.lightgbm\",\n    num_iterations = to_tune(c(500, 1000)),\n    learning_rate = to_tune(1e-3, 1e-1),\n    max_depth = to_tune(c(3, 5, 7, 9)),\n    min_data_in_leaf = to_tune(c(1, 5, 10))\n)\n\nmodel_boost_cv_tune = auto_tuner(\n    tuner = tnr(\"random_search\"),\n    learner = lrn_lgbm,\n    resampling = rsmp(\"cv\", folds = 5),\n    measure = msr(\"classif.acc\"),\n    terminator = trm(\"evals\", n_evals = 10)\n)\n\nmodel_boost_cv_tune$train(tsk_model_boost_cv_tune, row_ids = split$train)\nmodel_boost_cv_tune$predict(tsk_model_boost_cv_tune, row_ids = split$test)$score(msr(\"classif.acc\"))\n\n\n\nTest Accuracy: 0.864\nGuessing: 0.539\n\n\n\n\n\nLooks like we’ve done a lot better than guessing. Even if we don’t do better than our previous model, we should feel better that we’ve done our due diligence in trying to find the best set of underlying parameters, rather than just going with defaults or what seems to work best.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-compare",
    "href": "ml_common_models.html#sec-ml-common-compare",
    "title": "8  Common Models in Machine Learning",
    "section": "8.9 Comparing Models",
    "text": "8.9 Comparing Models\nWe can tune all the models and compare them head to head. We first split the same data into training and test sets (20% test). Then with training data, we tuned each model over different settings:\n\nElastic net: penalty and mixing ratio\nBoosting: number of trees, learning rate, and maximum depth, etc.\nNeural network: number of hidden layers, number of nodes in each layer, etc.\n\nAfter this, we used the tuned values to retrain on the complete training data set. At this stage it’s not necessary to investigate in most settings, but we show the results of the 10-fold cross-validation for the already-tuned models, to give a sense of the uncertainty in error estimation with a small sample like this. Even with the ‘best’ settings, we can see that there is definitely some variability across data splits.\n\n\n\n\n\n\n\n\nFigure 8.5: Cross-validation results for tuned models.\n\n\n\n\n\nWhen we look at the performance on the holdout set with our tuned models in the following table12, we see something you might be surprised about - the simplest model does really well! It is tied for or the best on three out of six metrics. Again, your results may vary depending on whether you used a seed, R vs. Python, and possibly other aspects of your environment.\n\n\n\nTable 8.2\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.3: Metrics for tuned models on holdout data.\n\n\n\n\n\n\n  \n    \n      model\n      acc\n      tpr\n      tnr\n      f1\n      ppv\n      npv\n    \n  \n  \n    Elastic Net\n0.88\n0.83\n0.92\n0.85\n0.87\n0.89\n    Boost\n0.87\n0.83\n0.89\n0.83\n0.83\n0.89\n    MLP\n0.83\n0.83\n0.83\n0.80\n0.77\n0.88\n  \n  \n  \n\n\n\n\n\n\n\nIt’s important to note that none of these results are statistically different from each other. As an example, the elastic net model had an accuracy of 0.88, but the interval estimate for such a small holdout sample is very wide - from 0.77 to 0.95. The interval estimate for the difference in TPR between the elastic net and boosting models is from -0.21 to 0.2113. Again, don’t take this result too far, we’re dealing with a small data set and it is difficult to detect potentially complex relationships in such a setting. In addition, we could have done more to explore the parameter space of the models, but we’ll leave that for another time. But this was a good example of the importance of having an adequate baseline, and where complexity didn’t really help much, though all our approaches did well. In this case, any of these models would be a good choice for future prediction.\n\n\n\n\n\n\nTest metrics better than training?\n\n\n\n\n\nSome may wonder how the holdout results can be better than the cross-validation results, as they are for the elastic net model. This can definitely happen, and at least in this case probably just reflects the small sample size. The holdout set is a random sample of 20% of the complete data, which is 238 examples. Just a couple different predictions could result in a several percentage point difference in accuracy. Also, this could happen just by chance. In general though, you’d expect the holdout results to be a bit, or even significantly, worse than the cross-validation results.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-interpret",
    "href": "ml_common_models.html#sec-ml-common-interpret",
    "title": "8  Common Models in Machine Learning",
    "section": "8.10 Interpretation",
    "text": "8.10 Interpretation\nWhen it comes to machine learning, many models we use don’t have an easy interpretation, like with coefficients in a linear model. However, that doesn’t mean we can’t still figure out what’s going on. Let’s use the boosting model as an example.\n\n8.10.1 Feature Importance\nThe default importance metric for a lightgbm model is the number of splits in which a feature is used across trees, and this will depend a lot on the chosen parameters of the best model. But there are other ways to think about what importance means that will be specific to a model, data setting, and ultimate goal of the modeling process. For this data and the model, depending on the settings, you might see that the most important features are age, cholesterol, and max heart rate.\n\nPythonR\n\n\n\n# Get feature importances\nbest_model = model_boost_cv_tune.best_estimator_\nbest_model.feature_importances_\n\n# you remember which feature is which, right? if not, do this:\npd.DataFrame({\n    'Feature': best_model.feature_name_,\n    'Importance': best_model.feature_importances_\n}).sort_values('Importance', ascending=False)\n\n\n\nR shows the proportion of splits in which a feature is used across trees rather than the raw number.\n\n# Get feature importances\nmodel_boost_cv_tune$learner$importance()\n\n\n\n\n\n\n\n\nTable 8.4: Top 4 features from a tuned LGBM model.\n\n\n\n\n\n\n  \n    \n      Feature\n      value\n    \n  \n  \n    num_major_vessels\n0.23\n    age\n0.16\n    thalassemia\n0.10\n    st_depression\n0.10\n  \n  \n  \n\n\n\n\n\n\n\nNow let’s think about a visual display to aid our understanding. Here we show a partial dependence plot (Section 3.3.6) to see the effects of cholesterol and being male. From this we can see that males are expected to have a higher probability of heart disease, and that cholesterol has a positive relationship with heart disease, though this occurs mostly after midpoint for cholesterol (shown by vertical line). The plot shown is a prettier version of what you’d get with the following code, but the model predictions are the same.\n\nPythonR\n\n\n\nPartialDependenceDisplay.from_estimator(\n    model_boost_cv_tune, \n    df_heart.drop(columns='heart_disease'), \n    features=['cholesterol', 'male'], \n    categorical_features=['male'], \n    percentiles=(0, .9),\n    grid_resolution=75\n)\n\n\n\nFor R we’ll use the iml package.\n\nlibrary(iml)\n\nprediction = Predictor$new(\n    model_boost_cv_tune$model$learner,\n    data = df_heart,\n    type = 'prob', \n    class = 'yes'\n)\n\n# interaction plot, select a singe feature for a single feature plot\neffect_dat = FeatureEffect$new(\n    prediction, \n    feature = c('cholesterol', 'male'),\n    method = \"pdp\", \n)\n\neffect_dat$plot(show.data = TRUE)\n\n\n\n\n\n\n\n\n\n\nFigure 8.6: Partial dependence plot for cholesterol",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-other-models",
    "href": "ml_common_models.html#sec-ml-common-other-models",
    "title": "8  Common Models in Machine Learning",
    "section": "8.11 Other ML Models for Tabular Data",
    "text": "8.11 Other ML Models for Tabular Data\nWhen you research classical machine learning models for the kind of data we’ve been exploring, you’ll find a variety of methods. Popular approaches from the past include k-nearest neighbors regression, principal components regression, support vector machines (SVM), and more. You don’t see these used in practice as much though for several reasons:\n\nSome, like k-nearest neighbors regression, generally don’t predict as well as other models.\nOthers, like linear discriminant analysis, make strong assumptions about how the data is distributed.\nSome models, like SVM, tend to work well only with ‘clean’ and well-structured data of the same type.\nMany of these models are computationally demanding, making them less practical for large datasets.\nLastly, some of these models are less interpretable, making it hard to understand their predictions without an obvious gain in performance.\n\nWhile some of these classical models might still work well in unique situations, when you have tools that can handle a lot of data complexity and predict very well (and usually better) like tree-based methods, there’s not much reason to use the historical alternatives. If you’re interested in learning more about them or think one of them is just ‘neat’, you could potentially use it as a baseline model. Alternatively, you could maybe employ them as part of an ensemble or stacked model, where you combine the predictions of multiple models to produce a single prediction. This is a common approach in machine learning, and is often used in Kaggle competitions.\nThere are also other methods that are more specialized, such as those for text, image, and audio data. We will provide an overview of these elsewhere (Chapter 9). As of this writing, the main research effort for new models for tabular data regards deep learning methods like large language models (LLMs). While typically used for text data, they can be adapted for tabular data as well. They are very powerful, but also computationally expensive. The issue is primarily whether a model can be devised that can consistently beat boosting and other approaches, and while it hasn’t happened yet, there is a good chance it will in the near future. For now, the best approach is to use the best model that works for your data, and to be open to new methods as they come along.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-wrap",
    "href": "ml_common_models.html#sec-ml-common-wrap",
    "title": "8  Common Models in Machine Learning",
    "section": "8.12 Wrapping Up",
    "text": "8.12 Wrapping Up\nIn this chapter we’ve provided a few common and successful models you can implement with much success in machine learning. You don’t really need much beyond these for tabular data unless your unique data condition somehow requires it. But a couple things are worth mentioning before moving on…\n\nFeature engineering will typically pay off more in performance than the model choice.\n\n\nThinking hard about the problem and the data is more important than the model choice.\n\n\nThe best model is simply the one that works best for your situation.\n\nYou’ll always get more payoff by coming up with better features to use in the model, as well as just using better data that’s been ‘fixed’ because you’ve done some good exploratory data analysis. Thinking harder about the problem means you will waste less time going down dead ends, and you typically can find better data to use to solve the problem by thinking more clearly about the question at hand. And finally, it’s good to not be stuck on one model, and be willing to use whatever it takes to get things done efficiently.\n\n8.12.1 The common thread\nWhen it comes to machine learning, you can use any model you feel like, and this could be standard statistical models like we’ve covered elsewhere. Both boosting and neural networks, like GAMs and related techniques, can be put under a common heading of basis function models. GAMs with certain types of smooth functions are approximations of gaussian processes, and gaussian processes are equivalent to a neural network with an infinitely wide hidden layer (Neal (1996)). Even the most complicated deep learning model typically has components that involve feature combinations and transformations that we use in far simpler models.\n\n\n8.12.2 Choose your own adventure\nIf you haven’t had much exposure to statistical approaches we suggest heading to any chapter of Part I. Otherwise, consider an overview of more machine learning techniques (Chapter 9), data (Chapter 10), or causal modeling (Chapter 11).\n\n\n8.12.3 Additional resources\nAdditional resources include those mentioned in Section 7.9.3, but here are some more to consider:\n\nGoogle’s Course Decision Forests\nInterpretable ML (Molnar (2023))\nInterpretable Machine Learning with Python (Masis (2023))\nMachine Learning Q & AI (Raschka (2023))\n\nDeep Learning:\n\nCommon activation functions\nAn overview of deep learning applications for tabular data by Michael (Clark (2021), Clark (2022))\nDive into Deep Learning (Zhang et al. (2023))\nFast AI course (Howard (2024))",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#sec-ml-common-exercise",
    "href": "ml_common_models.html#sec-ml-common-exercise",
    "title": "8  Common Models in Machine Learning",
    "section": "8.13 Exercise",
    "text": "8.13 Exercise\nTune a model of your choice to predict whether a movie is good or bad with the movie review data. Use the categorical target, and use one-hot encoded features if needed. Make sure you use a good baseline model for comparison!\n\n\n\n\nClark, Michael. 2021. “This Is Definitely Not All You Need,” July. https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/.\n\n\n———. 2022. “Deep Learning for Tabular Data,” May. https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/.\n\n\nHoward, Jeremy. 2024. “Practical Deep Learning for Coders - Practical Deep Learning.” Practical Deep Learning for Coders. https://course.fast.ai/.\n\n\nMasis, Serg. 2023. “Interpretable Machine Learning with Python - Second Edition.” Packt. https://www.packtpub.com/product/interpretable-machine-learning-with-python-second-edition/9781803235424.\n\n\nMcCulloch, Warren S., and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” The Bulletin of Mathematical Biophysics 5 (4): 115–33. https://doi.org/10.1007/BF02478259.\n\n\nMolnar, Christoph. 2023. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.\n\n\nNeal, Radford M. 1996. “Priors for Infinite Networks.” In Bayesian Learning for Neural Networks, edited by Radford M. Neal, 29–53. New York, NY: Springer. https://doi.org/10.1007/978-1-4612-0745-0_2.\n\n\nRaschka, Sebastian. 2023. Machine Learning Q and AI. https://nostarch.com/machine-learning-q-and-ai.\n\n\nRasmussen, Carl Edward, and Christopher K. I. Williams. 2005. Gaussian Processes for Machine Learning. The MIT Press. https://doi.org/10.7551/mitpress/3206.001.0001.\n\n\nZhang, Aston, Zack Lipton, Mu Li, and Alex Smola. 2023. “Dive into Deep Learning — Dive into Deep Learning 1.0.3 Documentation.” https://d2l.ai/index.html.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_common_models.html#footnotes",
    "href": "ml_common_models.html#footnotes",
    "title": "8  Common Models in Machine Learning",
    "section": "",
    "text": "There would be far less hype and wasted time if those in ML and DL research simply did this rather than just reporting the chosen metric of their model ‘winning’ against other models. It’s not that hard to do, yet most do not provide any ranged estimate for their metric, let alone test statistical differences from other models. You don’t even have to bootstrap many common metric estimates for binary classification since they are just proportions. It’d also be nice if they used a more meaningful baseline than logistic regression, but that’s a different story. And one more thing, ranks also have uncertainty. So just saying you’re #1 isn’t enough to prove it.↩︎\nA single regression/classification tree actually could serve as a decent baseline model, especially given the interpretability, and modern methods try to make them more stable.↩︎\nThis is pretty much the same concept as with stochastic gradient in general. Larger learning rates allow for quicker parameter exploration, but may overshoot the optimal value. Smaller learning rates are more conservative, but may take longer to find the optimal value.↩︎\nSome also prefer catboost. The authors have not actually been able to practically implement catboost in a setting where it was more predictive or as efficient/speedy as xgboost or lightgbm, but some have had notable success with it.↩︎\nIt’s not clear why most model functions still have no default for this sort of thing in 2024.↩︎\nMost consider the scientific origin with McCulloch and Pitts (1943).↩︎\nOn the conceptual side, they served as a rudimentary model of neuronal functioning in the brain, and a way to understand how the brain processes information. The models sprung from the cognitive revolution, a backlash against the behaviorist approach to psychology, and used the computer as a metaphor for how the brain might operate.↩︎\nThe term ‘hidden’ is used because these nodes are between the input or output. It does not imply a latent/hidden variable in the sense it is used in structural equation or measurement models, but there is a lot of common ground. See the connection with principal components analysis for example(Section 9.2.1.2).↩︎\nWe have multiple options for our activation functions, and probably the most common activation function in deep learning is the rectified linear unit or ReLU. Other commonly used are the sigmoid function, which is exactly the same as what we used in logistic regression, the hyperbolic tangent function, variants of the ReLU, and of course the linear/identity function, which not to do any transformation at all.↩︎\nIt’s not exactly clear why computer scientists chose to call this the bias, but it’s the same as the intercept in a linear model, or conceptually as an offset or constant. It has nothing to do with the word bias as used in every other modeling context.↩︎\nA really good tool for a standard MLP type approach with automatic categorical embeddings is fastai’s tabular learner.↩︎\nThis table was based on Python randomized CV search, but a similar R approach produced similar results, and they can both vary quite a bit even with just a random seed change.↩︎\nWe just used the prop.test function in R for these values with the key question of whether these proportions are different. A lot of the metrics people look at from confusion matrices are proportions.↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Common Models in Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html",
    "href": "ml_more.html",
    "title": "9  More Machine Learning",
    "section": "",
    "text": "9.1 Key Ideas\nAs we wrap up our discussion on machine learning, here are some things to keep in mind:",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-key",
    "href": "ml_more.html#sec-ml-more-key",
    "title": "9  More Machine Learning",
    "section": "",
    "text": "ML can be applied to virtually any modeling or data domain\nOther widely used areas and applications of ML include unsupervised learning, reinforcement learning, computer vision, natural language processing, and more generally, artificial intelligence.\nWhile tabular data has traditionally been the primary format for modeling, the landscape has changed dramatically, and you may need to incorporate other data to reach your modeling goals.\n\n\n9.1.1 Why this matters\nIt’s very important to know just how unlimited the modeling universe is, but also to recognize the common thread that connects all models. Even when we get into other data situations and complex models, we can always fall back on the core approaches we’ve already seen and know well at this point, and know that those ideas can potentially be applied in any modeling situation.\n\n\n9.1.2 Good to know\nFor the content in this chapter, a basic idea of modeling and machine learning would probably be enough. We’re not going to get too technical in this section.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-unsuper",
    "href": "ml_more.html#sec-ml-more-unsuper",
    "title": "9  More Machine Learning",
    "section": "9.2 Unsupervised Learning",
    "text": "9.2 Unsupervised Learning\nAll the models considered thus far would fall under the name of supervised learning. That is, we have a target variable that we are trying to predict with various features, and we use the data to train a model to predict the target. However, there are settings in which we do not have a target variable, or we do not have a target variable for all of the data. In these cases, we can still use what’s often referred to as unsupervised learning to learn about the data. Unsupervised learning is a type of machine learning that involves training a model without an explicit target variable in the sense that we’ve seen. But to be clear, a model and target is still definitely there! Unsupervised learning attempts learn patterns in the data in a general sense, and can be used in a wide range of applications, including clustering, anomaly detection, and dimensionality reduction. Although much of data science instruction seems to treat these as fundamentally different modeling approaches, just like much of what we’ve seen, it’s probably best to think of these as different flavors of a more general approach.\nTraditionally, one of the more common applications of unsupervised learning falls under the heading of dimension reduction, or data compression, such that we reduce features to a smaller latent, or hidden, or unobserved, subset that accounts for most of the (co-)variance of the larger set. Alternatively, we reduce the rows to a small number of hidden, or unobserved, clusters. For example, we start with 100 features and reduce them to 10 features that still account for most of what’s important in the original set, or we classify each observation as belonging to 2-3 clusters. Either way, the primary goal is to reduce the dimensionality of the data, not predict an explicit target.\n\n\n\n\n\n\n\n\nFigure 9.1: Two Variables with Three Overlapping Clusters\n\n\n\n\n\nClassical methods in this domain include principal components analysis (PCA), singular value decomposition (SVD), and factor analysis, which are geared toward reducing column dimensions, as well as cluster methods such as k-means and hierarchical clustering for reducing observations into clusters. Sometimes these methods are often used as preprocessing steps for supervised learning problems, or as a part of exploratory data analysis, but often they are an end in themselves. Most of us are familiar with recommender systems, whether via Netflix or Amazon, which suggest products or movies, and we’re all now becoming extremely familiar with text analysis methods via chatbots and similar. While the underlying models are notably more complex these days, they actually just started off as SVD (recommender systems) or a form of factor analysis (text analysis via latent semantic analysis/latent dirichlet allocation). Having a conceptual understanding of the simpler methods can aid in understanding the more complex ones.\n\n\n\n\n\n\nDimension Reduction in Preprocessing\n\n\n\n\n\n In general, you probably should not use a dimension reduction technique as a preprocessing step for a supervised learning problem. Instead, use a modeling approach that can handle high-dimensional data, has a built-in way to reduce features (e.g., lasso, boosting, dropout), or use a dimension reduction technique that is specifically designed for supervised learning (e.g., partial least squares). Creating a reduced set of features without any connection to the target target will generally be suboptimal for the supervised learning problem.\n\n\n\n\n9.2.1 Connections\n\n9.2.1.1 Clusters are categorical latent features\nIn both clustering rows and reducing columns, we’re essentially reducing the dimension of the features. For methods like PCA and factor analysis, we’re explicitly reducing the number of data columns to a smaller set of numeric features. For example, we might take answers to responses to dozens of questions from a personality inventory, and reduce them to five key features that represent general aspects of personality. These new features are on their own scale, often standardized, but they still reflect at least some of the original items’ variability 1.\nNow, imagine if we reduced the features to a single categorical variable. Now you have cluster analysis! You can discretize any continuous feature to a coarse couple of categories, and this goes for latent variables as well as those we actually observe in our data. For example, if we do a factor analysis with one latent feature, we could either convert it to a probability of some class with an appropriate transformation, or just say that scores higher than some cutoff are in cluster A and the others are in cluster B. Indeed, there is a whole class of clustering models called mixture models that do just that- they estimate the latent probability of class membership. Many of these approaches are conceptually similar or even identical, and the primary difference is how we think about and interpret the results.\n\n\n9.2.1.2 PCA as a neural network\nConsider the following neural network, called an autoencoder. Its job is to shrink the features down to a smaller, simpler representation, and then rebuild it from the compressed state to an output that matches the original as closely as possible. It’s trained by minimizing the error between the original data and the reconstructed data. The autoencoder is a special case of a neural network used as a component of many larger architectures such as those seen with large language models, but can be used for dimension reduction in and of itself if we are specifically interested in the compression layer, sometimes called a bottleneck.\n\n\n\n\n\n\nFigure 9.2: PCA or Autoencoder\n\n\n\nConsider the following setup for such a situation:\n\nSingle hidden layer\nNumber of hidden nodes = number of inputs\nLinear activation function\n\nAn autoencoder in this case would be equivalent to principal components analysis. In the approach described, PCA perfectly reconstructs the original data when considering all components, and so the error would be zero. But that doesn’t give us any dimension reduction, we still have as many nodes in the compression layer as we did inputs. So with PCA we often only retain a small number of components that capture the data variance by some arbitrary amount. The discarded nodes are actually still estimated though.\nNeural networks however are not bound to linear activation functions, the size of the inputs or even a single layer, and so they provide a much more flexible approach that can compress the data at a certain layer, but still have very good reconstruction error. Typical autoencoders would have multiple layers with notably more nodes than inputs. It’s not as easily interpretable as typical factor analytic techniques, and we still have to sort out the architecture. However, it’s a good example of how the same underlying approach can be used for different purposes.\n\n\n\n\n\n\n\nFigure 9.3: Conceptual Diagram of an Autoencoder\n\n\n\n\n\n\n\n\n\n\n\nAutoencoders and LLMs\n\n\n\n\n\nAutoencoders are special cases of encoder-decoder models, which are used in many applications, including machine translation, image captioning, and more. Autoencoders have the same inputs and outputs, but in other scenarios, a similar type of architecture might be used to classify or generate text, as with large language models.\n\n\n\n\n\n9.2.1.3 Latent linear models\nAnother thing to be aware of is that some dimension reduction techniques can be thought of as latent linear models. The following depicts factor analysis as a latent linear model. The ‘targets’ are the observed features, and we predict each one by some linear combination of latent variables.\n\\[\n\\begin{aligned}\nx_1 &= \\beta_{11} h_1 + \\beta_{12} h_2 + \\beta_{13} h_3 + \\beta_{14} h_4 + \\epsilon_1 \\\\\nx_2 &= \\beta_{21} h_1 + \\beta_{22} h_2 + \\beta_{23} h_3 + \\beta_{24} h_4 + \\epsilon_2 \\\\\nx_3 &= \\beta_{31} h_1 + \\beta_{32} h_2 + \\beta_{33} h_3 + \\beta_{34} h_4 + \\epsilon_3 \\\\\n\\end{aligned}\n\\]\nIn this scenario, the \\(h\\) are estimated latent variables, and \\(\\beta\\) are the coefficients, which in some contexts are called loadings. The \\(\\epsilon\\) are the residuals, which are assumed to be independent and normally distributed as with a standard linear model. The \\(\\beta\\) are usually estimated by maximum likelihood. The latent variables are not observed, but are to be estimated as part of the modeling process2, and typically standardized with mean 0 and standard deviation of 1. The number of latent variables we use is a hyperparameter in the ML sense, and so can be determined by the usual means3. To tie some more common models together:\n\nPCA is a factor analysis with no (residual) variance, and the latent variables are orthogonal (independent).\nProbabilistic PCA is a factor analysis with constant residual variance.\nFactor analysis is a factor analysis with varying residual variance.\nIndependent component analysis is a factor analysis that does not assume an underlying gaussian data generating process.\nNon-negative matrix factorization and latent dirichlet allocation are factor analyses applied to counts (think poisson and multinomial regression).\n\n\n\n\n9.2.2 Other unsupervised learning techniques\nThere are several techniques that are used to visualize high-dimensional data in simpler ways, such as multidimensional scaling, t-SNE, and (H)DBSCAN. These are often used as a part of exploratory data analysis to identify groups.\nCluster analysis is a method with a long history and many different approaches, including hierarchical clustering algorithms (agglomerative, divisive), k-means, and more. Distance matrices are often the first step for these clustering approaches, and there are many ways to calculate distances between observations. Conversely, some methods use adjacency matrices, which focus on similarity of observations rather than differences (like correlations), are can be used for graph-based approaches to find hidden clusters.\nAnomaly/outlier detection is an approach for finding ‘unusual’ data points, or otherwise small, atypical clusters. This is often done by looking for data points that are far from the rest of the data, or that are not well explained by the model. This approach is often used for situations like fraud detection or network intrusion detection. Standard clustering or modeling techniques might be used to identify outliers, or specialized techniques might be used.\n\n\n\n\n\n\nFigure 9.4: Network Graph\n\n\n\nNetwork analysis is a type of unsupervised learning that involves analyzing the relationships between entities. It is a graph-based approach that involves identifying nodes (e.g., people) and edges (e.g., do they know each other?) in a network. It is used in a wide range of applications, like identifying communities within a network, or to see how they evolve over time. It is also used to identify relationships between entities, such as people, products, or documents. One might be interested in such things as which nodes that have the most connections, or the general ‘connectedness’ of a network. Network analysis or similar graphical models typically have their own clustering techniques that are based on the edge (connection) weights between individuals, such as modularity, or the number of edges between individuals, such as k-clique.\nIn summary, there are many methods that fall under the umbrella of unsupervised learning, but even when you don’t think you have an explicit target variable, you can still understand or frame these as models in a similar way to help you understand the data. It’s important to not get hung up on trying to distinguish modeling approaches with somewhat arbitrary labels, and focus more on what their modeling goal is and how best to achieve it!\n\n\n\n\n\n\nGenerative vs. Discriminative Models\n\n\n\n\n\nIn general, many unsupervised learning and many deep learning techniques involved in computer vision and natural language processing are often thought of as generative models. These attempt to model the underlying data generating process, i.e. the features, but possibly a target variable also. In contrast, most supervised learning models are often thought of as discriminative models, that try to model the conditional distribution of the target given the features only.\nThese labels are a bit problematic though. Any probabilistic model can be used to generate data, even if it is only for the target, so calling a model generative isn’t all that clarifying. And models that might be thought of as discriminative in a machine learning context might not be in others (e.g. Bayesian).",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-reinforcement",
    "href": "ml_more.html#sec-ml-more-reinforcement",
    "title": "9  More Machine Learning",
    "section": "9.3 Reinforcement Learning",
    "text": "9.3 Reinforcement Learning\n\n\n\nReinforcement Learning\n\n\nReinforcement learning (RL) is a type of modeling approach that involves training an agent to make decisions in an environment. The agent receives feedback in the form of rewards or punishments for its actions, and the goal is to maximize its rewards over time by learning which actions lead to positive or negative outcomes. Typical data involves a sequence of states, actions, and rewards, and the agent learns a policy that maps states to actions. The agent learns by interacting with the environment, and the environment changes based on the agent’s actions.\nThe agent’s goal is to learn a policy, which is a set of rules that dictate which actions to take in different situations. The agent learns by trial and error, adjusting its policy based on the feedback it receives from the environment. The classic example is a game like chess or a simple video game. In these scenarios, The agent learns which moves (actions) lead to winning the game (positive reward) and which moves lead to losing the game (negative reward). Over time, the agent improves its policy to make better moves that increase its chances of winning.\nOne of the key challenges in reinforcement learning is balancing exploration and exploitation. Exploration is about trying new actions that could lead to higher rewards, while exploitation is about sticking to the actions that have already been found to give good rewards.\nReinforcement learning has many applications, including robotics, games, and autonomous driving, but there is little restriction on where it might be applied. It is often a key part of some deep learning models, where reinforcement is supplied via human feedback or other means to an otherwise automatic modeling process. In general, RL is a powerful tool that might be useful where traditional programming approaches may not be as feasible.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-non-tabular",
    "href": "ml_more.html#sec-ml-more-non-tabular",
    "title": "9  More Machine Learning",
    "section": "9.4 Working with Specialized Data Types",
    "text": "9.4 Working with Specialized Data Types\n\nWhile our focus in this book is on tabular data due to its ubiquity, there are many other types of data that can be used for machine learning and modeling in general, some of which can still potentially be used in that manner, but which often start as a different format or must be considered in a special way. You’ll often hear this labeled as ‘unstructured’, but that’s probably not the best conceptual way to think about it, as the data is still structured in some way, sometimes in a strict format (e.g. images). Here we’ll briefly discuss some of the other types of data you’ll potentially come across.\n\n9.4.1 Spatial\n\n\n\nSpatial Data (code available from Kyle Walker)\n\n\nSpatial data, which includes geographic and similar information, can be quite complex. It often comes in specific formats (e.g. shapefiles), and may require specialized tools to work with it. Spatial specific features may include continuous variables like latitude and longitude, or tracking data from a device like a smartwatch. Other spatial features are more discrete, such as states or political regions within a country.\nIn general, we could use these features as we would others in the tabular setting, but we often want to take into account the uniqueness of a particular region, or the correlation of spatial regions. Historically, most spatial data can be incorporated into approaches like mixed models or generalized additive models, but in certain applications, such as satellite imagery, deep learning models are more the norm, and the models often transition into image processing techniques.\n\n\n9.4.2 Audio\n\n\n\nSound wave\n\n\nAudio data is a type of time series data that is also the focus for many modeling applications. Think of the sound of someone speaking or music playing, as it changes over time. Such data is often represented as a waveform, which is a plot of the amplitude of the sound wave over time.\nThe goal of modeling audio data may include speech recognition, language translation, music generation, and more. Like spatial data, audio data is typically stored in specific formats and can be quite large. Also like spatial data, the specific type of data and research question may allow for a tabular format. In that case the modeling approaches used are similar to those for other time series data.\nDeep learning methods have proven very effective for analyzing audio data, and can even create songs people actually like, even recently helping the Beatles to release one more song. Nowadays, you can generate an entire song in any genre you want, just by typing a text prompt!\n\n\n9.4.3 Computer vision\n\n\n\n\n\n\n\n\n\nFigure 9.5: Convolutional Neural Network\n\n\n\nComputer vision involves a range of models and techniques for analyzing and interpreting image-based data. It includes tasks like image classification (labeling an image), object detection (finding the location of objects are in an image), image segmentation (identifying the boundaries of objects in an image), and object tracking (following objects as they move over time).\nIn general, your raw data is an image, which is represented as a matrix of pixel values. For example, each row of the matrix could be a grayscale value for a pixel, or it could be an array of Red, Green, and Blue (RGB) values for each pixel. The modeling goal is to extract features from the image that can be used for the task at hand. For example, you might extract features such as color, texture, and shape. You can then use these features to train a model to classify images or whatever your task may be.\nImage processing is a broad field with many applications. It is used in medical imaging, satellite imagery, self-driving cars, and more. And while it can be really fun to classify objects such as cats and dogs, or generate images from text and vice versa, it can be quite challenging due to the size of the data, issues specific to video/image quality, and the model complexity. Even if your base data is often the same or very similar, the model architecture and training process can vary widely depending on the task at hand.\nThese days we generally don’t have to start from scratch though, there are pretrained models that can be used for image processing tasks, which you can then fine-tune for your specific task. These models are often based on convolutional neural networks (CNNs), which are a type of deep learning model. CNNs are designed to take advantage of the spatial structure of images, and they use a series of convolutional layers to extract features from the image. These features are then passed through a series of fully connected layers to make a prediction. CNNs have been used to achieve state-of-the-art results on a wide range of image processing tasks, and are the standard for many image processing applications. More recently, diffusion models, which seek to reconstruct images after successively adding noise to the initial input, have been shown to be quite effective for a wide range of tasks involving image generation.\n\n\n9.4.4 Natural language processing\n\n\n\n\nPartial GPT4 output from a prompt: Write a very brief short story about using models in data science. It should reflect the style of Donald Barthelme.\n\n\nOne of the hottest areas of modeling development in recent times regards natural language processing, as evidenced by the runaway success of models like ChatGPT. Natural language processing (NLP) is a field of study that focuses on understanding human language, and along with computer vision, is a very visible subfield of artificial intelligence. NLP is used in a wide range of applications, including machine translation, speech recognition, text classification, and more. NLP is behind some of the most exciting modeling applications today, with tools that continue to amaze with their capabilities to generate summaries of articles, answering questions, write code, and even pass the bar exam with flying colors!\nEarly efforts in this field were based on statistical models, and then variations on things like PCA, but it took a lot of data pre-processing work to get much from those approaches, and results could still be unsatisfactory. More recently, deep learning models became the standard application, and there is no looking back in that regard. Current state of the art models have been trained on massive amounts of data, even much of the internet, and can be used for a wide range of tasks. But you don’t have to train such a model yourself- now you can simply use a pretrained model like GPT-4 for your own tasks. In some cases much of the trouble comes with generating the best prompt to produce the desired results. However, the field and the models are evolving extremely rapidly, and things are getting easier all the time. In the meantime, feel free to just play around with ChatGPT yourself.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-pretrained",
    "href": "ml_more.html#sec-ml-more-pretrained",
    "title": "9  More Machine Learning",
    "section": "9.5 Pretrained Models & Transfer Learning",
    "text": "9.5 Pretrained Models & Transfer Learning\nPretrained models are those that have been trained on a large amount of data, and can be used for a wide range of tasks, even on data they were not trained on. They are widely employed in image and natural language processing. The basic idea is that if you can use a model that was trained on the entire internet of text, why start from scratch? Image processing models already understand things like edges and colors, so there is little need to reinvent the wheel when you know those features would be useful for your own task. These are viable in tasks where the inputs are similar to the data the model was trained on, as is the case with images and text.\nYou can use a pretrained model as a starting point for your own model, and then fine-tune it for your specific task, and this is more generally called transfer learning. The gist is that you only need to train part of the model on your specific data, or possibly even not at all. You can just feed your data in and get predictions from the ready-to-go model! This obviously can save a lot of time and resources, assuming you don’t have to pay much to use the model in the first place, and can be especially useful when you don’t have a lot of data to train your model on.\n\n9.5.1 Self-supervised learning\nSelf-supervised learning is a type of machine learning technique that involves training a model on a task that can be generated from the data itself. In this case there is no labeled data unlike the standard pre-trained setting. For example, you might train a model to predict the next word in a sentence. The idea is that the model learns to extract (represent) useful features from the data by trying to predict the missing information, which is imposed via a mask and may change from sample to sample. We also may use a different type of objective function, such as minimizing the distance between the embeddings for similar inputs and maximizing the distance between embeddings for dissimilar inputs.\nThis can be a useful approach when you don’t have labeled data, or just when you don’t have a lot of labeled data. Once trained, it can be used as other pre-trained models to predict other unlabeled data. Self-supervised learning is often used in natural language processing, but can be applied to other types of data as well.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-combine",
    "href": "ml_more.html#sec-ml-more-combine",
    "title": "9  More Machine Learning",
    "section": "9.6 Combining Models",
    "text": "9.6 Combining Models\nIt’s also important to note that these types of data and their associated models are not mutually exclusive. For example, you might have a video that contains both audio and visual information pertinent to the task. Or you might want to produce images from text inputs. In these cases, you can use a combination of models to extract features from the data, which may just be more features in a tabular format, or be as complex as a multimodal deep learning architecture.\nMany computer vision, audio, natural language and other modeling approaches incorporate transformers. They are based on the idea of attention, which is a mechanism that allows the model to focus on certain parts of the input sequence and less on others. Transformers are used in many state-of-the-art models with different data types, such as those that combine text and images. The transformer architecture, although complex, underpins many of today’s most sophisticated models, so is worth being aware of even if it isn’t your data domain.\nAs an example, we added a transformer-based approach to process the reviews in the movie review data set used in other chapters. We kept to the same basic data setup, and we ended up with notably better performance than the other models demonstrated, pushing toward 90% accuracy on test, even without fiddling too much with many hyper parameters. It’s a good example of a case where we have standard tabular data, but need to deal with additional data structure in a different way. By combining the approaches to obtain a final output for prediction, we obtained better results than we would with a single model. This won’t always be the case, but keep it in mind when you are dealing with different data sources or types.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-ai",
    "href": "ml_more.html#sec-ml-more-ai",
    "title": "9  More Machine Learning",
    "section": "9.7 Artificial Intelligence",
    "text": "9.7 Artificial Intelligence\n\n\n\nAI\n\n\nThe prospect of combining models for computer vision, natural language processing, audio processing, and other domains can produce tools that mimic many aspects of what we call intelligence4. Current efforts in artificial intelligence produce models that can pass law and medical exams, create better explanations of images and text than average human effort, and produce conversation on par with humans. AI even helped to create this book!\nIn many discussions of ML and AI, many put ML as a subset of AI, but this is a bit off the mark from a modeling perspective in our opinion5. In terms of models, practically most of what we’d call modern AI almost exclusively employs deep learning models, while the ML approach to training and evaluating models can be used for any underlying model, from simple linear models to the most complex deep learning models, and whether the application falls under the domain of AI or not. Furthermore, statistical model applications have never seriously attempted what we might call AI.\nIf AI is some ‘autonomous and general set of tools that attempt to engage the world in a human-like way or better’, it’s not clear why it’d be compared to ML in the first place. That’s kind of like saying the brain is a subset of cognition. The brain does the work, much like ML does the modeling work with data, and gives rise to what we call cognition, but generally we would not compare the brain to cognition. We also wouldn’t call ML a subset of climate science or medicine for similar reasons - they are domains in which it is used, much like the domain of artificial intelligence.\nThe main point is to not get too hung up on the labels, and focus on the modeling goal and how best to achieve it. Deep learning models, and machine learning in general, can be used for AI or non-AI settings, as we have seen for ourselves. And models still employ the perspective of the ML approach when ultimately used for AI - the steps taken from data to model output are largely the same, as we are concerned with validation and generalization.\nMany of the non-AI settings we use modeling for may well be things we can eventually rely on AI to do. At present though, the computational limits, and the amount of data that would be required for AI models to do well, or the ability of AI to be able to deal with situations in which there is only small bits of data to train on, are still hindrances in current applications of AI. However, we feel it’s likely these issues will eventually be overcome. Even then, a statistical approach may still have a place when the data is small.\nArtificial general intelligence (AGI) is the holy grail of AI, and like AI itself is not consistently defined. In general, the idea behind AGI is the creation of some autonomous agent that can perform any task that a human can perform, many that humans cannot, and generalize abilities to new problems that have not even been seen yet. It seems we are getting closer to AGI all the time, but it’s not yet clear when it will be achieved, or even what it will look like when it is, especially since no one has an agreed upon definition of what intelligence is in the first place.\nAll that being said, to be perfectly honest, you may well be reading a history book. Given recent advancements just in the last couple years, it almost seems unlikely that the data science being performed five years from now will resemble much of how things are done today6. We are already capable of making faster and further advancements in many domains due to AI, and it’s likely that the next generation of data scientists will be able to do so even more easily. The future is here, and it is amazing. Buckle up!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#sec-ml-more-wrap",
    "href": "ml_more.html#sec-ml-more-wrap",
    "title": "9  More Machine Learning",
    "section": "9.8 Wrapping Up",
    "text": "9.8 Wrapping Up\nWe hope you’ve enjoyed the journey and have a better understanding of the core concepts. By now you also have a couple modeling tools in hand, and also have a good idea of where things can go. We encourage you to continue learning and experimenting with what you’ve seen, and to apply what you’ve learned to your own problems. The best way to learn is by doing, so don’t be afraid to get your hands dirty and start building models!\n\n9.8.1 The common thread\nEven the most complex models can be thought of as a series of steps that go from input to output. In between can get very complicated, but often the underlying operations are the same ones you saw used with the simplest models. One of the key goals of any model is to generalize to new data, and this is the same no matter what type of data you’re working with or what type of model you’re using.\n\n\n9.8.2 Choose your own adventure\nThe sky’s the limit with machine learning and modeling, so go where your heart leads you, and have some fun! If you started here, feel free to go back to Part I for a more traditional and statistical modeling overview. Otherwise, head to Part III for an overview of a few more modeling topics such as causal modeling (Chapter 11), data issues (Chapter 10), and things to avoid (Chapter 12).\n\n\n9.8.3 Additional resources\n\nCourses on ML and DL: FastAI (Howard (2024)), Coursera, edX, DeepLearning.AI, and many others are great places to get more formal training.\nKaggle: Even if you don’t compete, you can learn a lot from what others are doing.\nUnsupervised learning overview at Google\nMachine Learning and AI: Beyond the Basics (Raschka (2023))\nVisualizing Transformer Models (Vig (2019))\n\n\n\n\n\nHoward, Jeremy. 2024. “Practical Deep Learning for Coders - Practical Deep Learning.” Practical Deep Learning for Coders. https://course.fast.ai/.\n\n\nRaschka, Sebastian. 2023. Machine Learning Q and AI. https://nostarch.com/machine-learning-q-and-ai.\n\n\nVig, Jesse. 2019. “Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention.” Medium. https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_more.html#footnotes",
    "href": "ml_more.html#footnotes",
    "title": "9  More Machine Learning",
    "section": "",
    "text": "Ideally we’d capture all the variability, but that’s not going to happen, and some techniques or results may only capture a relatively small percentage. In our personality example, this could be because the questions don’t adequately capture the underlying personality constructs (i.e. an issue of the reliability of instrument), or because personality is just not that simple and we’d need more dimensions.↩︎\nThey can also be derived in post-processing depending on the estimation approach.↩︎\nActually, in application as typically seen in social sciences, cross-validation is very rarely employed, and the number of latent variables is determined by some combination of theory, model comparison for training data only, or trial and error. Not that we’re advocating for that, but it’s a common practice.↩︎\nIt seems most discussions of AI in the public sphere haven’t really defined intelligence very clearly in the first place, and the academic realm has struggled with the concept for centuries.↩︎\nIn almost every instance of this we’ve seen printed, there isn’t any actual detail or specific enough definitions to make the comparison meaningful to begin with, so don’t take it too seriously.↩︎\nA good reference for this sentiment is a scene from Star Trek in which Scotty has to use a contemporary computer.↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Machine Learning</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "10  Dealing with Data",
    "section": "",
    "text": "10.1 Key Ideas",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-key-ideas",
    "href": "data.html#sec-data-key-ideas",
    "title": "10  Dealing with Data",
    "section": "",
    "text": "Data transformations can provide many modeling benefits.\nLabel and text-based data still needs a numeric representation, and this can be accomplished in a variety of ways.\nThe data type for the target may suggest a particular model, but does not necessitate one.\nThe data structure, e.g. temporal, spatial, censored, etc., may suggest a particular modeling domain to use.\nMissing data can be handled in a variety of ways, and the simpler approaches are typically not great.\nClass imbalance is a very common issue in classification problems, and there are a number of ways to deal with it.\nLatent variables are everywhere!\n\n\n10.1.1 Why this matters\nKnowing your data is one of the most important aspects of any application of data science. It’s not just about knowing what you have, but also what you can do with it. The data you have will influence the models you can potentially use, the features you can create and manipulate, and have a say on the results you can expect.\n\n\n10.1.2 Good to Know\nWe’re talking very generally about data here, so not much background is needed. The models mentioned are covered in other chapters, or build upon those, but we’re not doing any actual modeling here.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-transformations",
    "href": "data.html#sec-data-transformations",
    "title": "10  Dealing with Data",
    "section": "10.2 Feature & Target Transformations",
    "text": "10.2 Feature & Target Transformations\nTransforming variables from one form to another provides several benefits in modeling, whether applied to the target, features, or both, and should regularly be used for most model situations. Some of these benefits include:\n\nMore comparable feature effects and related parameters\nFaster estimation\nEasier convergence\nHelping with heteroscedasticity\n\nFor example, just centering features, i.e. subtracting the mean, provides a more interpretable intercept that will fall within the actual range of the target variable. After centering, the intercept tells us what the value of the target variable is when the features are at their means (or reference value if categorical). Centering also puts the intercept within the expected range of the target, which often makes for easier parameter estimation. So even if easier interpretation isn’t a major concern, variable transformations can help with convergence and speed up estimation, so can always be of benefit.\n\n10.2.1 Numeric variables\nThe following table shows the interpretation of some very common transformations applied to numeric variables- logging and standardizing to mean zero, standard deviation of one.\n\n\n\n\nTable 10.1: Common numeric transformations\n\n\n\n\n\n\n  \n    \n      Target\n      Feature\n      Change in X\n      Change in Y\n      Benefits\n    \n  \n  \n    y\nx\n1 unit\nB unit\nInterpretation\n    log(y)\nx\n1 unit\n100 * (exp(B) -1) \nHeteroscedasticity in y\n    log(y)\nlog(x)\n1% change\nB% change\nInterpretation, deal with feature extremes\n    y\nscale(x)\n1 standard deviation\nB unit\nInterpretation, estimation\n    scale(y)\nscale(x)\n1 standard deviation\nB standard deviation\nInterpretation, estimation\n  \n  \n  \n\n\n\n\n\n\n\nFor example, it is very common to use standardized variables, or simply ‘scaling’ them. Some also call this normalizing but this term can mean a lot of things, so one should be clear in their communication. If \\(y\\) and \\(x\\) are both standardized, a one unit (i.e. one standard deviation) change in \\(x\\) leads to a \\(\\beta\\) standard deviation change in \\(y\\). So, if \\(\\beta\\) was .5, a standard deviation change in \\(x\\) leads to a half standard deviation change in \\(y\\). In general, there is nothing to lose by standardizing, so you should employ it often.\nAnother common transformation, particularly in machine learning, is min-max scaling. This involves changing variables to range from some minimum value to some maximum value, and usually this means zero and one respectively. This transformation can make numeric and categorical indicators more comparable, or at least put them on the same scale for estimation purposes, and so can help with convergence and speed up estimation.\n\nPythonR\n\n\nWhen using sklearn, it’s a bit of a verbose process to do such a simple transformation. However it is beneficial when you want to do more complicated things, especially when using data pipelines.\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport numpy as np\n\n# Create a sample dataset\nimport numpy as np\n\n# Create a random sample of integers\ndata = np.random.randint(low=0, high=100, size=(5, 3))\n\n# Apply StandardScaler\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n\n# Apply MinMaxScaler\nminmax_scaler = MinMaxScaler()\nminmax_scaled_data = minmax_scaler.fit_transform(data)\n\n\n\nR, being made for statistics, makes it easy to do simple transformations, but you can also use tools like recipes and mlr3 pipeline operations when needed to make sure your preprocessing is applied appropriately.\n\n# Create a sample dataset\ndata = matrix(sample(1:100, 15), nrow = 5)\n\n# Standardization\nscaled_data = scale(data)\n\n# Min-Max Scaling\nminmax_scaled_data = apply(data, 2, function(x) {\n    (x - min(x)) / (max(x) - min(x))\n})\n\n\n\n\nUsing a log transformation for numeric targets and features is straightforward, and comes with several benefits. For example, it can help with heteroscedasticity, which is when the variance of the target is not constant across the range of the predictions1 (demonstrated below). It can also help to keep predictions positive after transformation, allows for interpretability gains, and more. One issue with logging is that it is not a linear transformation, which can help capture nonlinear feature-target relationships, but can also make some post-modeling transformations more less straightforward. Also if you have a lot of zeros, ‘log plus one’ transformations are not going to be enough to help you overcome that hurdle2. Logging also won’t help much when the variables in question have few distinct values, like ordinal variables, which we’ll discuss later in Section 10.2.3.\n\n\n\n\n\n\n\n\nFigure 10.1: Log transformation and heteroscedasticity\n\n\n\n\n\n\n\n\n\n\n\nCategorizing continuous variables\n\n\n\n\n\nIt is rarely necessary or a good idea to transform a numeric feature or target to a categorical one. Doing so potentially throws away useful information by making the feature a less reliable measure of the underlying construct. For example, discretizing age to ‘young’ and ‘old’ does not help your model, and you can always get predictions for what you would consider ‘young’ and ‘old’ after the fact. Though extremely common, particularly in machine learning contexts when applied to target variables, it’s just not a statistically or practically sound thing to do, and can ultimately hinder interpretation.\n\n\n\n\n\n10.2.2 Categorical variables\nDespite their ubiquity in data, we can’t analyze raw text information as it is. Character strings, and labeled features like factors, must be converted to a numeric representation before we can analyze them. For categorical features, we can use something called effects coding to test for specific types of group differences. Far and away the most common type is called dummy coding or one-hot encoding3, which we visited previously Section 2.7.2. In these situations we create columns for each category, and the value of the column is 1 if the observation is in that category, and 0 otherwise. Here is a one-hot encoded version of the season feature that was demonstrated previously.\n\n\n\n\nTable 10.2: One-hot encoding\n\n\n\n\n\n\n  \n    \n      seasonFall\n      seasonSpring\n      seasonSummer\n      seasonWinter\n      season\n    \n  \n  \n    1\n0\n0\n0\nFall\n    1\n0\n0\n0\nFall\n    1\n0\n0\n0\nFall\n    1\n0\n0\n0\nFall\n    0\n0\n1\n0\nSummer\n    0\n0\n1\n0\nSummer\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Coding Explained\n\n\n\n\n\nFor statistical models, when doing one-hot encoding all relevant information is incorporated in k-1 groups, where k is the number of groups, so one category will be dropped from the model matrix. This dropped category is called the reference. In a standard linear model, the intercept represents the mean of the target for the reference category. The coefficients for the other categories are the difference between the mean for the reference category and the group mean of the category being considered.\nAs an example, in the case of the season feature, if the dropped category is winter, the intercept tells us the mean rating for winter, and the coefficients for the other categories are the difference between the value for winter and the mean of the target for fall, summer and spring.\nIn other models, we include all categories in the model. The model learns how to use them best and might only consider some or one of them at a time.\n\n\n\nWhen we encode categories for statistical analysis, we can summarize their impact on the target variable single measure. For a model with only categorical features, we can use an ANOVA (Section 2.7.2.1) for this. But a similar approach can also be used for mixed models, splines, and other models. Techniques like SHAP also provide a way to summarize the total effect of a categorical feature.\n\n10.2.2.1 Text embeddings\nWhen it comes to other string representations like text, we use other methods to represent them numerically. One important way to encode text is through an embedding. This is a way of representing the text as a vector of numbers, at which point the embedding feature is used in the model like anything else. The way to do this usually involves a model itself, one that learns the best way to represent the text or categories numerically. This is commonly used in deep learning and natural language processing in particular. However, embeddings can also be used as a preprocessing step in any modeling situation.\nTo understand how embeddings work, consider a one-hot encoded matrix for a categorical variable. This matrix then connects to a hidden layer of a neural network, and the weights of that layer are the embeddings for the categorical variable. While this isn’t the exact method used (there are more efficient methods that don’t require the actual matrix), the concept is the same. In addition, we normally don’t even use whole words. Instead, we break the text into smaller units called tokens, like characters or subwords, and then use embeddings for those units. Tokenization is used in many of the most successful models for natural language processing, including those such as ChatGPT.\n\n\n\n\n\n\nFigure 10.2: Conceptual example of an embedding\n\n\n\n\n\n10.2.2.2 Multiclass targets\nWe’ve talked and demonstrated binary targets a lot, but what about when there are more than two classes? In statistical settings, we can use a multinomial regression, which is a generalization of (binomial) logistic regression to more than two classes via the multinomial distribution. Depending on the tool, you may have to use a multivariate target of the counts, though most commonly they would be zeros and ones for a classification model, which then is just a one-hot encoded target. The following table demonstrates how this might look.\n\n\n\n\nTable 10.3: Multinomial data example\n\n\n\n\n\n\n  \n    \n      x1\n      x2\n      target\n      Class A\n      Class B\n      Class C\n    \n  \n  \n    −1.41\n6\nA\n1\n0\n0\n    −1.13\n3\nC\n0\n0\n1\n    0.70\n6\nC\n0\n0\n1\n    0.32\n8\nB\n0\n1\n0\n    −0.84\n5\nA\n1\n0\n0\n  \n  \n  \n\n\n\n\n\n\n\nWith Bayesian tools, it’s common to use the categorical distribution, which is a different generalization of the Bernoulli distribution to more than two classes. Unlike the multinomial, it is not a count distribution, but an actual distribution over categories.\nIn the machine learning context, we can use a variety of models we’d use for binary classification. How the model is actually implemented will depend on the tool, but one of the more popular methods is to use one-vs-all or one-vs-one strategies, where you treat each class as the target in a binary classification problem. In the first case you would have a model for each class that predicts whether an observation is in that class or not. In the second case, you would have a model for each pair of classes. You should generally be careful with either approach if interpretation is important, as it can make the feature effects very difficult to understand. As an example, we can’t expect feature X to have the same effect on the target in a model for class A vs B, as it does in a model for class A vs. (B & C) or A & C. It also can be misleading when the models are conducted as if the categories are independent.\nRegardless of the context, interpretation is now spread across multiple target outputs, and so it can be difficult to understand the overall effect of a feature on the target. Even in the statistical setting, you now have coefficients that regard relative effects for one class versus a reference group, and so they cannot tell you a general effect of a feature on the target. This is where tools like marginal effects and SHAP can be useful (Chapter 3).\n\n\n10.2.2.3 Multilabel targets\nMultilabel targets are a bit more complicated, and are not as common as multiclass targets. In this case, each observation can have multiple labels. For example, if we wanted to predict genre based on the movie review data, we could choose to allow a movie to be both a comedy and action film, a sci-fi horror, or a romantic comedy. In this setting, labels are not mutually exclusive.\n\n\n\n\n\n\nCategorical Objective Functions\n\n\n\n\n\nIn many situations where you have a categorical target you will use a form of cross-entropy loss for the objective function. You may see other names such as log loss or logistic loss or negative log likelihood depending on the context, but usually it’s just a different name for the same underlying objective.\n\n\n\n\n\n\n10.2.3 Ordinal variables\nSo far in our discussion of categorical data, it’s assumed to have no order. But it’s quite common to have labels like “low”, “medium”, and “high”, or “very bad”, “bad”, “neutral”, “good”, “very good”, or simply are a few numbers, like ratings from 1 to 5. Ordinal data is categorical data that has a known ordering, but which still has arbitrary labels. Let us repeat that, ordinal data is categorical data.\n\n10.2.3.1 Ordinal features\nThe simplest way to treat ordinal features is as if they were numeric. If you do this, then you’re just pretending that it’s not categorical, and this is usually fine. Most of the transformations we mentioned probably aren’t going to be as useful, but you can still use them if you want. For example, logging ratings 1-5 isn’t going to do anything for you model-wise, but it technically doesn’t hurt anything. But you should know that typical statistics like means and standard deviations don’t really make sense for ordinal data, so the main reason for treating them as numeric is for modeling convenience.\nIf you choose to treat an ordinal feature as categorical, you can ignore the ordering and do the same as you would with categorical data. This would allow for some nonlinearity since the category means whatever they need to be. There are also some specific techniques to coding ordinal data for use in linear models, but they are not commonly used, and they generally aren’t going to help the model performance or interpreting the feature, so we do not recommend them. You could however use old-school effects coding that you would incorporate traditional ANOVA models, but again, you’d need a good reason to do so.\nThe take home message for ordinal features is generally simple. Treat them as you would numeric features or non-ordered categorical features. Either is fine.\n\n\n10.2.3.2 Ordinal targets\nOrdinal targets can be trickier to deal with. If you treat them as numeric, you’re assuming that the difference between 1 and 2 is the same as the difference between 2 and 3, and so on. This is probably not true. If you treat them as categorical and use standard models for that setting, you’re assuming that there is no connection between categories. So what should you do?\nThere are a number of ways to model ordinal targets, but probably the most common is the proportional odds model. This model can be seen as a generalization of the logistic regression model, and is very similar to it, and actually identical if you only had two categories. It basically is a model of 2 vs. 1, 3 vs. (2, 1), 4 vs. (3, 2, 1), etc. But other models beyond proportional odds are also possible, and your results could return something that gives coefficients for the model for the 1-2 category change, the 2-3 category change, and so on.\nOrdinality of a categorical outcome is largely ignored in machine learning applications. The outcome is either treated as numeric or multiclass classification. This is not necessarily a bad thing, especially if prediction is the primary goal. But if you need a categorical prediction, treating the target as numeric means you have to make an arbitrary choice to classify the predictions. And if you treat it as multiclass, you’re ignoring the ordinality of the target, which may not work as well.\n\n\n10.2.3.3 Rank data\nThough ranks are ordered, with rank data we are referring to cases where the observations are uniquely ordered. An ordinal vector with numeric labels could be something like [2, 1, 1, 3, 4, 2], whereas rank data would be [2, 1, 3, 4, 5, 6]. For example, in sports, the actual finish of the runners in a race is what we’d want to predict. Assuming you have a modeling tool that actually handles this situation, the objective will be different from other scenarios. Statistical modeling methods include using the Plackett-Luce distribution (or the simpler variant Bradley-Terry model). In machine learning, you might use so-called learning to rank methods, like the RankNet and LambdaRank algorithms, and other variants for deep learning models.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-missing",
    "href": "data.html#sec-data-missing",
    "title": "10  Dealing with Data",
    "section": "10.3 Missing Data",
    "text": "10.3 Missing Data\n\n\n\n\n\n\n\n  \n    \n      x1\n      x2\n      x3\n    \n  \n  \n    4\n0\n?\n    7\n3\nB\n    ?\n5\nA\n    8\n?\nB\n    ?\n3\nC\n  \n  \n  \n\n\n\n\nFigure 10.3\n\n\n\n\nMissing data is a common challenge in data science, and there are a number of ways to deal with it, usually by substituting, or imputing, the substituted value for the missing one. Here we’ll provide an overview of common techniques to deal with missing data.\n\n10.3.1 Complete case analysis\nThe first way to deal with missing data is the simplest - complete case analysis. Here we only use observations that have no missing data and drop the rest. Unfortunately, this can lead to a lot of lost data, and can lead to biased statistical results if the data is not missing completely at random. There are special cases of some models that by their nature can ignore the missingness under an assumption of missing at random, but even those models would likely benefit from some sort of imputation. If you don’t have much missing data though, dropping the missing data is fine for practical purposes4. How much is too much? Unfortunately that depends on the context, but if you have more than 10% missing, you should probably be looking at alternatives.\n\n\n10.3.2 Single value imputation\nSingle value imputation: Replace missing values with the mean, median, mode or some other typical value of the feature.This will probably rarely help your model for a variety of reasons. Consider a numeric feature that is 50% missing, and for which you replace the missing with the mean. How good do you think that feature will be when at least half the values are identical? Whatever variance it normally would have and share with the target is probably reduced, and possibly dramatically. Furthermore, you’ve also attenuated correlations it has with the other features, which may mute other modeling issues that you would otherwise deal with in some way (e.g. collinearity), or cause you to miss out on interactions.\nSingle value imputation makes perfect sense if you know that the missingness should be a specific value, like a count feature where missing means a count of zero. If you don’t have much missing data, it’s unlikely this would have any real benefit over complete case analysis, except if it allows you to use all the other features that would otherwise be dropped. But then, why not just drop this feature and keep the others?\n\n\n10.3.3 Model-based imputation\nModel-based imputation is more complicated, but can be very effective. In essence, you run a model for complete cases in which the feature with missing values is now the target, and all the other features and primary target are used to predict it. You then use that model to predict the missing values, and use those predictions as the imputed values. After these predictions are made, you move on to the next feature and do the same. There are no restrictions on which model you use for which feature. If the other features in the imputation model also have missing data, you can use something like mean imputation to get more complete data if necessary as a first step, and then when their turn comes, impute those values.\nAlthough the implication is that you would have one model per feature and then be done, you can do this iteratively for several rounds, such that the initial imputed values are then used in subsequent models to reimpute other feature’s missing values. You can do this as many times as you want, but the returns will diminish. In this setting, we are assuming you’ll have a single value imputed for each missing one, but this approach is the precursor for our next method.\n\n\n10.3.4 Multiple imputation\nMultiple imputation (MI) is a more complicated technique, but can be very useful under some situations, depending on what you’re willing to sacrifice for having better uncertainty estimates versus a deeper dive into the model. The idea is that you create multiple imputed datasets, each of which is based on the predictive distribution of the model used in model-based imputation. Say we use a linear regression assuming a normal distribution to impute feature A. We would then draw repeatedly from the predictive distribution of that model to create multiple datasets with (randomly) imputed values for feature A\nLet’s say we do this 10 times, and we now have 10 imputed data sets. We now run our actual model of interest on each of these datasets, and final model results are averaged in some way to get final parameter estimates. Doing so acknowledges that your single imputation methods have uncertainty in those imputed values, and that uncertainty is incorporated into the final model results.\nMI can in theory handle any source of missingness and can be a very powerful technique. But it has some drawbacks that are often not mentioned. One is that you need a specified target distribution for all imputation models used, in order to generate random draws. Your final model presumably is also a probabilistic model with coefficients and variances you are trying to estimate and understand. MI probably isn’t going to help boosting or deep learning models that have native methods for dealing with missing values, or at least offer little if anything over single value imputation. If you have very large data and a complicated model, you could be waiting a long time, and as modeling is an iterative process itself, this can be rather tedious to work through. Finally, few data or post-model processing tools that you commonly use will work with MI results, especially visualization tools. So you will have to hope that whatever package you use for MI has what you need. As an example, you’d have to figure out how you’re going to impute interaction terms if you have them.\nPractically speaking, MI takes a lot of effort to often come to the same conclusions you would have with a single imputation method, or possibly fewer conclusions for anything beyond GLM coefficients and their standard errors. But if you want your uncertainty estimate for those models to be better, MI can be an option.\n\n\n10.3.5 Bayesian imputation\nOne final option is to run a Bayesian model where the missing values are treated as parameters to be estimated, and they would have priors just like other parameters as well. MI is basically a variant of Bayesian imputation that can be applied to the non-Bayesian model setting, so why not just use the actual Bayesian method? Some modeling packages can allow you to try this very easily, and it can be very effective. But it is also very computationally intensive, and can be very slow as you may be increasing the number of parameters to estimate dramatically. At least it would be more fun than standard MI!",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-class-imbalance",
    "href": "data.html#sec-data-class-imbalance",
    "title": "10  Dealing with Data",
    "section": "10.4 Class Imbalance",
    "text": "10.4 Class Imbalance\n\n\n\n\n\n\n\n\nFigure 10.4: Class Imbalance\n\n\n\n\n\nClass imbalance refers to the situation where the target variable has a large difference in the number of observations in each class. For example, if you have a binary target, and 90% of the observations are in one class, and 10% in the other, you would have class imbalance. You’ll almost never see a 50/50 split in the real world, but the issue is that as we move further away from that point, we can start to see problems in model estimation, prediction, and interpretation. With our example, if we just predict the majority class in a binary classification problem, our accuracy would be 90%! Under other circumstances that might be a great result, but in this case it’s not. So right off the bat one of our favorite metrics to use for classification models isn’t going to help us much.\nFor classification problems, class imbalance is the rule, not the exception. This is because nature just doesn’t sort itself into nice and even bins. The majority of people in a random sample do not have cancer, the vast majority of people have not had a heart attack in the past year, most people do not default on their loans, and so on.\nThere are a number of ways to help deal with class imbalance, and the method that works best will depend on the situation. Some of the most common are:\n\nUse different metrics: Use metrics that are less affected by class imbalance, such as area under a receiver operating characteristic curve (AUC), or those that balance the tradeoff between precision and recall, like the F1 score, or balance recall and true negative rate, like the balanced accuracy score.\nOversampling/Undersampling: Randomly sample from the minority (majority) class to increase (decrease) the number of observations in that class.\nWeighted objectives: Weight the loss function to give more weight to the minority class. Although commonly employed, and a simple thing to use with models like lightgbm and xgboost, it often fails to help, and can cause other issues.\nThresholding: Change the threshold for classification to be more sensitive to the minority class. Nothing says you have to use 0.5 as the threshold for classification, and you can change it to be more sensitive to the minority class. This is a very simple approach, and may be all you need.\n\nThese are not necessarily mutually exclusive. For example, in the imbalanced setting it’s probably a good idea to switch your focus to a metric besides accuracy even as you employ other techniques.\n\n10.4.1 Calibration issues in classification\nProbability calibration is often an issue in classification problems, and is a bit more complicated than just class imbalance, but is often discussed in the same setting. Having calibrated probabilities refers to the situation where the predicted probabilities of the target match up well to the actual probabilities. For example, if you predict that 10% of people will default on their loans, and 10% of people actually do default on their loans, one would say your model is well calibrated. Conversely, if you predict that 10% of people will default on their loans, but 20% of people actually do default on their loans, your model is not so well-calibrated.\nOne way to assess calibration is to use a calibration curve, which is a plot of the predicted probabilities vs. the observed proportions. We bin the observations in some way, and then calculate the average predicted probability and the average observed proportion of the target in each bin. If the model is well-calibrated, the points should fall along the 45-degree line. If the model is not well-calibrated, the points will fall above or below the line. In the following, one model seems to align well with the observed proportions based on the chosen bins. The other model (dashed line) is not so well calibrated, and is overshooting with its predictions. For example, if the model’s average prediction predicts a 0.5 probability of default, the actual proportion of defaults is only around 0.6.\n\n\n\n\n\n\n\n\nFigure 10.5: Calibration Plot\n\n\n\n\n\nWhile the issue is an important one, it’s good to keep the issue of calibration and imbalance separate. As miscalibration implies bias, bias can happen irrespective of the class proportions, and can be due to a variety of factors related to the model, target, or features. Furthermore, miscalibration is not inherent to any particular model.\nWe should note that the assessment of calibration in this manner has a few issues. For one, the observed ‘probabilities’ are proportions based on arbitrarily chosen bins, and the observed values are measured with some measurement error and have a natural variability that will partly reflect sample size5. These plots are often presented such that observed proportions are labeled as the “true” probabilities. However, you do not have the true probabilities, just the observed class labels, so whether your model’s predicted probabilities match observed proportions is a bit of a different question. The predictions obviously have uncertainty as well, and this will depend on the model, sample size, and other factors. And finally, the number of bins chosen can also affect the appearance of the plot in a notable way if the sample size is small.\nAll this is to say that each point in a calibration plot, along with the reference line itself, has some error bar around it, and the differences between models and the ‘best case scenario’ would need additional steps to suss out if we are interested in doing so in a statistically rigorous fashion. Some methods are available to calibrate probabilities, but they are not commonly implemented in practice, and often involve a model-based technique, with all of its own assumptions and limitations. It’s also not exactly clear that forcing your probabilities to be on the line is helping solve the actual modeling goal in any way6. But if you are interested, you can read more here.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-censoring",
    "href": "data.html#sec-data-censoring",
    "title": "10  Dealing with Data",
    "section": "10.5 Censoring and Truncation",
    "text": "10.5 Censoring and Truncation\n\n\n\n\n\n\n\n\nFigure 10.6: Censoring for Time Until Death\n\n\n\n\n\nSometimes, we just don’t see all the data there is to see. Censoring is a situation where the target variable is not fully observed. This is common in techniques where the target is the time to an event, like ‘death’, but the event has not yet occurred for some observations (thankfully!). Specifically this is called right censoring, and is the most common type of censoring, and depicted in the above plot, where several individuals are only observed to a certain age and were still alive at that time. There is also left censoring, where the censoring happens from the other direction, and data before a certain point is unknown. Finally, there is interval censoring, where the event of interest occurs within some interval, but the exact value is unknown.\nSurvival analysis7 is a common modeling technique in this situation, but you may also be able to keep things even more simple via something like tobit regression. In this model, you assume that the target is fully observed, but that the values are censored, and you model the probability of censoring. This is a common technique in econometrics, and can keep you in a traditional linear model context.\n\n\n\n\n\n\n\n\nFigure 10.7: Truncation\n\n\n\n\n\nTruncation is a situation where the target variable is only observed if it is above or below some value, even though we know other possibilities exist. One of the issues is that default distributional methods assume a distribution that is not necessarily bounded in the way that the data exhibits. In our plot above, we restrict our data to 70 and below for practical or other reasons, but typical modeling methods predicting age would not respect that.\nYou could truncate predictions after the fact, but this is a bit of a hack, and often results in lumpiness in the predictions at the boundary that likely isn’t realistic. Alternatively, Bayesian methods allow you to model the target as a distribution with truncated distributions, and so you can model the probability of the target being above or below some value. There are also models such as hurdle models that might prove useful where the truncation is theoretically motivated, e.g. a zero-inflated Poisson model for count data where the zero counts are due to a separate process than the non-zero counts.\n\n\n\n\n\n\nCensoring vs. Truncation\n\n\n\n\n\nOne way to distinguish censored and truncated data is that censored data is usually due to some external process such that the target is not observed, but could be possible (capping reported income at $1 million). Truncated data, on the other hand, is due to some internal process that prevents the target from being observed, and is often derived from sample selection (we only want to model non-millionaires). We would not want predictions past the censored point to be unlikely, but we would want predictions past the truncated point to be impossible. Trickier still is that for bounded or truncated distributions that might be applied to the truncated scenario, such as folded vs. truncated distributions, they would not result in the same probability distributions even if they can be applied to the same data situation.\n\n\n\nFolded and Truncated Distributions\n\n\nImage from StackExchange",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-time",
    "href": "data.html#sec-data-time",
    "title": "10  Dealing with Data",
    "section": "10.6 Time Series",
    "text": "10.6 Time Series\n\n\n\n\n\n\n\n\nFigure 10.8: Time Series Data\n\n\n\n\n\nTime series data is any data that incorporates values over a period of time. This could be something like a state’s population over years, or the max temperature of an area over days. Time series data is very common in data science, and there are a number of ways to model such data.\n\n10.6.1 Time-based targets\nAs in other settings, the most common approach when the target is some value that varies time is to use a linear model of some kind. While the target varies over time, the features may be time-varying or not. There are traditional autoregressive models that use the target’s past values as features, for example, autoregressive moving average (ARIMA) models. Others can incorporate historical information in other ways such as is done in Bayesian methods to marketing data or in reinforcement learning (Section 9.3). Still others can get quite complex, such recurrent neural networks and their generalizations that form the backbone of modern AI models. Lately transformer-based models have looked promising.\nLongitudinal data8 is a special case of time series data, where the target is a function of time, but it is typically grouped in some fashion. An example is a model for school performance for students over several semesters, where values are clustered within students over time. In this case, you can use some sort of time series regression, but you can also use a mixed model (Section 6.3), where you model the target as a function of time, but also include a random effect for the grouping variable, in this case, students. This is a very common approach in many domains, and can be very effective in terms of performance as well. It can also be used for time series data that is not longitudinal, where the random effects are based on autoregressive covariance matrices. In this case, an ARIMA component is added to the linear model as a random effect to account for the time series nature of the data. This is fairly common in Bayesian contexts.\nIn general lots of models can be found specific to time series data, and the choice of model will depend on the data, the questions we want to ask, and the goals we have.\n\n\n\n\n\n\nTime Series vs. Longitudinal\n\n\n\n\n\nThe primary distinguishing feature for referring data as ‘time-series’ and ‘longitudinal’ is the number of time points, where the latter typically has relatively few. This arbitrary though.\n\n\n\n\n\n10.6.2 Time-based features\nWhen it comes to time-series features, we can apply time-specific transformations. One technique is the fourier transform, which can be used to decompose a time series into its component frequencies, much like how we use PCA (Section 9.2). This can be useful for identifying periodicity in the data, which can be used as a feature in a model.\nIn marketing contexts, some perform adstocking with features. This method models the delayed effect of features over time, such that they may have their most important impact immediately, but still can impact the present target from past values. For example, a marketing campaign might have the most significant impact immediately after it’s launched, but it can still influence the target variable at later time points. Adstocking helps capture this delayed effect without having to include multiple lagged features in the model. That said, including lagged features is also an option. In this case, you would have a feature for the current time point (t), the same feature for the previous time point (t-1), the feature for the time point before that (t-2), and so on.\nIf you have the year as a feature, you can use it as a numeric feature or as a categorical feature. If you treat it as numeric, you need to consider what a zero means. In a linear model, the intercept usually represents the outcome when all features are zero. But with a feature like year, a zero year isn’t meaningful in most contexts. To solve this, you can shift the values so that the earliest time point, like the first year in your data, becomes zero. This way, the intercept in your model will represent the outcome for this first time point, which is more meaningful. The same goes if you are using months or days as a numeric feature. It doesn’t really matter which year/month/day is zero, just that zero refers to one of the actual time points observed.\nDates and/or times can be a bit trickier. Often you can just split dates out into year, month, day, etc., and proceed as discussed. In other cases you’d want to track the time period to assess possible seasonal effects. You can use something like a cyclic approach (e.g. cyclic spline or sine/cosine transformation) to get at yearly or within-day seasonal effects. As mentioned, a fourier transform can also be used to decompose the time series into its component frequencies for use as model features. Time components like hours, minutes, and seconds can often be dealt with in similar ways, but you will more often deal with the periodicity in the data. For example, if you are looking at hourly data, you may want to consider the 24-hour cycle.\n\n\n\n\n\n\nCalendars are hard.\n\n\n\n\n\nWeeks are not universal. Some start on Sunday, others Monday. Some data contexts only consider weekdays. Some systems may have 52 or 53 weeks in a year, and dates may not be in the same week from one year to the next, etc. So use caution when considering weeks as a feature.\n\n\n\n\n10.6.2.1 Covariance structures\nIn many cases you’ll have features that vary over time but are not a time-oriented feature like year or month. For example, you might have a feature that is the number of people who visited a website over days. This is a time-varying feature, but it’s not a time metric in and of itself.\nIn general, we’d like to account for the time-dependent correlations in our data, and the main way to do so is to posit a covariance structure that accounts for this in some fashion. This helps us understand how data points are related to each other over time, and requires us to estimate the correlations between observations. As a starting point, consider linear regression. In a standard linear regression model, we assume that the samples are independent of one another, with a constant variance and no covariance.\nInstead, we can also use something like a mixed model, where we include a random effect for each group and estimate the variance attributable to the grouping effect. By default, this ultimately assumes a constant correlation from time point to time point, but many tools allow you to specify a more complex covariance structure. A common method is to use autoregressive covariance structure that allows for correlations further apart in time to lessen. In this sense the covariance comes in as an added random effect, rather than being a model in and of itself as with ARIMA. Many such approaches to covariance structures are special cases of gaussian processes, which are a very general technique to model time series, spatial, and other types of data.\n\n\n\n\n\n\n\n\nFigure 10.9: AR (1) Covariance Structure Visualized",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-spatial",
    "href": "data.html#sec-data-spatial",
    "title": "10  Dealing with Data",
    "section": "10.7 Spatial Data",
    "text": "10.7 Spatial Data\n\n\n\n\n\n\nFigure 10.10: Spatial Weighting Applied to the Dallas-Fort Worth Area Census Tracts\n\n\n\nWe visited spatial data in a discussion on non-tabular data (Section 9.4.1), but here we want to talk about it from a modeling perspective, especially within the tabular domain. Say you have a target that is a function of location, such as the proportion of people voting a certain way in a county, or the number of crimes in a city. You can use a spatial regression model, where the target is a function of location among other features that may or may not be spatially oriented. Two approaches already discussed may be applied in the case of having continuous spatial features, such as latitude and longitude, or discrete features like county. For the continuous case, we could use a GAM (Section 6.4), where we use a smooth interaction of latitude and longitude. For the discrete setting, we can use a mixed model, where we include a random effect for county.\nThere are other traditional techniques to spatial regression, especially in the continuous spatial domain, such as using a spatial lag, where we incorporate information about the neighborhood of a observation’s location into the model (e.g. a weighted mean of neighboring values, as in the visualization above based on code from Walker (2023)). Such models fall under names such as CAR (conditional autoregressive), SAR (spatial autoregressive), BYM, kriging, and so on. These models can be very effective, but are in general different forms of random effects models very similar to those used for time-based settings, and likewise can be seen as special cases of gaussian processes.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#multivariate-targets",
    "href": "data.html#multivariate-targets",
    "title": "10  Dealing with Data",
    "section": "10.8 Multivariate Targets",
    "text": "10.8 Multivariate Targets\nOften you will encounter settings where the target is not a single value, but a vector of values. This is often called a multivariate target in statistical settings, or just the norm for deep learning. For example, you might be interested in predicting the number of people who will buy a product, the number of people who will click on an ad, and the number of people who will sign up for a newsletter. This is a common setting in marketing, where you are interested in multiple outcomes. The main idea is that there is a relationship among the targets, and you want to take this into account.\nOne model example we’ve already seen is the case where we have more than two categories for the target. Some default approaches may take that input and just do a one-vs-all, for each category, but this kind of misses the point. Others will simultaneously model the multiple targets in some way. On the other hand, it can be difficult to interpret results with multiple targets. Because of this, you’ll often see results presented in terms of the respective targets anyway, and often even ignoring parameters specifically associated with such a model9.\nIn deep learning contexts, the multivariate setting is ubiquitous. For example, if you want to classify the content of an image, you might have to predict something like different species of animals, or different types of car models. In natural language processing, you might want to predict the probability of different words in a sentence. In some cases, there are even multiple kinds of targets considered simultaneously! It can get very complex, but often in these settings prediction performance far outweighs the need to interpret specific parameters, and so it’s a good fit.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-latent",
    "href": "data.html#sec-data-latent",
    "title": "10  Dealing with Data",
    "section": "10.9 Latent Variables",
    "text": "10.9 Latent Variables\n\n\n\n\n\n\n\nFigure 10.11: Latent Variable Model (Bifactor)\n\n\n\nLatent variables are a fundamental aspect of modeling, and simply put, are variables that are not directly observed, but are inferred from other variables. Here are some examples of what might be called latent variables:\n\nThe linear combination of features in a linear regression model is a latent variable, but usually we only think of it as such before the link transformation in GLMs (Chapter 5).\nThe error term in any model is a latent variable representing all the unknown/unobserved/unmodeled factors that influence the target (Equation 2.3).\nThe principal components in PCA (Chapter 9).\nThe measurement error in any feature or target.\nThe factor scores in a factor analysis model or structural equation (visualization above).\nThe true target underlying the censored values (Section 10.5).\nThe clusters in cluster analysis/mixture models (Section 9.2.1.1).\nThe random effects in a mixed model (Section 6.3).\nThe hidden states in a hidden Markov model.\nThe hidden layers in a deep learning model (Section 8.7).\n\nIt’s easy to see from such a list that latent variables are very common in modeling, so it’s good to get comfortable with the concept. Whether they’re appropriate to your specific situation will depend on a variety of factors, but they can be very useful in many settings, if not a required part of the modeling approach.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-aug",
    "href": "data.html#sec-data-aug",
    "title": "10  Dealing with Data",
    "section": "10.10 Data Augmentation",
    "text": "10.10 Data Augmentation\nData augmentation is a technique where you artificially increase the size of your dataset by creating new data points based on the existing data. This is a common technique in deep learning for computer vision, where you might rotate, flip, or crop images to create new training data. This can help improve the performance of your model, especially when you have a small dataset. Techniques are also available for text.\nIn the tabular domain, data augmentation is less common, but still possible. You’ll see it most commonly applied with class-imbalance settings (Section 10.4), where you might create new data points for the minority class to balance the dataset. This can be done by randomly sampling from the existing data points, or by creating new data points based on the existing data points. For the latter, SMOTE and many variants of it are quite common.\nUnfortunately for tabular data, these techniques are not nearly as successful, nor consistently so. Part of the issue is that tabular data is very noisy and fraught with measurement error, so in a sense, such techniques are just adding noise to the modeling process10. Simple random upsampling can potentially lead to an overconfident model that still doesn’t generalize well. In the end, the best approach is to get more and/or better data, but hopefully more successful methods will be developed in the future.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-wrap",
    "href": "data.html#sec-data-wrap",
    "title": "10  Dealing with Data",
    "section": "10.11 Wrapping Up",
    "text": "10.11 Wrapping Up\nThere’s a lot going on with data before you ever get to modeling, and which will affect every aspect of your modeling approach. This chapter outlines common data types, issues, and associated modeling aspects, but in the end, you’ll always have to make decisions based on your specific situation, and they will often not be easy ones. These are only some of the things to consider, so be ready for surprises, and be ready to learn from them!\n\n10.11.1 The common thread\nMany of the transformations and missing data techniques could possibly be applied in many modeling settings. Likewise, you may find yourself dealing with different target variable issues like imbalance or censoring, and deal with temporal, spatial or other structures, in a variety of models. The key is to understand the data, the target, and the features, and to make the best decisions you can based on that understanding.\n\n\n10.11.2 Choose your own adventure\nConsider revisiting a model covered in the other parts of this book in light of the data issues discussed here. For example,might you deal with class imbalance for a boosted tree model? How would you deal with spatial structure in a neural network? How would you deal with a multivariate target in a time series model?\n\n\n10.11.3 Additional resources\nHere are some additional resources to help you learn more about the topics covered in this chapter.\nTransformations\n\nAbout Feature Scaling and Normalization (Raschka (2014))\nWhat are Embedding (Boykis (2023))\n\nClass Imbalance\n\nBrief Overview (Google)\nHandling imbalanced datasets in machine learning (Rocca (2019))\nImbalanced Outcomes: Challenges & Solutions (Monroe and Clark (2024), in preparation)\nA Gentle Introduction to Imbalanced Classification (Brownlee (2019))\n\nCalibration\n\nWhy some algorithms produce calibrated probabilities (StackExchange)\nPredicting good probabilities with supervised learning (Niculescu-Mizil and Caruana (2005))\n\nSurvival, Ordinal and Other Models\n\nRegression Modeling Strategies is a great resource for statistical modeling in general (Harrell (2015))\n\nTime Series\n\nForecasting: Principles and Practice (Hyndman and Athanasopoulos (2021))\n\nLatent Variables\nThere’s a ton of stuff, but one of your humble authors has an applied treatment that is not far from the conceptual approach of this text, along with a bit more general exploration. However, we’d recommend more of an ‘awareness’ of latent variable modeling, as you will likely be more interested in the specific application for your data and model, which can be very different from these contexts.\nThinking about Latent Variables (Clark (2018b)) Graphical and Latent Variable Modeling (Clark (2018a))\nData Augmentation\nWhat is Data Augmentation? (Amazon (2024))\n\n\n\n\nAmazon. 2024. “What Is Data Augmentation? - Data Augmentation Techniques Explained - AWS.” Amazon Web Services, Inc. https://aws.amazon.com/what-is/data-augmentation/.\n\n\nBoykis, Vicki. 2023. “What Are Embeddings?” http://vickiboykis.com/what_are_embeddings/index.html.\n\n\nBrownlee, Jason. 2019. “A Gentle Introduction to Imbalanced Classification.” MachineLearningMastery.com. https://machinelearningmastery.com/what-is-imbalanced-classification/.\n\n\nClark, Michael. 2018a. Graphical & Latent Variable Modeling. https://m-clark.github.io/sem/.\n\n\n———. 2018b. “Thinking about Latent Variables.” https://m-clark.github.io/docs/FA_notes.html.\n\n\nHarrell, Frank E. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. 2nd ed. Springer Series in Statistics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nHyndman, Rob, and George Athanasopoulos. 2021. Forecasting: Principles and Practice (3rd Ed). https://otexts.com/fpp3/.\n\n\nMonroe, Elizabeth, and Michael Clark. 2024. “Imbalanced Outcomes: Challenges and Solutions.”\n\n\nNiculescu-Mizil, Alexandru, and Rich Caruana. 2005. “Predicting Good Probabilities with Supervised Learning.” In Proceedings of the 22nd International Conference on Machine Learning - ICML ’05, 625–32. Bonn, Germany: ACM Press. https://doi.org/10.1145/1102351.1102430.\n\n\nRaschka, Sebastian. 2014. “About Feature Scaling and Normalization.” Sebastian Raschka, PhD. https://sebastianraschka.com/Articles/2014_about_feature_scaling.html.\n\n\nRocca, Baptiste. 2019. “Handling Imbalanced Datasets in Machine Learning.” Medium. https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28.\n\n\nWalker, Kyle E. 2023. Analyzing US Census Data. https://walker-data.com/census-r.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "data.html#footnotes",
    "href": "data.html#footnotes",
    "title": "10  Dealing with Data",
    "section": "",
    "text": "For the bazillionth time, logging does not make data ‘normal’ so that you can meet your normality assumption in linear regression. The only time that would work is if your data is log-normally distributed to begin with.↩︎\nThat doesn’t mean you won’t see many people try (and fail).↩︎\nNote that one-hot encoding can refer to just the 1/0 coding for all categories, or to the specific case of dummy coding where one category is dropped. Make sure the context is clear.↩︎\nWhile many statisticians will possibly huff and puff at the idea of dropping data, there are two things to consider. With minimal missingness you’ll likely never come to a different conclusion unless you have very little data to come to a conclusion about, which is already the bigger problem. Secondly, it’s impossible to prove one way or another if the data is missing at random, because doing so would require knowing the missing values.↩︎\nNote that each bin will reflect the portion of the test set size in this situation. If you have a small test set, the observed proportions will be more variable, and the calibration plot will be more variable as well.↩︎\nOftentimes we are only interested in the ordering of the predictions, and not the actual probabilities. For example, if we are trying to identify the top 10% of people most likely to default on their loans, we’ll just take the top 10% of predictions, and the actual probabilities are irrelevant for that goal.↩︎\nSurvival analysis is also called event history analysis, and is widely used in biostatistics, sociology, demography, and other disciplines where the target is the time to an event, such as death, marriage, divorce, etc.↩︎\nYou may also hear the term panel data in econometrics-oriented disciplines.↩︎\nIt was common in social sciences back in the day to run a Multivariate ANOVA, and then if the result was statistically significant, and mostly because few practitioners knew what to do with the result, they would run separate ANOVAs for each target.↩︎\nCompare to the image settings where there is relatively little measurement error, by just rotating an image, you are still preserving the underlying structure of the data.↩︎",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Dealing with Data</span>"
    ]
  },
  {
    "objectID": "causal.html",
    "href": "causal.html",
    "title": "11  Causal Modeling",
    "section": "",
    "text": "11.1 Key Ideas",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#sec-causal-key-ideas",
    "href": "causal.html#sec-causal-key-ideas",
    "title": "11  Causal Modeling",
    "section": "",
    "text": "No model can tell you whether a relationship is causal or not. Causality is inferred, not proven, based on the available evidence.\nThe exact same models would be used for similar data settings to answer a causal question or a purely predictive question. The primary difference is in the interpretation of the results.\nExperimental design, such as randomized control trials, are the gold standard for causal inference. But the gold standard is often not practical, and not without its limitations even when it is.\nCausal inference is often done with observational data, which is often the only option, and that’s okay.\nCounterfactual thinking is at the heart of causal inference, but is useful for all modeling contexts.\nSeveral models exist which are typically employed to answer a more causal-oriented question. These include structural equation models, graphical models, uplift modeling, and more.\nInteractions are the norm, if not the reality. Causal inference generally regards a single effect. If the normal setting is that such an effect would always vary depending on other features, you should question why you want to aggregate your results to a single ‘effect’, since that effect would be potentially misleading.\n\n\n11.1.1 Why it matters\nOften we need a precise statement about the feature-target relationship, not just whether there is some relationship. For example, we might want to know whether a drug works well, or whether showing an advertisement results in a certain amount of new sales. Whether or not random assignment was used, we generally need to know whether the effect is real, and the size of the effect, and often, the uncertainty in that estimate. Causal modeling is, like machine learning, more of an approach than a specific model, and that approach may involve the design or implementing models we’ve already seen in a different way to answer the key question. Without more precision in our understanding, we could miss the effect, or overstate it, and make bad decisions as a result.\n\n\n11.1.2 Good to know\nThis section is pretty high level, and we are not going to go into much detail here so even just some understanding of correlation and modeling would likely be enough.\n\n\n\n\n\n\n\n\nFigure 11.1: A causal DAG",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#sec-causal-classic",
    "href": "causal.html#sec-causal-classic",
    "title": "11  Causal Modeling",
    "section": "11.2 Classic Experimental Design",
    "text": "11.2 Classic Experimental Design\n\n\n\n\n\n\n\nFigure 11.2: Random Assignment\n\n\n\nMany are familiar with the basic idea of an experiment, where we have a treatment group and a control group, and we want to measure the difference between the two groups. The ‘treatment’ could regard a new drug, a marketing campaign, or a new app’s feature. If we randomly assign our observational units to the two groups, say, one that gets the new app feature and the other doesn’t, we can be more confident that the two groups are essentially the same aside from the treatment, and that any difference in the outcome, for example, customer satisfaction with the app, is due to the treatment.\nThis is the basic idea behind a randomized control trial, or RCT. We can randomly assign the groups in a variety of ways, but you can think of it as flipping a coin, and assigning each sample to the treatment when the coin comes up on one side, and to the control when it comes up on the other. The idea is that the only difference between the two groups is the treatment, and so any difference in the outcome can be attributed to the treatment. This is visualized in Figure 11.2, where the colors represent different groups, and the groups are essentially the same aside from the treatment.\nMany of those who have taken a statistics course have been exposed to the simple t-test to determine whether two groups are different. The t-test tells us whether the difference in means between the two groups is statistically significant. However, it definitely does not tell us whether the treatment itself caused the difference, whether the effect is large, nor whether the effect is real, or even if the treatment is a good idea to do in the first place. It just tells us whether the two groups are statistically different.\nTurns out, a t-test is just a linear regression model. It’s a special case of linear regression where there is only one independent variable, and it is a categorical variable with two levels. The coefficient from the linear regression would tell you the mean difference of the outcome between the two groups. Under the same conditions, the t-statistic from the linear regression and the t-test would have identical statistical results.\nAnalysis of variance, or ANOVA, allows the t-test to be extended to more than two groups, and multiple features, and is also commonly employed to analyze the results of experimental design settings. But ANOVA is still just a linear regression. Even when we get into more complicated design settings such as repeated measures and mixed design, it’s still just a linear regression, we’d just be using mixed models (Section 6.3).\nIf linear regression didn’t suggest any notion of causality to you before, it certainly shouldn’t now either. The model is identical whether there was an experimental design with random assignment or not. The only difference is that the data was collected in a different way, and the theoretical assumptions and motivations are different. Even the statistical assumptions are the same whether you use random assignment or there are one or more groups, or whether the treatment is continuous or categorical.\nExperimental design1 can give us more confidence in the causal explanation of model results, whatever model is used, and this is why we like to use it when we can. It helps us control for the unobserved factors that might otherwise be influencing the results. If we can be fairly certain the observations are essentially the same except for the treatment, then we can be more confident that the treatment is the cause of the difference, and we can be more confident in the causal interpretation of the results. But it doesn’t change the model itself, and the results of a model don’t prove a causal relationship by themselves. Your experimental study will also be limited by the quality of the data, and the population it generalizes to. Even with strong design and modeling, if care isn’t taken in the modeling process to even assess the generalization of the results (Section 7.4), you may find they don’t hold up2.\n\n\n\n\n\n\nA/B Testing\n\n\n\n\n\nA/B testing is just marketing-speak for a project focused on comparing two groups. It implies randomized assignment, but you’d have to understand the context to know if that is actually the case.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#sec-causal-natural",
    "href": "causal.html#sec-causal-natural",
    "title": "11  Causal Modeling",
    "section": "11.3 Natural Experiments",
    "text": "11.3 Natural Experiments\n\n\n\n\n\n\nFigure 11.3: Covid Vaccinations and Deaths in the US\n\n\n\nAs we noted, random assignment or a formal experiment is not always possible or practical to implement. But sometimes we get to do it anyway, or at least we can get pretty close! Sometimes, the world gives us a natural experiment, where the assignment to the groups is essentially random, or where there is clear break before and after some event occurs, such that we examine the change as we would in pre-post design.\nFor example, the covid pandemic allowed us to potentially examine vaccination effects, governmental policy effects, the effectiveness of remote work, and more. This was not a tightly controlled experiment, but it’s something we can treat very similar to an experiment, and we can compare the differences in various outcomes before and after the pandemic to see what changes took place.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#sec-causal-inference",
    "href": "causal.html#sec-causal-inference",
    "title": "11  Causal Modeling",
    "section": "11.4 Causal Inference",
    "text": "11.4 Causal Inference\nReasoning about causality is a very old topic, philosophically dating back millennia, and more formally hundreds of years. Random assignment is a relatively new idea, say 150 years old, but was posited even before Wright, Fisher, and Neyman and the 20th century rise of statistics. But with stats and random assignment we had a way to start using models to help us reason about causal relationships. Pearl and others came along to provide a perspective from computer science, and things have been progressing along. We were actually using programming approaches to do causal inference back in the 1970s even! Economists got into the game too (e.g., Heckman), and eventually most scientific academic disciplines were well acquainted with causal inference in some fashion.\nNow we can use recently developed modeling approaches to help us reason about causal relationships, which can be both a blessing and a curse. Our models can be more complex, and we can use more data, which can potentially give us more confidence in our conclusions. But we can still be easily fooled by our models, as well as by ourselves. So we’ll need to be careful in how we go about things, but let’s see what some of our options are!",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#sec-causal-models",
    "href": "causal.html#sec-causal-models",
    "title": "11  Causal Modeling",
    "section": "11.5 Models for Causal Inference",
    "text": "11.5 Models for Causal Inference\nAny model can be used to answer a causal question, and which one you use will depend on the data setting and the question you are asking. The following covers a few models that might be seen in various academic and professional settings.\n\n11.5.1 Linear regression\nYep, linear regression. The old standby is possibly the mostly widely used model for causal inference, historically speaking and even today. We’ve seen linear regression as a graphical model Figure 2.2, and in that sense, it can serve as the starting point for structural equation models and related models that we’ll talk about next that many consider to be true causal models. It can also be used as a baseline model for other more complex causal model approaches. Linear regression can potentially tell us for any particular feature, what that feature’s relationship with the target is, holding the other features constant. This ceteris paribus interpretation - ‘all else being equal’ - already gets us into a causal mindset.\nHowever, your standard linear model doesn’t care where the data came from or what the underlying structure should be. It only does what you ask of it, and will tell you about group differences whether they come from a randomized experiment or not. For example, if you don’t include features that would have a say in how the treatment comes about (confounders), you could get a biased estimate of the effect3. Basic linear regression also cannot tell you whether X is the effect of Y or vice versa. As such, linear regression by itself cannot save us from the difficulties of causal inference, nor really be considered a causal model. But it can be useful as a starting point in conjunction with other approaches.\n\n\n\n\n\n\nWeighting and Sampling Methods\n\n\n\n\n\nCommon techniques for traditional statistical models used for causal inference include a variety of weighting or sampling methods. These methods are used to adjust the data so that the ‘treatment’ groups are more similar, and its effect can be more accurately estimated. Sampling methods include techniques such as stratification and matching, which focus on the selection of the sample as a means to balance treatment and control groups. Weighting methods include inverse probability weighting and propensity score weighting, which focus on adjusting the weights of the observations to make the groups more similar.\nThese methods are not models themselves, and potentially can be used with just about any model that attempts to estimate the effect of a treatment. An excellent overview of using such methods vs. standard regression/ML can be found on Cross Validated (https://stats.stackexchange.com/a/544958).\n\n\n\n\n\n11.5.2 Graphical models & structural equation models\nGraphical and Structural Equation Models (SEM) are flexible approaches to regression and classification (see Figure 11.1), and have one of the longest histories of formal statistical modeling, dating back over a century4. They are widely employed in the social sciences, and are often used to model both observed and latent variables (Section 10.9), with either serving as features or targets. They are also used to model causal relationships, to the point that historically they were even called ‘causal graphical models’ or ‘causal structural models’. SEMs are a special case of graphical models, which are common tools in computer science and non-social science disciplines.\nFormal graphical models like SEM provide a much richer set of tools for controlling various confounding, interaction, and indirect effects than simpler linear models. For this reason, they can be very useful for causal inference. Unfortunately for those looking for causal effects, the basic input for SEM is a correlation matrix, and the basic output is a correlation matrix. Insert your favorite modeling quote here - you know which one! The point is that SEM, like linear regression, can no more tell you whether a relationship is causal than the linear regression or t-test could5.\n\n\n\n\n\n\nCausal Language\n\n\n\n\n\nIt’s often been suggested that we keep certain phrasing (e.g. feature X has an effect on target Y) only for the causal model setting. But the model we use can only tell us that the data is consistent with the effect we’re trying to understand, not that it actually exists. In everyday language, we often use causal language whenever we think the relationship is or should be causal, and that’s fine, and we think that’s okay in a modeling context too, as long as you are clear about the limits of your generalizability.\n\n\n\n\n\n11.5.3 Counterfactual thinking\n\nWhen we think about causality, we really ought to think about counterfactuals. What would have happened if I had done something different? What would have happened if I had done something sooner rather than later? What would have happened if I had done nothing at all? It’s natural to question our own actions in this way, but we can think like this in a modeling context too. In terms of our treatment effect example, we can summarize counterfactual thinking as:\n\nThe question is not whether there is a difference between A and B but whether there would still be a difference if A was B and B was A.\n\nThis is the essence of counterfactual thinking. It’s not about whether there is a difference between two groups, but whether there would still be a difference if those in one group had actually been treated differently. In this sense, we are concerned with the potential outcomes of the treatment, however defined.\nHere is a more concrete example:\n\nRoy is shown ad A, and buys the product.\nPris is shown ad B, and does not buy the product.\n\nWhat are we to make of this? Which ad is better? A seems to be, but maybe Pris wouldn’t have bought the product if shown that ad either, and maybe Roy would have bought the product if shown ad B too! With counterfactual thinking, we are concerned with the potential outcomes of the treatment, which in this case is whether or not to show the ad.\nLet’s say ad A is the new one, i.e., our treatment group, and B is the status quo ad, our control group. Without randomization, our real question can’t be answered by a simple test of whether means or predictions are different among the two groups, as this estimate would be biased if the groups are already different in some way to start with. The real effect is whether, for those who saw ad A, what the difference in the outcome would be if they hadn’t seen it.\nFrom a prediction stand point, we can get an initial estimate straightforwardly. We demonstrated this before in Section 3.3.4, but can revisit it briefly here. For those in the treatment, we can just plug in their feature values with treatment set to ad A. Then we just make a prediction with treatment set to ad B6.\n\nPythonR\n\n\n\nmodel.predict(X.assign(treatment = 'A')) - \n    model.predict(X.assign(treatment = 'B'))\n\n\n\n\npredict(model, X |&gt; mutate(treatment = 'A')) - \n    predict(model, X |&gt; mutate(treatment = 'B'))\n\n\n\n\nWith counterfactual thinking explicitly in mind, we can see that the difference in predictions is the difference in the potential outcomes of the treatment. This is a very simple demo to illustrate how easy it is to start getting some counterfactual results from our models. But it’s typically not quite that simple in practice, and there are many ways to get this estimate wrong as well. As in other circumstances, the data, or our assumptions about the problem can potentially lead us astray. Assuming those aspects of our modeling endeavor are in order, this is one way to get an estimate of the causal effect of the treatment.\n\n\n11.5.4 Uplift modeling\nThe counterfactual prediction we just did provides a result that can be called the uplift or gain from the treatment, especially when compared to a baseline metric. Uplift modeling is a general term applied to models where counterfactual thinking is at the forefront, especially in a marketing context. Uplift modeling is not a specific model per se, but any model that is used to answer a question about the potential outcomes of a treatment. The key question is what is the gain, or uplift, in applying a treatment vs. not? Typically any statistical model can be used to answer this question, and often the model is a classification model, whether Roy both the product or not.\nIt is common in uplift modeling to distinguish certain types of individuals or instances, and we think it’s useful to extend this to other modeling contexts as well. In the context of our previous example they are:\n\nSure things: those who would buy the product whether or not shown the ad.\nLost causes: those who would not buy the product whether or not shown the ad.\nSleeping dogs: those who would buy the product if not shown the ad, but not if they are shown the ad. Also referred to as the ‘Do not disturb’ group!\nPersuadables: those who would buy the product if shown the ad, but not if not shown the ad.\n\nWe can generalize beyond the marketing context to just think about response to any treatment we might be interested in. It’s worthwhile to think about which aspects of your data could correspond to these groups. One of the additional goals in uplift modeling is to identify persuadables for additional treatment efforts, and to avoid wasting money on the lost causes. But to get there, we have to think causally first!\n\n\n\n\n\n\nUplift Modeling in R and Python\n\n\n\n\n\nThere are more widely used tools for uplift modeling and meta-learners in Python than in R, but there are some options in R as well. In Python you can check out causalml and sci-kit uplift for some nice tutorials and documentation.\n\n\n\n\n\n11.5.5 Meta-Learning\nMeta-learners are used in machine learning contexts to assess potentially causal relationships between some treatment and outcome. The core model can actually be any kind you might want to use, but in which extra steps are taken to assess the causal relationship. The most common types of meta-learners are:\n\n\nS-learner - single model for both groups; predict the (counterfactual) difference as when all observations are treated vs when all are not, similar to our code demo above.\nT-learner - two models, one for each treatment group; predict the difference as when all observations are treated vs when all are not for both models, and take the difference\nX-learner - a more complicated modification to the T-learner also using a multi-step approach. \n\nSome additional variants of these models exist, and they can be used in a variety of settings, not just uplift modeling. The key idea is to use the model to predict the potential outcomes of the treatment, and then to take the difference between the two predictions as the causal effect.\n\n\n\n\n\n\nMeta-Learners vs. Meta-Analysis\n\n\n\n\n\nMeta-learners are not to be confused with meta-analysis, which is also related to understanding causal effects. Meta-analysis attempts to combine the results of multiple studies to get a better estimate of the true effect. The studies are typically conducted by different researchers and in different settings. The term meta-learning has also been used to refer to what is more commonly called ensemble learning, the approach used in random forests and boosting. It is also used by other people that don’t bother to look things up before naming their technical terms.\n\n\n\n\n\n11.5.6 Others models used for causal inference\nNote that there are many models that would fall under the umbrella of causal inference, and several that are discipline specific, but really are only a special application of some of the ones we’ve already seen. A few you might come across:\n\nG-computation, doubly robust estimation, targeted maximum likelihood estimation7\nMarginal structural models\nInstrumental variables and two-stage least squares\nPropensity score matching/weighting\nRegression discontinuity design\nMediation/moderation analysis\nMeta-analysis\nBayesian networks\n\nIn general, any modeling technique might be employed as part of a causal modeling approach. To actually make causal statements, you’ll generally need to ensure that the assumptions for those claims are tenable.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#causal-wrap",
    "href": "causal.html#causal-wrap",
    "title": "11  Causal Modeling",
    "section": "11.6 Wrapping Up",
    "text": "11.6 Wrapping Up\nWe’ve been pretty loose in our presentation here, and glossed over many details with causal modeling. Our main goal is to give you some idea of the domain, but more so the models used and things to think about when you want to answer a causal question with your data.\nModels used in statistical analysis and machine learning are not causal models, but when we take a causal model from the realm of ideas and apply it to the real world, a causal model becomes a statistical/ML model with more assumptions, and with additional steps taken to address those assumptions8. These assumptions are required in order to make stronger causal statements, but neither the assumptions, data, nor model can prove that the underlying theory is causally correct. Things like random assignment, sampling, a complex model and good data can possibly help the situation, but they can’t save you from a fundamental misunderstanding of the problem, or data that may still be consistent with that misunderstanding. Nothing about employing a causal model inherently makes better predictions either.\nCausal modeling is hard, and most of the difficulty lies outside of the realm of models and data. The model implemented reflects the causal theory, which can be a correct or incorrect idea about how the world works. In the end, the main thing is that when we want to make causal statements, we’ll make do with what data we have, and be careful that we rule out some of the other obvious explanations and issues. The better we can control the setting, or the better we can do things from a modeling standpoint, the more confident we can be in making causal claims. Causal modeling is an exercise in reasoning, which makes it such an interesting endeavor.\n\n11.6.1 The common thread\nEngaging in causal modeling may not even require you to learn any new models, but you will typically have to do more to be able to make causal statements. The key is to think about the problem in a different way, and to be more careful about the assumptions you are making. You may need to do more to ensure that your data and model are consistent with the assumptions you are making.\n\n\n11.6.2 Choose your own adventure\nFrom here you might revisit some of the previous models and think about how you might use them to answer a causal question. You might also look into some of the other models we’ve mentioned here, and see how they are used in practice via the additional resources below.\n\n\n11.6.3 Additional resources\nWe have only scratched the surface here, and there is a lot more to learn. Here are some resources to get you started:\n\nCausal Inference in R Barrett, McGowan, and Gerke (2024)\nCausal Inference The Mixtape Cunningham (2023)\nCausal Inference for the Brave and True Facure Alves (2022)\nApplied Causal Inference Powered by ML and AI Chernozhukov et al. (2024)\nThe C-Word Hernán (2018)",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#causal-exercise",
    "href": "causal.html#causal-exercise",
    "title": "11  Causal Modeling",
    "section": "11.7 Exercise",
    "text": "11.7 Exercise\nIf you look into causal modeling, you’ll find mention of problematic covariates such as colliders or confounders. Can you think of a way to determine if something is a collider or confounder that would not involve a statistical approach or model?\n\n\n\n\nBarrett, Malcolm, Lucy D’Agostino McGowan, and Travis Gerke. 2024. Causal Inference in R. https://www.r-causal.org/.\n\n\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis. 2024. “Applied Causal Inference Powered by ML and AI.” arXiv. http://arxiv.org/abs/2403.02467.\n\n\nCunningham, Scott. 2023. Causal Inference The Mixtape. https://mixtape.scunning.com/.\n\n\nFacure Alves, Matheus. 2022. “Causal Inference for The Brave and True — Causal Inference for the Brave and True.” https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHernán, Miguel A. 2018. “The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data.” American Journal of Public Health 108 (5): 616–19. https://doi.org/10.2105/AJPH.2018.304337.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "causal.html#footnotes",
    "href": "causal.html#footnotes",
    "title": "11  Causal Modeling",
    "section": "",
    "text": "Note that experimental design is not just any setting that uses random assignment, but more generally how we introduce control in the sample settings.↩︎\nMany experimental design settings involve sometimes very small samples due to the cost of the treatment implementation and other reasons. This often limits exploration of more complex relationships (e.g. interactions), and it is relatively rare to see any assessment of performance generalization. It would probably worry many to know how many experimental results are based on p-values with small data, and this is the part of the problem seen with the replication crisis in science.↩︎\nA reminder that a conclusion of ‘no effect’ is also a causal statement, and can be just as biased as any other statement. Also, you can come to the same practical conclusion with a biased estimate as with an unbiased one.↩︎\nWright is credited with coming up with what would be called path analysis in the 1920s, which is a precursor to and part of SEM.↩︎\nYour authors have to admit some bias here. We’ve spent a lot of our past dealing with SEMs, and almost every application we saw had too little data and too little generalization, and were grossly overfit. Many SEM programs even added multiple ways to overfit the data even further, and it is difficult to trust the results reported in many papers that used them. But that’s not the fault of SEM in general- it can be a useful tool when used correctly, and it can help answer causal questions, but it is not a magic bullet, and it doesn’t make anyone look fancier by using it.↩︎\nThis is basically the S-Learner approach to meta-learning, which we’ll discuss in a bit. It is generally too weak↩︎\nThe G-computation approach and S-learners are essentially the same approach, but came about from different domain contexts.↩︎\nGentle reminder that making an assumption does not mean the assumption is correct, or even provable.↩︎",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Modeling</span>"
    ]
  },
  {
    "objectID": "danger_zone.html",
    "href": "danger_zone.html",
    "title": "12  Danger Zone",
    "section": "",
    "text": "12.1 Linear Models and Related Statistical Endeavors\nStatistical models are a powerful tool for understanding the relationships between variables in a dataset. They are also excellent at helping us to understand the uncertainty in our data and the aspects of the model we wish to estimate. However, there are many ways in which statistical models can be misused.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Danger Zone</span>"
    ]
  },
  {
    "objectID": "danger_zone.html#sec-danger-linear",
    "href": "danger_zone.html#sec-danger-linear",
    "title": "12  Danger Zone",
    "section": "",
    "text": "12.1.1 Statistical significance\nOne of the most common mistakes when conducting statistical linear models is simply relying too heavily on the statistical result. Statistical significance is simply not enough to determine feature importance or model performance. When complex statistical models are applied to small data, the results are typically very noisy and statistical significance can be misleading. This also means that ‘big’ effects can be a reflection of that noise, rather than something meaningful.\nFocusing on statistical significance can lead you down other dangerous paths. For example, relying on statistical tests of assumptions instead of visualizations or practical metrics can lead you to believe that your model is valid when it is not. Using a statistical testing approach to select features can often result in incorrect choices about feature contributions, as well as poorer models.\nA related issue is p-hacking, which occurs when you try many different models, features, or other aspects of the model until you find one that is statistically significant. This is a problem because it can lead to spurious results, and can make it difficult to generalize the results of the model (overfitting). It also means you ignored null results, which can be just as informative as significant ones, a problem known as the file drawer problem.\n\n\n12.1.2 Ignoring complexity\nWhile techniques like standard linear/logistic regression and GLMs are valid and very useful, for many modeling contexts they may be too simplified to capture the complexity of the data generating process. This can lead to underfitting, where the model is too simple to capture the underlying structure of the data.\nOn the other side of the coin, many applications of statistical models ignore model assessment on a separate dataset, which can lead to overfitting. This makes generalization of statistical and other results more problematic. Such applications typically use a single model as well, and so may not be indicative of the best approach that could be taken.\n\n\n12.1.3 Using outdated techniques\nIf you wanted to go on a road trip, would you prefer a 1973 Ford Pinto or a Tesla Model S? If you want to browse the web, would you prefer to use a computer from the 90s and 56k modem, or a modern laptop with a high-speed internet connection? In both cases, you could potentially get to your destination or browse the web, but the experience would be much different, and you would likely have a clear preference1. The same goes with the models you use for your data analysis.\nThis is not specific to the statistical linear modeling realm, but there are many applications of statistical models that rely on outdated techniques, metrics, or other tools that solve problems that don’t exist anymore. For example, using stepwise/best subset regression for feature selection is not really viable when more principled approaches like the lasso are available. Likewise, we can’t really think of a case where something like MANOVA/discriminant function analysis would provide the best answer to a data problem, or where a pseudo-R2 would be a metric that would help us understand a model better or make a decision about it.\nStatistical analysis has been around a long time, and many of the techniques that have been developed are still valid, useful, and very powerful. But some reflect the limitations of the time in which they were developed. Others were an attempt to take something that was straightforward for simpler settings (e.g. linear regression) and apply to settings where it doesn’t make sense (nonlinear, non-gaussian, etc.). Even when still valid, there may be better alternatives available now.\n\n\n12.1.4 Simpler is not necessarily more interpretable\nStandard linear models are often used because of their interpretability, but in many of these modeling situations, interpretability can be difficult to obtain without using the same amount of effort one would for more complex models. Many statistical/linear models employ interactions, or nonlinear feature-target relationships (e.g. GLM/GAMs). If your goal is interpretability, these settings can be as difficult to interpret as features in a random forest. They still have the added benefit of more reliable uncertainty estimation, but you should not assume you will have a result as simple as a coefficient in a linear regression just because you didn’t use a deep learning model.\n\n\n12.1.5 Model comparison\nWhen comparing models, especially in the statistical modeling realm, many will use a statistical test to compare them. An example would be using an ANOVA or likelihood ratio test to compare a model with and without interactions. Unfortunately this doesn’t actually tell us how the models perform under realistic settings, and it comes with the usual statistical significance issues, like using an arbitrary threshold for claiming statistical significance. You could basically claim that one terrible model is statistically better than another terrible model, but there isn’t much value in that.\nSome like to look at R2 to compare models2, but it has a lot of problems. People think it’s more interpretable than other options, yet there is no value of ‘good’ you can universally apply, even in very similar scenarios. It can arbitrarily increase with the number of features whether they are actually predictive or not, and it doesn’t tell you how well the model will perform on new data. It can also simply reflect that you have time-series data, as you are just witnessing spurious correlations over time. In short, you can use it to get a sense of how your predictions correlate with the target, but that is fairly limited.\nThe following plot shows 250 simulations with a sample size of 500 and 250 completely meaningless features. The R2 values would all suggest the model is somewhat useful with an average of .5. The adjusted R^2 values average zero, which is correct, but they can only average that by being negative, which is a meaningless value. Many\n\n\n\n\n\nThe problem of R2\n\n\n\n\n\nOther commonly used metrics, like AIC, might be better in theory, but they approximate the model selection one would get through cross-validation, so why not just do the cross-validation? Furthermore, as long as you are using those metrics on the training data, you\n\n\n\n\n\n\nGarden of Forking Paths\n\n\n\nA common issue in statistical and machine learning modeling is the garden of forking paths. This is the idea that there are many different ways to analyze a dataset, and that the results of these analyses can be very different. When you don’t have a lot of data, or when the data is complex and the data generating process is not well understood, there can be a lot of forks that lead to many different models with varying results. In these cases, the interpretation of a single model from the many that are actually employed can be misleading, and can lead to incorrect conclusions about the data.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Danger Zone</span>"
    ]
  },
  {
    "objectID": "danger_zone.html#sec-danger-estimation",
    "href": "danger_zone.html#sec-danger-estimation",
    "title": "12  Danger Zone",
    "section": "12.2 Estimation",
    "text": "12.2 Estimation\n\n12.2.1 What if I just tweak this…\nFrom traditional statistical models to deep learning, the more you know about the underlying modeling process, the more apt you are to tweak some aspect of the model to try and improve performance. When you start thinking about changing optimizer options, link/activation functions, learning rates, etc., you can easily get lost in the weeds. This would be okay if you knew ahead of time it would make a big difference. However, in many, or maybe even most cases, this sort of tweaking doesn’t improve model results, or there are ways to not have to make the choice in the first place. More to the point, if this sort of estimation parameter tweaking does make a notable difference, that probably means you have a bigger problem with your model architecture or data.\nUnless you are using a bleeding edge technique, much of the work has been done for you by folks who had a lot more time to work on these aspects of the model. As such, spending a lot of time trying to decide on ML vs. REML, which variant of an Adam optimizer to use, learning rate of 1e-3 vs. 1e-4, 5 vs. 10 fold CV, or whether you should use ReLU, GELU, or Swish activations, a dropout rate of .1 or .2, etc., a lot of these choices won’t matter, or shouldn’t if more important aspects of your model and data are in order.\n\n\n12.2.2 Everything is fine\nThere is a flip side to the previous point, and that is that many assume that the default settings for complex models are good enough. We all do this when venturing into the unknown, but this is not always the case. Many of the more complex models have defaults geared toward a ‘just works’ setting rather than a ‘production’ setting. For example, the default number of boosting rounds for xgboost will rarely be adequate3.\n\n\n12.2.3 Just bootstrap it!\nWhen it comes to uncertainty estimation, many common modeling tools leave that to the user, and when the developers are pressed on how to get uncertainty estimates, they often will suggest to just bootstrap the result. While the bootstrap is a powerful tool for inference, it isn’t appropriate just because you decide to use it. The suggestion to use bootstrapping is often made in the context of a complex modeling situation where it would be very (prohibitively) computationally expensive, and in other cases the properties of the results are not well understood. Other methods of prediction inference, such as conformal prediction, may be better suited to the task. In general, if a package developer suggests you bootstrap because their package doesn’t have any means of uncertainty estimation, you should be wary.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Danger Zone</span>"
    ]
  },
  {
    "objectID": "danger_zone.html#sec-danger-ml",
    "href": "danger_zone.html#sec-danger-ml",
    "title": "12  Danger Zone",
    "section": "12.3 Machine Learning",
    "text": "12.3 Machine Learning\n\n12.3.1 General ML modeling issues\nWe see a lot of issues with machine learning approaches, and many of them are the same as those that come up with statistical models, but some are more unique to the machine learning world. A starting point is that many forget to create a baseline model, and instead jump right into a complicated model. This is a problem because it is hard to improve performance if you don’t know what a good baseline score is. So create that baseline model and iterate from there.\nA related point is that many will jump into machine learning models without fully investigating the data. Standard exploratory data analysis (EDA) is a prerequisite for any modeling, and can go a long way toward saving time and effort in the modeling process. It’s here you’ll find problematic cases and features, and can explore ways to deal with it.\nWhen choosing a model or set of models, one should have a valid reason for the choice. Some less stellar reasons include using a model just because it seems popular, or just because it’s the only one you know. In addition, as mentioned with other types of models, you want to avoid using older methods that really don’t perform well in most situations compared to others4.\n\n\n12.3.2 Classification\nMachine learning is not synonymous with a classification problem, but this point seems to be lost on many. As an example, many will split their target just so they can do classification, when the target is a more expressive continuous variable. This is a problem because you are unnecessarily diminishing the reliability of the target score, and losing information about it. This can lead to a well known statistical issue - attenuation of the correlation between variables.\n\n\n12.3.2.0.1 R\n\nsimulate_binarize = function(N = 1000, correlation = .5, num_simulations = 100, bin_y_only = FALSE) {\n    correlations = list()\n    \n    for (i in 1:num_simulations) {\n        # Simulate two variables with the given correlation\n        \n        xy = MASS::mvrnorm(\n            n = N, \n            mu = c(0, 0), \n            Sigma = matrix(c(1, correlation, correlation, 1), \n            nrow = 2),\n            empirical = FALSE\n        )\n\n        # binarize on median split\n        \n        if (bin_y_only) {\n            x_bin = xy[, 1]\n        } else {\n            x_bin = ifelse(xy[, 1] &gt;= median(xy[, 1]), 1, 0)\n        }\n        \n        y_bin = ifelse(xy[, 2] &gt;= median(xy[, 2]), 1, 0)\n        \n        raw_correlation = cor(xy[, 1], xy[, 2])\n        binarized_correlation = cor(x_bin, y_bin)\n        \n        correlations[[i]] = tibble(\n            sim = i,\n            raw_correlation,\n            binarized_correlation\n        )\n    }\n\n    cors =  bind_rows(correlations)\n    cors\n}\n\nsimulate_binarize(correlation = .25, num_simulations = 5)\n\n\n\n12.3.2.0.2 Python\n\nimport numpy as np\nimport pandas as pd\n\ndef simulate_binarize(N = 1000, correlation = .5, num_simulations = 100, bin_y_only = False):\n    correlations = []\n    \n    for i in range(num_simulations):\n        # Simulate two variables with the given correlation\n        xy = np.random.multivariate_normal(\n            mean = [0, 0], \n            cov = [[1, correlation], [correlation, 1]], \n            size = N\n        )\n\n        # binarize on median split\n        if bin_y_only:\n            x_bin = xy[:, 0]\n        else:\n            x_bin = np.where(xy[:, 0] &gt;= np.median(xy[:, 0]), 1, 0)\n        y_bin = np.where(xy[:, 1] &gt;= np.median(xy[:, 1]), 1, 0)\n        \n        raw_correlation = np.corrcoef(xy[:, 0], xy[:, 1])[0, 1]\n        binarized_correlation = np.corrcoef(x_bin, y_bin)[0, 1]\n        \n        correlations.append({\n            'sim': i,\n            'raw_correlation': raw_correlation,\n            'binarized_correlation': binarized_correlation\n        })\n\n    cors = pd.DataFrame(correlations)\n    return cors\n\nsimulate_binarize(correlation = .25, num_simulations = 5)\n\n\n\nThe following plot shows the case where we only binarize the target variable for 500 simulations. The correlation between the raw and binarized variables is .25, .5, or .75, but the attenuated correlation in the binarized case is still notable. This is because the binarization process has removed the correlation between the variables.\n\n\n\n\n\n\nFigure 12.1: Density plots of raw and binarized correlations\n\n\n\nCommon issues with ML classification don’t end here however. Another problem is that many will use a simple .5 cutoff for binary classification, when it is probably not the best choice in most classification settings. Related to this, many only focus on accuracy as a metric for performance. Others are more useful in many situations, or just add more information to assess the model. Each metric has its own pros and cons, so you should evaluate your model’s performance with a suite of metrics.\n\n\n12.3.3 Ignoring uncertainty\nIt is very common in ML practice to ignore uncertainty in your predictions or metrics. This is a problem because there is always uncertainty, and acknowledging that it is there will be helpful for mitigating performance expectations. This is especially true when you are using a model in a production setting, where the model’s performance can have real-world consequences.\nIt is difficult, and often computationally hard to get uncertainty estimates for many of the black-box techniques that are popular in ML. Some might suggest that there is enough data such that uncertainty is not needed, but this would have to be demonstrated in some fashion, and there is always increased uncertainty for prediction on new data. In general, there are ways to get uncertainty estimates for these models, e.g., bootstrapping, conformal prediction, and it is worth the effort to do so.\n\n\n12.3.4 Hyperfocus on feature importance\nResearchers and businesses often have questions about which features in an ML model are important. Yet this can be a difficult question to answer, and the answer is often not practically useful. For example, most models used in ML are going to have interactions, so the importance of a feature is going to be dependent on the other features in the model. If you can’t disentangle the effects of one feature from another, where either may or may not be important on their own, or maybe neither is predictive except in the presence of the other, then trying to talk about a single feature’s relative worth is often a misguided endeavor, even if you use an importance metric that purports to account for the interaction.\nEven if we can deem a variable ‘important’, this doesn’t imply a causal relationship, and it doesn’t mean that the variable is the best of the features you have. In addition, other metrics, which might be just as valid, may provide a different rank ordering of importance.\nWhat’s more, just because an importance metric may deem a feature as not important, that doesn’t mean it has no effect on the target. It may be that the feature is correlated with other features that are more important, and so the importance metric is just reflecting that. It may also just mean that the importance metric is not well suited to assessing that particular feature’s contribution.\nThe reality is that multiple valid measures of importance can come to different conclusions about the relative importance of a feature, even within the same model setting. One should be very cautious in how they interpret these.\n\n\n\n\n\n\nSHAP for Feature Importance\n\n\n\nSHAP values are meant to assess local, i.e. observation level, feature contributions to a prediction. Although used as such, they are not meant to be a metric for assessing global feature importance. Because of this they can be misleading, and often average SHAP values will just reflect the distribution of the feature more than its importance, and could be notably inconsistent with other metrics even in simple settings.\n\n\n\n\n12.3.5 Other common pitfalls\nA few other common pitfalls in ML modeling include:\n\nForgetting that the data is more important than your modeling technique. You will almost always get more mileage out of improving your data than you will out of improving your model.\nData Leakage: Letting training data leak into the test set. As a simple example, consider if we use random validation splits with time-based data. This would allow the model to train with future data it will ultimately be assessed on. That may be an obvious example, but there are many more subtle ways this can happen. Data leakage gives your model an unfair advantage when it is time for testing, leading you to believe that your model is doing better than it really is.\nForgetting you will ultimately need to be able to explain your model to someone else. The only good model is a useful one; if you can’t explain it to someone, you can’t expect others to trust you with it or your results.\nAssuming that grid search is good enough for all or even most cases. Not only is it computationally expensive, but you can easily miss valid tuning parameter values that are outside of the grid. Many other methods are available that more efficiently search the space and are as easy to implement.\nThinking deep learning will solve all your problems. If you are dealing with standard, tabular data, i.e. very noisy, deep learning will often just increase computational complexity and time, with no guarantee of increased performance. Hopefully this will change in the future, but for now, you should not expect major performance gains.\nComparing models on different datasets. If you run different models on separate data, there is no objective way to compare them. As an example, the accuracy may be higher on one dataset just because the baseline rate is much higher.\n\n\nThe list goes on. In short, many of the pitfalls in ML modeling are the same as those in statistical modeling, but there are some unique to the ML world. The most important thing to remember is that due diligence is key when conducting any modeling exercise, and ML doesn’t change that. You should always be able to explain your model and code to someone else, that you should always be able to defend your choices, and that you should always be able to defend your choices.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Danger Zone</span>"
    ]
  },
  {
    "objectID": "danger_zone.html#sec-danger-data",
    "href": "danger_zone.html#sec-danger-data",
    "title": "12  Danger Zone",
    "section": "12.4 Data",
    "text": "12.4 Data\nWhen it comes to data, plenty can go wrong before even starting with any modeling attempt. Let’s take a look at some issues that can regularly arise.\n\n12.4.1 Transformations\nMany models will fail miserably without some sort of scaling or transformation of the data. A few techniques, like tree-based approaches, do not benefit, but practically all others do. At the very least, models will converge faster and possibly be more interpretable. You should not use transformations that would lose the expressivity of the data, because as we noted with binarization (Section 12.3.2), some can do more harm than good. But you should always consider the need for transformations, and not just assume that the data is in a form that is ready for modeling.\n\n\n12.4.2 Simple imputation techniques\nImputation may be required when you have missing data, but it can be done in ways that don’t help your model. Simple imputation techniques, like using the mean or modal category, can produce faulty, or at best, noisier, results. First you should consider why you want to keep a feature that doesn’t have a lot of data - do you even trust the values that are present? If you really need to impute, use an actual model to do so, but recognize that the resulting value has uncertainty associated with it. There are practical problems with implementing multiple-imputation Section 10.3.4, so there is no free lunch when estimating that uncertainty. But at least having a better imputation model will provide a better guess than a mean, and still better is to use a model that would handle the missing values natively, like tree-based methods that can split on the missingness.\n\n\n12.4.3 Outliers are real!\nOne common practice in modeling is to drop or modify values considered as “outliers”. However, extreme values in the target variable are often a natural part of the data. Assuming there is no actual error in recording them, often, a simple transformation can address the issue. If extremes persist after modeling, it indicates that the model is unable to capture the underlying data structure, rather than an inherent problem with the data itself. Additionally, even values that may not appear extreme can still have large residuals, so it’s important not to solely focus on the most extreme raw values.\nIn terms of features, extreme values can cause strange effects, but often they reflect a data problem (e.g. incorrect values), or can be resolved using the transformations you should already be considering. In other cases, they don’t really cause any modeling problems at all. And again, some techniques are fairly robust to feature extremes, like tree-based methods.\n\n\n12.4.4 Big data isn’t always as big as you think\nConsider a model setting with 100,000 samples. Is this large? Let’s say you have a rare outcome that occurs 1% of the time. This means you have 1000 samples where outcome label of interest occurs. Now consider a categorical feature (A) that has four categories, and one of those categories is relatively small, say 5% of the data, or 5000 cases, and you want to interact it with another categorical feature (B), one whose categories are all equally distributed. Assuming no particular correlation between the two, you’d be down to ~1% of the data for the least category of A across the levels of B. Now if there is an actual interaction, some of those interaction cells may have only a dozen or so positive target values. Odds are pretty good that you don’t have enough data to make a reliable estimate of the interaction effect.\nOh wait, did you want to use cross-validation also? A simple random sample approach might result in some validation sets with no positive values at all! Don’t forget that you may have already split your 100,000 samples into training and test sets, so you have even less data to start with! The following table shows the final cell count for a dataset with these properties.\nThe point is that it’s easy to forget that large data can get small very quickly due to class imbalance, interactions, etc. There is not much you can do about this, but you should not be surprised when these situations are not very revealing in terms of your model results.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Danger Zone</span>"
    ]
  },
  {
    "objectID": "danger_zone.html#sec-danger-causal",
    "href": "danger_zone.html#sec-danger-causal",
    "title": "12  Danger Zone",
    "section": "12.5 Causal Inference",
    "text": "12.5 Causal Inference\nCausal inference and modeling is hard. Very hard.\n\n12.5.1 The hard work is done before data analysis\nThe most important part of causal modeling is the conceptualization of the problem and the general design of the study to answer the specific questions related to that problem. You have to think very hard about the available data, what variables may be confounders, which effects may be indirect, and many other aspects of the process you want to measure. A causal model is the one you draw up, possibly before you even start collecting data, and it is the one you use to guide your data collection and ultimately your data modeling process.\n\n\n12.5.2 Models can’t prove causality\nCausal modeling focuses on addressing issues like confounding, selection bias, and measurement error, which can skew interpretations about cause and effect. While predictive accuracy is key in some scenarios, understanding these issues is crucial for making valid causal claims.\nA common mistake in modeling is assuming that a model can prove causality. You can have a very performant model, but the model results cannot prove that one variable causes another just because it is highly predictive. There is also nothing in the estimation process that can magically extract a causal relationship even if it exists. Reality is even more complex than our models, and no model can account for every possibility. Causal modeling attempts to account for some of these possibilities, but it is limited by our own biases in thinking about a particular problem.\nPredictive features in a model might suggest a causal link, act as stand-ins for one, or merely reflect spurious associations. Conversely, true causal effects of a feature may not be large, but it doesn’t mean they are unimportant. Assuming you have done the hard work of developing the causal structure beforehand, model results can provide more confidence in your ultimate causal conclusions, and that is very useful, despite lingering uncertainties.\n\n\n12.5.3 Random assignment is not enough\nMany believe experimental design is the gold standard for making causal claims, and it is certainly a good way to control for various aspects that can make causal claims difficult. Consider a randomized control trial (RCT) where you randomly assign people to a treatment or control group. The left panel shows the overall treatment effect, where the main effect would suggest a causal conclusion of no treatment effect. However, the right panel shows the treatment effect by another group factor, and it is clear that the treatment effect is not the same across groups.\n\n\n\nMain effect vs. interaction effect of treatment group\n\n\nSo random assignment cannot save us from misunderstanding the causal mechanisms at play. Other issues to think about are that the treatment may be implemented poorly, participants may not be compliant, or the treatment may not even be well defined, and these are not uncommon situations. This comes back to having the causal structure understood as best you can before any analysis.\n\n\n12.5.4 Ignoring causal issues\nCausal modeling is concerned with things like confounding, selection bias, measurement error, reverse causality and more. These are all issues that can lead to incorrect causal conclusions. A lot of this can be ignored when predictive performance is of primary importance, and some can be ignored when we are not interested in making causal claims. But when you are interested in making causal claims, you will have some homework to learn and revisit these topics in order for your model to help you make said claims, regardless of the modeling technique you choose to implement.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Danger Zone</span>"
    ]
  },
  {
    "objectID": "danger_zone.html#sec-danger-conclusion",
    "href": "danger_zone.html#sec-danger-conclusion",
    "title": "12  Danger Zone",
    "section": "12.6 Wrapping Up",
    "text": "12.6 Wrapping Up\nThough we’ve covered many common issues in modeling here, there are plenty more ways we can trip ourselves up. The important thing to remember is that we’re all prone to making and repeating mistakes in modeling. But awareness and effort can go a long way, and we can more easily avoid these problems with practice. The main thing is to try and do better each time, and learn from any mistakes you do make.\n\n12.6.1 The common thread\nMany of the issues here are model agnostic and could creep into any modeling exercise you undertake.\n\n\n12.6.2 Choose your own adventure\nIf you’ve made it through the previous chapters, there’s only one place to go. But you might revisit some of those in light of the common problems we’ve discussed here.\n\n\n12.6.3 Additional resources\nMostly we recommend the same resources we did in the corresponding sections of the previous chapters. However, a couple others to consider are:\n\nShalizi (2015) (start with the fantastic concluding comment)\nQuestionable Practices in Machine Learning (Leech et al. 2024)\n\n\n\n\n\nLeech, Gavin, Juan J. Vazquez, Misha Yagudin, Niclas Kupper, and Laurence Aitchison. 2024. “Questionable Practices in Machine Learning.” arXiv. https://doi.org/10.48550/arXiv.2407.12220.\n\n\nShalizi, Cosma. 2015. “F-Tests, R2, and Other Distractions.” https://www.stat.cmu.edu/~cshalizi/mreg/15/.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Danger Zone</span>"
    ]
  },
  {
    "objectID": "danger_zone.html#footnotes",
    "href": "danger_zone.html#footnotes",
    "title": "12  Danger Zone",
    "section": "",
    "text": "Granted, if it was a Pinto wagon, the choice could be more difficult.↩︎\nAdjusted R2 doesn’t help, the same issues are present and it would not be any practically different than R2 except for very small data situations, where it might even be negative!↩︎\nThe number is actually dependent on other parameters, like whether early stopping is used, the number of classes, etc.↩︎\nAs we mentioned in the statistical section, many older methods are still valid and useful, but it’s not clear what would be gained by using things like a standard support vector machine or hidden markov model related to more recently developed or other techniques that have shown more flexibility.↩︎",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Danger Zone</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "13  Until Next Time…",
    "section": "",
    "text": "13.1 How to Think About Models\nWhen we first started our discussion of models in data science, we talked about how a model is a simplified representation of reality. They start as ideas based on our intuition or experience, and they can sometimes be very simple ones. But at some point we start to think of them more formally, as a step towards testing those ideas in the real world. For statistics, machine learning, and data science more generally, models are then put into mathematical equations that give us a common language to reference them by. This does not have to be complex though. As an example, most of the models you’ve seen so far can be expressed as follows:\nIn words, this equation says that the target variable \\(y\\) is a function of the feature inputs \\(X\\), along with anything else that we don’t include in that set. This is the basic form of a model, and it’s the same for linear regression, logistic regression, and even random forests and neural networks1.\nTo aid our understanding beyond the math, we try to visually express models in the form of graphical models, or even in more complex ways with neural networks2, as in the following images.\nBut even now these models are still at the idea stage, and we ultimately need to see how they work in the world, make predictions, and help us to make important decisions. We’ve seen how to do this with linear models of various forms, and more unusual model implementations in the form of tree-based models, and even highly complex neural networks. These are the tools that allow us to take our ideas and turn them into something that can be used to make decisions, and that’s the real power of using models in data science.\nAt this point, we can break our thinking about models into the following components:\nModel\nIn data science, a model refers to a unique (mathematical) implementation we’re using to answer our questions. It specifies the architecture of the model, which might be a simple linear component, a series of trees, or neural network. In addition, the model specifies the functional form, the \\(f()\\) in our equation, that translates inputs to outputs, and the parameters required to make that transformation. In code, the model is implemented with functions such as lm in R, or in Python, an XGBoostClassifier or PyTorch nn.Model class.\nTask\nThe task can be thought of as the goal of our model, which might be defined as regression, classification, ranking, or next word prediction. It is closely tied to the objective or loss function, which is like a measure of correspondence between the model output and the target we’re trying to understand. The objective function provides the model a goal - minimize target-output discrepancy or maximize similarity. As an example, if our target is numeric and our task is ‘regression’, we can use mean squared error as an objective function, which provides a measure of the prediction-target discrepancy.\nAlgorithm\nVarious algorithms allow us to estimate the parameters of the model, typically in an iterative fashion, moving from one guess to a hopefully better one. This could be maximum likelihood, bayesian estimation, or stochastic gradient descent, or a specific implementation of these, such as penalized likelihood, hamilton monte carlo, or backpropagation.\nSo when we think about models, we start with an idea, but in the end it needs to be expressed in a form that suggests an architecture that can take in data and make outputs in the form of predictions or what can be amenable to such. With that in place, we need an algorithm that can obtain parameter estimates of the model, and a way to evaluate how well the model is doing. While this is enough, it only gets us the bare minimum of a result. There are many more things we have to do to help us interpret results, understand its performance, and get a sense of its limitations.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "conclusion.html#conc-models-think",
    "href": "conclusion.html#conc-models-think",
    "title": "13  Until Next Time…",
    "section": "",
    "text": "A generic model\n\n\n\n\n\n\n\nLogistic Regression Model\n\n\n\n\n\nPlate Notation for Latent Dirichlet Allocation\n\n\n\n\n\nJust part of the Nano GPT model",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "conclusion.html#key-steps-in-modeling",
    "href": "conclusion.html#key-steps-in-modeling",
    "title": "13  Until Next Time…",
    "section": "13.2 Key Steps in Modeling",
    "text": "13.2 Key Steps in Modeling\nWhen it comes to modeling, there are a few key steps that you should always keep in mind. These are not necessarily exhaustive, but we feel they’re a good way to think about how to approach modeling in data science.\nDefine the problem\nStart by clearly defining the problem you want to solve. It is often easy to express in very general terms, but it’s often challenging to pin down the problem statement more precisely, in a way that can actually help you solve it. What are you trying to predict? What data do you have to work with? What are the constraints on your data and model? What are the consequences of the results, whatever they may be? Why do you even care about any of this? These are all questions you should try to answer before diving into modeling.\nKnow your data well\nDuring our time consulting in industry and academia, we’ve seen many cases where the available data is simply not suited to answer the question at hand3. This leads to wasted time, money, and other resources. If the data doesn’t have the appropriate content you can’t possibly answer your question with it.\nIn addition, if your data is fraught with issues due to inadequate exploration, cleaning , or transformation, then you’re going to have a hard time getting valuable results. It is very common to be dealing with data that has issues that even those who collected it are unaware of, so always be looking out for ways to improve your data.\nHave multiple models at your disposal\nGo into a modeling project with a couple models in mind that you think might be useful. This could even be as simple as increasing complexity within a single model approach - you don’t have to get too fancy! You should have a few models that you’re comfortable with and that you know how to use, and for which you know the strengths and weaknesses. Whenever possible, make time to explore more complex or less familiar approaches that you also think may be suitable to the problem. As we’ve discussed (Section 3.2.3), model comparison can help you have more confidence in the results of the model that’s finally chosen. Just like in a lot of other situations, you don’t want to ‘put all your eggs in one basket’, and you’ll always have more to talk about and consider if you have multiple models to work with.\nCommunicate your results\nIf you don’t know the model and underlying data well enough to explain the results to others, you’re not going to be able to use them effectively in the first place. Conversely, you also may know the technical side very well, but if you’re unable to communicate the results in simpler terms that others can understand, you’re going to have a hard time convincing others of the value of your work.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "conclusion.html#more-models",
    "href": "conclusion.html#more-models",
    "title": "13  Until Next Time…",
    "section": "13.3 More Models",
    "text": "13.3 More Models\nWhen choosing a model, there’s a lot at your disposal. The world of data science is vast, and we’ve only scratched the surface of what’s out there. Here are a few more models that you may encounter in your data science journey:\nStatistical Models\nIn the statistical realm there are many more models that focus on different target distributions and types. For instance, we might use a beta distribution/likelihood for targets between 0 and 1, ordinal logistic regression for ordinal targets, or survival models for time-to-event outcomes. Some models are field-specific, like two-stage least squares in econometrics. Most of these models are essentially linear models with slight modifications.\nNonlinear models are another realm, which are a bit different from the nonlinear aspects of GLMs, GAMs, or deep learning. These models assume a specific (non-linear) functional form, and can be used to explore relationships that are not well captured by standard linear models. Examples range from something as simple as a polynomial regression or logistic growth model, to more complex biological and epidemiological models. These approaches are not as flexible as Generalized Additive Models (GAMs) or as predictive as neural networks, but they can potentially be useful in the right context.\nAs a final consideration, there are ‘multivariate’ techniques like Principal Component Analysis (PCA), factor analysis, and similar which are still pretty widely used. There are also cases where the primary target is multivariate in nature, meaning a standard regression with multiple outcomes. These are more common within some areas like economics and psychology.\nMachine Learning\nIn a purely machine learning context, you may find other models beyond those just mentioned in the statistical realm. These models prioritize prediction and would not produce standard statistical output like coefficients and uncertainty estimates by default. Examples include support vector machines, k-nearest neighbors, and other techniques. Most of these traditional ‘machine learning models’ have fallen out of favor due to their inflexibility with heterogeneous data or poor performance compared to more modern approaches. However, even then, their spirit may live on in modern applications.\nYou’ll also find models that focus on ranking, either with an outcome of ranks requiring a specific loss function (e.g. LambdaRank), or where ranking is used to simplify decision-making through post-estimation ranking of predictions (e.g., decile ranking, uplift modeling). In addition, you can find machine learning techniques extended to survival, ordinal, and other situations that are more common in the statistical realm.\nOther areas of machine learning, like reinforcement learning, recommender systems, and unsupervised learning provide additional models that can be used in various scenarios. Plenty is left for you to explore here as well!\nDeep Learning\nWhen it comes to deep learning, it seems there is a new model every day, and it’s hard to keep up. In general,convolutional neural networks are the go-to for computer vision tasks, while transformers are commonly used for natural language processing, but both have been applied to the other domain with success. For tabular data you’ll typically see some variant of Multilayer Perceptrons (MLPs), often with embeddings for categorical features. Some have attempted transformers and CNNs here as well, but results are mixed.\nThe deep learning landscape also includes models like deep graphical networks, and deep Q learning for reinforcement learning, specific models for image segmentation (e.g. SAM), recurrent neural networks and LSTM for time-series data, and generative adversarial networks for a variety of tasks. Some specific techniques are falling out of favor as transformer-based architectures are being applied to seemingly everything, but the field is dynamic, and it remains to be seen which methods will prevail in the long run.\n\n\n\n\n\n\n\nA List of Models\n\n\n\n\n\nYou can find a list of some specific models for each of these categories in the appendix (Appendix D). It is by no means an exhaustive list, but it should give you a good starting point for exploring additional models once you’re finished here.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "conclusion.html#conc-models-families",
    "href": "conclusion.html#conc-models-families",
    "title": "13  Until Next Time…",
    "section": "13.4 Families of Models",
    "text": "13.4 Families of Models\nThough there are many models out there, even if we restrict the discussion to tabular data, we can group them in a fairly simple way that would cover most of the standard problems you’ll come across.\n\nGLM and Related: Interpretable Insights\nHere we have standard linear models with a focus on interpretability. Basically anything you’d find in a traditional stats or econometrics textbook would belong to this ‘family’.\n\nIncludes: GLM, survival, ordinal, time-series, other distributions (beta, tweedie)\nBest for: small data situations (samples and features), a baseline model, a causal model, post-model analysis of the results from more complex models\nPrimary strength: ease of estimation, interpretability, uncertainty estimation\nPrimary weakness: relatively poor prediction, may not capture natural data complexity\n\nPenalized Regression and Friends: Predictive Progress\nThis family encompasses techniques that could be used as stepping stones towards machine learning. These include linear models enhanced with regularization, and advanced statistical models that deliberately incorporate nonlinearities and other complexities. Moreover, our emphasis begins to shift more towards prediction in this context, though these models still provide relatively easier interpretation compared to the next group.\n\nIncludes: lasso/ridge, mixed models, GAMs, Bayesian*\nBest for: small to large data, possibly a relatively large number of features (esp. lasso), baseline model\nPrimary strength: increased predictive capability while maintaining interpretability\nPrimary weakness: interpretability can decrease, estimation difficulty can start to arise (convergence issues, uncertainty)\n\nTrees & Nets: Technological Titans\nThis family includes tree-based models and neural networks, which are almost exclusively focused on predictive performance by default, and represent a significant increase in complexity and computational requirements.\n\nIncludes random forests, gradient boosting, neural networks (‘basis function models’)\nBest for: prediction/performance\nPrimary strength: prediction, ability to handle potentially very large data and numbers of features\nPrimary weakness: interpretability and uncertainty estimation\n\nThinking about families or groups of models can do a lot to help demystify the modeling process. You could come up with other schemas within a specific data domain or group of models, there’s no solid rule here. But it can be helpful to compartmentalize the models so that you don’t get overwhelmed by what are often minor details that won’t significantly impact the practical application.\nThe differences between the model families are not substantial, particularly between the first two. Specific models may only differ in the likelihood function, the penalty term, or just a shift in focus. The third group is a bit different, but it mostly just extends the application of nonlinear and interaction effects we can implement from the first groups, allowing for more computational capacity and flexibility. But if you’re new to modeling or dabbling in a new area, we think this grouping can quickly help you understand what you’re looking at and what you might want to use for a given problem. As you do more modeling, you’ll likely come up with your own.\n\n13.4.1 A simple modeling toolbox\nIn practice, just a handful of techniques of the ones you’ve seen in this text can provide a lot of modeling power. Here’s a simple toolbox that can cover a lot of the ground you’d need in a typical data science project:\n\nPenalized Regression: Lasso, ridge and similar keep it linear while increasing predictive power and accommodating more features.\nGeneralized Additive Models (GAM): These models simplify to GLM and mixed models if needed, handle nonlinear relationships and interactions, and use a penalized approach. They can also be extended to time-series and spatial data contexts with ease, making GAMs a very versatile option.\nBoosting/Tree-based Models: At the time of this writing, boosting methods consistently deliver the best predictive performance for tabular data, and are quite computationally efficient. That’s reason enough to know how to use them.\nA Basic Deep Learning Model (MLP): A Multilayer Perceptron (MLP), especially one that incorporates embeddings for categorical/text features, is a very powerful tool. It can be combined with other deep learning models applied to other types of data. We’re still working towards an implementation that can handle any tabular data we throw at it, but we’re not quite there yet.\n\nBesides the models, it’s crucial to understand how to evaluate your models (cross-validation, metrics), how to interpret them (coefficients, SHAP, feature importance, uncertainty), and how to manage the data you’re working with. We’ve covered a lot of this in the text, but there’s always more to learn, and more to practice.\n\n\n\n\n\n\nThe Tweener\n\n\n\n\n\nDespite getting an occasional shout-out, GAMs appear to still be quite underrated in the data science community, probably because the tools in Python to implement and explore them are relatively lacking4. On the plus side, the tools in R are excellent, though they can take some getting used to.\nGAMs are a great way to handle nonlinear relationships, interactions, and add penalization, all in a way that can be more interpretable than boosting or a neural network. They can be used in a wide variety of data situations, and can be a great way to get more predictive power while staying within a traditional linear model setting. If you’re looking for a way to get a bit more out of your linear models without diving into deep learning, GAMs are a great place to start, and are often a tough model to beat for those that know how to use them.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "conclusion.html#conc-models-how2choose",
    "href": "conclusion.html#conc-models-how2choose",
    "title": "13  Until Next Time…",
    "section": "13.5 How to Choose?",
    "text": "13.5 How to Choose?\nPeople love to say that ‘all models are wrong, but some are useful’5. We prefer to think of this a bit differently. There is no (necessarily) wrong model to use to answer your question, and there’s no guarantee that you would come to a different conclusion from using a simple correlation than you would from a complex neural network. But some models can be more useful depending on the context, and some are more useful depending on the question you’re asking.\nIn the end, nothing says you can’t use multiple models to answer your question, and in fact, this is often a good idea assuming you have the time and resources to do so. As we’ve talked about, you can use a simple model to get a baseline, and then use a more complex model to see if you can improve on that. You can use a model that’s easy to interpret to get a sense of what’s going on, and then use a model that’s better at prediction. Even when your primary focus is prediction, you can often combine models to potentially get a better result.\nAnd that’s the main thing, you don’t have to restrict yourself when it comes to modeling in data science, and you shouldn’t. The key is to understand what you’re trying to do, and to use the right tools for the job.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "conclusion.html#conc-models-hard",
    "href": "conclusion.html#conc-models-hard",
    "title": "13  Until Next Time…",
    "section": "13.6 The Hard Part",
    "text": "13.6 The Hard Part\n\nModeling is just one aspect of the data science process, and the hard part of that process is often not so much the model itself, but everything else that goes into it and what you do with it after. It can be difficult to come up with the original idea for a model, and even harder to get it to work in practice.\nThe Data\nModel performance is largely going to come from the quality of the data and how you’ve prepared it, from ensuring its integrity to feature engineering. Some models will definitely work better than others in certain situations, but there are no guarantees, and often the practical difference in performance is minimal. But you can potentially improve performance by understanding your data better, and by understanding the limitations of your model. Having more domain knowledge can help reduce noise and irrelevant information that you might have otherwise retained, and can provide insights for feature engineering. Thorough data exploration can reveal bugs and issues to be fixed, and will help you understand the relationships between your features and your target.\nThe Interpretation\nOnce you have a model, you need to understand what it’s telling you. This can be as simple as looking at the coefficients of a linear regression, or as complex as trying to understand the weights of a hidden layer in a neural network. Once you get past a linear regression though, expect model interpretation to get hard. But whatever model you use, you need to be able to explain what the model is doing, and how you’re ultimately coming to your conclusions. This can be difficult, and often requires a lot of work. Even if you’ve used a model often, it may still be difficult to understand in a new data environment. Model interpretation can take a lot of effort, but it’s important to do what’s necessary to trust your model results, and help others trust them as well.\nWhat You Do With It\nOnce you have the model and you (think you) understand it, you need to be able to use it effectively. If you’ve gone to this sort of trouble, you must have had a good reason for undertaking what can be a very difficult task. We use models to make business decisions, inform policy, understand the world around us, and to make our lives better. However, using a model effectively means understanding its limitations and the practical, ethical, scientific, and other consequences of the decisions you make based on it. It’s at this point that the real value of your model is realized.\nIn the end models are a tool to help you solve a problem. They do not solve the problem for you, and they do not absolve you of the responsibility of understanding the problem and the consequences of your decisions.",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "conclusion.html#conc-models-adventure",
    "href": "conclusion.html#conc-models-adventure",
    "title": "13  Until Next Time…",
    "section": "13.7 Choose Your Own Adventure",
    "text": "13.7 Choose Your Own Adventure\nWe’ve covered a lot of ground in this text, and we hope you’ve learned something new along the way. But there’s so much more to learn, and so much more to do. We hope that you’ll be able to take what you’ve learned here and apply it to your own work, and that you’ll continue to learn and grow as a data scientist.\nSo where do you go from here? The world of data science is vast and infinite. Choose your own adventure!\nMichael & Seth",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "conclusion.html#footnotes",
    "href": "conclusion.html#footnotes",
    "title": "13  Until Next Time…",
    "section": "",
    "text": "Neural networks are a bit different in that they can be thought of as a series of (typically nested) functions that are applied to the data, but they can still be expressed in this form. The functions are just more complex, and the parameters are estimated in a different way.↩︎\nThe LDA model depicted from Wikipedia was one of the early machine learning models for understanding natural language, and in particular to extract topics from text. It was a lot of fun to play with these, but it took a lot of pre-processing of text to get them to work at all, and they were performed pretty poorly in practice. That model may look like something peculiar, but it’s not much more than a flexible PCA on a matrix of word counts, or from another perspective, a Bayesian multinomial model.↩︎\nThis is a common problem where data is often collected for one purpose and then used for another, as with general purpose surveys or administrative data. Sometimes it can be that the available data is simply not enough to say anything without a lot of uncertainty, as in the case of demographic data regarding minority groups for which there may be little to no instances in a particular sample. Zero/Few-shot learning isn’t applicable here, because there isn’t a model pre-trained on millions/billions of similar examples to transfer knowledge from.↩︎\nUntil Python can go from model to visualizing the marginal effect with uncertainty in two or three lines of code (even if a Bayesian implementation), possibly on millions of observations in a few seconds, and even visualizing the derivatives (also with uncertainty estimates), it’s not going to be as easy to use as R for GAMs. But here’s hoping the current efforts continue there.↩︎\nGeorge Box, a famous statistician, said this in 1976.↩︎",
    "crumbs": [
      "Other Considerations",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Until Next Time...</span>"
    ]
  },
  {
    "objectID": "dataset_descriptions.html",
    "href": "dataset_descriptions.html",
    "title": "Appendix A — Dataset Descriptions",
    "section": "",
    "text": "A.1 Movie reviews\nThe movie reviews dataset was a fun way to use an LLM to create movie titles and reviews in a specific way, as well as other features. With features in hand, we then generated a rating outcome with specific feature-target relationships. It has 1000 rows and the following columns:\nThe movie reviews data is available directly in the book’s repo.\nShort link:",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Dataset Descriptions</span>"
    ]
  },
  {
    "objectID": "dataset_descriptions.html#sec-dd-movie-reviews",
    "href": "dataset_descriptions.html#sec-dd-movie-reviews",
    "title": "Appendix A — Dataset Descriptions",
    "section": "",
    "text": "title: The title of the movie\nreview_year: The year the review was written\nage: The age of the reviewer\nchildren_in_home: The number of children in the reviewer’s home\neducation: The education level of the reviewer (Post-Graduate, Completed College, Completed High School)\ngender: The gender of the reviewer (male or female)\nwork_status: The work status of the reviewer (Employed, Retired, Unemployed, Student)\ngenre: The genre of the movie\nrelease_year: The year the movie was released\nlength_minutes: The length of the movie in minutes\nseason: The season the movie was released (e.g. Fall, Winter)\ntotal_reviews: The total number of reviews for the movie\nrating: The rating of the movie\nreview_text: The text of the review\nword_count: The number of words in the review\nreview_year_0: The review year starting from 0\nrelease_year_0: The release year starting from 0\nage_sc, length_minutes_sc, total_reviews_sc, word_count_sc: Scaled versions of age, length_minutes, total_reviews, and word_count\nrating_good: A binary version of rating, where 1 is a good rating (&gt;= 3) and 0 is a bad rating (&lt; 3)\n\n\n\n\nmovies: https://tinyurl.com/moviereviewsdata",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Dataset Descriptions</span>"
    ]
  },
  {
    "objectID": "dataset_descriptions.html#sec-dd-world-happiness-report",
    "href": "dataset_descriptions.html#sec-dd-world-happiness-report",
    "title": "Appendix A — Dataset Descriptions",
    "section": "A.2 World Happiness Report",
    "text": "A.2 World Happiness Report\nThe World Happiness Report is a survey of the state of global happiness that ranks countries by how ‘happy’ their citizens perceive themselves to be. You can also find additional details in their supplemental documentation. Our 2018 data is from what was originally reported at that time (figure 2.2) and it also contains a life ladder score from the most recent survey, which is similar and very highly correlated.\nThe dataset contains the following columns:\n\ncountry: The country name\nyear: The year of the survey\nlife_ladder: The happiness score\nlog_gdp_per_capita: The log of GDP per capita\nsocial_support: The social support score\nhealthy_life_expectancy_at_birth: The healthy life expectancy at birth\nfreedom_to_make_life_choices: The freedom to make life choices score\ngenerosity: The generosity score\nperceptions_of_corruption: The perceptions of corruption score\npositive_affect: The positive affect score\nnegative_affect: The negative affect score\nconfidence_in_national_government: The confidence in national government score\nhappiness_score: The happiness score\ndystopia_residual: The dystopia residual score (difference from a ‘least happy’ country)\n\nIn addition there are standardized/scaled versions of the features, which are suffixed with _sc.\nData for all years and 2018 only are available directly in the book’s repo.\nShort links:\n\nall years: https://tinyurl.com/worldhappinessallyears\n2018: https://tinyurl.com/worldhappiness2018",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Dataset Descriptions</span>"
    ]
  },
  {
    "objectID": "dataset_descriptions.html#sec-dd-heart-disease-uci",
    "href": "dataset_descriptions.html#sec-dd-heart-disease-uci",
    "title": "Appendix A — Dataset Descriptions",
    "section": "A.3 Heart Disease UCI",
    "text": "A.3 Heart Disease UCI\nThis classic dataset comes from the UCI ML repository. We took a version from Kaggle, and features and target were renamed to be more intelligible. Here is a brief description from UCI:\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to date. The “goal” field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n\n\nage: age in years\nmale: ‘yes’ or ‘no’\nchest_pain_type: ‘typical’, ‘atypical’, ‘non-anginal’, ‘asymptomatic’\nresting_bp: resting blood pressure (mm Hg)\ncholesterol: serum cholesterol (mg/dl)\nfasting_blood_sugar: ‘&gt; 120 mg/dl’ or ‘&lt;= 120 mg/dl’\nresting_ecg: ‘normal’, ‘left ventricular hypertrophy’, ‘ST-T wave abnormality’\nmax_heart_rate: maximum heart rate achieved\nexercise_induced_angina: ‘yes’ or ‘no’\nst_depression: ST depression induced by exercise relative to rest\nslope: ‘upsloping’, ‘flat’, ‘downsloping’\nnum_major_vessels: number of major vessels (0-3) colored by fluoroscopy\nthalassemia: ‘normal’, ‘fixed defect’, ‘reversible defect’\nheart_disease: ‘yes’ or ‘no’\n\nThe processed data and processed data with scaled numeric variables are available directly in the book’s repo.\nShort links:\n\nprocessed: https://tinyurl.com/heartdiseaseprocessed\nnumeric features only: https://tinyurl.com/heartdiseaseprocessednumeric",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Dataset Descriptions</span>"
    ]
  },
  {
    "objectID": "dataset_descriptions.html#sec-dd-fish",
    "href": "dataset_descriptions.html#sec-dd-fish",
    "title": "Appendix A — Dataset Descriptions",
    "section": "A.4 Fish",
    "text": "A.4 Fish\nA very simple data set with a count target variable available for an exercise in the GLM chapter. Also good if you want to try your hand at zero-inflated models. The background is that state wildlife biologists want to model how many fish are being caught by fishermen at a state park.\n\nnofish: We’ve never seen this explained. Originally 0 and 1, 0 is equivalent to livebait == ‘yes’, so it may be whether the primary motivation of the camping trip is for fishing or not.\nlivebait: whether live bait was used or not\ncamper: whether or not they brought a camper\npersons: how many total persons on the trip\nchild: how many children present\ncount: number of fish caught\n\nShort Link:\n\nfish: https::/tinyurl.com/fishcountdata.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Dataset Descriptions</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html",
    "href": "matrix_operations.html",
    "title": "Appendix B — Matrix Operations",
    "section": "",
    "text": "B.1 Addition\nMatrix addition, along with subtraction, is the easiest concept when dealing with matrices. While it is easy to grasp, you will not find it featured as prominently as matrix multiplication.\nThere is one rule for matrix addition: the matrices need to have the same dimensions. From a practical code perspective, if one is a scalar, addition of the scalar will be applied to every element in the matrix.\nLet’s check out these two matrices:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n\\\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n\\]\nYou probably noticed that we gave each scalar within the matrix a label associated with its row and column position. We can use these to see how we will produce the new matrix:\nNow, we can set this up as an addition problem to produce Matrix C:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\nA_{11} + B_{11}& A_{12} + B_{12} & A_{13} + B_{13}\\\\\nA_{21} + B_{21}& A_{22} + B_{22} & A_{23} + B_{23}\n\\end{bmatrix}\n}\n\\]\nNow we can pull in the real numbers:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n1 + 7  & 2 + 8 & 3 + 9\\\\\n4 + 9 & 5 + 8 & 6 + 7\n\\end{bmatrix}\n}\n\\]\nGiving us Matrix C:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n8 & 10 & 12 \\\\\n13 & 13 & 13\n\\end{bmatrix}\n}\n\\]",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#subtraction",
    "href": "matrix_operations.html#subtraction",
    "title": "Appendix B — Matrix Operations",
    "section": "B.2 Subtraction",
    "text": "B.2 Subtraction\nTake everything that you just saw with addition and replace it with subtraction! But just like addition, every matrix needs to have the same dimensions if you are going to use subtraction. \nHere is the result:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n-6 & -6 & -6 \\\\\n-5 & -3 & -1\n\\end{bmatrix}\n}\n\\]\nAdding and subtracting matrices in R and Python is pretty simple.\nIn R, we can create a matrix a few ways: with the matrix function or by row binding numeric vectors.\n\nmatrix_A = rbind(1:3, 4:6)\n\n# The following is an equivalent\n# to rbind:\n# matrix_A = matrix(c(1:3, 4:6), \n#                    nrow = 2, \n#                    ncol = 3, byrow = TRUE)\n\nmatrix_B = rbind(7:9, 9:7)\n\nOnce we have those matrices created, we can use the standard + and - signs to add and subtract:\n\nmatrix_A + matrix_B\n\n     [,1] [,2] [,3]\n[1,]    8   10   12\n[2,]   13   13   13\n\nmatrix_A - matrix_B\n\n     [,1] [,2] [,3]\n[1,]   -6   -6   -6\n[2,]   -5   -3   -1\n\n\nThe task is just as easy in Python. We will import numpy and then use the matrix method to create the matrices:\n\nimport numpy as np\n\nmatrix_A = np.array([[1, 2, 3], [4, 5, 6]])\n\nmatrix_B = np.array([[7, 8, 9], [9, 8, 7]])\n\nJust like R, we can use + and - on those matrices.\n\nmatrix_A + matrix_B\n\narray([[ 8, 10, 12],\n       [13, 13, 13]])\n\nmatrix_A - matrix_B\n\narray([[-6, -6, -6],\n       [-5, -3, -1]])",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#transpose",
    "href": "matrix_operations.html#transpose",
    "title": "Appendix B — Matrix Operations",
    "section": "B.3 Transpose",
    "text": "B.3 Transpose\nYou might see a matrix denoted as \\(A^T\\) or \\(A'\\). The superscripted T stands for transpose. If we transpose a matrix, all we are doing is flipping the rows and columns along the matrix’s main diagonal. A visual example is much easier:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-&gt;\n\\stackrel{\\mbox{Matrix A transposed} }{\n\\begin{bmatrix}\n1 & 4 \\\\\n2 & 5 \\\\\n3 & 6\n\\end{bmatrix}\n}\n\\]\nIn R, all we need is the t function:\n\nt(matrix_A)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nIn Python, we can use numpy’s transpose method:\n\nmatrix_A.transpose()\n\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n\nmatrix_A.T\n\narray([[1, 4],\n       [2, 5],\n       [3, 6]])",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#multiplication",
    "href": "matrix_operations.html#multiplication",
    "title": "Appendix B — Matrix Operations",
    "section": "B.4 Multiplication",
    "text": "B.4 Multiplication\nNow, you probably have some confidence in doing matrix operations. Just as quickly as we built that confidence, it will be crushed when learning about matrix multiplication.\nWhen dealing with matrix multiplication, we have a huge change to our rule. No longer can our dimensions be the same! Instead, the matrices need to be conformable – the first matrix needs to have the same number of columns as the number of rows within the second matrix. In other words, the inner dimensions must match.\nLook one more time at these matrices:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n\\]\nMatrix A has dimensions of \\(2x3\\), as does Matrix B. Putting those dimensions side by side – \\(2x3 * 2x3\\) – we see that our inner dimensions are 3 and 2 and do not match.\nWhat if we transpose Matrix B?\n\\[\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n\\]\nNow we have something that works!\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n. & . \\\\\n. & . \\\\\n\\end{bmatrix}\n}\n\\]\nNow we have a \\(2x3 * 3x2\\) matrix multiplication problem! The resulting matrix will have the same dimensions as our two matrices’ outer dimensions: \\(2x2\\)\nHere is how we will get at \\(2x2\\) matrix:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n=\n\\] \\[\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n(A_{11}*B_{11})+(A_{12}*B_{21})+(A_{13}*B_{31}) & (A_{11}*B_{12})+(A_{12}*B_{22})+(A_{13}*B_{32}) \\\\\n(A_{21}*B_{11})+(A_{22}*B_{21})+(A_{23}*B_{31}) & (A_{21}*B_{12})+(A_{22}*B_{22})+(A_{23}*B_{32})\n\\end{bmatrix}\n}\n\\]\nThat might look like a horrible mess and likely isn’t easy to commit to memory. Instead, we’d like to show you a way that might make it easier to remember how to multiply matrices. It also gives a nice representation of why your matrices need to be conformable.\nWe can leave Matrix A exactly where it is, flip Matrix B\\(^T\\), and stack it right on top of Matrix A:\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n7_{b} & 8_{b} & 9_{b} \\\\\n\\\\\n1_{a} & 2_{a} & 3_{a} \\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n\\]\nNow, we can let those rearranged columns from Matrix B\\(^T\\) “fall down” through the rows of Matrix A:\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n\\\\\n1_{a}*7_{b} & 2_{a}*8_{b} & 3_{a}*9_{b}\\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & .\\\\\n. & .\n\\end{bmatrix}\n}\n\\]\nAdding those products together gives us 50 for \\(C_{11}\\).\nLet’s move that row down to the next row in the Matrix A, multiply, and sum the products.\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n\\\\\n1_{a} & 2_{a} & 3_{a}\\\\\n4_{a}*7_{b} & 5_{a}*8_{b} & 6_{a}*9_{b}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & .\\\\\n122 & .\n\\end{bmatrix}\n}\n\\]\nWe have 122 for \\(C_{21}\\). That first column from Matrix B\\(^T\\) won’t be used any more, but now we need to move the second column through Matrix A.\n\\[\n\\begin{bmatrix}\n1_{a}*9_{b} & 2_{a}*8_{b} & 3_{a}*7_{b}\\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & 46\\\\\n122 & .\n\\end{bmatrix}\n}\n\\]\nThat gives us 46 for \\(C_{12}\\).\nAnd finally:\n\\[\n\\begin{bmatrix}\n1_{a} & 2_{a} & 3_{a}\\\\\n4_{a}*9_{b} & 5_{a}*8_{b} & 6_{a}*7_{b}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & 46\\\\\n122 & 118\n\\end{bmatrix}\n}\n\\]\nWe have 118 for \\(C_{22}\\).\nNow that you know how these work, you can see how easy it is to handle these tasks in R and Python.\nIn R, we need to use a fancy operator: %*%. This is just R’s matrix multiplication operator. We will also use the transpose function: t.\n\nmatrix_A %*% t(matrix_B)\n\n     [,1] [,2]\n[1,]   50   46\n[2,]  122  118\n\n\nIn Python, we can just use the regular multiplication operator and the transpose method:\n\nmatrix_A @ matrix_B.transpose()\n\narray([[ 50,  46],\n       [122, 118]])\n\n\nYou can see that whether we do this by hand, R, or Python, we come up with the same answer! While these small matrices can definitely be done by hand, we will always trust the computer to handle larger matrices.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#division",
    "href": "matrix_operations.html#division",
    "title": "Appendix B — Matrix Operations",
    "section": "B.5 Division",
    "text": "B.5 Division\nThough addition, subtraction, and multiplication are all pretty straightforward, matrix division is not. In fact, there really isn’t such a thing as matrix division, we just use matrix multiplication in a particular way. This is similar to how we can divide two numbers, e.g. \\(a/b\\), but we can also multiply by the reciprocal, \\(a*(1/b)\\). In matrix terms this would look something like:\n\\[\nA * B^{-1} \\\\\n\\]\nor simply\n\\[\nAB^{-1}\n\\]\nWhile that may also seem straightforward on the surface, matrix inversion is not. The basic idea is that we are looking for a matrix that, when multiplied by the original matrix like \\(B\\), gives us the identity matrix. The identity matrix is a matrix that has 1s along the diagonal and 0s everywhere else.\n\\[\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nAnother caveat is that not all matrices have inverses. If the determinant of a matrix is 0, then it does not have an inverse. Technically, only square matrices can have inverses, but not all square matrices have inverses. We can, however, get a pseudo-inverse for non-square matrices.\n\nmatrix_B_inv = MASS::ginv(matrix_B)\nround(matrix_B %*% matrix_B_inv)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\nmatrix_B_inv = np.linalg.pinv(matrix_B)\nmatrix_B @ matrix_B_inv\n\narray([[ 1.00000000e+00, -7.49400542e-16],\n       [-1.11022302e-16,  1.00000000e+00]])\n\n\nMore to the point, when would we do this? In the world of modeling, we might use matrix inversion to solve a system of equations. This is most commonly implemented in linear regression, where we are trying to find the coefficients that minimize the error in our model. That problem has an analytical solution that involves matrix inversion.\n\\[\n\\beta = (X^TX)^{-1}X^Ty\n\\]\nLet’s see this for ourselves. We will create a simple linear regression model and solve for the coefficients using matrix inversion.\n\nset.seed(123)\nx = rnorm(100)\ny = 2*x + rnorm(100)\nX = cbind(1, x)\n\nbeta = MASS::ginv(t(X) %*% X) %*% t(X) %*% y\ntibble(\n  ours = beta[,1],\n  standard = coef(lm(y ~ x))\n)\n\n\nimport statsmodels.api as sm\nimport pandas as pd\n\nnp.random.seed(123)\nx = np.random.normal(size = 100)\ny = 2*x + np.random.normal(size = 100)\nX = np.c_[np.ones(100), x]\n\nbeta = np.linalg.pinv(X.T @ X) @ X.T @ y\nbeta\n\narray([-0.01908575,  1.98340745])\n\nmodel_sm = sm.OLS(y, X)\nresults_sm = model_sm.fit()\ncoefficients_sm = results_sm.params\n\npd.DataFrame({\n    'ours': beta,\n    'standard': coefficients_sm\n})\n\n       ours  standard\n0 -0.019086 -0.019086\n1  1.983407  1.983407",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#summary",
    "href": "matrix_operations.html#summary",
    "title": "Appendix B — Matrix Operations",
    "section": "B.6 Summary",
    "text": "B.6 Summary\nMatrix operations are in some ways intuitive, but not something we do everyday. However, having a grasp of the fundamentals enables an understanding of the underlying model mechanics. Many data scientists never have to do matrix operations by hand, but knowing the mechanics demystifies the modeling process, and can greatly expand a data scientist’s abilities. Whether linear regression or deep learning, matrix operations are at the core of many models.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "matrix_operations.html#footnotes",
    "href": "matrix_operations.html#footnotes",
    "title": "Appendix B — Matrix Operations",
    "section": "",
    "text": "Numpy arrays/matrices are in row major order, while R is column major order. You’ll note how with numpy we essentially provided two rows to the array function, which automatically created the 2 x 3 matrix. The R matrix is not the same, because by default it fills in the columns. If you add by_row = TRUE, you’d then get the same result as the numpy example. Column major is generally more intuitive for tabular data, because that’s how we think of data stored in tables, and why the pandas package in python is also column major/oriented. However, both R and Python are very flexible and more generally work in arrays. If you use both it can take a bit to settle with one if you’ve used the other (especially for ‘apply’ functions). The reticulate package has a vignette that provides a nice overview, while the rray package actually brings the numpy approach to the R landscape.↩︎",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Matrix Operations</span>"
    ]
  },
  {
    "objectID": "pyr.html",
    "href": "pyr.html",
    "title": "Appendix C — Python vs. R",
    "section": "",
    "text": "C.1 Our Background\nMichael has used both R and Python for many years and now uses Python primarily as a reflection of his own movement from academia to industry. Seth also has used both for many years and teaches with both in his courses every semester. As such, unlike many, we have a lot of experience with using both for serious data science, which we hope gives us a pretty good perspective on the strengths and weaknesses of each. In the end though, this is just our opinion, and you would do well to consider your specific needs and preferences when choosing a language.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python vs. R</span>"
    ]
  },
  {
    "objectID": "pyr.html#sec-pyr-python",
    "href": "pyr.html#sec-pyr-python",
    "title": "Appendix C — Python vs. R",
    "section": "C.2 Python",
    "text": "C.2 Python\nPython is the leading language for machine learning, deep learning, and data science in general. Many other languages can perform ML and maybe even well, but Python is the most popular, has the most packages, and it’s where tools are typically implemented and developed first. Even if it isn’t your primary language, it should be for any implementation of machine learning or using deep learning models.\n\nC.2.1 Pros\n\nPopular Tools: Very powerful and very widely used tools.\nEfficiency: Typically efficient on memory and fast.\nModeling API: Consistent API across many modeling packages (e.g., sklearn)\nPipelines: Easy pipeline and reproducibility setup.\nML/DL Development: The standard for machine/deep learning packages.\nVersatility: Versatile, general programming language.\nOrigins: Was not developed by statisticians.\n\n\n\nC.2.2 Cons\n\nModel Exploration: It can be difficult to get model summaries, visualizations, and interpretable results.\nData Processing: Tedious and verbose; Pandas is not as user-friendly as tidyverse.\nEnvironment fragility: Python or package updates often break functionality; environments are often frozen to avoid this, which means using outdated versions that can cause any number of problems and retain bugs unnecessarily.\nDocumentation: Documentation is often poor and inconsistent since there is no standard. This hopefully will be alleviated in the future with AI tools that can write the documentation for you.\nInteractive Development: Jupyter notebooks lag behind RMarkdown, but Quarto shows promise for improving Python’s usability in this area2.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python vs. R</span>"
    ]
  },
  {
    "objectID": "pyr.html#sec-pyr-r",
    "href": "pyr.html#sec-pyr-r",
    "title": "Appendix C — Python vs. R",
    "section": "C.3 R",
    "text": "C.3 R\nR is the leading language for statistical analysis, visualization and reporting analytical results. Your authors can also definitively say that R is actually great at ML and at production level, as they have used it with data comprising millions of data points for very large and well-known companies. The default tools are not as fast or memory efficient relative to Python, but they are typically more user friendly, and usually have good to even excellent documentation, as package development has been largely standardized for some time. When it comes to statistical models like those from Part I, R is the best tool for the job hands down.\n\nC.3.1 Pros\n\nUser-friendly: Very user-friendly and fast data processing, with easy to use objects that contain the things you’d need for further processing\nCompatibility: Practically every tool you’d ever use works with data frames\nPost-processing: Easy post-processing of models with many packages\nDocumentation: Documentation is standardized for any CRAN and almost non-CRAN packages use the same approach. Unlike Python, examples are expected for documented functions, and the package will fail to build if any example fails.\nInteractive analysis: Interactive analysis was always the default approach for R\nPackage Development: CRAN packages are checked against the latest version of R and even other packages that depend on them.\n\n\n\nC.3.2 Cons\n\nSpeed: Relatively slow for many modeling tasks.\nMemory: R is very memory intensive, but with modern machines this is less of an issue for many tabular data applications.\nPipelines: Pipeline approach and/or reproducibility in a production-level sense has only recently been of focus.\nProduction tools: Production tools are still relatively recent and not as well-developed.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python vs. R</span>"
    ]
  },
  {
    "objectID": "pyr.html#sec-pyr-conclusion",
    "href": "pyr.html#sec-pyr-conclusion",
    "title": "Appendix C — Python vs. R",
    "section": "C.4 Conclusion",
    "text": "C.4 Conclusion\nIn summary, R is fantastic for Part I models (statistical modeling) and Python for Part II (machine learning). We can also say that both are great for causal modeling depending on what your approach is. We still feel R is notably easier to use for data processing, visualization, and model exploration, producing data science reports/documents, and obviously for statistical modeling. We would likely use Python for anything that requires a lot of data or is computationally expensive, and for machine learning in general. For deep learning models, Python is the obvious choice.\nWe like tools like Quarto because it makes it easy to use both, even simultaneously within the same document, so the great thing is that you don’t have to choose. More to the point, AI tools like CoPilot now make it easier to program in either, leaving you to focus on the data science instead of the programming.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python vs. R</span>"
    ]
  },
  {
    "objectID": "pyr.html#footnotes",
    "href": "pyr.html#footnotes",
    "title": "Appendix C — Python vs. R",
    "section": "",
    "text": "There are other languages that can be used for data science, but they are not nearly as common. For example, Julia is a language that is very fast for some things and generally has a lot of potential for data science. But honestly it came too late to the game, and it’s not as user-friendly as Python or R. Proprietary tools like Matlab, SAS, Stata, and similar had their day, and it has long since passed, and closed-source tools will never develop as rapidly as popular open source tools. Still other languages whose primary purpose is not data science may be useful for some tasks (e.g. Spark-ML), but they are not as well-suited for as Python or R.↩︎\nQuarto is a relatively new markdown tool that attempts to be language agnostic and is designed to be a successor to RMarkdown. In our experience it is already much better than Jupyter for producing an actual document, and nearly approaching the same level for interactive use.↩︎",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python vs. R</span>"
    ]
  },
  {
    "objectID": "more_models.html",
    "href": "more_models.html",
    "title": "Appendix D — More Models",
    "section": "",
    "text": "D.1 Linear Models\nSimplified Linear Models\nGeneralize Linear Models and related\nMultivariate/multiclass/multipart\nOther Random Effects\nAll of these are explicitly linear models or can be framed as such, and compared to what you’ve already seen, only require only a tweak or two - e.g. a different distribution, a different link function, penalizing the coefficients, etc. In other cases, we can bounce from one to the another. For example we can reshape our multivariate regression to be amenable to a mixed model approach, and get the exact same results. We can potentially add a random effect to any model, and that random effect can be based on time, spatial or other considerations. The important thing to know is that the linear model is a very flexible tool that expands easily, and allows you to model most of the types of outcomes were interested in. As such, it’s a very powerful approach to modeling.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>More Models</span>"
    ]
  },
  {
    "objectID": "more_models.html#sec-app-more-linear",
    "href": "more_models.html#sec-app-more-linear",
    "title": "Appendix D — More Models",
    "section": "",
    "text": "correlation\nt-test and ANOVA\nchi-square\n\n\n\nTrue GLM e.g. gamma\nOther distributions: beta regression, tweedie, t (so-called ‘robust’), truncated\nCensored outcomes: Survival models, tobit\nNonlinear regression\nModeling other parameters (e.g. heteroscedastic models)\n\n\n\nMultivariate regression (multiple targets)\nMultinomial/Categorical/Ordinal regression (&gt;2 classes)\nMANOVA/Linear Discriminant Analysis (these are identical, and can handle multiple outputs or &gt;=2 classes)\nZero (or some number) -inflated/hurdle/altered\nMixture models and Cluster analysis\nTwo-stage least squares, instrumental variables\nSEM, simultaneous equations\nPCA, Factor Analysis\nMixture models\nStructural Equation Modeling, Graphical models generally\n\n\n\nGaussian process regression\nSpatial models (CAR, SAR, etc.)\nTime series models (ARIMA and related, e.g. state space)\nFactor analysis",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>More Models</span>"
    ]
  },
  {
    "objectID": "more_models.html#sec-app-more-ml",
    "href": "more_models.html#sec-app-more-ml",
    "title": "Appendix D — More Models",
    "section": "D.2 Other Machine Learning Models",
    "text": "D.2 Other Machine Learning Models\nThere are models you’ll typically only see when you’re in a machine learning context, as they often do not provide specific parameters of interest like coefficients or variance components, nor have easy ways to estimate uncertainty. While that leads the focus toward prediction, most of these are no longer performant compared to tools used today. Still, they can be interesting historically or conceptually, some are special cases of more widely used techniques, and some can still be used as baseline models.\nStandard Regression/Classification\n\nk-Nearest neighbors regression\nNaive Bayes\nSupport Vector Machines, Boltzmann Machines\nProjection pursuit regression\n(Hidden) Markov Models\nUndirected graphs, Markov Random Fields, Network analysis\nSingle Decision trees, CART, C4.5, etc.\n\nLatent Models\n\nPCA, probabilistic PCA, ICA, SVD\nLatent Dirichlet Allocation, Latent Semantic Analysis\n(Non-negative) Matrix Factorization\nDirichlet process",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>More Models</span>"
    ]
  },
  {
    "objectID": "more_models.html#sec-app-more-dl",
    "href": "more_models.html#sec-app-more-dl",
    "title": "Appendix D — More Models",
    "section": "D.3 Other Deep Learning Models",
    "text": "D.3 Other Deep Learning Models\nWe haven’t delved into the world of deep learning as much as there hasn’t yet been a ‘foundational’ model for tabular data of the sort we’ve focused on. However most of the models that make headlines today are built upon simpler models, even going back to the basic multilayer perceptron. Here are some of the models you might see in the wild:\n\nConvolutional Neural Networks\nRecurrent Neural Networks\nLong Short-Term Memory Networks\nTransformers\nAutoencoders\nGenerative Adversarial Networks\nExtreme Learning Machines\nGraph Neural Networks\nFactorization Machines\nAttention Mechanisms\nReinforcement Learning\n\nConvolutional neural networks as currently implemented can be seen going back to LeNet in the late 1990s, and took off several years later with AlexNet and VGG. ResNet (residual networks) and Densenet are more recent examples of CNNs, though even they are have been around for several years at this point. Even so, several of these still serve as baseline models for image classification and object detection, either in practice or as a reference point for current model performance.\nNLP and language processing more generally can be seen as evolving from matrix factorization and LDA, to neural network models such as word2vec and GloVe. In addition, the temporal nature of text suggested time-based models even more statistical ones like hidden markov models way back in the day. But in the neural network domain, we have standard Recurrent networks, then LSTMs, GRUs, Seq2Seq, and more that continued the theme. Now the field is dominated by attention-based transformers, of which BERT variants and GPT are among the most famous examples of these, but there are many others that have been developed in the last few years, offered from Meta, Google, Anthropic and others.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>More Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix E — References",
    "section": "",
    "text": "These references tend to be more functional than academic, and hopefully will be more practically useful to you as well. If you prefer additional academic resources, you can always look at the references within these, or just search Google Scholar for any of the topics covered.\n\n\nAlbon, Chris. 2024. “Machine Learning\nNotes.” https://chrisalbon.com/Home.\n\n\nAmazon. 2024. “What Is Data\nAugmentation? - Data Augmentation\nTechniques Explained -\nAWS.” Amazon Web Services, Inc. https://aws.amazon.com/what-is/data-augmentation/.\n\n\nBarrett, Malcolm, Lucy D’Agostino McGowan, and Travis Gerke. 2024.\nCausal Inference in R. https://www.r-causal.org/.\n\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang, eds.\n2024. Applied Machine Learning\nUsing Mlr3 in R. https://mlr3book.mlr-org.com/.\n\n\nBoykis, Vicki. 2023. “What Are Embeddings?” http://vickiboykis.com/what_are_embeddings/index.html.\n\n\nBrownlee, Jason. 2019. “A Gentle\nIntroduction to Imbalanced\nClassification.”\nMachineLearningMastery.com. https://machinelearningmastery.com/what-is-imbalanced-classification/.\n\n\n———. 2021. “Gradient Descent With\nAdaGrad From Scratch.”\nMachineLearningMastery.com. https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/.\n\n\nCawley, Gavin C., and Nicola L. C. Talbot. 2010. “On\nOver-Fitting in Model Selection\nand Subsequent Selection Bias in\nPerformance Evaluation.” The\nJournal of Machine Learning Research 11 (August): 2079–2107.\n\n\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler,\nand Vasilis Syrgkanis. 2024. “Applied Causal\nInference Powered by ML and\nAI.” arXiv. http://arxiv.org/abs/2403.02467.\n\n\nClark, Michael. 2018a. Graphical & Latent\nVariable Modeling. https://m-clark.github.io/sem/.\n\n\n———. 2018b. “Thinking about Latent\nVariables.” https://m-clark.github.io/docs/FA_notes.html.\n\n\n———. 2021a. Model Estimation by\nExample. https://m-clark.github.io/models-by-example/.\n\n\n———. 2021b. “This Is Definitely Not All You Need,” July. https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/.\n\n\n———. 2022a. Generalized Additive\nModels. https://m-clark.github.io/generalized-additive-models/.\n\n\n———. 2022b. “Deep Learning for Tabular\nData,” May. https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/.\n\n\n———. 2023. Mixed Models with R. https://m-clark.github.io/mixed-models-with-R/.\n\n\nCohen, Jacob. 2009. Statistical Power Analysis for the Behavioral\nSciences. 2. ed., reprint. New York, NY: Psychology Press.\n\n\nComputing, UCLA Advanced Research. 2023. “FAQ:\nWhat Are Pseudo R-Squareds?” https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/.\n\n\nCunningham, Scott. 2023. Causal Inference\nThe Mixtape. https://mixtape.scunning.com/.\n\n\nDataBricks. 2019. “What Is AdaGrad?”\nDatabricks. https://www.databricks.com/glossary/adagrad.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap\nMethods and Their Application. Cambridge\nSeries in Statistical and\nProbabilistic Mathematics. Cambridge:\nCambridge University Press. https://doi.org/10.1017/CBO9780511802843.\n\n\nDobson, Annette J., and Adrian G. Barnett. 2018. An\nIntroduction to Generalized\nLinear Models. 4th ed. New York: Chapman;\nHall/CRC. https://doi.org/10.1201/9781315182780.\n\n\nDunn, Peter K., and Gordon K. Smyth. 2018. Generalized\nLinear Models With\nExamples in R. Springer.\n\n\nEfron, Bradley, and R. J. Tibshirani. 1994. An\nIntroduction to the Bootstrap. New York:\nChapman; Hall/CRC. https://doi.org/10.1201/9780429246593.\n\n\nFacure Alves, Matheus. 2022. “Causal Inference for\nThe Brave and True —\nCausal Inference for the Brave\nand True.” https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nFahrmeir, Ludwig, Thomas Kneib, Stefan Lang, and Brian D. Marx. 2021.\nRegression: Models, Methods and\nApplications. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-662-63882-8.\n\n\nFaraway, Julian. 2014. “Linear Models with\nR.” Routledge & CRC Press. https://www.routledge.com/Linear-Models-with-R/Faraway/p/book/9781439887332.\n\n\nFox, John. 2015. Applied Regression\nAnalysis and Generalized Linear\nModels. SAGE Publications.\n\n\nGelman, Andrew. 2013. “What Are the Key Assumptions of Linear\nRegression?  Statistical\nModeling, Causal Inference, and\nSocial Science.” https://statmodeling.stat.columbia.edu/2013/08/04/19470/.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data\nAnalysis, Third Edition. CRC\nPress.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Cambridge university\npress.\n\n\nGelman, Andrew, Jennifer Hill, Ben Goodrich, Jonah Gabry, Daniel\nSimpson, and Aki Vehtari. 2024. “Advanced Regression\nand Multilevel Models.” http://www.stat.columbia.edu/~gelman/armm/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. 1st ed. Cambridge\nUniversity Press. https://doi.org/10.1017/9781139161879.\n\n\nGoogle. 2023. “Machine Learning \nGoogle for Developers.” https://developers.google.com/machine-learning.\n\n\nGreene, William. 2017. Econometric Analysis - 8th\nEdition. https://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm.\n\n\nHardin, James W., and Joseph M. Hilbe. 2018. Generalized\nLinear Models and\nExtensions. Stata Press.\n\n\nHarrell, Frank E. 2015. Regression Modeling\nStrategies: With Applications to\nLinear Models, Logistic and\nOrdinal Regression, and Survival\nAnalysis. 2nd ed. Springer Series in\nStatistics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017.\nElements of Statistical Learning: Data\nMining, Inference, and Prediction. 2nd Edition. https://hastie.su.domains/ElemStatLearn/.\n\n\nHernán, Miguel A. 2018. “The C-Word:\nScientific Euphemisms Do\nNot Improve Causal\nInference From Observational\nData.” American Journal of Public Health\n108 (5): 616–19. https://doi.org/10.2105/AJPH.2018.304337.\n\n\nHoward, Jeremy. 2024. “Practical Deep\nLearning for Coders - Practical\nDeep Learning.” Practical Deep\nLearning for Coders. https://course.fast.ai/.\n\n\nHyndman, Rob, and George Athanasopoulos. 2021. Forecasting:\nPrinciples and Practice (3rd Ed). https://otexts.com/fpp3/.\n\n\nIvanova, Anna A, Shashank Srikant, Yotaro Sueoka, Hope H Kean, Riva\nDhamala, Una-May O’Reilly, Marina U Bers, and Evelina Fedorenko. 2020.\n“Comprehension of Computer Code Relies Primarily on Domain-General\nExecutive Brain Regions.” Edited by Andrea E Martin, Timothy E\nBehrens, William Matchin, and Ina Bornkessel-Schlesewsky. eLife\n9 (December): e58906. https://doi.org/10.7554/eLife.58906.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical\nLearning. Vol. 103. Springer Texts in\nStatistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nJiang, Lili. 2020. “A Visual Explanation\nof Gradient Descent Methods\n(Momentum, AdaGrad, RMSProp,\nAdam).” Medium. https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c.\n\n\nKoenker, Roger. 2005. Quantile Regression. Vol. 38. Cambridge\nuniversity press. https://books.google.com/books?hl=en&lr=&id=WjOdAgAAQBAJ&oi=fnd&pg=PT12&dq=koenker+quantile+regression&ots=CQFHSt5o-W&sig=G1TpKPHo-BRdJ8qWcBrIBI2FQAs.\n\n\nKuhn, Max, and Kjell Johnson. 2023. Applied Machine\nLearning for Tabular Data.\nhttps://aml4td.org/.\n\n\nKuhn, Max, and Julia Silge. 2023. Tidy Modeling with\nR. https://www.tmwr.org/.\n\n\nLeech, Gavin, Juan J. Vazquez, Misha Yagudin, Niclas Kupper, and\nLaurence Aitchison. 2024. “Questionable Practices in Machine\nLearning.” arXiv. https://doi.org/10.48550/arXiv.2407.12220.\n\n\nMasis, Serg. 2023. “Interpretable Machine\nLearning with Python - Second\nEdition.” Packt. https://www.packtpub.com/product/interpretable-machine-learning-with-python-second-edition/9781803235424.\n\n\nMcCullagh, P. 2019. Generalized Linear\nModels. 2nd ed. New York: Routledge. https://doi.org/10.1201/9780203753736.\n\n\nMcCulloch, Warren S., and Walter Pitts. 1943. “A Logical Calculus\nof the Ideas Immanent in Nervous Activity.” The Bulletin of\nMathematical Biophysics 5 (4): 115–33. https://doi.org/10.1007/BF02478259.\n\n\nMcElreath, Richard. 2020. “Statistical Rethinking:\nA Bayesian Course with\nExamples in R and STAN.”\nRoutledge & CRC Press. https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919.\n\n\nMolnar, Christoph. 2023. Interpretable Machine\nLearning. https://christophm.github.io/interpretable-ml-book/.\n\n\nMonroe, Elizabeth, and Michael Clark. 2024. “Imbalanced\nOutcomes: Challenges and\nSolutions.”\n\n\nMurphy, Kevin P. 2012. “Machine Learning:\nA Probabilistic\nPerspective.” MIT Press. https://mitpress.mit.edu/9780262018029/machine-learning/.\n\n\n———. 2023. “Probabilistic Machine\nLearning.” MIT Press. https://mitpress.mit.edu/9780262046824/probabilistic-machine-learning/.\n\n\nNavarro, Danielle. 2018. Learning Statistics with\nR. https://learningstatisticswithr.com.\n\n\nNeal, Radford M. 1996. “Priors for Infinite\nNetworks.” In Bayesian Learning for\nNeural Networks, edited by Radford M.\nNeal, 29–53. New York, NY: Springer. https://doi.org/10.1007/978-1-4612-0745-0_2.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized\nLinear Models.” Royal Statistical\nSociety. Journal. Series A: General 135 (3): 370–84. https://doi.org/10.2307/2344614.\n\n\nNiculescu-Mizil, Alexandru, and Rich Caruana. 2005. “Predicting\nGood Probabilities with Supervised Learning.” In Proceedings\nof the 22nd International Conference on Machine Learning -\nICML ’05, 625–32. Bonn, Germany: ACM Press. https://doi.org/10.1145/1102351.1102430.\n\n\nRaschka, Sebastian. 2014. “About Feature\nScaling and Normalization.”\nSebastian Raschka, PhD. https://sebastianraschka.com/Articles/2014_about_feature_scaling.html.\n\n\n———. 2022. Machine Learning with PyTorch\nand Scikit-Learn. https://sebastianraschka.com/books/machine-learning-with-pytorch-and-scikit-learn/.\n\n\n———. 2023. Machine Learning Q and\nAI. https://nostarch.com/machine-learning-q-and-ai.\n\n\nRasmussen, Carl Edward, and Christopher K. I. Williams. 2005.\nGaussian Processes for Machine\nLearning. The MIT Press. https://doi.org/10.7551/mitpress/3206.001.0001.\n\n\nRocca, Baptiste. 2019. “Handling Imbalanced Datasets in Machine\nLearning.” Medium. https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28.\n\n\nRovine, Michael J, and Douglas R Anderson. 2004. “Peirce and\nBowditch.” The American Statistician 58\n(3): 232–36. https://doi.org/10.1198/000313004X964.\n\n\nSchmidhuber, Juergen. 2022. “Annotated History of\nModern AI and Deep\nLearning.” arXiv. https://doi.org/10.48550/arXiv.2212.11279.\n\n\nShalizi, Cosma. 2015. “F-Tests, R2, and\nOther Distractions.” https://www.stat.cmu.edu/~cshalizi/mreg/15/.\n\n\nStatQuest with Josh Starmer. 2019a. “Gradient\nDescent, Step-by-Step.” https://www.youtube.com/watch?v=sDv4f4s2SB8.\n\n\n———. 2019b. “Stochastic Gradient\nDescent, Clearly\nExplained!!!” https://www.youtube.com/watch?v=vMh0zPT0tLI.\n\n\n———. 2021. “Bootstrapping Main\nIdeas!!!” https://www.youtube.com/watch?v=Xz0x-8-cgaQ.\n\n\nVig, Jesse. 2019. “Deconstructing BERT,\nPart 2: Visualizing the Inner\nWorkings of Attention.”\nMedium. https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1.\n\n\nWalker, Kyle E. 2023. Analyzing US Census\nData. https://walker-data.com/census-r.\n\n\nWeed, Ethan, and Danielle Navarro. 2021. Learning\nStatistics with Python — Learning\nStatistics with Python. https://ethanweed.github.io/pythonbook/landingpage.html.\n\n\nWikipedia. 2023. “Relationships Among Probability\nDistributions.” Wikipedia. https://en.wikipedia.org/wiki/Relationships_among_probability_distributions.\n\n\nWood, Simon N. 2017. Generalized Additive\nModels: An Introduction with\nR, Second Edition. 2nd ed.\nBoca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781315370279.\n\n\nWooldridge, Jeffrey M. 2012. Introductory Econometrics:\nA Modern Approach. 5th\nedition. Mason, OH: Cengage Learning.\n\n\nZhang, Aston, Zack Lipton, Mu Li, and Alex Smola. 2023. “Dive into\nDeep Learning — Dive into\nDeep Learning 1.0.3 Documentation.” https://d2l.ai/index.html.",
    "crumbs": [
      "Appendices",
      "Additional Topics",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>References</span>"
    ]
  }
]