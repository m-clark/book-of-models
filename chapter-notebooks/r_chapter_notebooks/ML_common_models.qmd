---
title: "Common Models in Machine Learning"
format: html
---

## Data Setup

```{r}
library(tidyverse)

df_heart = read_csv("https://tinyurl.com/heartdiseaseprocessed") |> 
    mutate(across(where(is.character), as.factor))

df_heart_num = read_csv("https://tinyurl.com/heartdiseaseprocessednumeric")

# for use with for mlr3
X = df_heart_num |> 
    as_tibble() |> 
    mutate(heart_disease = factor(heart_disease)) |> 
    janitor::clean_names() # remove some symbols

prevalence = mean(df_heart_num$heart_disease)
majority = pmax(prevalence, 1 - prevalence)
```


## Baseline Elastic Net Model

```{r}
library(mlr3)
library(mlr3learners)

tsk_elastic = as_task_classif(
    X,
    target = "heart_disease"
)

model_elastic = lrn(
    "classif.cv_glmnet", 
    nfolds = 5, 
    type.measure = "class", 
    alpha = 0.5
)

model_elastic_cv = resample(
    task = tsk_elastic,
    learner = model_elastic,
    resampling = rsmp("cv", folds = 5)
)

# model_elastic_cv$aggregate(msr('classif.acc')) # default output
```


## Tree-based Models

```{r}
# for lightgbm, you need mlr3extralearners and lightgbm package installed
# mlr3extralearners available from github
# remotes::install_github("mlr-org/mlr3extralearners@*release")
library(mlr3extralearners) 

set.seed(1234)

# Define task
# For consistency we use X, but lgbm can handle factors and missing data 
# and so we can use the original df_heart if desired
tsk_boost = as_task_classif(
    df_heart,                   # can use the 'raw' data
    target = "heart_disease"
)

# Define learner
model_boost = lrn(
    "classif.lightgbm",
    num_iterations = 1000,
    max_depth = 5,
    learning_rate = 1e-3
)

# Cross-validation
model_boost_cv = resample(
    task = tsk_boost,
    learner = model_boost,
    resampling = rsmp("cv", folds = 5)
)
```

## Neural Network Model

There's no simple MLP model in the R world, so we use pytorch via mlr3torch. It is also an experimental package, and we can't promise how well it might work for you (we had many issues, and it's documentation is not very helpful).

```{r}
library(mlr3torch)

learner_mlp = lrn(
    "classif.mlp",
    # defining network parameters
    neurons = c(200, 200, 200),
    # training parameters
    batch_size = 16,
    epochs = 50,
    # Defining the optimizer, loss, and callbacks
    optimizer = t_opt("adam", lr = 1e-3),
    loss = t_loss("cross_entropy"),
    # Measures to track
    measures_train = msrs(c("classif.logloss")),
    validate = .1, # Proportion of data to use for validation
    measures_valid = msrs(c("classif.logloss", "classif.ce")),
    # predict type (required by logloss)
    predict_type = "prob",
    seed = 123
)

tsk_mlp = as_task_classif(
    x = X,
    target = 'heart_disease'
)

# this will take a few seconds depending on your chosen settings and hardware
model_mlp_cv = resample(
    task = tsk_mlp,
    learner = learner_mlp,
    resampling = rsmp("cv", folds = 5),
)

model_mlp_cv$aggregate(msr("classif.acc")) # default output
```


## A Tuned Example

```{r}
set.seed(1234)
library(mlr3tuning)

tsk_model_boost_cv_tune = as_task_classif(
    df_heart,
    target = "heart_disease"
)

split = partition(tsk_model_boost_cv_tune, ratio = .8)

lrn_lgbm = lrn(
    "classif.lightgbm",
    num_iterations = to_tune(c(500, 1000)),
    learning_rate = to_tune(1e-3, 1e-1, logscale = TRUE),
    max_depth = to_tune(c(3, 5, 7, 9)),
    min_data_in_leaf = to_tune(c(1, 5, 10))
)

model_boost_cv_tune = auto_tuner(
    tuner = tnr("random_search"),
    learner = lrn_lgbm,
    resampling = rsmp("cv", folds = 5),
    measure = msr("classif.acc"),
    terminator = trm("evals", n_evals = 10)
)

model_boost_cv_tune$train(tsk_model_boost_cv_tune, row_ids = split$train)
model_boost_cv_tune$predict(tsk_model_boost_cv_tune, row_ids = split$test)$score(msr("classif.acc"))
```


## Comparing Models

This is one approach to comparing models with mlr3. As mentioned in the text, your results may vary depending on the data and the settings you choose, and will definitely be a little different from the depicted Python results.

For the MLP, there doesn't appear a way to tune over a list of layer values, so we search the space of them in an admittedly convoluted way. See the following link for details:

https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tune-trafo



```{r}
library(mlr3verse)
library(mlr3torch)

set.seed(1234)
split_prop = .8
split = rsample::initial_split(X, prop = split_prop)

df_train = rsample::training(split)
df_test  = rsample::testing(split)

tuner = tnr("random_search")
cv = rsmp("cv", folds = 10)
metric = msr("classif.acc")

task = as_task_classif(
    df_train,
    target = "heart_disease"
)

# Tune and setup glmnet
lrn_glmnet = lrn(
    "classif.cv_glmnet",
    predict_type = "prob",
    alpha = to_tune(0, 1.0), # mixing
    s = to_tune(1e-3, 1e-1) # penalty
)

lrn_glmnet_tune = AutoTuner$new(
    learner = lrn_glmnet,
    resampling = cv,
    measure = metric,
    tuner = tnr("random_search"),
    terminator = trm("evals", n_evals = 10)
)

lrn_glmnet_tune$train(task)

lrn_glmnet_tune$tuning_result$learner_param_vals

lrn_glmnet = lrn(
    "classif.glmnet",
    predict_type = "prob"
)

lrn_glmnet$param_set$values = lrn_glmnet_tune$tuning_result$learner_param_vals[[1]]


# Tune and setup lgbm
lrn_lgbm = lrn(
    "classif.lightgbm",
    predict_type = "prob",
    num_iterations = to_tune(c(250, 500, 1000)),
    learning_rate = to_tune(1e-3, 1e-1),
    max_depth = to_tune(c(2, 3, 5, 7, 9)),
    min_data_in_leaf = to_tune(c(1, 5, 10)),
    feature_fraction = to_tune(0.75, 1.0),
    lambda_l1 = to_tune(0, 1.0),
    lambda_l2 = to_tune(0, 1.0)
)

lrn_lgbm_tune = AutoTuner$new(
    learner = lrn_lgbm,
    resampling = cv,
    measure = metric,
    tuner = tnr("random_search"),
    terminator = trm("evals", n_evals = 10)
)

lrn_lgbm_tune$train(task)

lrn_lgbm = lrn(
    "classif.lightgbm",
    predict_type = "prob"
)

lrn_lgbm$param_set$values = lrn_lgbm_tune$tuning_result$learner_param_vals[[1]]

# Tune and setup MLP (will take a couple minutes for each eval)

search_space = ps(
    ## p_int for integer valued parameters
    nodes = p_int(100, 200),
    layers = p_int(lower = 2, upper = 2),
    .extra_trafo = function(x, param_set) {
        x$neurons = rep(x$nodes, x$layers)
        x$nodes = x$layers = NULL
        return(x)
    }
)


learner_mlp = lrn(
    "classif.mlp",
    # defining network parameters
    # neurons = c(c(100, 100), c(200, 200)), 
    # training parameters
    batch_size = 16,
    epochs = 200,
    # Defining the optimizer, loss, and callbacks
    optimizer = t_opt("adam", lr = 1e-3),
    loss = t_loss("cross_entropy"),
    # Measures to track
    measures_train = msrs(c("classif.logloss")),
    validate = .1, # Proportion of data to use for validation
    measures_valid = msrs(c("classif.logloss", "classif.ce")),
    # predict type (required by logloss)
    predict_type = "prob"
)

lrn_mlp_tune = AutoTuner$new(
    learner = learner_mlp,
    resampling = cv,
    measure = metric,
    search_space = search_space,
    tuner = tnr("random_search"),
    terminator = trm("evals", n_evals = 10)
)

lrn_mlp_tune$train(task)


lrn_mlp = lrn(
    "classif.mlp",
    predict_type = "prob",
    validate = .1
)

lrn_mlp$param_set$values = lrn_mlp_tune$tuning_result$learner_param_vals[[1]]
```

```{r}
benchmark_grid = benchmark_grid(
    task, 
    c(lrn_lgbm, lrn_glmnet, lrn_mlp), 
    rsmp("cv", folds = 10)
)

bmr = benchmark(benchmark_grid, store_models = FALSE)
```

```{r}
bmr$aggregate(msr("classif.acc"))
```


## Feature Importance

```{r}
# Get feature importances
model_boost_cv_tune$learner$importance()
```