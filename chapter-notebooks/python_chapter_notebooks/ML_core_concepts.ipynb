{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Concepts in Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    roc_auc_score, roc_curve, auc, confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micl/anaconda3/envs/book-of-models/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "df_reviews = pd.read_csv('https://tinyurl.com/moviereviewsdata')\n",
    "\n",
    "X = df_reviews[\n",
    "    [\n",
    "        'word_count',\n",
    "        'age',\n",
    "        'review_year',\n",
    "        'release_year',\n",
    "        'length_minutes',\n",
    "        'children_in_home',\n",
    "        'total_reviews',\n",
    "    ]\n",
    "]\n",
    "\n",
    "y = df_reviews['rating']\n",
    "y_class = df_reviews['rating_good']\n",
    "\n",
    "model_lin_reg = LinearRegression().fit(X, y)\n",
    "\n",
    "# note that sklearn uses regularization by default for logistic regression\n",
    "model_log_reg = LogisticRegression().fit(X, y_class)\n",
    "\n",
    "y_pred_linreg = model_lin_reg.predict(X)\n",
    "y_pred_logreg = model_log_reg.predict(X)\n",
    "\n",
    "# regression metrics\n",
    "rmse = root_mean_squared_error(y, y_pred_linreg)\n",
    "mae = mean_absolute_error(y, y_pred_linreg)\n",
    "r2 = r2_score(y, y_pred_linreg)\n",
    "\n",
    "# classification metrics\n",
    "accuracy = accuracy_score(y_class, y_pred_logreg)\n",
    "precision = precision_score(y_class, y_pred_logreg)\n",
    "recall = recall_score(y_class, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>0.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prediction   rmse\n",
       "0      Train  0.515\n",
       "1       Test  0.530"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_reviews[[\n",
    "        'word_count',\n",
    "        'age',\n",
    "        'review_year',\n",
    "        'release_year',\n",
    "        'length_minutes',\n",
    "        'children_in_home',\n",
    "        'total_reviews',\n",
    "    ]]\n",
    "\n",
    "y = df_reviews['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.25, \n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "model_linreg_train = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# get predictions\n",
    "y_pred_train = model_linreg_train.predict(X_train)\n",
    "y_pred_test = model_linreg_train.predict(X_test)\n",
    "\n",
    "# get RMSE\n",
    "rmse_train = root_mean_squared_error(y_train, y_pred_train)\n",
    "rmse_test = root_mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "pd.DataFrame(\n",
    "    dict(\n",
    "        prediction = ['Train', 'Test'],\n",
    "        rmse = [rmse_train, rmse_test]\n",
    "    )\n",
    ").round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.671"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_reviews.filter(regex='_sc$') # grab the standardized features\n",
    "y = df_reviews['rating_good']\n",
    "\n",
    "# Cs is the (inverse) penalty parameter;\n",
    "model_logistic_l2 = LogisticRegressionCV(\n",
    "    penalty='l2',      # penalty type\n",
    "    Cs=[1],            # penalty parameter value \n",
    "    cv=5, \n",
    "    max_iter=1000, \n",
    "    verbose=False\n",
    ").fit(X, y)\n",
    "\n",
    "# model_logistic_l2.scores_  # show the accuracy score for each fold\n",
    "\n",
    "# print the average accuracy score\n",
    "model_logistic_l2.scores_[1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset from the previous example into\n",
    "# training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.25, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# define the parameter values for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 2, 5, 10, 20],\n",
    "}\n",
    "\n",
    "# perform k-fold cross-validation to select the best penalty parameter\n",
    "# Note that LogisticRegression by default is ridge regression for scikit-learn\n",
    "model_logistic_grid = GridSearchCV(\n",
    "    LogisticRegression(), \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy'\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# if you want to inspect the results\n",
    "best_model = model_logistic_grid.best_estimator_  \n",
    "best_param = model_logistic_grid.best_params_['C']\n",
    "\n",
    "# apply the best model to the test set and calculate accuracy\n",
    "acc_train = model_logistic_grid.score(X_train, y_train)\n",
    "acc_test  = model_logistic_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.692"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pipeline\n",
    "logistic_cv_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    LogisticRegressionCV(penalty='l2', Cs=[1], cv=5, max_iter=1000),\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "logistic_cv_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Assess the pipeline on test\n",
    "y_pred = logistic_cv_pipeline.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Save the pipeline\n",
    "# from joblib import dump, load\n",
    "# dump(logistic_cv_pipeline, 'logistic_cv_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the metrics and model you want\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.metrics import accuracy_score, roc_auc_score, recall_score \n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# pipeline = make_pipeline(\n",
    "#     SimpleImputer(strategy='mean'),\n",
    "#     StandardScaler(),\n",
    "#     RandomizedSearchCV(\n",
    "#         DecisionTreeClassifier(), \n",
    "#         param_distributions={'max_depth': [2, 5, 7]}, \n",
    "#         cv=5, \n",
    "#         scoring='???',  # change to some other metric\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# # extract the best model from the pipeline\n",
    "# best_model = pipeline.named_steps['randomizedsearchcv'].best_estimator_\n",
    "\n",
    "# # extract the best parameter from the pipeline\n",
    "# best_model.max_depth\n",
    "\n",
    "# # ???(y_test, y_pred) # use your chosen metric on the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-of-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
