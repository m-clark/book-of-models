{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_happiness = (\n",
    "    pd.read_csv('https://tinyurl.com/worldhappiness2018')\n",
    "    .dropna()\n",
    "    .rename(columns = {'happiness_score': 'happiness'})\n",
    "    .filter(regex = '_sc|country|happ')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE comparison between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>5.086298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.637560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model       MSE\n",
       "0     A  5.086298\n",
       "1     B  0.637560"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_happiness['happiness']\n",
    "\n",
    "# Calculate the error for the guess of four\n",
    "prediction = np.min(df_happiness['happiness']) + 1 * df_happiness['life_exp_sc']\n",
    "mse_model_A   = np.mean((y - prediction)**2)\n",
    "\n",
    "# Calculate the error for our other guess\n",
    "prediction = y.mean() + .5 * df_happiness['life_exp_sc']\n",
    "mse_model_B  = np.mean((y - prediction)**2)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Model': ['A', 'B'],\n",
    "    'MSE': [mse_model_A, mse_model_B]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later comparison\n",
    "model_lr_happy = smf.ols('happiness ~ life_exp_sc', data = df_happiness).fit()\n",
    "\n",
    "def ols(par, X, y):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the predicted values\n",
    "    y_hat = X @ par  # @ is matrix multiplication\n",
    "    \n",
    "    # Calculate the mean of the squared errors\n",
    "    value = np.mean((y - y_hat)**2)\n",
    "    \n",
    "    # Return the objective value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.77700449624871"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "guesses = pd.DataFrame(\n",
    "    product(\n",
    "        np.arange(1, 7, 0.1),\n",
    "        np.arange(-1, 1, 0.1)\n",
    "    ),\n",
    "    columns = ['b0', 'b1']\n",
    ")\n",
    "\n",
    "# Example for one guess\n",
    "ols(\n",
    "    par = guesses.iloc[0,:],\n",
    "    X = df_happiness['life_exp_sc'],\n",
    "    y = df_happiness['happiness']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>objective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.490675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      b0   b1  objective\n",
       "899  5.4  0.9   0.490675"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses['objective'] = guesses.apply(\n",
    "    lambda x: ols(\n",
    "        par = x, \n",
    "        X = df_happiness['life_exp_sc'], \n",
    "        y = df_happiness['happiness']\n",
    "    ),\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "min_loss = guesses[guesses['objective'] == guesses['objective'].min()]\n",
    "\n",
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(const          5.444832\n",
       " life_exp_sc    0.887796\n",
       " dtype: float64,\n",
       " 0.4973994106686574)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_happy_life = sm.OLS(df_happiness['happiness'], sm.add_constant(df_happiness['life_exp_sc'])).fit()\n",
    "\n",
    "model_lr_happy_life.params, model_lr_happy_life.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 0.48851727833528863\n",
       "        x: [ 5.445e+00  8.878e-01]\n",
       "      nit: 5\n",
       "      jac: [-7.451e-09  0.000e+00]\n",
       " hess_inv: [[ 5.000e-01  3.539e-06]\n",
       "            [ 3.539e-06  5.045e-01]]\n",
       "     nfev: 21\n",
       "     njev: 7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_ols_optim = minimize(\n",
    "    fun  = ols,\n",
    "    x0   = np.array([1., 0.]),\n",
    "    args = (\n",
    "        np.array(df_happiness['life_exp_sc']), \n",
    "        np.array(df_happiness['happiness'])\n",
    "    ),\n",
    "    method  = 'BFGS',   # optimization algorithm\n",
    "    tol     = 1e-6,     # tolerance\n",
    "    options = {\n",
    "        'maxiter': 500  # max iterations\n",
    "    }\n",
    ")\n",
    "\n",
    "our_ols_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10798193, 0.22184167])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two example life expectancy scores, at the mean (0) and 1 sd above\n",
    "life_expectancy = np.array([0, 1])\n",
    "\n",
    "# observed happiness scores\n",
    "happiness = np.array([4, 5.2])\n",
    "\n",
    "# predicted happiness with rounded coefs\n",
    "mu = 5 + 1 * life_expectancy\n",
    "\n",
    "# just a guess for sigma\n",
    "sigma = .5\n",
    "\n",
    "# likelihood for each observation\n",
    "L = norm.pdf(happiness, loc = mu, scale = sigma)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35819022,  5.44483214,  0.88779604])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_likelihood(par, X, y):\n",
    "    \n",
    "    # setup\n",
    "    X = np.c_[np.ones(X.shape[0]), X] # add a column of 1s for the intercept\n",
    "    beta   = par[1:]         # coefficients\n",
    "    sigma  = np.exp(par[0])  # error sd, exp keeps positive\n",
    "    N = X.shape[0]\n",
    "\n",
    "    LP = X @ beta            # linear predictor\n",
    "    mu = LP                  # identity link in the glm sense\n",
    "\n",
    "    # calculate (log) likelihood\n",
    "    ll = norm.logpdf(y, loc = mu, scale = sigma)\n",
    "\n",
    "    value = -np.sum(ll)      # negative for minimization\n",
    "\n",
    "    return value\n",
    "\n",
    "our_max_like = minimize(\n",
    "    fun  = max_likelihood,\n",
    "    x0   = np.array([1, 0, 0]),\n",
    "    args = (\n",
    "        np.array(df_happiness['life_exp_sc']), \n",
    "        np.array(df_happiness['happiness'])\n",
    "    )\n",
    ")\n",
    "\n",
    "our_max_like['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use lambda_ because lambda is a reserved word in python\n",
    "def ridge(par, X, y, lambda_ = 0):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the predicted values\n",
    "    mu = X @ par\n",
    "    \n",
    "    # Calculate the error\n",
    "    value = np.sum((y - mu)**2)\n",
    "    \n",
    "    # Add the penalty\n",
    "    value = value + lambda_ * np.sum(par**2)\n",
    "    \n",
    "    return value\n",
    "\n",
    "our_ridge = minimize(\n",
    "    fun  = ridge,\n",
    "    x0   = np.array([0, 0, 0, 0]),\n",
    "    args = (\n",
    "        np.array(df_happiness.drop(columns=['happiness', 'country'])),\n",
    "        np.array(df_happiness['happiness']), \n",
    "        0.1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.439975  ,  0.52422716, -0.1053189 ,  0.43749604])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_ridge['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassification Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_rate(par, X, y, class_threshold = .5):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the 'linear predictor'\n",
    "    mu = X @ par \n",
    "    \n",
    "    # Convert to a probability ('sigmoid' function)\n",
    "    p = 1 / (1 + np.exp(-mu))\n",
    "    \n",
    "    # Convert to a class\n",
    "    predicted_class = np.where(p > class_threshold, 1, 0)\n",
    "    \n",
    "    # Calculate the mean error\n",
    "    value = np.mean(y - predicted_class)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(par, X, y):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the predicted values\n",
    "    y_hat = X @ par\n",
    "    \n",
    "    # Convert to a probability ('sigmoid' function)\n",
    "    y_hat = 1 / (1 + np.exp(-y_hat))\n",
    "    \n",
    "    # likelihood\n",
    "    ll = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n",
    "\n",
    "    value = -np.sum(ll)\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16365245,  1.81715104, -0.46478325,  1.13108169])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_happiness_bin = df_happiness.copy()\n",
    "df_happiness_bin['happiness'] = np.where(df_happiness['happiness'] > 5.5, 1, 0)\n",
    "\n",
    "model_logloss = minimize(\n",
    "    log_loss,\n",
    "    x0 = np.array([0, 0, 0, 0]),\n",
    "    args = (\n",
    "        df_happiness_bin[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']],\n",
    "        df_happiness_bin['happiness']\n",
    "    )\n",
    ")\n",
    "\n",
    "model_glm = smf.glm(\n",
    "    'happiness ~ life_exp_sc + corrupt_sc + gdp_pc_sc',\n",
    "    data   = df_happiness_bin,\n",
    "    family = sm.families.Binomial()\n",
    ").fit()\n",
    "\n",
    "model_logloss['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    par, \n",
    "    X, \n",
    "    y, \n",
    "    tolerance = 1e-3, \n",
    "    maxit = 1000, \n",
    "    learning_rate = 1e-3\n",
    "):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    # initialize\n",
    "    beta = par\n",
    "    loss = np.sum((X @ beta - y)**2)\n",
    "    tol = 1\n",
    "    iter = 1\n",
    "\n",
    "    while (tol > tolerance and iter < maxit):\n",
    "        LP = X @ beta\n",
    "        grad = X.T @ (LP - y)\n",
    "        betaCurrent = beta - learning_rate * grad\n",
    "        tol = np.max(np.abs(betaCurrent - beta))\n",
    "        beta = betaCurrent\n",
    "        loss = np.append(loss, np.sum((LP - y)**2))\n",
    "        iter = iter + 1\n",
    "\n",
    "    output = {\n",
    "        'par': beta,\n",
    "        'loss': loss,\n",
    "        'MSE': np.mean((LP - y)**2),\n",
    "        'iter': iter,\n",
    "        'fitted': LP\n",
    "    }\n",
    "\n",
    "    return output\n",
    "\n",
    "our_gd = gradient_descent(\n",
    "    par = np.array([0, 0, 0, 0]),\n",
    "    X = df_happiness[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']].to_numpy(),\n",
    "    y = df_happiness['happiness'].to_numpy(),\n",
    "    learning_rate = 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.43691264,  0.52121949, -0.10746734,  0.43896778])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_gd['par']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x6/4jhswqxj0sqf_gkgq6lw6l880000gn/T/ipykernel_51608/2270994482.py:55: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  fits[i] = LP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 5.42765734,  0.53391505, -0.15014284,  0.39964098]),\n",
       " 0.36857925293717886)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    par, # parameter estimates\n",
    "    X,   # model matrix\n",
    "    y,   # target variable\n",
    "    learning_rate = 1, # the learning rate\n",
    "    stepsize_tau = 0,  # if > 0, a check on the LR at early iterations\n",
    "    average = False    # a variation of the approach\n",
    "):\n",
    "    # initialize\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # shuffle the data\n",
    "    idx = np.random.choice(\n",
    "        df_happiness.shape[0], \n",
    "        df_happiness.shape[0], \n",
    "        replace = False\n",
    "    )\n",
    "    X = X[idx, :]\n",
    "    y = y[idx]\n",
    "    \n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    beta = par\n",
    "\n",
    "    # Collect all estimates\n",
    "    betamat = np.zeros((X.shape[0], beta.shape[0]))\n",
    "\n",
    "    # Collect fitted values at each point))\n",
    "    fits = np.zeros(X.shape[0])\n",
    "\n",
    "    # Collect loss at each point\n",
    "    loss = np.zeros(X.shape[0])\n",
    "\n",
    "    # adagrad per parameter learning rate adjustment\n",
    "    s = 0\n",
    "\n",
    "    # a smoothing term to avoid division by zero\n",
    "    eps = 1e-8\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        Xi = X[None, i, :]\n",
    "        yi = y[i]\n",
    "\n",
    "        # matrix operations not necessary here,\n",
    "        # but makes consistent with previous gd func\n",
    "        LP = Xi @ beta\n",
    "        grad = Xi.T @ (LP - yi)\n",
    "        s = s + grad**2 # adagrad approach\n",
    "\n",
    "        # update\n",
    "        beta = beta - learning_rate / \\\n",
    "            (stepsize_tau + np.sqrt(s + eps)) * grad\n",
    "\n",
    "        betamat[i, :] = beta\n",
    "\n",
    "        fits[i] = LP\n",
    "        loss[i] = np.sum((LP - yi)**2)\n",
    "\n",
    "    LP = X @ beta\n",
    "    lastloss = np.sum((LP - y)**2)\n",
    "\n",
    "    output = {\n",
    "        'par': beta,          # final estimates\n",
    "        'par_chain': betamat, # estimates at each iteration\n",
    "        'MSE': lastloss / X.shape[0],\n",
    "        'predictions': LP\n",
    "    }\n",
    "\n",
    "    return output\n",
    "\n",
    "X = df_happiness[['life_exp_sc', 'corrupt_sc', 'gdp_pc_sc']].to_numpy()\n",
    "y = df_happiness['happiness'].to_numpy()\n",
    "\n",
    "our_sgd = stochastic_gradient_descent(\n",
    "    par = np.array([np.mean(df_happiness['happiness']), 0, 0, 0]),\n",
    "    X = X,\n",
    "    y = y,\n",
    "    learning_rate = .15,\n",
    "    stepsize_tau = .1\n",
    ")\n",
    "\n",
    "our_sgd['par'], our_sgd['MSE']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-of-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
