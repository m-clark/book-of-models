# Until Next Time... {#conclusion}

> All models are stardust.

As we wrap things up, let's revisit some of the key points we've covered in this text, and talk more about the modeling process in general.

## How to Think About Models {#conc-models-think}

When we first started our discussion of models in data science, we talked about how a model is a simplified representation of reality. They start as ideas based on our intuition or experience, and they can sometimes be very simple ones. But at some point we start to think of them more formally, as a step towards testing those ideas in the real world. Models are then put into mathematical equations to help us express them in ways that we can more easily reference and be more explicit about. Even then, pretty much all models you've seen so far can be expressed as follows:

<!-- the correct display of annotation seems to only work for pdf (which will not do anything for the color); so here we add image see annotated_equations.qmd -->
![A generic model of any kind](img/eq-model-basic.png){#eq-generic-model}

In words, this equation says that the target variable $y$ is a function of the feature inputs $X$, along with anything else that we don't include in that set. This is the basic form of a model, and it's the same for linear regression, logistic regression, and even random forests and neural networks, though in the latter case it is more like functions within functions within functions, each of which might represent a certain layer of the network.

To aid our understanding, we try to get more visual with models, expressing them as graphical models or even in more complex ways with neural networks[^lda], as in the following images. 

[^lda]: The LDA model depicted [from Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Model) was one of the [early machine learning models](https://dl.acm.org/doi/10.5555/944919.944937) for understanding natural language, and in particular to extract topics from text. It was a lot of fun to play with these, but it took a lot of pre-processing of text to get them to work at all, and they were performed pretty poorly in practice. That model may look like something peculiar, but it's not much more than a flexible PCA on a matrix of word counts, or from another perspective, a Bayesian multinomial model.

![Logistic Regression Model](img/graphical_logistic.svg)


![Plate Notation for LDA](/img/plate_notation_LDA.png){width=66%}


![[Just part of the Nano GPT model](https://bbycroft.net/llm)](/img/nano_gpt.png){width=66%}


But even now these models are still just at the idea stage, and we ultimately need to see how they work in the world, make predictions, and help us to make important decisions. We've seen how to do this with linear models of various forms, and more unusual model implementations in the form of tree-based models, and even highly complex neural networks. These are the tools that allow us to take our ideas and turn them into something that can be used to make decisions, and that's the real power of models in data science.

At this point, we can maybe break our thinking about models into the following components:

**Model**

In data science, a model refers to a unique (mathematical) implementation we're using to answer our questions. It specifies the **architecture** of the model, which might be a simple linear component, a series of trees, or neural network. In addition, the model specifies the **functional form**, the $f()$ in our equation, that translates inputs to outputs, and the **parameters** required to make that transformation. In code, the model is implemented with functions such as `lm` in R, or in Python, an `XGBoostClassifier`, or a pytorch `nn.Model` class.


**Task**

The task can be thought of as the goal of our model, which might be defined as regression, classification, ranking, or next word prediction. It is closely tied to the objective function, which is like a measure of correspondence between the model output and the target we're trying to understand. The objective functions provides the model a goal- minimize target-output discrepancy or maximize similarity. As an example, if our target is numeric and our task is 'regression', we can use mean squared error, which provides a measure of the prediction-target discrepancy, as an objective function.

**Algorithm**

Algorithms allow us to estimate the parameters of the model, typically in an iterative fashion, moving from one guess to a hopefully better one. This could be maximum likelihood, bayesian estimation, or stochastic gradient descent, or specific implementations of these, such as penalized likelihood, hamilton monte carlo, or backpropagation.

So when we think about models, we start with an idea, but in the end it needs to be expressed in a form that suggests an architecture that can take in data and make outputs in the form of predictions or what can be amenable to such. With that in place, we need an algorithm that can obtain parameter estimates of the model, and a way to evaluate how well the model is doing. While this is enough, it only gets us the bare minimum of result. There are many more things we have to do to help us interpret results, understand its performance, and get a sense of its limitations.



## Families of Models {#conc-models-families}

Though there are many models for tabular data out there, we can group them in a fairly simple way that would cover most of the standard problems you'll come across.



**GLM and Related**

Here we have standard linear models with a focus on interpretability. Basically anything you'd find in a traditional stats or econometrics textbook would belong to this 'family'.

- Includes: GLM, survival, ordinal, time-series, other distributions (beta, tweedie)
- Best for: small data situations (samples and features), a baseline model, a causal model, post-model analysis of the results from more complex models
- Primary strength: ease of estimation, interpretability, uncertainty estimation
- Primary weakness: relatively poor prediction, may not capture natural complexity

**Penalized Regression & Friends**

This includes things we might use in a transition toward machine learning such as linear models with regularization, or more complex statistical models that explicitly include nonlinearities and other complexity. In addition, here our focus shifts more towards prediction.

- Includes: lasso/ridge, mixed models, GAMs, Bayesian\*
- Best for: small to large data, possibly a relatively large number of features (esp. lasso), baseline model
- Primary strength: increased predictive capability while maintaining interpretability
- Primary weakness: interpretability can decrease, estimation difficulty can start to arise (convergence issues, uncertainty)


**Trees & Nets**

This family includes tree-based models and neural networks, which are almost exclusively focused on prediction performance by default, and represent a significant increase in complexity and computational requirements.

- Includes random forests, gradient boosting, neural networks ('basis function models')
- Best for: prediction/performance
- Primary strength: prediction, ability to handle potentially very large data and numbers of features
- Primary weakness: interpretability and uncertainty estimation

The differences from one model family to the next are not large, especially for our first two, which may only differ in the likelihood function, the penalty term, or just a shift in focus. The third group is a bit different, but it mostly just extends the application of nonlinear and interaction effects we can implement from the first groups, allowing for more computational capacity and flexibility.



### A Simple Modeling Toolbox

In practice, just a couple models from the ones you've seen in this text can provide a lot of power. Here's a simple toolbox that can cover a lot of the ground you'd need to cover in a typical data science project:

- Penalized regression: Lasso, ridge and similar keep it linear while increasing predictive power and allowing for additional capacity to handle more features.
- GAM: simplifies to GLM and mixed models if desired, can handle nonlinear relationships, interactions, and uses a penalized approach, but can also just be a linear regression. It also can be used in time-series and spatial data contexts with ease, making it a very versatile option.
- Boosting/tree-based models: As of this writing, boosting approaches are consistently the best predictive performance for tabular data, and quite computationally efficient. That's reason enough to know how to use them.
- A Basic MLP: An MLP, particularly one that incorporates embeddings for categorical/text features, is a very powerful tool, and you can combine it with other deep learning models applied to other types of data. Ultimately, we may come to an implementation that is able to handle anything we throw at in the tabular world, but we're not quite there yet.

Besides the models, having a good idea of how to evaluate your models (cross-validation, metrics), how to interpret them (coefficients, SHAP, feature importance, uncertainty), and how to handle the data you're working with is just as important. We've covered a lot of this in the text, but there's always more to learn, and more to practice.



:::{.callout type='info' title="The Tweener" collapse="true"}
Despite getting an occasional shoutout, GAMs appear to still be quite underrated in the data science community, probably because the tools in Python to implement and explore them are relatively lacking[^gamr]. On the plus side, the tools in R are super fantastic.

Whatever you use, GAMs are a great way to handle nonlinear relationships, interactions, and penalization in a way that can be more interpretable than boosting or a neural network. They can be used in a wide variety of contexts, and can be a great way to get more predictive power while staying within a traditional linear model setting. They can also be used in time-series and spatial data contexts. So if you're looking for a way to get a bit more out of your linear models without diving into deep learning, GAMs are a great place to start and are often a tough model to beat for those that know how to use them.
:::

[^gamr]: Until Python can go from model to visualizing the marginal effect with uncertainty in two or three lines of code (even if a Bayesian implementation), possibly on millions of observations in a few seconds, and even visualizing the derivatives (also with uncertainty estimates), it's not going to be as easy to use as R for GAMs. But here's hoping the current efforts continue there.



## More Models 







For Numeric & Binary/multiclass

Awareness: time series, dimension reduction (e.g. PCA, embeddings, time-based)

Periphery: ordinal, survival, ranks, spatial,  etc. dive in as needed

Metrics: RMSE, likelihood/log loss, AUC, Prec/Recall, F1, AUPRC, brier


## How to Choose?

People love to say that 'all models are wrong, but some are useful'[^box]. We prefer to think of this a bit differently. There is no (necessarily) wrong model to use to answer your question, and there's no guarantee that you would come to a different conclusion from using a simple correlation than you would from a complex neural network. But some models are typically more useful depending on the context, and some are more useful depending on the question you're asking.

[^box]: George Box, a famous statistician, said this in 1976.

In the end, nothing says you can't use multiple models to answer your question, and in fact, this is often a good idea assuming you have the time and resources to do so. As we've talked about, you can use a simple model to get a baseline, and then use a more complex model to see if you can improve on that. You can use a model that's easy to interpret to get a sense of what's going on, and then use a model that's better at prediction. Even when your primary focus is prediction, you can combine models to potentially get a better result, or just to have more to talk about. 

And that's the main thing, you don't have to restrict yourself when it comes to modeling in data science, and you shouldn't. The key is to understand what you're trying to do, and to use the right tools for the job.



## Key Steps in Modeling

1. **Understand the problem**: Your general problem before starting out is usually easy to express, though it can be difficult to pin down.

What are you trying to predict? What are the features you have to work with? What are the constraints on your model? What are the consequences of your predictions? Why do you even care about any of this?

2. Understand the data: So often in consulting in industry and academia we've seen cases where the available data is simply not suited to answer the question at hand.

## The Hard Part {#conc-models-hard}

The hard part of modeling is not so much the model itself, but everything else that goes into it and what you do with it after. It is often difficult to come up with the original idea for a model, but it's even harder to get it to work in practice. This is because the model is only a part of the process, and the rest of the process is where most of the work is done.


### The Data {#conc-models-hard-data}

Almost all of your model performance is going to come from the data you have and how you've preprocessed it, from insuring its integrity to feature engineering. Some models will definitely work better than others in certain situations, but there are no guarantees, and often there will be little practical difference in performance. But you can gain in performance by understanding your data better, and by understanding the limitations of your model. Better data typically requires domain knowledge to help reduce noise and irrelevant information, and more domain knowledge can provide insights to create new features that may help your model perform better. Simply exploring the data fully will uncover bugs and issues that can be fixed, and will help you understand the relationships between your features and your target.

### The Interpretation {#conc-models-hard-interpretation}

Once you have a model, you need to understand what it's telling you. This can be as simple as looking at the coefficients of a linear model, or as complex as trying to understand the output of a neural network. Once you get past a linear regression though, expect things to get hard. But you need to be able to explain what the model is doing, and how you're coming to the conclusions you've come to. This can be difficult, especially with complex models, but it's important to be able to do this in order to trust your model and to be able to use it effectively.


### What You Do With It {#conc-models-hard-do}

Once you have the model and you (think you) understand it, you need to be able to use it. If you've gone to this sort of trouble, odds are good that you had a good reason for undertaking what can be a very difficult task. We use models to make business decisions, to inform policy, to understand the world around us, and to make our lives better. But the model is only a tool, and it's up to you to use it effectively. This means understanding the limitations of your model, understanding the uncertainty in your predictions, and possibly most importantly, understanding the practical, ethical, scientific and other *consequences* of your decisions. It's at this point that the real value of your model is realized.






## Choose Your Own Adventure {#conc-models-adventure}