# Until Next Time... {#conclusion}

> All models are stardust.

As we wrap things up, let's revisit some of the key points we've covered in this text, and talk more about the modeling process in general.

## How to Think About Models {#conc-models-think}

When we first started our discussion of models in data science, we talked about how a model is a simplified representation of reality. They start as ideas based on our intuition or experience, and they can sometimes be very simple ones. But at some point we start to think of them more formally, as a step towards testing those ideas in the real world. Models are then put into mathematical equations to help us express them in ways that we can more easily reference and be more explicit about. Even then, pretty much all models you've seen so far can be expressed as follows:

<!-- the correct display of annotation seems to only work for pdf (which will not do anything for the color); so here we add image see annotated_equations.qmd -->
![A generic model of any kind](img/eq-model-basic.png){#eq-generic-model}

In words, this equation says that the target variable $y$ is a function of the feature inputs $X$. This is the basic form of a model, and it's the same for linear regression, logistic regression, and even random forests and neural networks, though in the latter case it is more like functions within functions within functions, each of which might represent a certain layer of the network.

To aid our understanding, we try to get more visual with models, expressing them as graphical models or even in more complex ways with neural networks[^lda], as in the following images. 

[^lda]: The LDA model depicted [from Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Model) was one of the [early machine learning models](https://dl.acm.org/doi/10.5555/944919.944937) for understanding natural language, and in particular to extract topics from text. It was a lot of fun to play with these, but it took a lot of pre-processing of text to get them to work at all, and they were performed pretty poorly in practice. That model may look like something peculiar, but it's not much more than a flexible PCA on a matrix of word counts, or from another perspective, a Bayesian multinomial model.

![Logistic Regression Model](img/graphical_logistic.svg)


![Plate Notation for LDA](/img/plate_notation_LDA.png){width=66%}


![[Just part of the Nano GPT model](https://bbycroft.net/llm)](/img/nano_gpt.png){width=66%}


But even now these models are still just at the idea stage, and we ultimately need to see how they work in the world, make predictions, and help us to make important decisions. We've seen how to do this with linear models of various forms, and more unusual model implementations in the form of tree-based models, and even highly complex neural networks. These are the tools that allow us to take our ideas and turn them into something that can be used to make decisions, and that's the real power of models in data science.

At this point, we can maybe break our thinking about models into the following components:

**Model**

In data science, a model refers to a unique (mathematical) implementation we're using to answer our questions. It specifies the **architecture** of the model, which might be a simple linear component, a series of trees, or neural network. In addition, the model specifies the **functional form**, the $f()$ in our equation, that translates inputs to outputs, and the **parameters** required to make that transformation. In code, the model is implemented with functions such as `lm` in R, or in Python, an `XGBoostClassifier`, or a pytorch `nn.Model` class.


**Task**

The task can be thought of as the goal of our model, which might be defined as regression, classification, ranking, or next word prediction. It is closely tied to the **objective function**, which is like a measure of correspondence between the model output and the target we're trying to understand. The objective functions provides the model a goal- minimize target-output discrepancy or maximize similarity. As an example, if our target is numeric and our task is 'regression', we can use mean squared error, which provides a measure of the prediction-target discrepancy, as an objective function.

**Algorithm**

Algorithms allow us to estimate the parameters of the model, typically in an iterative fashion, moving from one guess to a hopefully better one. This could be **maximum likelihood**, **bayesian estimation**, or **stochastic gradient descent**, or specific implementations of these, such as penalized likelihood, hamilton monte carlo, or backpropagation.

So when we think about models, we start with an idea, but in the end it needs to be expressed in a form that suggests an architecture that can take data and make predictions. With that in place, we need an algorithm that can obtain parameter estimates of the model, and a way to evaluate how well the model is doing. While this is enough, it only gets us the bare minimum of result. There are many more things we have to do to help us interpret results, understand its performance, and get a sense of its limitations.



## Families of Models {#conc-models-families}

Though there are many models for tabular data out there, we can group them in a simple way that would cover most of the standard problems you'll come across.



**GLM and Related**

Here we have standard linear models with a focus on interpretability. Basically anything you'd find in a traditional stats or econometrics textbook would belong to this 'family'.

- Includes: GLM, survival, ordinal, time-series, other distributions (beta, tweedie)
- Best for: small data situations (samples and features), a baseline model, a causal model, post-model analysis of the results from more complex models
- Primary strength: ease of estimation, interpretability, uncertainty estimation
- Primary weakness: relatively poor prediction, may not capture natural complexity

**Penalized Regression & Friends**

This includes things we might use in a transition toward machine learning such as linear models with regularization, or more complex statistical models that explicitly include nonlinearities and other complexity. In addition, here our focus shifts more towards prediction.

- Includes: lasso/ridge, mixed models, GAMs, Bayesian\*
- Best for: small to large data, possibly a relatively large number of features (esp. lasso), baseline model
- Primary strength: increased predictive capability while maintaining interpretability
- Primary weakness: interpretability can decrease, estimation difficulty can start to arise (convergence issues, uncertainty)


**Trees & Nets**

This family includes tree-based models and neural networks, which are almost exclusively focused on prediction performance by default, and represent a significant increase in complexity and computational requirements.

- Includes random forests, gradient boosting, neural networks ('basis function models')
- Best for: prediction/performance
- Primary strength: prediction, ability to handle potentially very large data and numbers of features
- Primary weakness: interpretability and uncertainty estimation

The differences from one model family to the next are not large, especially for our first two, which may only differ in the likelihood function, the penalty term, or just a shift in focus. The third group is a bit different, but it mostly just extends the application of nonlinear and interaction effects we can implement from the first groups, allowing for more computational capacity and flexibility.

### A Simple Modeling Toolbox

In practice, just a couple models from the ones you've seen in this text can provide a lot of power. Here's a simple toolbox that can cover a lot of the ground you'd need to cover in a typical data science project:

- Penalized regression: keep it linear while increasing predictive power
- GAM: simplifies to GLM and mixed models if desired, can handle nonlinear relationships, interaction, and a penalized approach, but can also just be a linear regression. Also can be used in time-series and spatial data contexts.
- Boosting/tree-based models: as of this writing, boosting approaches are consistently the best predictive performance for tabular data.
- A Basic MLP: better would be one that incorporates embeddings for categorical/text features. But with this you could combine predictions with other deep learning models.

Besides the models, having a good idea of how to evaluate your models (cross-validation, metrics), how to interpret them (coefficients, SHAP, feature importance), and how to handle the data you're working with is just as important. We've covered a lot of this in the text, but there's always more to learn, and more to practice.



:::{.callout type='info' title="The Tweener" collapse="true"}
Despite getting an occasional shoutout, GAMs appear to still be grossly underrated in the data science community, probably because the tools in Python to implement and explore them are relatively lacking[^gamr]. On the plus side, the tools in R are super fantastic.

Whatever you use, GAMs are a great way to handle nonlinear relationships, interactions, and penalization in a way that can be more interpretable than boosting or a neural network. They can be used in a wide variety of contexts, and can be a great way to get more predictive power while staying within a traditional linear model setting. They can also be used in time-series and spatial data contexts. So if you're looking for a way to get a bit more out of your linear models without diving into deep learning, GAMs are a great place to start and are often a tough model to beat for those that know how to use them.
:::

[^gamr]: Until Python can go from model to visualizing the marginal effect with uncertainty in two or three lines of code (even if a Bayesian implementation), possibly on millions of observations in a few seconds, and even visualizing the derivatives (also with uncertainty estimates), it's not going to be as easy to use as R for GAMs. But here's hoping the current efforts continue there.






<!-- TODO: Maybe move to appendix -->
:::{.content-visible when-format="html"}


## More Models

This is the list of models and related that includes, some we've covered, some we've mentioned and some we've not seen at at all. 


### Linear Models

Simplified Linear Models

- correlation
- t-test and ANOVA
- chi-square

Generalize Linear Models and related

- True GLM e.g. logistic, poisson
- Other distributions: beta regression, tweedie, t (so-called robust), truncated
- Penalized regression: ridge, lasso, elastic net
- Censored outcomes: Survival models, tobit
- Modeling other parameters (e.g. heteroscedastic models that also predict scale in Gaussian linear regression)

Multivariate/multiclass/multipart

- Multivariate regression (multiple targets)
- Multinomial/Categorical/Ordinal regression (>2 classes)
- MANOVA/Linear Discriminant Analysis (these are identical, and can handle multiple outputs or >=2 classes)
- Zero (or some number) -inflated/hurdle/altered
- Mixture models and Cluster analysis
- Two-stage least squares, instrumental variables
- SEM, simultaneous equations


Random Effects

- Mixed effects models (random intercepts/coefficients; "fixed-effects" models are just a restricted case)
- Generalized additive models (GAMMs)
- Gaussian process regression
- Spatial models (CAR, SAR, etc.)
- Time series models (ARIMA and related, e.g. state space)
- Factor analysis

Latent Linear Models

- PCA, Factor Analysis
- Mixture models
- Structural Equation Modeling, Graphical models generally

All of these are explicitly linear models or can be framed as such, and most are either identical in description to what you've already seen or require only a tweak or two - e.g. a different distribution, a different link function, penalizing the coefficients, etc. In other cases, we can bounce from one to the another. For example we can reshape our multivariate outcome to be amenable to a mixed model approach, and get the exact same results. We can potentially add a random effect to any model, and that random effect can be based on time, spatial or other considerations. The important thing to know is that the linear model is a very flexible tool that expands easily, and allows you to model most of the types of outcomes were interested in. As such, it's a very powerful approach to modeling.


### Other ML

There are models you'll typically only see when you're in a machine learning context. They often do not output specific parameters of interest like coefficients or variance components, nor have easy ways to estimate uncertainty. Some are very little used anymore, but are interesting historically or conceptually.

Standard Regression/Classification

- k-Nearest Neighbors
- Naive Bayes
- Support Vector Machines, Boltzmann Machines
- Hidden Markov Models
- undirected graphs, Markov Random Fields, network analysis
- (Single) Decision trees, CART, C4.5, etc.

Latent Models

- PCA, probabilistic PCA, ICA
- Latent Dirichlet Allocation
- Factorization Machines
- (Non-negative) Matrix Factorization
- Dirichlet process

todo: checkout ESL

Historical models like single decision tree, knn-regression, svm, naive bayes, etc. Most of these are note used so much anymore but may be interesting. 

Miscellaneous combinations of models like ensembles/stacking, meta-learners, random forests and bagging, and more. (we demo'd boosting)

Recommender systems, graphical models, dimension reduction

Unsupervised/semi-supervised learning


### Other DL

could mention specific/historical models here, like resnet, bert, GANs, LSTM, etc.
talk about DL applied to tabular
autoencoders, VAEs, etc.
misc - extreme learning machines, etc.

:::





For Numeric & Binary/multiclass

Awareness: time series, dimension reduction (e.g. PCA, embeddings, time-based)

Periphery: ordinal, survival, ranks, spatial,  etc. dive in as needed

Metrics: RMSE, likelihood/log loss, AUC, Prec/Recall, F1, AUPRC, brier


## How to Choose?

People love to say that 'all models are wrong, but some are useful'[^box]. We prefer to think of this a bit differently. There is no wrong model to use to answer your question, and there's no guarantee that you would come to a different conclusion from using a simple correlation than you would from a complex neural network. But some models are typically more useful depending on the context.

[^box]: George Box, a famous statistician, said this in 1976.


## Key Steps in Modeling

1. **Understand the problem**: What are you trying to predict? What are the features you have to work with? What are the constraints on your model? What are the consequences of your predictions? Why do you even care about any of this?


## The Hard Part {#conc-models-hard}

The hard part of modeling is not so much the model itself, but everything else that goes into it and what you do with it after. It is often difficult to come up with the original idea for a model, but it's even harder to get it to work in practice. This is because the model is only a part of the process, and the rest of the process is where most of the work is done.


### The Data {#conc-models-hard-data}

Almost all of your model performance is going to come from the data you have and how you've preprocessed it, from insuring its integrity to feature engineering. Some models will definitely work better than others in certain situations, but there are no guarantees, and often there will be little practical difference in performance. But you can gain in performance by understanding your data better, and by understanding the limitations of your model. Better data typically requires domain knowledge to help reduce noise and irrelevant information, and more domain knowledge can provide insights to create new features that may help your model perform better. Simply exploring the data fully will uncover bugs and issues that can be fixed, and will help you understand the relationships between your features and your target.

### The Interpretation {#conc-models-hard-interpretation}

Once you have a model, you need to understand what it's telling you. This can be as simple as looking at the coefficients of a linear model, or as complex as trying to understand the output of a neural network. Once you get past a linear regression though, expect things to get hard. But you need to be able to explain what the model is doing, and how you're coming to the conclusions you've come to. This can be difficult, especially with complex models, but it's important to be able to do this in order to trust your model and to be able to use it effectively.


### What You Do With It {#conc-models-hard-do}

Once you have the model and you (think you) understand it, you need to be able to use it. If you've gone to this sort of trouble, odds are good that you had a good reason for undertaking what can be a very difficult task. We use models to make business decisions, to inform policy, to understand the world around us, and to make our lives better. But the model is only a tool, and it's up to you to use it effectively. This means understanding the limitations of your model, understanding the uncertainty in your predictions, and possibly most importantly understanding the practical, ethical, scientific and other consequences of your decisions. It's at this point that the real value of your model is realized.






